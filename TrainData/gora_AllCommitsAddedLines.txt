 * Optional base class for Gora based {@link Reducer}s.
 * A query to a data store to retrieve objects. Queries are constructed by 
   * is not necessary and it is potentially dangerous. So use this 
   * different array implementation.  By default, this returns a 
   * GenericData.Array instance.*/
   * @param clazz the Persistent class
   * @param fields the name of the fields of the class
 * Copied from Avro trunk, when 1.4 is released and Gora switches to it
 * written by one to the other, and validates the input. However, Gora needs to 
 * {@link InputFormat} to fetch the input from Gora data stores. The
 * Optional base class for Gora based {@link Mapper}s.
import org.apache.gora.store.DataStoreFactory;
 * <p>
 * Hadoop jobs can be either configured through static 
 * <code>setInput()</code> methods, or from {@link GoraMapper}.
 * 
 * @see GoraMapper
  
  /**
   * Sets the input parameters for the job
   * @param job the job to set the properties for
   * @param query the query to get the inputs from
   * @param dataStoreClass the datastore class
   * @param inKeyClass Map input key class
   * @param inValueClass Map input value class
   * @param reuseObjects whether to reuse objects in serialization
   * @throws IOException
   */
  public static <K1, V1 extends Persistent> void setInput(
      Job job, 
      Class<? extends DataStore<K1,V1>> dataStoreClass, 
      Class<K1> inKeyClass, 
      Class<V1> inValueClass,
      boolean reuseObjects)
  throws IOException {

    DataStore<K1,V1> store = DataStoreFactory.getDataStore(dataStoreClass
        , inKeyClass, inValueClass);
    setInput(job, store.newQuery(), store, reuseObjects);
  }
}
 * Base class for Gora based {@link Mapper}s.
  extends Mapper<K1, V1, K2, V2> {
  /**
   * Initializes the Mapper, and sets input parameters for the job. All of 
   * the records in the dataStore are used as the input. If you want to 
   * include a specific subset, use one of the overloaded methods which takes
   * query parameter.
   * @param job the job to set the properties for
   * @param query the query to get the inputs from
   * @param dataStoreClass the datastore class
   * @param inKeyClass Map input key class
   * @param inValueClass Map input value class
   * @param outKeyClass Map output key class
   * @param outValueClass Map output value class
   * @param mapperClass the mapper class extending GoraMapper
   * @param partitionerClass optional partitioner class
   * @param reuseObjects whether to reuse objects in serialization
   */
  void initMapperJob(
      Job job,
      Class<? extends DataStore<K1,V1>> dataStoreClass,
      Class<K1> inKeyClass, 
      Class<V1> inValueClass,
      Class<K2> outKeyClass, 
      Class<V2> outValueClass,
      Class<? extends Partitioner> partitionerClass, 
      boolean reuseObjects)
  throws IOException {
    
    //set the input via GoraInputFormat
    GoraInputFormat.setInput(job, dataStoreClass, inKeyClass, inValueClass, reuseObjects);

    job.setMapperClass(mapperClass);
    job.setMapOutputKeyClass(outKeyClass);
    job.setMapOutputValueClass(outValueClass);

    if (partitionerClass != null) {
      job.setPartitionerClass(partitionerClass);
    }
  }
  
  /**
   * Initializes the Mapper, and sets input parameters for the job. All of 
   * the records in the dataStore are used as the input. If you want to 
   * include a specific subset, use one of the overloaded methods which takes
   * query parameter.
   * @param job the job to set the properties for
   * @param query the query to get the inputs from
   * @param dataStoreClass the datastore class
   * @param inKeyClass Map input key class
   * @param inValueClass Map input value class
   * @param outKeyClass Map output key class
   * @param outValueClass Map output value class
   * @param mapperClass the mapper class extending GoraMapper
   * @param reuseObjects whether to reuse objects in serialization
   */  
  @SuppressWarnings("rawtypes")
  public static <K1, V1 extends Persistent, K2, V2>
  void initMapperJob(
      Job job,
      Class<? extends DataStore<K1,V1>> dataStoreClass,
      Class<K1> inKeyClass, 
      Class<V1> inValueClass,
      Class<K2> outKeyClass, 
      Class<V2> outValueClass,
      Class<? extends GoraMapper> mapperClass,
      boolean reuseObjects)
  throws IOException {
    initMapperJob(job, dataStoreClass, inKeyClass, inValueClass, outKeyClass
        , outValueClass, mapperClass, null, reuseObjects);
  }
  
  /**
   * Initializes the Mapper, and sets input parameters for the job
   * @param job the job to set the properties for
   * @param query the query to get the inputs from
   * @param dataStore the datastore as the input
   * @param outKeyClass Map output key class
   * @param outValueClass Map output value class
   * @param mapperClass the mapper class extending GoraMapper
   * @param partitionerClass optional partitioner class
   * @param reuseObjects whether to reuse objects in serialization
   */
  @SuppressWarnings("rawtypes")
  public static <K1, V1 extends Persistent, K2, V2>
  void initMapperJob(
      Job job, 
      Query<K1,V1> query,
      DataStore<K1,V1> dataStore,
      Class<K2> outKeyClass, 
      Class<V2> outValueClass,
      Class<? extends GoraMapper> mapperClass,
      Class<? extends Partitioner> partitionerClass, 
      boolean reuseObjects)
  /**
   * Initializes the Mapper, and sets input parameters for the job
   * @param job the job to set the properties for
   * @param dataStore the datastore as the input
   * @param outKeyClass Map output key class
   * @param outValueClass Map output value class
   * @param mapperClass the mapper class extending GoraMapper
   * @param reuseObjects whether to reuse objects in serialization
   */
  void initMapperJob(
      Job job, 
      DataStore<K1,V1> dataStore,
      Class<K2> outKeyClass, 
      Class<V2> outValueClass,
      Class<? extends GoraMapper> mapperClass, 
      boolean reuseObjects)
  throws IOException {
    initMapperJob(job, dataStore.newQuery(), dataStore, 
        outKeyClass, outValueClass, mapperClass, reuseObjects);
  }
  
  /**
   * Initializes the Mapper, and sets input parameters for the job
   * @param job the job to set the properties for
   * @param query the query to get the inputs from
   * @param dataStore the datastore as the input
   * @param outKeyClass Map output key class
   * @param outValueClass Map output value class
   * @param mapperClass the mapper class extending GoraMapper
   * @param reuseObjects whether to reuse objects in serialization
   */
  @SuppressWarnings({ "rawtypes" })
  public static <K1, V1 extends Persistent, K2, V2>
  void initMapperJob(
      Job job, 
      Query<K1,V1> query, 
      DataStore<K1,V1> dataStore,
      Class<K2> outKeyClass, 
      Class<V2> outValueClass,
      Class<? extends GoraMapper> mapperClass, 
      boolean reuseObjects)
/**
 * {@link OutputFormat} for Hadoop jobs that want to store the job outputs 
 * to a Gora store. 
 * <p>
 * Hadoop jobs can be either configured through static 
 * <code>setOutput()</code> methods, or if the job is not map-only from {@link GoraReducer}.
 * @see GoraReducer 
 */
    setOutput(job, dataStore.getClass(), dataStore.getKeyClass()
        , dataStore.getPersistentClass(), reuseObjects);
  /**
   * Sets the output parameters for the job 
   * @param job the job to set the properties for
   * @param dataStoreClass the datastore class
   * @param keyClass output key class
   * @param persistentClass output value class
   * @param reuseObjects whether to reuse objects in serialization
   */
      Class<K> keyClass, Class<V> persistentClass,
    job.setOutputKeyClass(keyClass);
    job.setOutputValueClass(persistentClass);
 * Base class for Gora based {@link Reducer}s.
  extends Reducer<K1, V1, K2, V2> {
 
  /**
   * Initializes the Reducer, and sets output parameters for the job. 
   * @param job the job to set the properties for
   * @param dataStoreClass the datastore class
   * @param keyClass output key class
   * @param persistentClass output value class
   * @param reducerClass the reducer class extending GoraReducer
   * @param reuseObjects whether to reuse objects in serialization
   */
  void initReducerJob(
      Job job, 
      Class<? extends DataStore<K2,V2>> dataStoreClass,
      Class<K2> keyClass, 
      Class<V2> persistentClass,
      Class<? extends GoraReducer<K1, V1, K2, V2>> reducerClass, 
      boolean reuseObjects) {
    
    GoraOutputFormat.setOutput(job, dataStoreClass, keyClass, persistentClass, reuseObjects);
    
    job.setReducerClass(reducerClass);
  }
  
  /**
   * Initializes the Reducer, and sets output parameters for the job. 
   * @param job the job to set the properties for
   * @param dataStore the datastore as the output
   * @param reducerClass the reducer class extending GoraReducer
   */
  public static <K1, V1, K2, V2 extends Persistent>
  void initReducerJob(
      Job job, 
      DataStore<K2,V2> dataStore,

  /**
   * Initializes the Reducer, and sets output parameters for the job. 
   * @param job the job to set the properties for
   * @param dataStore the datastore as the output
   * @param reducerClass the reducer class extending GoraReducer
   * @param reuseObjects whether to reuse objects in serialization
   */
  void initReducerJob(
      Job job, 
      DataStore<K2,V2> dataStore,
      Class<? extends GoraReducer<K1, V1, K2, V2>> reducerClass, 
      boolean reuseObjects) {

import org.apache.gora.util.GoraException;
      , Class<K> keyClass, Class<T> persistent) throws GoraException {
  D createDataStore(Class<D> dataStoreClass , Class<K> keyClass, 
      Class<T> persistent, String schemaName) throws GoraException {
      , Class<T> persistent, Properties properties, String schemaName) 
  throws GoraException {
    } catch (GoraException ex) {
      throw ex;
    } catch(Exception ex) {
      throw new GoraException(ex);
      , Class<K> keyClass, Class<T> persistent, Properties properties) 
  throws GoraException {
      Class<T> persistentClass) throws GoraException {
      throws GoraException {
    try {
      Class<? extends DataStore<K,T>> c
      return getDataStore(c, keyClass, persistentClass);
    } catch(GoraException ex) {
      throw ex;
    } catch (Exception ex) {
      throw new GoraException(ex);
    }
    throws GoraException {
    try {
      Class k = Class.forName(keyClass);
      Class p = Class.forName(persistentClass);
      return getDataStore(dataStoreClass, k, p);
    } catch(GoraException ex) {
      throw ex;
    } catch (Exception ex) {
      throw new GoraException(ex);
    }
      Class<K> keyClass, Class<T> persistent) throws GoraException {
    return getDataStore(defaultDataStoreClass, keyClass, persistent);
import org.apache.gora.util.GoraException;
    MockDataStore dataStore;
    try {
      dataStore = DataStoreFactory.getDataStore(MockDataStore.class
          , String.class, MockPersistent.class);
      return dataStore;
    } catch (GoraException ex) {
      throw new RuntimeException(ex);
    }
      if(ReflectionUtils.hasConstructor(keyClass)) {
        this.keyConstructor = ReflectionUtils.getConstructor(keyClass);
        this.key = keyConstructor.newInstance(ReflectionUtils.EMPTY_OBJECT_ARRAY);
      }
    else if(keyConstructor == null) {
      throw new RuntimeException("Key class does not have a no-arg constructor");
    }
   * Returns a new instance of the key object. If the object cannot be instantiated 
   * (it the class is a Java primitive wrapper, or does not have no-arg 
   * constructor) it throws an exception. Only use this function if you can 
   * make sure that the key class has a no-arg constructor.   
   * Returns whether the class defines an empty argument constructor.
   */
  public static boolean hasConstructor(Class<?> clazz) 
  throws SecurityException, NoSuchMethodException {
    if(clazz == null) {
      throw new IllegalArgumentException("class cannot be null");
    }
    Constructor<?>[] consts = clazz.getConstructors();

    boolean found = false;
    for(Constructor<?> cons : consts) {
      if(cons.getParameterTypes().length == 0) {
        found = true;
      }
    }

    return found;
  }

  /**
}
        if(!ex.getMessage().contains("closed")) {
          throw new IOException(ex); 
        }
    return new GoraRecordWriter(store, context);
    return new GoraRecordReader<K, T>(partitionQuery, context);
import org.apache.hadoop.conf.Configuration;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class GoraRecordReader<K, T extends Persistent> extends RecordReader<K,T> {
  public static final Logger LOG = LoggerFactory.getLogger(GoraRecordReader.class);

  private static final String BUFFER_LIMIT_READ_NAME = "gora.buffer.read.limit";
  private static final int BUFFER_LIMIT_READ_VALUE = 10000;
  private GoraRecordCounter counter = new GoraRecordCounter();
  
  public GoraRecordReader(Query<K,T> query, TaskAttemptContext context) {

    Configuration configuration = context.getConfiguration();
    int recordsMax = configuration.getInt(BUFFER_LIMIT_READ_NAME, BUFFER_LIMIT_READ_VALUE);
    
    // Check if result set will at least contain 2 rows
    if (recordsMax <= 1) {
      LOG.info("Limit "  recordsMax  " changed to "  BUFFER_LIMIT_READ_VALUE);
      recordsMax = BUFFER_LIMIT_READ_VALUE;
    }
    
    counter.setRecordsMax(recordsMax);
    LOG.info("gora.buffer.read.limit = "  recordsMax);
    
    this.query.setLimit(recordsMax);
    if (counter.isModulo()) {
      boolean firstBatch = (this.result == null);
      if (! firstBatch) {
        this.query.setStartKey(this.result.getKey());
        if (this.query.getLimit() == counter.getRecordsMax()) {
          this.query.setLimit(counter.getRecordsMax()  1);
        }
      }
      
      if (! firstBatch) {
        // skip first result
        this.result.next();
      }
    counter.increment();
    return this.result.next();
  private static final String BUFFER_LIMIT_WRITE_NAME = "gora.buffer.write.limit";
  private static final int BUFFER_LIMIT_WRITE_VALUE = 10000;
  private GoraRecordCounter counter = new GoraRecordCounter();
    int recordsMax = configuration.getInt(BUFFER_LIMIT_WRITE_NAME, BUFFER_LIMIT_WRITE_VALUE);
    counter.setRecordsMax(recordsMax);
    LOG.info("gora.buffer.write.limit = "  recordsMax);
    counter.increment();
    if (counter.isModulo()) {
      LOG.info("Flushing the datastore after "  counter.getRecordsNumber()  " records");
import org.apache.hadoop.hbase.regionserver.StoreFile.BloomType;
      , String maxVersions, String timeToLive, String inMemory) {
      columnDescriptor.setBloomFilterType(BloomType.valueOf(bloomFilter));
              , bloomFilter, maxVersions, timeToLive, inMemory);
  void setKeySpace(String keySpace);
  void setConsistencyLevel(ConsistencyLevel level);
  Row get(String key, Select select) throws IOException;
  RowIterable getRange(String startKey, String endKey, int rowCount, Select select) 
    throws IOException;
  RowIterable getTokenRange(String startToken, String endToken,
      int rowCount, Select select) throws IOException;
  void mutate(String key, Mutate mutation) throws IOException;
  Map<String, Map<String, String>> describeKeySpace() throws IOException;
  List<TokenRange> describeRing() throws IOException;
  List<String> describeSplits(String startToken, String endToken, int size)
    throws IOException;
  void close();
  K newKey() throws Exception;
   * @return a new instance of the Persistent class
  T newPersistent();
  K getCachedKey();
  T getCachedPersistent();
  Class<K> getKeyClass();
  Class<T> getPersistentClass();
  StateManager getStateManager();
  Persistent newInstance(StateManager stateManager);
  String[] getFields();
  String getField(int index);
  int getFieldIndex(String field);
  void clear();
  boolean isNew();
  void setNew();
  void clearNew();
  boolean isDirty();
  boolean isDirty(int fieldIndex);
  boolean isDirty(String field);
  void setDirty();
  void setDirty(int fieldIndex);
  void setDirty(String field);
  void clearDirty(int fieldIndex);
  void clearDirty(String field);
  void clearDirty();
  boolean isReadable(int fieldIndex);
  boolean isReadable(String field);
  void setReadable(int fieldIndex);
  void setReadable(String field);
  void clearReadable(int fieldIndex);
  void clearReadable(String field);
  void clearReadable();
  Persistent clone();
  void setManagedPersistent(Persistent persistent);
  boolean isNew(Persistent persistent);
  void setNew(Persistent persistent);
  void clearNew(Persistent persistent);
  boolean isDirty(Persistent persistent);
  boolean isDirty(Persistent persistent, int fieldIndex);
  void setDirty(Persistent persistent);
  void setDirty(Persistent persistent, int fieldIndex);
  void clearDirty(Persistent persistent, int fieldIndex);
  void clearDirty(Persistent persistent);
  boolean isReadable(Persistent persistent, int fieldIndex);
  void setReadable(Persistent persistent, int fieldIndex);
  void clearReadable(Persistent persistent, int fieldIndex);
  void clearReadable(Persistent persistent);
  State getState(K key);
  void putState(K key, State state);
  Map<K, State> states();
  void clearStates();
  String[] getLocations();
  void setDataStore(DataStore<K,T> dataStore);
  DataStore<K,T> getDataStore();
  Result<K,T> execute() throws IOException;
//  void compile();
//  void setQueryString(String queryString);
//  String getQueryString();
  void setFields(String... fieldNames);
  String[] getFields();
  void setKey(K key);
  void setStartKey(K startKey);
  void setEndKey(K endKey);
  void setKeyRange(K startKey, K endKey);
  K getKey();
  K getStartKey();
  K getEndKey();
  void setTimestamp(long timestamp);
  void setStartTime(long startTime);
  void setEndTime(long endTime);
  void setTimeRange(long startTime, long endTime);
  long getTimestamp();
  long getStartTime();
  long getEndTime();
  void setLimit(long limit);
  long getLimit();
  DataStore<K,T> getDataStore();
  Query<K, T> getQuery();
  boolean next() throws IOException;
  K getKey();
  T get();
  Class<K> getKeyClass();
  Class<T> getPersistentClass();
  long getOffset();
  float getProgress() throws IOException;
  void close() throws IOException;
  void initialize(Class<K> keyClass, Class<T> persistentClass,
  void setKeyClass(Class<K> keyClass);
  Class<K> getKeyClass();
  void setPersistentClass(Class<T> persistentClass);
  Class<T> getPersistentClass();
  String getSchemaName();
  void createSchema() throws IOException;
  void deleteSchema() throws IOException;
  void truncateSchema() throws IOException;
  boolean schemaExists() throws IOException;
  K newKey() throws IOException;
  T newPersistent() throws IOException;
  T get(K key) throws IOException;
  T get(K key, String[] fields) throws IOException;
  void put(K key, T obj) throws IOException;
  boolean delete(K key) throws IOException;
  long deleteByQuery(Query<K, T> query) throws IOException;
  Result<K,T> execute(Query<K, T> query) throws IOException;
  Query<K, T> newQuery();
  List<PartitionQuery<K,T>> getPartitions(Query<K,T> query)
  void flush() throws IOException;
  void setBeanFactory(BeanFactory<K,T> beanFactory);
  BeanFactory<K,T> getBeanFactory();
  void close() throws IOException;
  Configuration getConf();
  void setConf(Configuration conf);
  void readFields(DataInput in) throws IOException;
  void write(DataOutput out) throws IOException;
  void setInputPath(String inputPath);
  void setOutputPath(String outputPath);
  String getInputPath();
  String getOutputPath();
  void setInputStream(InputStream inputStream);
  void setOutputStream(OutputStream outputStream);
  InputStream getInputStream();
  OutputStream getOutputStream();
      inStore = DataStoreFactory.getDataStore(dataStoreClass, Long.class, Pageview.class);
            String fieldType = type(fieldSchema);
            line(2, "return ("fieldType") get("i");");
            fieldType = type(fieldSchema.getValueType());
            line(1, "public GenericArray<"fieldType"> get"camelKey"() {");
            line(2, "return (GenericArray<"fieldType">) get("i");");
            fieldType = type(fieldSchema.getValueType());
            line(1, "public Map<Utf8, "fieldType"> get"camelKey"() {");
            line(2, "return (Map<Utf8, "fieldType">) get("i");");
            line(1, "public "fieldType" getFrom"camelKey"(Utf8 key) {");
            line(1, "public "fieldType" removeFrom"camelKey"(Utf8 key) {");
import java.util.List;
import java.util.Map;

import org.apache.gora.query.Query;
public class CassandraQuery<K, T extends Persistent> extends QueryBase<K, T> {
  private Query<K, T> query;
  
  /**
   * Maps Avro fields to Cassandra columns.
   */
  private Map<String, List<String>> familyMap;
  
  public void setFamilyMap(Map<String, List<String>> familyMap) {
    this.familyMap = familyMap;
  }
  public Map<String, List<String>> getFamilyMap() {
    return familyMap;
  }
  
  /**
   * @param family the family name
   * @return an array of the query column names belonging to the family
   */
  public String[] getColumns(String family) {
    
    List<String> columnList = familyMap.get(family);
    String[] columns = new String[columnList.size()];
    for (int i = 0; i < columns.length; i) {
      columns[i] = columnList.get(i);
    }
    return columns;
  }
  public Query<K, T> getQuery() {
    return query;
  }
  public void setQuery(Query<K, T> query) {
    this.query = query;
  }
  
  

import java.util.List;
import java.util.Map;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Field;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class CassandraResult<K, T extends Persistent> extends ResultBase<K, T> {
  public static final Logger LOG = LoggerFactory.getLogger(CassandraResult.class);
  
  private int rowNumber;
  private CassandraResultSet cassandraResultSet;
  
  /**
   * Maps Cassandra columns to Avro fields.
   */
  private Map<String, String> reverseMap;
  public CassandraResult(DataStore<K, T> dataStore, Query<K, T> query) {
    if (this.rowNumber < this.cassandraResultSet.size()) {
      updatePersistent();
    this.rowNumber;
    return (this.rowNumber <= this.cassandraResultSet.size());

  /**
   * Load key/value pair from Cassandra row to Avro record.
   * @throws IOException
   */
  private void updatePersistent() throws IOException {
    CassandraRow cassandraRow = this.cassandraResultSet.get(this.rowNumber);
    
    // load key
    this.key = (K) cassandraRow.getKey();
    
    // load value
    Schema schema = this.persistent.getSchema();
    List<Field> fields = schema.getFields();
    
    for (CassandraColumn cassandraColumn: cassandraRow) {
      
      // get field name
      String family = cassandraColumn.getFamily();
      String fieldName = this.reverseMap.get(family  ":"  cassandraColumn.getName());
      
      // get field
      int pos = this.persistent.getFieldIndex(fieldName);
      Field field = fields.get(pos);
      
      // get value
      cassandraColumn.setField(field);
      Object value = cassandraColumn.getValue();
      
      this.persistent.put(pos, value);
      // this field does not need to be written back to the store
      this.persistent.clearDirty(pos);
  public void close() throws IOException {
    // TODO Auto-generated method stub
    

  @Override
  public float getProgress() throws IOException {
    return (((float) this.rowNumber) / this.cassandraResultSet.size());
  }

  public void setResultSet(CassandraResultSet cassandraResultSet) {
    this.cassandraResultSet = cassandraResultSet;
  }
  
  public void setReverseMap(Map<String, String> reverseMap) {
    this.reverseMap = reverseMap;
  }

}
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import me.prettyprint.cassandra.model.BasicColumnFamilyDefinition;
import me.prettyprint.cassandra.service.ThriftCfDef;
import me.prettyprint.hector.api.ddl.ColumnFamilyDefinition;
import me.prettyprint.hector.api.ddl.ColumnType;
import me.prettyprint.hector.api.ddl.ComparatorType;

import org.jdom.Document;
import org.jdom.Element;
import org.jdom.JDOMException;
import org.jdom.input.SAXBuilder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  
  public static final Logger LOG = LoggerFactory.getLogger(CassandraMapping.class);
  
  private static final String MAPPING_FILE = "gora-cassandra-mapping.xml";
  private static final String KEYSPACE_ELEMENT = "keyspace";
  private static final String NAME_ATTRIBUTE = "name";
  private static final String MAPPING_ELEMENT = "class";
  private static final String COLUMN_ATTRIBUTE = "qualifier";
  private static final String FAMILY_ATTRIBUTE = "family";
  private static final String SUPER_ATTRIBUTE = "type";
  private static final String CLUSTER_ATTRIBUTE = "cluster";
  private static final String HOST_ATTRIBUTE = "host";
  private String hostName;
  private String clusterName;
  private String keyspaceName;
  
  
  /**
   * List of the super column families.
   */
  private List<String> superFamilies = new ArrayList<String>();
  /**
   * Look up the column family associated to the Avro field.
   */
  private Map<String, String> familyMap = new HashMap<String, String>();
  
  /**
   * Look up the column associated to the Avro field.
   */
  private Map<String, String> columnMap = new HashMap<String, String>();

  /**
   * Look up the column family from its name.
   */
  private Map<String, BasicColumnFamilyDefinition> columnFamilyDefinitions = new HashMap<String, BasicColumnFamilyDefinition>();

  public String getHostName() {
    return this.hostName;
  public String getClusterName() {
    return this.clusterName;
  public String getKeyspaceName() {
    return this.keyspaceName;

  @SuppressWarnings("unchecked")
  public void loadConfiguration() throws JDOMException, IOException {
    SAXBuilder saxBuilder = new SAXBuilder();
    Document document = saxBuilder.build(getClass().getClassLoader().getResourceAsStream(MAPPING_FILE));
    Element root = document.getRootElement();
    
    Element keyspace = root.getChild(KEYSPACE_ELEMENT);
    this.keyspaceName = keyspace.getAttributeValue(NAME_ATTRIBUTE);
    this.clusterName = keyspace.getAttributeValue(CLUSTER_ATTRIBUTE);
    this.hostName = keyspace.getAttributeValue(HOST_ATTRIBUTE);
    
    // load column family definitions
    List<Element> elements = keyspace.getChildren();
    for (Element element: elements) {
      BasicColumnFamilyDefinition cfDef = new BasicColumnFamilyDefinition();
      
      String familyName = element.getAttributeValue(NAME_ATTRIBUTE);
      
      String superAttribute = element.getAttributeValue(SUPER_ATTRIBUTE);
      if (superAttribute != null) {
        this.superFamilies.add(familyName);
        cfDef.setColumnType(ColumnType.SUPER);
        cfDef.setSubComparatorType(ComparatorType.UTF8TYPE);
      }
      
      cfDef.setKeyspaceName(this.keyspaceName);
      cfDef.setName(familyName);
      cfDef.setComparatorType(ComparatorType.UTF8TYPE);
      cfDef.setDefaultValidationClass(ComparatorType.UTF8TYPE.getClassName());
      
      this.columnFamilyDefinitions.put(familyName, cfDef);

    }
    
    // load column definitions    
    Element mapping = root.getChild(MAPPING_ELEMENT);
    elements = mapping.getChildren();
    for (Element element: elements) {
      String fieldName = element.getAttributeValue(NAME_ATTRIBUTE);
      String familyName = element.getAttributeValue(FAMILY_ATTRIBUTE);
      String columnName = element.getAttributeValue(COLUMN_ATTRIBUTE);
      BasicColumnFamilyDefinition columnFamilyDefinition = this.columnFamilyDefinitions.get(familyName);
      if (columnFamilyDefinition == null) {
        LOG.warn("Family "  familyName  " was not declared in the keyspace.");
      }
      
      this.familyMap.put(fieldName, familyName);
      this.columnMap.put(fieldName, columnName);
      
    }    
  public String getFamily(String name) {
    return this.familyMap.get(name);

  public String getColumn(String name) {
    return this.columnMap.get(name);
  }

  /**
   * Read family super attribute.
   * @param family the family name
   * @return true is the family is a super column family
   */
  public boolean isSuper(String family) {
    return this.superFamilies.indexOf(family) != -1;
  }

  public List<ColumnFamilyDefinition> getColumnFamilyDefinitions() {
    List<ColumnFamilyDefinition> list = new ArrayList<ColumnFamilyDefinition>();
    for (String key: this.columnFamilyDefinitions.keySet()) {
      ColumnFamilyDefinition columnFamilyDefinition = this.columnFamilyDefinitions.get(key);
      ThriftCfDef thriftCfDef = new ThriftCfDef(columnFamilyDefinition);
      list.add(thriftCfDef);
    }
    
    return list;
  }


import me.prettyprint.hector.api.beans.ColumnSlice;
import me.prettyprint.hector.api.beans.HColumn;
import me.prettyprint.hector.api.beans.HSuperColumn;
import me.prettyprint.hector.api.beans.Row;
import me.prettyprint.hector.api.beans.SuperRow;
import me.prettyprint.hector.api.beans.SuperSlice;
import org.apache.gora.cassandra.query.CassandraResultSet;
import org.apache.gora.cassandra.query.CassandraRow;
import org.apache.gora.cassandra.query.CassandraSubColumn;
import org.apache.gora.cassandra.query.CassandraSuperColumn;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.persistency.impl.StateManagerImpl;
import org.apache.gora.query.impl.PartitionQueryImpl;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class CassandraStore<K, T extends Persistent> extends DataStoreBase<K, T> {
  public static final Logger LOG = LoggerFactory.getLogger(CassandraStore.class);
  
  private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
  /**
   * The values are Avro fields pending to be stored.
   */
  private Map<K, T> buffer = new HashMap<K, T>();
  
  public CassandraStore() throws Exception {
    this.cassandraClient.init();
  public void close() throws IOException {
    LOG.debug("close");
    flush();
  public void createSchema() {
    LOG.debug("create schema");
    this.cassandraClient.checkKeyspace();
    LOG.debug("delete "  key);
    return false;
  public long deleteByQuery(Query<K, T> query) throws IOException {
    LOG.debug("delete by query "  query);
    return 0;
  }
  public void deleteSchema() throws IOException {
    LOG.debug("delete schema");
    this.cassandraClient.dropKeyspace();
  }

  @Override
  public Result<K, T> execute(Query<K, T> query) throws IOException {
    
    Map<String, List<String>> familyMap = this.cassandraClient.getFamilyMap(query);
    Map<String, String> reverseMap = this.cassandraClient.getReverseMap(query);
    
    CassandraQuery<K, T> cassandraQuery = new CassandraQuery<K, T>();
    cassandraQuery.setQuery(query);
    cassandraQuery.setFamilyMap(familyMap);
    
    CassandraResult<K, T> cassandraResult = new CassandraResult<K, T>(this, query);
    cassandraResult.setReverseMap(reverseMap);

    CassandraResultSet cassandraResultSet = new CassandraResultSet();
    
    // We query Cassandra keyspace by families.
    for (String family : familyMap.keySet()) {
      if (this.cassandraClient.isSuper(family)) {
        addSuperColumns(family, cassandraQuery, cassandraResultSet);
         
      } else {
        addSubColumns(family, cassandraQuery, cassandraResultSet);
      
      }
      
    }
    
    cassandraResult.setResultSet(cassandraResultSet);
    
    
    return cassandraResult;

  }

  private void addSubColumns(String family, CassandraQuery<K, T> cassandraQuery,
      CassandraResultSet cassandraResultSet) {
    // select family columns that are included in the query
    List<Row<String, String, String>> rows = this.cassandraClient.execute(cassandraQuery, family);
    
    for (Row<String, String, String> row : rows) {
      String key = row.getKey();
      
      // find associated row in the resultset
      CassandraRow cassandraRow = cassandraResultSet.getRow(key);
      if (cassandraRow == null) {
        cassandraRow = new CassandraRow();
        cassandraResultSet.putRow(key, cassandraRow);
        cassandraRow.setKey(key);
      }
      
      ColumnSlice<String, String> columnSlice = row.getColumnSlice();
      
      for (HColumn<String, String> hColumn : columnSlice.getColumns()) {
        CassandraSubColumn cassandraSubColumn = new CassandraSubColumn();
        cassandraSubColumn.setValue(hColumn);
        cassandraSubColumn.setFamily(family);
        cassandraRow.add(cassandraSubColumn);
      }
      
    }
  }

  private void addSuperColumns(String family, CassandraQuery<K, T> cassandraQuery, 
      CassandraResultSet cassandraResultSet) {
    
    List<SuperRow<String, String, String, String>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
    for (SuperRow<String, String, String, String> superRow: superRows) {
      String key = superRow.getKey();
      CassandraRow cassandraRow = cassandraResultSet.getRow(key);
      if (cassandraRow == null) {
        cassandraRow = new CassandraRow();
        cassandraResultSet.putRow(key, cassandraRow);
        cassandraRow.setKey(key);
      }
      
      SuperSlice<String, String, String> superSlice = superRow.getSuperSlice();
      for (HSuperColumn<String, String, String> hSuperColumn: superSlice.getSuperColumns()) {
        CassandraSuperColumn cassandraSuperColumn = new CassandraSuperColumn();
        cassandraSuperColumn.setValue(hSuperColumn);
        cassandraSuperColumn.setFamily(family);
        cassandraRow.add(cassandraSuperColumn);
      }
    }
  }

  /**
   * Flush the buffer. Write the buffered rows.
   * @see org.apache.gora.store.DataStore#flush()
   */
  @Override
  public void flush() throws IOException {
    for (K key: this.buffer.keySet()) {
      T value = this.buffer.get(key);
      Schema schema = value.getSchema();
      for (Field field: schema.getFields()) {
        if (value.isDirty(field.pos())) {
          addOrUpdateField((String) key, field, value.get(field.pos()));
        }
      }
    }
    
    this.buffer.clear();
  }

  @Override
  public T get(K key, String[] fields) throws IOException {
    LOG.info("get "  key);
    return null;
  }

  @Override
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
      throws IOException {
    // just a single partition
    List<PartitionQuery<K,T>> partitions = new ArrayList<PartitionQuery<K,T>>();
    partitions.add(new PartitionQueryImpl<K,T>(query));
    return partitions;
  }

  @Override
  public String getSchemaName() {
    LOG.info("get schema name");
    return null;
  /**
   * Duplicate instance to keep all the objects in memory till flushing.
   * @see org.apache.gora.store.DataStore#put(java.lang.Object, org.apache.gora.persistency.Persistent)
   */
  public void put(K key, T value) throws IOException {
    T p = (T) value.newInstance(new StateManagerImpl());
    Schema schema = value.getSchema();
    for (Field field: schema.getFields()) {
      if (value.isDirty(field.pos())) {
        Object fieldValue = value.get(field.pos());
        
        // check if field has a nested structure (map or record)
        Schema fieldSchema = field.schema();
        Type type = fieldSchema.getType();
        switch(type) {
          case RECORD:
            Persistent persistent = (Persistent) fieldValue;
            Persistent newRecord = persistent.newInstance(new StateManagerImpl());
            for (Field member: fieldSchema.getFields()) {
              newRecord.put(member.pos(), persistent.get(member.pos()));
            fieldValue = newRecord;
            break;
          case MAP:
            StatefulHashMap<?, ?> map = (StatefulHashMap<?, ?>) fieldValue;
            StatefulHashMap<?, ?> newMap = new StatefulHashMap(map);
            fieldValue = newMap;
            break;
        
        p.put(field.pos(), fieldValue);
    }
    
    this.buffer.put(key, p);
 }

  /**
   * Add a field to Cassandra according to its type.
   * @param key     the key of the row where the field should be added
   * @param field   the Avro field representing a datum
   * @param value   the field value
   */
  private void addOrUpdateField(String key, Field field, Object value) {
    Schema schema = field.schema();
    Type type = schema.getType();
    //LOG.info(field.name()  " "  type.name());
    switch (type) {
      case STRING:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case INT:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case LONG:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case BYTES:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case FLOAT:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case RECORD:
        if (value != null) {
          if (value instanceof PersistentBase) {
            PersistentBase persistentBase = (PersistentBase) value;
            for (Field member: schema.getFields()) {
              
              // TODO: hack, do not store empty arrays
              Object memberValue = persistentBase.get(member.pos());
              if (memberValue instanceof GenericArray<?>) {
                GenericArray<String> array = (GenericArray<String>) memberValue;
                if (array.size() == 0) {
                  continue;
                }
              }
              
              this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
            }
          } else {
            LOG.info("Record not supported: "  value.toString());
            
          }
        }
        break;
      case MAP:
        if (value != null) {
          if (value instanceof StatefulHashMap<?, ?>) {
            //TODO cast to stateful map and only write dirty keys
            Map<Utf8, Object> map = (Map<Utf8, Object>) value;
            for (Utf8 mapKey: map.keySet()) {
              
              // TODO: hack, do not store empty arrays
              Object keyValue = map.get(mapKey);
              if (keyValue instanceof GenericArray<?>) {
                GenericArray<String> array = (GenericArray<String>) keyValue;
                if (array.size() == 0) {
                  continue;
                }
              }
              
              this.cassandraClient.addSubColumn(key, field.name(), mapKey.toString(), keyValue);              
            }
          } else {
            LOG.info("Map not supported: "  value.toString());
          }
        }
        break;
      default:
        LOG.info("Type not considered: "  type.name());      

  @Override
  public boolean schemaExists() throws IOException {
    LOG.info("schema exists");
    return false;
  }

}
  public static final String BUFFER_LIMIT_READ_NAME = "gora.buffer.read.limit";
  public static final int BUFFER_LIMIT_READ_VALUE = 10000;
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.apache.hadoop.conf.Configuration;
      = DataStoreFactory.getDataStore(dataStoreClass, String.class, WebPage.class, new Configuration());
    Configuration conf = new Configuration();
      dataStore = DataStoreFactory.getDataStore(dataStoreClass, keyClass, persistentClass, conf);
      dataStore = DataStoreFactory.getDataStore(keyClass, persistentClass, conf);
    Configuration conf = new Configuration();
          String.class, WebPage.class, conf);
          String.class, TokenDatum.class, conf);
      inStore = DataStoreFactory.getDataStore(String.class, WebPage.class, conf);
      outStore = DataStoreFactory.getDataStore(String.class, TokenDatum.class, conf);
        , inKeyClass, inValueClass, job.getConfiguration());
      DataStoreFactory.createDataStore(dataStoreClass, keyClass, rowClass, context.getConfiguration());
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
      DataStore<K, T> dataStore, Class<K> keyClass, Class<T> persistent,
      Properties properties) throws IOException {
      , Class<K> keyClass, Class<T> persistent, Configuration conf) throws GoraException {
    return createDataStore(dataStoreClass, keyClass, persistent, conf, properties);
      Class<T> persistent, Configuration conf, String schemaName) throws GoraException {
    return createDataStore(dataStoreClass, keyClass, persistent, conf, properties, schemaName);
      , Class<T> persistent, Configuration conf, Properties properties, String schemaName) 
      if ((dataStore instanceof Configurable) && conf != null) {
        ((Configurable)dataStore).setConf(conf);
      }
      , Class<K> keyClass, Class<T> persistent, Configuration conf, Properties properties) 
    return createDataStore(dataStoreClass, keyClass, persistent, conf, properties, null);
      Class<T> persistentClass, Configuration conf) throws GoraException {
      dataStore = createDataStore(dataStoreClass, keyClass, persistentClass,
          conf, properties);
      String dataStoreClass, Class<K> keyClass, Class<T> persistentClass, Configuration conf)
      return getDataStore(c, keyClass, persistentClass, conf);
      String dataStoreClass, String keyClass, String persistentClass, Configuration conf)
      return getDataStore(dataStoreClass, k, p, conf);
      Class<K> keyClass, Class<T> persistent, Configuration conf) throws GoraException {
    return getDataStore(defaultDataStoreClass, keyClass, persistent, conf);
import org.apache.hadoop.conf.Configuration;
          , String.class, MockPersistent.class, new Configuration());
    this.conf = HBaseConfiguration.create(getConf());
    admin = new HBaseAdmin(this.conf);
import org.apache.hadoop.conf.Configuration;
    Configuration conf = new Configuration();    

      inStore = DataStoreFactory.
          getDataStore(dataStoreClass, Long.class, Pageview.class, conf);
      outStore = DataStoreFactory.
          getDataStore(dataStoreClass, 
			 String.class, MetricDatum.class, conf);
	inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, conf);
	outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, conf);
import org.apache.hadoop.conf.Configuration;
    dataStore = DataStoreFactory.getDataStore(Long.class, Pageview.class,
            new Configuration());
 * <p><a name="visibility"><b>Note:</b> Results of updates ({@link #put(Object, Persistent)},
 * {@link #delete(Object)} and {@link #deleteByQuery(Query)} operations) are
 * guaranteed to be visible to subsequent get / execute operations ONLY
 * after a subsequent call to {@link #flush()}.
   * Inserts the persistent object with the given key. If an 
   * object with the same key already exists it will silently
   * be replaced. See also the note on 
   * <a href="#visibility">visibility</a>.
   * @return whether the object was successfully deleted
   * See also the note on <a href="#visibility">visibility</a>.
   * Forces the write caches to be flushed. DataStore implementations may
   * optimize their writing by deferring the actual put / delete operations
   * until this moment.
   * See also the note on <a href="#visibility">visibility</a>.
  /**
   * Close the DataStore. This should release any resources held by the
   * implementation, so that the instance is ready for GC.
   * All other DataStore methods cannot be used after this
   * method was called. Subsequent calls of this method are ignored.
   */
import java.util.LinkedHashMap;
import java.util.Set;
   *
   * We want to iterate over the keys in insertion order.
   * We don't want to lock the entire collection before iterating over the keys, since in the meantime other threads are adding entries to the map.
  private Map<K, T> buffer = new LinkedHashMap<K, T>();
    
    Set<K> keys = this.buffer.keySet();
    
    // this duplicates memory footprint
    K[] keyArray = (K[]) keys.toArray();
    
    // iterating over the key set directly would throw ConcurrentModificationException with java.util.HashMap and subclasses
    for (K key: keyArray) {
      if (value == null) {
        LOG.info("Value to update is null for key "  key);
        continue;
      }
    // remove flushed rows
    for (K key: keyArray) {
      this.buffer.remove(key);
    }
    // this performs a structural modification of the map
    table = new HTable(conf, mapping.getTableName());
 * <p> Note: HBaseStore is not yet thread-safe.
  final String tableName;
  final byte[] family;
  final byte[] qualifier;
    this.family = family==null ? null : Arrays.copyOf(family, family.length);
    this.qualifier = qualifier==null ? null : 
      Arrays.copyOf(qualifier, qualifier.length);
  /**
   * @return the family (internal array returned; do not modify)
   */
  /**
   * @return the qualifer (internal array returned; do not modify)
   */
 * Mapping definitions for HBase. Thread safe.
  private final Map<String, HTableDescriptor> tableDescriptors;
  private final String tableName; 
  private final Map<String, HBaseColumn> columnMap;
  public HBaseMapping(Map<String, HTableDescriptor> tableDescriptors,
      String tableName, Map<String, HBaseColumn> columnMap) {
    super();
    this.tableDescriptors = tableDescriptors;
    this.columnMap = columnMap;

  
  /**
   * A builder for creating the mapper. This will allow building a thread safe
   * {@link HBaseMapping} using simple immutabilty.
   *
   */
  public static class HBaseMappingBuilder {
    private Map<String, HTableDescriptor> tableDescriptors 
      = new HashMap<String, HTableDescriptor>();
    private String tableName; 
    private Map<String, HBaseColumn> columnMap = 
      new HashMap<String, HBaseColumn>();
    
    public void addTable(String tableName) {
      if(!tableDescriptors.containsKey(tableName)) {
        tableDescriptors.put(tableName, new HTableDescriptor(tableName));
      }
    }
    
    public String getTableName() {
      return tableName;
    }
    
    public void setTableName(String tableName) {
      this.tableName = tableName;
    }
    
    public void addColumnFamily(String tableName, String familyName,
        String compression, String blockCache, String blockSize,
        String bloomFilter ,String maxVersions, String timeToLive, 
        String inMemory) {
      
      HColumnDescriptor columnDescriptor = addColumnFamily(tableName, 
          familyName);
      
      if(compression != null)
        columnDescriptor.setCompressionType(Algorithm.valueOf(compression));
      if(blockCache != null)
        columnDescriptor.setBlockCacheEnabled(Boolean.parseBoolean(blockCache));
      if(blockSize != null)
        columnDescriptor.setBlocksize(Integer.parseInt(blockSize));
      if(bloomFilter != null)
        columnDescriptor.setBloomFilterType(BloomType.valueOf(bloomFilter));
      if(maxVersions != null)
        columnDescriptor.setMaxVersions(Integer.parseInt(maxVersions));
      if(timeToLive != null)
        columnDescriptor.setTimeToLive(Integer.parseInt(timeToLive));
      if(inMemory != null)
        columnDescriptor.setInMemory(Boolean.parseBoolean(inMemory));
      
      getTable(tableName).addFamily(columnDescriptor);
    }
    
    public HTableDescriptor getTable(String tableName) {
      return tableDescriptors.get(tableName);
    }
    
    public HColumnDescriptor addColumnFamily(String tableName, 
        String familyName) {
      HTableDescriptor tableDescriptor = getTable(tableName);
      HColumnDescriptor columnDescriptor =  tableDescriptor.getFamily(
          Bytes.toBytes(familyName));
      if(columnDescriptor == null) {
        columnDescriptor = new HColumnDescriptor(familyName);
        tableDescriptor.addFamily(columnDescriptor);
      }
      return columnDescriptor;
    }
    
    public void addField(String fieldName, String tableName, String family, 
        String qualifier) {
      byte[] familyBytes = Bytes.toBytes(family);
      byte[] qualifierBytes = qualifier == null ? null : 
        Bytes.toBytes(qualifier);
      
      HBaseColumn column = new HBaseColumn(tableName, familyBytes, 
          qualifierBytes);
      columnMap.put(fieldName, column);
    }
    
    /**
     * @return A newly constructed mapping.
     */
    public HBaseMapping build() {
      return new HBaseMapping(tableDescriptors, tableName, columnMap);
    }
  }
import org.apache.gora.hbase.store.HBaseMapping.HBaseMappingBuilder;
 * DataStore for HBase. Thread safe.
  private volatile HBaseAdmin admin;
  private volatile HBaseTableConnection table;
  private volatile Configuration conf;
  private final boolean autoCreateSchema = true;
  private volatile HBaseMapping mapping;
    table = new HBaseTableConnection(getConf(), mapping.getTableName(), true);
    HBaseMappingBuilder mappingBuilder = new HBaseMappingBuilder();
        mappingBuilder.addTable(tableName);
          mappingBuilder.addColumnFamily(tableName, familyName, compression, 
              blockCache, blockSize, bloomFilter, maxVersions, timeToLive, 
              inMemory);
        if(classElement.getAttributeValue("keyClass").equals(
            keyClass.getCanonicalName())
          String tableName = getSchemaName(
              classElement.getAttributeValue("table"), persistentClass);
          mappingBuilder.addTable(tableName);
          mappingBuilder.setTableName(tableName);
            mappingBuilder.addField(fieldName, mappingBuilder.getTableName(), 
                family, qualifier);
            mappingBuilder.addColumnFamily(mappingBuilder.getTableName(), 
                family);//implicit family definition
    return mappingBuilder.build();
    table.close();
        List<Element> fieldElements = tableElement.getChildren("family");
  /**
   * 
   * @param startKey
   *          an inclusive start key
   */
  /**
   * 
   * @param endKey
   *          an inclusive end key
   */
  /**
   * Set the range of keys over which the query will execute.
   * 
   * @param startKey
   *          an inclusive start key
   * @param endKey
   *          an inclusive end key
   */
import org.apache.gora.util.ClassLoadingUtils;
    Class<K> keyClass = (Class<K>) ClassLoadingUtils.loadClass(args[0]);
    Class<T> persistentClass = (Class<T>) ClassLoadingUtils.loadClass(args[1]);
import org.apache.gora.util.ClassLoadingUtils;
      dataStore = (DataStore<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(dataStoreClass), conf);
import org.apache.gora.util.ClassLoadingUtils;
      Class k = ClassLoadingUtils.loadClass(keyClass);
      Class p = ClassLoadingUtils.loadClass(persistentClass);
import org.apache.gora.util.ClassLoadingUtils;
      Class<K> keyClass = (Class<K>) ClassLoadingUtils.loadClass(Text.readString(in));
      Class<T> persistentClass = (Class<T>)ClassLoadingUtils.loadClass(Text.readString(in));
    Class<T> c = (Class<T>) ClassLoadingUtils.loadClass(objClass);
    Class<T> c = (Class<T>)ClassLoadingUtils.loadClass(clazz);
      T obj = (T) DefaultStringifier.load(conf, dataKey, ClassLoadingUtils.loadClass(className));
    Class<?> clazz = ClassLoadingUtils.loadClass(classStr);
import org.apache.gora.util.ClassLoadingUtils;
      ClassLoadingUtils.loadClass(jdbcDriverClass);
  private StringSerializer stringSerializer = new StringSerializer();
  @SuppressWarnings("unchecked")
public void addSubColumn(String key, String fieldName, String memberName, Object value) {
  private StringSerializer stringSerializer = new StringSerializer();
  @SuppressWarnings("unchecked")
public void addSubColumn(String key, String fieldName, String memberName, Object value) {
      this.cluster.addKeyspace(keyspaceDefinition, true);
      this.cluster.addKeyspace(keyspaceDefinition, true);
import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import me.prettyprint.hector.api.HConsistencyLevel;
   * In this method, we also utilise Hector's {@ConfigurableConsistencyLevel}
   * logic. It is set by passing a ConfigurableConsistencyLevel object right 
   * when the Keyspace is created. Currently consistency level is .ONE which 
   * permits consistency to wait until one replica has responded. 
      // Create a customized Consistency Level
      ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();

      // Define CL.ONE for ColumnFamily "ColumnFamily"
      clmap.put("ColumnFamily", HConsistencyLevel.ONE);

      // In this we use CL.ONE for read and writes. But you can use different CLs if needed.
      configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
      configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);

      // Then let the keyspace know
      HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);

import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import me.prettyprint.hector.api.HConsistencyLevel;
   * In this method, we also utilise Hector's {@ConfigurableConsistencyLevel}
   * logic. It is set by passing a ConfigurableConsistencyLevel object right 
   * when the Keyspace is created. Currently consistency level is .ONE which 
   * permits consistency to wait until one replica has responded. 
      // Create a customized Consistency Level
      ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();

      // Define CL.ONE for ColumnFamily "ColumnFamily"
      clmap.put("ColumnFamily", HConsistencyLevel.ONE);

      // In this we use CL.ONE for read and writes. But you can use different CLs if needed.
      configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
      configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);

      // Then let the keyspace know
      HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);

    CassandraQuery<K,T> query = new CassandraQuery<K,T>();
    query.setDataStore(this);
    query.setKeyRange(key, key);
    query.setFields(fields);
    query.setLimit(1);
    Result<K,T> result = execute(query);
    boolean hasResult = result.next();
    return hasResult ? result.get() : null;
  public T get(K key) throws IOException {
    return get(key, getFieldsToQuery(null));
    CassandraQuery<K,T> query = new CassandraQuery<K,T>();
    query.setDataStore(this);
    query.setKeyRange(key, key);
    query.setFields(fields);
    query.setLimit(1);
    Result<K,T> result = execute(query);
    boolean hasResult = result.next();
    return hasResult ? result.get() : null;
  public T get(K key) throws IOException {
    return get(key, getFieldsToQuery(null));
import java.io.OutputStream;
import java.util.HashMap;
import java.util.Map;
import org.apache.avro.io.DecoderFactory;
  /**
   * Threadlocals maintaining reusable binary decoders and encoders.
   */
  public static final ThreadLocal<BinaryDecoder> decoders =
      new ThreadLocal<BinaryDecoder>();
  public static final ThreadLocal<BinaryEncoderWithStream> encoders =
      new ThreadLocal<BinaryEncoderWithStream>();
  
  /**
   * A BinaryEncoder that exposes the outputstream so that it can be reset
   * every time. (This is a workaround to reuse BinaryEncoder and the buffers,
   * normally provided be EncoderFactory, but this class does not exist yet 
   * in the current Avro version).
   */
  public static final class BinaryEncoderWithStream extends BinaryEncoder {
    public BinaryEncoderWithStream(OutputStream out) {
      super(out);
    }
    
    protected OutputStream getOut() {
      return out;
    }
  }
  
  /*
   * Create a threadlocal map for the datum readers and writers, because
   * they are not thread safe, at least not before Avro 1.4.0 (See AVRO-650).
   * When they are thread safe, it is possible to maintain a single reader and
   * writer pair for every schema, instead of one for every thread.
   */
  
  public static final ThreadLocal<Map<String, SpecificDatumReader<?>>> 
    readerMaps = new ThreadLocal<Map<String, SpecificDatumReader<?>>>() {
      protected Map<String,SpecificDatumReader<?>> initialValue() {
        return new HashMap<String, SpecificDatumReader<?>>();
      };
  };
  
  public static final ThreadLocal<Map<String, SpecificDatumWriter<?>>> 
    writerMaps = new ThreadLocal<Map<String, SpecificDatumWriter<?>>>() {
      protected Map<String,SpecificDatumWriter<?>> initialValue() {
        return new HashMap<String, SpecificDatumWriter<?>>();
      };
  };
      Map<String, SpecificDatumReader<?>> readerMap = readerMaps.get();
      SpecificDatumReader<?> reader = readerMap.get(schema.getFullName());
      if (reader == null) {
        reader = new SpecificDatumReader(schema);     
        readerMap.put(schema.getFullName(), reader);
      }
      
      // initialize a decoder, possibly reusing previous one
      BinaryDecoder decoderFromCache = decoders.get();
      BinaryDecoder decoder=DecoderFactory.defaultFactory().
          createBinaryDecoder(val, decoderFromCache);
      // put in threadlocal cache if the initial get was empty
      if (decoderFromCache==null) {
        decoders.set(decoder);
      }
      
  @SuppressWarnings({ "rawtypes", "unchecked" })
      Map<String, SpecificDatumWriter<?>> writerMap = writerMaps.get();
      SpecificDatumWriter writer = writerMap.get(schema.getFullName());
      if (writer == null) {
        writer = new SpecificDatumWriter(schema);
        writerMap.put(schema.getFullName(),writer);
      }
      
      BinaryEncoderWithStream encoder = encoders.get();
      if (encoder == null) {
        encoder = new BinaryEncoderWithStream(new ByteArrayOutputStream());
        encoders.set(encoder);
      }
      //reset the buffers
      ByteArrayOutputStream os = (ByteArrayOutputStream) encoder.getOut();
      os.reset();
      
import java.io.OutputStream;
import java.util.HashMap;
import java.util.Map;
import org.apache.avro.io.DecoderFactory;
  /**
   * Threadlocals maintaining reusable binary decoders and encoders.
   */
  public static final ThreadLocal<BinaryDecoder> decoders =
      new ThreadLocal<BinaryDecoder>();
  public static final ThreadLocal<BinaryEncoderWithStream> encoders =
      new ThreadLocal<BinaryEncoderWithStream>();
  
  /**
   * A BinaryEncoder that exposes the outputstream so that it can be reset
   * every time. (This is a workaround to reuse BinaryEncoder and the buffers,
   * normally provided be EncoderFactory, but this class does not exist yet 
   * in the current Avro version).
   */
  public static final class BinaryEncoderWithStream extends BinaryEncoder {
    public BinaryEncoderWithStream(OutputStream out) {
      super(out);
    }
    
    protected OutputStream getOut() {
      return out;
    }
  }
  
  /*
   * Create a threadlocal map for the datum readers and writers, because
   * they are not thread safe, at least not before Avro 1.4.0 (See AVRO-650).
   * When they are thread safe, it is possible to maintain a single reader and
   * writer pair for every schema, instead of one for every thread.
   */
  
  public static final ThreadLocal<Map<String, SpecificDatumReader<?>>> 
    readerMaps = new ThreadLocal<Map<String, SpecificDatumReader<?>>>() {
      protected Map<String,SpecificDatumReader<?>> initialValue() {
        return new HashMap<String, SpecificDatumReader<?>>();
      };
  };
  
  public static final ThreadLocal<Map<String, SpecificDatumWriter<?>>> 
    writerMaps = new ThreadLocal<Map<String, SpecificDatumWriter<?>>>() {
      protected Map<String,SpecificDatumWriter<?>> initialValue() {
        return new HashMap<String, SpecificDatumWriter<?>>();
      };
  };
      Map<String, SpecificDatumReader<?>> readerMap = readerMaps.get();
      SpecificDatumReader<?> reader = readerMap.get(schema.getFullName());
      if (reader == null) {
        reader = new SpecificDatumReader(schema);     
        readerMap.put(schema.getFullName(), reader);
      }
      
      // initialize a decoder, possibly reusing previous one
      BinaryDecoder decoderFromCache = decoders.get();
      BinaryDecoder decoder=DecoderFactory.defaultFactory().
          createBinaryDecoder(val, decoderFromCache);
      // put in threadlocal cache if the initial get was empty
      if (decoderFromCache==null) {
        decoders.set(decoder);
      }
      
  @SuppressWarnings({ "rawtypes", "unchecked" })
      Map<String, SpecificDatumWriter<?>> writerMap = writerMaps.get();
      SpecificDatumWriter writer = writerMap.get(schema.getFullName());
      if (writer == null) {
        writer = new SpecificDatumWriter(schema);
        writerMap.put(schema.getFullName(),writer);
      }
      
      BinaryEncoderWithStream encoder = encoders.get();
      if (encoder == null) {
        encoder = new BinaryEncoderWithStream(new ByteArrayOutputStream());
        encoders.set(encoder);
      }
      //reset the buffers
      ByteArrayOutputStream os = (ByteArrayOutputStream) encoder.getOut();
      os.reset();
      
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 * Store family, qualifier tuple 
  public HBaseColumn(byte[] family, byte[] qualifier) {

  @Override
  public String toString() {
    return "HBaseColumn [family="  Arrays.toString(family)  ", qualifier="
         Arrays.toString(qualifier)  "]";
  }
  
  
 * It holds a definition for a single table. 
  private final HTableDescriptor tableDescriptor;
  public HBaseMapping(HTableDescriptor tableDescriptor,
      Map<String, HBaseColumn> columnMap) {
    this.tableDescriptor = tableDescriptor;
  
    return tableDescriptor.getNameAsString();
    return tableDescriptor;
  public static class HBaseMappingBuilder { 
    private Map<String, Map<String, HColumnDescriptor>> tableToFamilies = 
      new HashMap<String, Map<String, HColumnDescriptor>>();
    private String tableName;
    public void addFamilyProps(String tableName, String familyName,
      // We keep track of all tables, because even though we
      // only build a mapping for one table. We do this because of the way
      // the mapping file is set up. 
      // (First family properties are defined, whereafter columns are defined).
      //
      // HBaseMapping in fact does not need to support multiple tables,
      // because a Store itself only supports a single table. (Every store 
      // instance simply creates one mapping instance for itself).
      //
      // TODO A nice solution would be to redefine the mapping file structure.
      // For example nest columns in families. Of course this would break compatibility.
      
      
      Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);;
      
      
      HColumnDescriptor columnDescriptor = getOrCreateFamily(familyName, families);
    }

    public void addColumnFamily(String tableName, String familyName) {
      Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);
      getOrCreateFamily(familyName, families);
    public void addField(String fieldName, String family, String qualifier) {
      HBaseColumn column = new HBaseColumn(familyBytes, qualifierBytes);

    private HColumnDescriptor getOrCreateFamily(String familyName,
        Map<String, HColumnDescriptor> families) {
      HColumnDescriptor columnDescriptor = families.get(familyName);
      if (columnDescriptor == null) {
        columnDescriptor=new HColumnDescriptor(familyName);
        families.put(familyName, columnDescriptor);
      }
      return columnDescriptor;
    }

    private Map<String, HColumnDescriptor> getOrCreateFamilies(String tableName) {
      Map<String, HColumnDescriptor> families;
      families = tableToFamilies.get(tableName);
      if (families == null) {
        families = new HashMap<String, HColumnDescriptor>();
        tableToFamilies.put(tableName, families);
      }
      return families;
    }
    
    public void renameTable(String oldName, String newName) {
      Map<String, HColumnDescriptor> families = tableToFamilies.remove(oldName);
      if (families == null) throw new IllegalArgumentException(oldName  " does not exist");
      tableToFamilies.put(newName, families);
    }
    
      if (tableName == null) throw new IllegalStateException("tableName is not specified");
      
      Map<String, HColumnDescriptor> families = tableToFamilies.get(tableName);
      if (families == null) throw new IllegalStateException("no families for table "  tableName);
      
      HTableDescriptor tableDescriptors = new HTableDescriptor(tableName);
      for (HColumnDescriptor desc : families.values()) {
        tableDescriptors.addFamily(desc);
      }
      return new HBaseMapping(tableDescriptors, columnMap);

    table = new HBaseTableConnection(getConf(), getSchemaName(), true);
    //return the name of this table
    if(schemaExists()) {
    if(!schemaExists()) {
    admin.disableTable(getSchemaName());
    admin.deleteTable(getSchemaName());
          
          mappingBuilder.addFamilyProps(tableName, familyName, compression, 
          String tableNameFromMapping = classElement.getAttributeValue("table");
            mappingBuilder.addField(fieldName, family, qualifier);
            mappingBuilder.addColumnFamily(tableNameFromMapping, family);
          
          String tableName = getSchemaName(tableNameFromMapping, persistentClass);
          
          
          if (!tableNameFromMapping.equals(tableName)) {
            log.info("Keyclass and nameclass match but mismatching table names " 
                 " mappingfile schema is '"  tableNameFromMapping 
                 "' vs actual schema '"  tableName  "' , assuming they are the same.");
            mappingBuilder.renameTable(tableNameFromMapping, tableName);
          }
          mappingBuilder.setTableName(tableName);
          //we found a matching key and value class definition,
          //do not continue on other class definitions
 * Store family, qualifier tuple 
  public HBaseColumn(byte[] family, byte[] qualifier) {

  @Override
  public String toString() {
    return "HBaseColumn [family="  Arrays.toString(family)  ", qualifier="
         Arrays.toString(qualifier)  "]";
  }
  
  
 * It holds a definition for a single table. 
  private final HTableDescriptor tableDescriptor;
  public HBaseMapping(HTableDescriptor tableDescriptor,
      Map<String, HBaseColumn> columnMap) {
    this.tableDescriptor = tableDescriptor;
  
    return tableDescriptor.getNameAsString();
    return tableDescriptor;
  public static class HBaseMappingBuilder { 
    private Map<String, Map<String, HColumnDescriptor>> tableToFamilies = 
      new HashMap<String, Map<String, HColumnDescriptor>>();
    private String tableName;
    public void addFamilyProps(String tableName, String familyName,
      // We keep track of all tables, because even though we
      // only build a mapping for one table. We do this because of the way
      // the mapping file is set up. 
      // (First family properties are defined, whereafter columns are defined).
      //
      // HBaseMapping in fact does not need to support multiple tables,
      // because a Store itself only supports a single table. (Every store 
      // instance simply creates one mapping instance for itself).
      //
      // TODO A nice solution would be to redefine the mapping file structure.
      // For example nest columns in families. Of course this would break compatibility.
      
      
      Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);;
      
      
      HColumnDescriptor columnDescriptor = getOrCreateFamily(familyName, families);
    }

    public void addColumnFamily(String tableName, String familyName) {
      Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);
      getOrCreateFamily(familyName, families);
    public void addField(String fieldName, String family, String qualifier) {
      HBaseColumn column = new HBaseColumn(familyBytes, qualifierBytes);

    private HColumnDescriptor getOrCreateFamily(String familyName,
        Map<String, HColumnDescriptor> families) {
      HColumnDescriptor columnDescriptor = families.get(familyName);
      if (columnDescriptor == null) {
        columnDescriptor=new HColumnDescriptor(familyName);
        families.put(familyName, columnDescriptor);
      }
      return columnDescriptor;
    }

    private Map<String, HColumnDescriptor> getOrCreateFamilies(String tableName) {
      Map<String, HColumnDescriptor> families;
      families = tableToFamilies.get(tableName);
      if (families == null) {
        families = new HashMap<String, HColumnDescriptor>();
        tableToFamilies.put(tableName, families);
      }
      return families;
    }
    
    public void renameTable(String oldName, String newName) {
      Map<String, HColumnDescriptor> families = tableToFamilies.remove(oldName);
      if (families == null) throw new IllegalArgumentException(oldName  " does not exist");
      tableToFamilies.put(newName, families);
    }
    
      if (tableName == null) throw new IllegalStateException("tableName is not specified");
      
      Map<String, HColumnDescriptor> families = tableToFamilies.get(tableName);
      if (families == null) throw new IllegalStateException("no families for table "  tableName);
      
      HTableDescriptor tableDescriptors = new HTableDescriptor(tableName);
      for (HColumnDescriptor desc : families.values()) {
        tableDescriptors.addFamily(desc);
      }
      return new HBaseMapping(tableDescriptors, columnMap);

    table = new HBaseTableConnection(getConf(), getSchemaName(), true);
    //return the name of this table
    if(schemaExists()) {
    if(!schemaExists()) {
    admin.disableTable(getSchemaName());
    admin.deleteTable(getSchemaName());
          
          mappingBuilder.addFamilyProps(tableName, familyName, compression, 
          String tableNameFromMapping = classElement.getAttributeValue("table");
            mappingBuilder.addField(fieldName, family, qualifier);
            mappingBuilder.addColumnFamily(tableNameFromMapping, family);
          
          String tableName = getSchemaName(tableNameFromMapping, persistentClass);
          
          
          if (!tableNameFromMapping.equals(tableName)) {
            log.info("Keyclass and nameclass match but mismatching table names " 
                 " mappingfile schema is '"  tableNameFromMapping 
                 "' vs actual schema '"  tableName  "' , assuming they are the same.");
            mappingBuilder.renameTable(tableNameFromMapping, tableName);
          }
          mappingBuilder.setTableName(tableName);
          //we found a matching key and value class definition,
          //do not continue on other class definitions
          String tableName = getSchemaName(tableNameFromMapping, persistentClass);
          
          //tableNameFromMapping could be null here
          if (!tableName.equals(tableNameFromMapping)) {
            log.info("Keyclass and nameclass match but mismatching table names " 
                 " mappingfile schema is '"  tableNameFromMapping 
                 "' vs actual schema '"  tableName  "' , assuming they are the same.");
            if (tableNameFromMapping != null) {
              mappingBuilder.renameTable(tableNameFromMapping, tableName);
            }
          }
          mappingBuilder.setTableName(tableName);
            mappingBuilder.addColumnFamily(tableName, family);
          String tableName = getSchemaName(tableNameFromMapping, persistentClass);
          
          //tableNameFromMapping could be null here
          if (!tableName.equals(tableNameFromMapping)) {
            log.info("Keyclass and nameclass match but mismatching table names " 
                 " mappingfile schema is '"  tableNameFromMapping 
                 "' vs actual schema '"  tableName  "' , assuming they are the same.");
            if (tableNameFromMapping != null) {
              mappingBuilder.renameTable(tableNameFromMapping, tableName);
            }
          }
          mappingBuilder.setTableName(tableName);
            mappingBuilder.addColumnFamily(tableName, family);
 * uses the JOOQ API and various JDBC drivers to communicate with the DB. 
 * Through use of the JOOQ API this SqlStore aims to support numerous SQL 
 * database stores namely;
 * DB2 9.7
 * Derby 10.8
 * H2 1.3.161
 * HSQLDB 2.2.5
 * Ingres 10.1.0
 * MySQL 5.1.41 and 5.5.8
 * Oracle XE 10.2.0.1.0 and 11g
 * PostgreSQL 9.0
 * SQLite with inofficial JDBC driver v056
 * SQL Server 2008 R8
 * Sybase Adaptive Server Enterprise 15.5
 * Sybase SQL Anywhere 12
 *
 * This DataStore is currently in development, and requires a complete
 * re-write as per GORA-86
 * Please see https://issues.apache.org/jira/browse/GORA-86
  // TODO implement DataBaseTable sqlTable
  //private DataBaseTable sqlTable;
  public void initialize() throws IOException {
      //TODO
  //TODO
  
  private void setColumnConstraintForQuery() throws IOException {
  //TODO
  
  
  //TODO
  private void getColumnConstraint() throws IOException {
  //TODO
  //TODO
  //TODO
  return false;
  //TODO
  return false;
  
  //TODO
  return 0;
  //TODO
  //TODO
  return null;
  //TODO
  return null;
  private void constructWhereClause() throws IOException {
  //TODO
  private void setParametersForPreparedStatement() throws SQLException, IOException {
  //TODO
  //TODO
  return null;
  protected byte[] getBytes() throws SQLException, IOException {
  protected Object readField() throws SQLException, IOException {
  //TODO
  return null;
  //TODO Implement this using Hadoop support
  return null;
  //TODO
  //TODO
  
  protected void setBytes() throws SQLException   {
  //TODO
  protected void setField() throws IOException, SQLException {
  //TODO
  //TODO
  return null;
  //TODO
  protected String getIdentifier() {
  //TODO
  return null;
  private void addColumn() {
  //TODO
  
  protected void createSqlTable() {
  //TODO
  
  private void addField() throws IOException {
  //TODO
  protected SqlMapping readMapping() throws IOException {
  //TODO
  return null;
 * uses the JOOQ API and various JDBC drivers to communicate with the DB. 
 * Through use of the JOOQ API this SqlStore aims to support numerous SQL 
 * database stores namely;
 * DB2 9.7
 * Derby 10.8
 * H2 1.3.161
 * HSQLDB 2.2.5
 * Ingres 10.1.0
 * MySQL 5.1.41 and 5.5.8
 * Oracle XE 10.2.0.1.0 and 11g
 * PostgreSQL 9.0
 * SQLite with inofficial JDBC driver v056
 * SQL Server 2008 R8
 * Sybase Adaptive Server Enterprise 15.5
 * Sybase SQL Anywhere 12
 *
 * This DataStore is currently in development, and requires a complete
 * re-write as per GORA-86
 * Please see https://issues.apache.org/jira/browse/GORA-86
  // TODO implement DataBaseTable sqlTable
  //private DataBaseTable sqlTable;
  public void initialize() throws IOException {
      //TODO
  //TODO
  
  private void setColumnConstraintForQuery() throws IOException {
  //TODO
  
  
  //TODO
  private void getColumnConstraint() throws IOException {
  //TODO
  //TODO
  //TODO
  return false;
  //TODO
  return false;
  
  //TODO
  return 0;
  //TODO
  //TODO
  return null;
  //TODO
  return null;
  private void constructWhereClause() throws IOException {
  //TODO
  private void setParametersForPreparedStatement() throws SQLException, IOException {
  //TODO
  //TODO
  return null;
  protected byte[] getBytes() throws SQLException, IOException {
  protected Object readField() throws SQLException, IOException {
  //TODO
  return null;
  //TODO Implement this using Hadoop support
  return null;
  //TODO
  //TODO
  
  protected void setBytes() throws SQLException   {
  //TODO
  protected void setField() throws IOException, SQLException {
  //TODO
  //TODO
  return null;
  //TODO
  protected String getIdentifier() {
  //TODO
  return null;
  private void addColumn() {
  //TODO
  
  protected void createSqlTable() {
  //TODO
  
  private void addField() throws IOException {
  //TODO
  protected SqlMapping readMapping() throws IOException {
  //TODO
  return null;
	public static final String SCHEMA_NAME = "schema.name";
  /**
   * Do not use! Deprecated because it shares system wide state. 
   * Use {@link #createProps()} instead.
   */
  @Deprecated()
  public static final Properties properties = createProps();
  
  /**
   * Creates a new {@link Properties}. It adds the default gora configuration
   * resources. This properties object can be modified and used to instantiate
   * store instances. It is recommended to use a properties object for a single
   * store, because the properties object is passed on to store initialization
   * methods that are able to store the properties as a field.   
   * @return The new properties object.
   */
  public static Properties createProps() {
    Properties properties = new Properties();
      InputStream stream = DataStoreFactory.class.getClassLoader()
        .getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
      if(stream != null) {
        try {
          properties.load(stream);
          return properties;
        } finally {
          stream.close();
        }
      } else {
        log.warn(GORA_DEFAULT_PROPERTIES_FILE  " not found, properties will be empty.");
      }
      return properties;
    } catch(Exception e) {
      throw new RuntimeException(e);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
    return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), null);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param schemaName A default schemaname that will be put on the properties.
   * @return A new store instance.
   * @throws GoraException
   */
    return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), schemaName);
  /**
   * Instantiate a new {@link DataStore}.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param properties The properties to be used be the store.
   * @param schemaName A default schemaname that will be put on the properties.
   * @return A new store instance.
   * @throws GoraException
   */
  /**
   * Instantiate a new {@link DataStore}. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param properties The properties to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
    return createDataStore(dataStoreClass, keyClass, persistentClass, conf, createProps(), null);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class <i>as string</i>.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
      return createDataStore(c, keyClass, persistentClass, conf, createProps(), null);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class <i>as string</i>.
   * @param keyClass The key class <i>as string</i>.
   * @param persistent The value class <i>as string</i>.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  @SuppressWarnings({ "unchecked" })
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
      Class<? extends DataStore<K,T>> c
          = (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
      Class<K> k = (Class<K>) ClassLoadingUtils.loadClass(keyClass);
      Class<T> p = (Class<T>) ClassLoadingUtils.loadClass(persistentClass);
      return createDataStore(c, k, p, conf, createProps(), null);
  /**
   * Instantiate <i>the default</i> {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  @SuppressWarnings("unchecked")
    Properties createProps = createProps();
    Class<? extends DataStore<K, T>> c;
    try {
      c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
    } catch (Exception ex) {
      throw new GoraException(ex);
    return createDataStore(c, keyClass, persistent, conf, createProps, null);
  private static String getDefaultDataStore(Properties properties) {
    return getProperty(properties, GORA_DEFAULT_DATASTORE_KEY);
   * Set a property
    if(value != null) {
    }
   * Sets a property for the datastore of the given class
  void setProperty(Properties properties, Class<D> dataStoreClass, String baseKey, String value) {
   * Gets the default schema name of a given store class 
   * Sets the default schema name.
    if (schemaName != null) {
      setProperty(properties, SCHEMA_NAME, schemaName);
    }
   * Sets the default schema name to be used by the datastore of the given class
import java.util.Map.Entry;
import org.apache.gora.util.WritableUtils;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
      Properties props = WritableUtils.readProperties(in);
      initialize(keyClass, persistentClass, props);
    WritableUtils.writeProperties(out, properties);
   * Returns the name of the schema to use for the persistent class. 
   * 
   * First the schema name in the defined properties is returned. If null then
   * the provided mappingSchemaName is returned. If this is null too,
   * the class name, without the package, of the persistent class is returned.
	public static final String SCHEMA_NAME = "schema.name";
  /**
   * Do not use! Deprecated because it shares system wide state. 
   * Use {@link #createProps()} instead.
   */
  @Deprecated()
  public static final Properties properties = createProps();
  
  /**
   * Creates a new {@link Properties}. It adds the default gora configuration
   * resources. This properties object can be modified and used to instantiate
   * store instances. It is recommended to use a properties object for a single
   * store, because the properties object is passed on to store initialization
   * methods that are able to store the properties as a field.   
   * @return The new properties object.
   */
  public static Properties createProps() {
    Properties properties = new Properties();
      InputStream stream = DataStoreFactory.class.getClassLoader()
        .getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
      if(stream != null) {
        try {
          properties.load(stream);
          return properties;
        } finally {
          stream.close();
        }
      } else {
        log.warn(GORA_DEFAULT_PROPERTIES_FILE  " not found, properties will be empty.");
      }
      return properties;
    } catch(Exception e) {
      throw new RuntimeException(e);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
    return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), null);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param schemaName A default schemaname that will be put on the properties.
   * @return A new store instance.
   * @throws GoraException
   */
    return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), schemaName);
  /**
   * Instantiate a new {@link DataStore}.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param properties The properties to be used be the store.
   * @param schemaName A default schemaname that will be put on the properties.
   * @return A new store instance.
   * @throws GoraException
   */
  /**
   * Instantiate a new {@link DataStore}. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param properties The properties to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
    return createDataStore(dataStoreClass, keyClass, persistentClass, conf, createProps(), null);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class <i>as string</i>.
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
      return createDataStore(c, keyClass, persistentClass, conf, createProps(), null);
  /**
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class <i>as string</i>.
   * @param keyClass The key class <i>as string</i>.
   * @param persistent The value class <i>as string</i>.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  @SuppressWarnings({ "unchecked" })
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
      Class<? extends DataStore<K,T>> c
          = (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
      Class<K> k = (Class<K>) ClassLoadingUtils.loadClass(keyClass);
      Class<T> p = (Class<T>) ClassLoadingUtils.loadClass(persistentClass);
      return createDataStore(c, k, p, conf, createProps(), null);
  /**
   * Instantiate <i>the default</i> {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  @SuppressWarnings("unchecked")
    Properties createProps = createProps();
    Class<? extends DataStore<K, T>> c;
    try {
      c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
    } catch (Exception ex) {
      throw new GoraException(ex);
    return createDataStore(c, keyClass, persistent, conf, createProps, null);
  private static String getDefaultDataStore(Properties properties) {
    return getProperty(properties, GORA_DEFAULT_DATASTORE_KEY);
   * Set a property
    if(value != null) {
    }
   * Sets a property for the datastore of the given class
  void setProperty(Properties properties, Class<D> dataStoreClass, String baseKey, String value) {
   * Gets the default schema name of a given store class 
   * Sets the default schema name.
    if (schemaName != null) {
      setProperty(properties, SCHEMA_NAME, schemaName);
    }
   * Sets the default schema name to be used by the datastore of the given class
import java.util.Map.Entry;
import org.apache.gora.util.WritableUtils;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
      Properties props = WritableUtils.readProperties(in);
      initialize(keyClass, persistentClass, props);
    WritableUtils.writeProperties(out, properties);
   * Returns the name of the schema to use for the persistent class. 
   * 
   * First the schema name in the defined properties is returned. If null then
   * the provided mappingSchemaName is returned. If this is null too,
   * the class name, without the package, of the persistent class is returned.
  public void initialize() throws Exception {
    this.cassandraClient.initialize();
  public void initialize() throws Exception {
    this.cassandraClient.initialize();
    boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
    table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
    boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
    table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
    for (HTable table : pool) {
      table.flushCommits();
    }
    for (HTable table : pool) {
      table.flushCommits();
    }
import java.util.HashMap;
import java.util.Map.Entry;
        } 
        else {
          persistent.clearDirty(i);
    Map<Utf8, State> tempStates = null;
      tempStates = new HashMap<Utf8, State>();
        tempStates.put(key, state);
    super.readMap(map, expected, in);
    map.clearStates();
    if (readDirtyBits) {
      for (Entry<Utf8, State> entry : tempStates.entrySet()) {
        map.putState(entry.getKey(), entry.getValue());
      }
    }
    return map;
      ((StatefulHashMap)old).reuse();
  /**
   * Create an empty instance.
   */
  /**
   * Create an instance with initial entries. These entries are added stateless;
   * in other words the statemap will be clear after the construction.
   * 
   * @param m The map with initial entries.
   */
    for (java.util.Map.Entry<K, V> entry : m.entrySet()) {
      put(entry.getKey(), entry.getValue());
    }
    clearStates();
    keyStates.remove(key);
    V old = super.put(key, value);
    //if old value is different or null, set state to dirty
    if (!value.equals(old)) {
      keyStates.put(key, State.DIRTY);
    }
    return old;
    keyStates.put((K) key, State.DELETED);
    return null;
    // We do not remove the actual entry from the map.
    // When we keep the entries, we can compare previous state to make Datastore
    // puts more efficient. (In the case of new puts that are in fact unchanged)
    // The problem with clear() is that we cannot delete entries that were not
    // initially set on the input.  This means that for a clear() to fully
    // reflect on a datastore you have to input the full map from the store.
    // This is acceptable for now. Another way around this is to implement
    // some sort of "clear marker" that indicates a map should be fully cleared,
    // with respect to any possible new entries.
    // Do not actually clear the map, i.e. with super.clear()
    // When we keep the entries, we can compare previous state to make Datastore
    // puts more efficient. (In the case of new puts that are in fact unchanged)

  /* (non-Javadoc)
   * @see org.apache.gora.persistency.StatefulMap#reuse()
   */
  public void reuse() {
    super.clear();
    clearStates();
  }
  /**
   * Reuse will clear the map completely with states. This is different
   * from {@link #clear()} in that the latter only sets entries to deleted.
   */
  void reuse();
  
import org.apache.gora.persistency.StatefulHashMap;
        case MAP: 
          if(get(i) != null) {
            if (get(i) instanceof StatefulHashMap) {
              ((StatefulHashMap)get(i)).reuse(); 
            } else {
              ((Map)get(i)).clear();
            }
          }
          break;
import java.util.HashMap;
import java.util.Map.Entry;
        } 
        else {
          persistent.clearDirty(i);
    Map<Utf8, State> tempStates = null;
      tempStates = new HashMap<Utf8, State>();
        tempStates.put(key, state);
    super.readMap(map, expected, in);
    map.clearStates();
    if (readDirtyBits) {
      for (Entry<Utf8, State> entry : tempStates.entrySet()) {
        map.putState(entry.getKey(), entry.getValue());
      }
    }
    return map;
      ((StatefulHashMap)old).reuse();
  /**
   * Create an empty instance.
   */
  /**
   * Create an instance with initial entries. These entries are added stateless;
   * in other words the statemap will be clear after the construction.
   * 
   * @param m The map with initial entries.
   */
    for (java.util.Map.Entry<K, V> entry : m.entrySet()) {
      put(entry.getKey(), entry.getValue());
    }
    clearStates();
    keyStates.remove(key);
    V old = super.put(key, value);
    //if old value is different or null, set state to dirty
    if (!value.equals(old)) {
      keyStates.put(key, State.DIRTY);
    }
    return old;
    keyStates.put((K) key, State.DELETED);
    return null;
    // We do not remove the actual entry from the map.
    // When we keep the entries, we can compare previous state to make Datastore
    // puts more efficient. (In the case of new puts that are in fact unchanged)
    // The problem with clear() is that we cannot delete entries that were not
    // initially set on the input.  This means that for a clear() to fully
    // reflect on a datastore you have to input the full map from the store.
    // This is acceptable for now. Another way around this is to implement
    // some sort of "clear marker" that indicates a map should be fully cleared,
    // with respect to any possible new entries.
    // Do not actually clear the map, i.e. with super.clear()
    // When we keep the entries, we can compare previous state to make Datastore
    // puts more efficient. (In the case of new puts that are in fact unchanged)

  /* (non-Javadoc)
   * @see org.apache.gora.persistency.StatefulMap#reuse()
   */
  public void reuse() {
    super.clear();
    clearStates();
  }
  /**
   * Reuse will clear the map completely with states. This is different
   * from {@link #clear()} in that the latter only sets entries to deleted.
   */
  void reuse();
  
import org.apache.gora.persistency.StatefulHashMap;
        case MAP: 
          if(get(i) != null) {
            if (get(i) instanceof StatefulHashMap) {
              ((StatefulHashMap)get(i)).reuse(); 
            } else {
              ((Map)get(i)).clear();
            }
          }
          break;
  public static final Log LOG = LogFactory.getLog(HBaseStore.class);
        LOG.warn(DEPRECATED_MAPPING_FILE  " is deprecated, please rename the file to "
        LOG.warn(DEPRECATED_MAPPING_FILE  " is deprecated, please rename the file to "
            LOG.info("Keyclass and nameclass match but mismatching table names " 
  public static final Log LOG = LogFactory.getLog(HBaseStore.class);
        LOG.warn(DEPRECATED_MAPPING_FILE  " is deprecated, please rename the file to "
        LOG.warn(DEPRECATED_MAPPING_FILE  " is deprecated, please rename the file to "
            LOG.info("Keyclass and nameclass match but mismatching table names " 
      if (this.result != null) {
        this.result.close();
      }
      
    if (result != null) {
      result.close();
    }
      if (this.result != null) {
        this.result.close();
      }
      
    if (result != null) {
      result.close();
    }
      tl.invalidateCache();
        tl.invalidateCache();
      tl.invalidateCache();
        tl.invalidateCache();
   * @param persistentClass The value class.
   * @param persistentClass The value class.
   * @param persistentClass The value class <i>as string</i>.
   * @param persistentClass The value class.
   * @param persistentClass The value class.
   * @param persistentClass The value class <i>as string</i>.
      manager.get(Long.parseLong(args[1]));
      manager.get(Long.parseLong(args[1]));
            fieldType = type(fieldSchema.getElementType());
            fieldType = type(fieldSchema.getElementType());
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
  private static final Log log = LogFactory.getLog(GoraCompiler.class);
	log.info("Compiling "  src  " to "  dest );
      System.err.println("Usage: Compiler <schema file> <output dir>");
 * <a href="http://gora.apache.org/docs/current/tutorial.html"> 
	log.info("Storing Pageview in: "  dataStore.toString());
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
  private static final Log log = LogFactory.getLog(GoraCompiler.class);
	log.info("Compiling "  src  " to "  dest );
      System.err.println("Usage: Compiler <schema file> <output dir>");
 * <a href="http://gora.apache.org/docs/current/tutorial.html"> 
	log.info("Storing Pageview in: "  dataStore.toString());
  private Map<String, BasicColumnFamilyDefinition> columnFamilyDefinitions = 
		  new HashMap<String, BasicColumnFamilyDefinition>();
  
  /**
   * Simply gets the Cassandra host name.
   * @return hostName
   */
  
  /**
   * Simply gets the Cassandra cluster (the machines (nodes) 
   * in a logical Cassandra instance) name.
   * Clusters can contain multiple keyspaces. 
   * @return clusterName
   */
  /**
   * Simply gets the Cassandra namespace for ColumnFamilies, typically one per application
   * @return
   */
  /**
   * Primary class for loading Cassandra configuration from the 'MAPPING_FILE'.
   * 
   * @throws JDOMException
   * @throws IOException
   */
    if (document == null) {
      LOG.warn("Mapping file '"  MAPPING_FILE  "' could not be found!");
    }
    if (keyspace == null) {
    	LOG.warn("Error locating Cassandra Keyspace element!");
    } else {
    	LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    }
    if (this.keyspaceName == null) {
    	LOG.warn("Error locating Cassandra Keyspace name attribute!");
    } else {
    	LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
    }
    if (this.clusterName == null) {
    	LOG.warn("Error locating Cassandra Keyspace cluster attribute!");
    } else {
    	LOG.info("Located Cassandra Keyspace cluster: '"  CLUSTER_ATTRIBUTE  "'");
    }
    if (this.hostName == null) {
    	LOG.warn("Error locating Cassandra Keyspace host attribute!");
    } else {
    	LOG.info("Located Cassandra Keyspace host: '"  HOST_ATTRIBUTE  "'");
    }
      if (familyName == null) {
      	LOG.warn("Error locating column family name attribute!");
      } else {
      	LOG.info("Located column family name: '"  NAME_ATTRIBUTE  "'");
      }
    	LOG.info("Located super column family");
        LOG.info("Added super column family: '"  familyName  "'");
  private Map<String, BasicColumnFamilyDefinition> columnFamilyDefinitions = 
		  new HashMap<String, BasicColumnFamilyDefinition>();
  
  /**
   * Simply gets the Cassandra host name.
   * @return hostName
   */
  
  /**
   * Simply gets the Cassandra cluster (the machines (nodes) 
   * in a logical Cassandra instance) name.
   * Clusters can contain multiple keyspaces. 
   * @return clusterName
   */
  /**
   * Simply gets the Cassandra namespace for ColumnFamilies, typically one per application
   * @return
   */
  /**
   * Primary class for loading Cassandra configuration from the 'MAPPING_FILE'.
   * 
   * @throws JDOMException
   * @throws IOException
   */
    if (document == null) {
      LOG.warn("Mapping file '"  MAPPING_FILE  "' could not be found!");
    }
    if (keyspace == null) {
    	LOG.warn("Error locating Cassandra Keyspace element!");
    } else {
    	LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    }
    if (this.keyspaceName == null) {
    	LOG.warn("Error locating Cassandra Keyspace name attribute!");
    } else {
    	LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
    }
    if (this.clusterName == null) {
    	LOG.warn("Error locating Cassandra Keyspace cluster attribute!");
    } else {
    	LOG.info("Located Cassandra Keyspace cluster: '"  CLUSTER_ATTRIBUTE  "'");
    }
    if (this.hostName == null) {
    	LOG.warn("Error locating Cassandra Keyspace host attribute!");
    } else {
    	LOG.info("Located Cassandra Keyspace host: '"  HOST_ATTRIBUTE  "'");
    }
      if (familyName == null) {
      	LOG.warn("Error locating column family name attribute!");
      } else {
      	LOG.info("Located column family name: '"  NAME_ATTRIBUTE  "'");
      }
    	LOG.info("Located super column family");
        LOG.info("Added super column family: '"  familyName  "'");
    return this.list.hashCode();
    return this.list.hashCode();
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(WebPageDataCreator.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(GoraCompiler.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  public static final Logger log = LoggerFactory.getLogger(DataStoreFactory.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  public static final Logger LOG = LoggerFactory.getLogger(HBaseStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(SqlStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(LogAnalytics.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(LogManager.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(WebPageDataCreator.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(GoraCompiler.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  public static final Logger log = LoggerFactory.getLogger(DataStoreFactory.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  public static final Logger LOG = LoggerFactory.getLogger(HBaseStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(SqlStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(LogAnalytics.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(LogManager.class);
  private CassandraResultSet<K> cassandraResultSet;
    CassandraRow<K> cassandraRow = this.cassandraResultSet.get(this.rowNumber);
    this.key = cassandraRow.getKey();
  public void setResultSet(CassandraResultSet<K> cassandraResultSet) {
public class CassandraResultSet<K> extends ArrayList<CassandraRow<K>> {
  private HashMap<K, Integer> indexMap = new HashMap<K, Integer>();
  public CassandraRow<K> getRow(K key) {
  public void putRow(K key, CassandraRow<K> cassandraRow) {
public class CassandraRow<K> extends ArrayList<CassandraColumn> {
  private K key;
  public K getKey() {
  public void setKey(K key) {
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
  private HColumn<String, ByteBuffer> hColumn;
    ByteBuffer valueByteBuffer = hColumn.getValue();
        value = new Utf8(StringSerializer.get().fromByteBuffer(valueByteBuffer));
        value = valueByteBuffer;
        value = IntegerSerializer.get().fromByteBuffer(valueByteBuffer);
        value = LongSerializer.get().fromByteBuffer(valueByteBuffer);
        value = FloatSerializer.get().fromByteBuffer(valueByteBuffer);
        break;
      case DOUBLE:
        value = DoubleSerializer.get().fromByteBuffer(valueByteBuffer);
        String valueString = StringSerializer.get().fromByteBuffer(valueByteBuffer);
  public void setValue(HColumn<String, ByteBuffer> hColumn) {
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
  private HSuperColumn<String, String, ByteBuffer> hSuperColumn;
        for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          ByteBuffer memberByteBuffer = hColumn.getValue();
              memberValue = new Utf8(StringSerializer.get().fromByteBuffer(memberByteBuffer));
              memberValue = memberByteBuffer;
              break;
            case INT:
              memberValue = IntegerSerializer.get().fromByteBuffer(memberByteBuffer);
              break;
            case LONG:
              memberValue = LongSerializer.get().fromByteBuffer(memberByteBuffer);
              break;
            case FLOAT:
              memberValue = FloatSerializer.get().fromByteBuffer(memberByteBuffer);
              break;
            case DOUBLE:
              memberValue = DoubleSerializer.get().fromByteBuffer(memberByteBuffer);
          for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
  public void setValue(HSuperColumn<String, String, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.SerializerTypeInferer;
import me.prettyprint.hector.api.Serializer;
import org.apache.avro.util.Utf8;
  private Mutator<K> mutator;
  private Class<K> keyClass;
  private ByteBufferSerializer byteBufferSerializer = new ByteBufferSerializer();
  private Serializer<K> keySerializer;
  public void initialize(Class<K> keyClass) throws Exception {
    this.keyClass = keyClass;
    this.keySerializer = SerializerTypeInferer.getSerializer(keyClass);
    this.mutator = HFactory.createMutator(this.keyspace, this.keySerializer);
  public void addColumn(K key, String fieldName, Object value) {

    ByteBuffer byteBuffer = null;
      byteBuffer = (ByteBuffer) value;
    }
    else if (value instanceof Utf8) {
      byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
    }
    else if (value instanceof Float) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Float ) returns ObjectSerializer !?
      byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
    }
    else if (value instanceof Double) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Double ) returns ObjectSerializer !?
      byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
    }
    else {
      byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
    this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, stringSerializer, byteBufferSerializer));
public void addSubColumn(K key, String fieldName, String memberName, Object value) {
    ByteBuffer byteBuffer = null;
      byteBuffer = (ByteBuffer) value;
    }
    else if (value instanceof Utf8) {
      byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
    }
    else if (value instanceof Float) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Float ) returns ObjectSerializer !?
      byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
    }
    else if (value instanceof Double) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Double ) returns ObjectSerializer !?
      byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
    }
    else {
      byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
    this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(memberName, byteBuffer, stringSerializer, byteBufferSerializer)), this.stringSerializer, this.stringSerializer, this.byteBufferSerializer));
  public List<Row<K, String, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
    if (limit < 1) {
      limit = Integer.MAX_VALUE;
    K startKey = query.getStartKey();
    K endKey = query.getEndKey();
    RangeSlicesQuery<K, String, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, stringSerializer, byteBufferSerializer);
    QueryResult<OrderedRows<K, String, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
    OrderedRows<K, String, ByteBuffer> orderedRows = queryResult.get();
  public List<SuperRow<K, String, String, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
    if (limit < 1) {
      limit = Integer.MAX_VALUE;
    K startKey = query.getStartKey();
    K endKey = query.getEndKey();
    RangeSuperSlicesQuery<K, String, String, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, this.stringSerializer, this.stringSerializer, this.byteBufferSerializer);
    QueryResult<OrderedSuperRows<K, String, String, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
    OrderedSuperRows<K, String, String, ByteBuffer> orderedRows = queryResult.get();
import java.nio.ByteBuffer;
import java.util.Properties;
    // this.cassandraClient.initialize();
  }

  public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) throws IOException {
    super.initialize(keyClass, persistent, properties);
    try {
      this.cassandraClient.initialize(keyClass);
    }
    catch (Exception e) {
      throw new IOException(e.getMessage(), e);
    }
    List<Row<K, String, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
    for (Row<K, String, ByteBuffer> row : rows) {
      K key = row.getKey();
      CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
        cassandraRow = new CassandraRow<K>();
      ColumnSlice<String, ByteBuffer> columnSlice = row.getColumnSlice();
      for (HColumn<String, ByteBuffer> hColumn : columnSlice.getColumns()) {
    List<SuperRow<K, String, String, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
    for (SuperRow<K, String, String, ByteBuffer> superRow: superRows) {
      K key = superRow.getKey();
      CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
      SuperSlice<String, String, ByteBuffer> superSlice = superRow.getSuperSlice();
      for (HSuperColumn<String, String, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
          addOrUpdateField(key, field, value.get(field.pos()));
    Query<K,T> query = new CassandraQuery<K, T>(this);
    query.setFields(getFieldsToQuery(null));
    return query;
  private void addOrUpdateField(K key, Field field, Object value) {
      case DOUBLE:
              if (memberValue instanceof Utf8) {
                memberValue = memberValue.toString();
              }
              if (keyValue instanceof Utf8) {
                keyValue = keyValue.toString();
              }
  private CassandraResultSet<K> cassandraResultSet;
    CassandraRow<K> cassandraRow = this.cassandraResultSet.get(this.rowNumber);
    this.key = cassandraRow.getKey();
  public void setResultSet(CassandraResultSet<K> cassandraResultSet) {
public class CassandraResultSet<K> extends ArrayList<CassandraRow<K>> {
  private HashMap<K, Integer> indexMap = new HashMap<K, Integer>();
  public CassandraRow<K> getRow(K key) {
  public void putRow(K key, CassandraRow<K> cassandraRow) {
public class CassandraRow<K> extends ArrayList<CassandraColumn> {
  private K key;
  public K getKey() {
  public void setKey(K key) {
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
  private HColumn<String, ByteBuffer> hColumn;
    ByteBuffer valueByteBuffer = hColumn.getValue();
        value = new Utf8(StringSerializer.get().fromByteBuffer(valueByteBuffer));
        value = valueByteBuffer;
        value = IntegerSerializer.get().fromByteBuffer(valueByteBuffer);
        value = LongSerializer.get().fromByteBuffer(valueByteBuffer);
        value = FloatSerializer.get().fromByteBuffer(valueByteBuffer);
        break;
      case DOUBLE:
        value = DoubleSerializer.get().fromByteBuffer(valueByteBuffer);
        String valueString = StringSerializer.get().fromByteBuffer(valueByteBuffer);
  public void setValue(HColumn<String, ByteBuffer> hColumn) {
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
  private HSuperColumn<String, String, ByteBuffer> hSuperColumn;
        for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          ByteBuffer memberByteBuffer = hColumn.getValue();
              memberValue = new Utf8(StringSerializer.get().fromByteBuffer(memberByteBuffer));
              memberValue = memberByteBuffer;
              break;
            case INT:
              memberValue = IntegerSerializer.get().fromByteBuffer(memberByteBuffer);
              break;
            case LONG:
              memberValue = LongSerializer.get().fromByteBuffer(memberByteBuffer);
              break;
            case FLOAT:
              memberValue = FloatSerializer.get().fromByteBuffer(memberByteBuffer);
              break;
            case DOUBLE:
              memberValue = DoubleSerializer.get().fromByteBuffer(memberByteBuffer);
          for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
  public void setValue(HSuperColumn<String, String, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.SerializerTypeInferer;
import me.prettyprint.hector.api.Serializer;
import org.apache.avro.util.Utf8;
  private Mutator<K> mutator;
  private Class<K> keyClass;
  private ByteBufferSerializer byteBufferSerializer = new ByteBufferSerializer();
  private Serializer<K> keySerializer;
  public void initialize(Class<K> keyClass) throws Exception {
    this.keyClass = keyClass;
    this.keySerializer = SerializerTypeInferer.getSerializer(keyClass);
    this.mutator = HFactory.createMutator(this.keyspace, this.keySerializer);
  public void addColumn(K key, String fieldName, Object value) {

    ByteBuffer byteBuffer = null;
      byteBuffer = (ByteBuffer) value;
    }
    else if (value instanceof Utf8) {
      byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
    }
    else if (value instanceof Float) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Float ) returns ObjectSerializer !?
      byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
    }
    else if (value instanceof Double) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Double ) returns ObjectSerializer !?
      byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
    }
    else {
      byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
    this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, stringSerializer, byteBufferSerializer));
public void addSubColumn(K key, String fieldName, String memberName, Object value) {
    ByteBuffer byteBuffer = null;
      byteBuffer = (ByteBuffer) value;
    }
    else if (value instanceof Utf8) {
      byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
    }
    else if (value instanceof Float) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Float ) returns ObjectSerializer !?
      byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
    }
    else if (value instanceof Double) {
      // workaround for hector-core-1.0-1.jar
      // because SerializerTypeInferer.getSerializer(Double ) returns ObjectSerializer !?
      byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
    }
    else {
      byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
    this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(memberName, byteBuffer, stringSerializer, byteBufferSerializer)), this.stringSerializer, this.stringSerializer, this.byteBufferSerializer));
  public List<Row<K, String, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
    if (limit < 1) {
      limit = Integer.MAX_VALUE;
    K startKey = query.getStartKey();
    K endKey = query.getEndKey();
    RangeSlicesQuery<K, String, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, stringSerializer, byteBufferSerializer);
    QueryResult<OrderedRows<K, String, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
    OrderedRows<K, String, ByteBuffer> orderedRows = queryResult.get();
  public List<SuperRow<K, String, String, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
    if (limit < 1) {
      limit = Integer.MAX_VALUE;
    K startKey = query.getStartKey();
    K endKey = query.getEndKey();
    RangeSuperSlicesQuery<K, String, String, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, this.stringSerializer, this.stringSerializer, this.byteBufferSerializer);
    QueryResult<OrderedSuperRows<K, String, String, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
    OrderedSuperRows<K, String, String, ByteBuffer> orderedRows = queryResult.get();
import java.nio.ByteBuffer;
import java.util.Properties;
    // this.cassandraClient.initialize();
  }

  public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) throws IOException {
    super.initialize(keyClass, persistent, properties);
    try {
      this.cassandraClient.initialize(keyClass);
    }
    catch (Exception e) {
      throw new IOException(e.getMessage(), e);
    }
    List<Row<K, String, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
    for (Row<K, String, ByteBuffer> row : rows) {
      K key = row.getKey();
      CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
        cassandraRow = new CassandraRow<K>();
      ColumnSlice<String, ByteBuffer> columnSlice = row.getColumnSlice();
      for (HColumn<String, ByteBuffer> hColumn : columnSlice.getColumns()) {
    List<SuperRow<K, String, String, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
    for (SuperRow<K, String, String, ByteBuffer> superRow: superRows) {
      K key = superRow.getKey();
      CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
      SuperSlice<String, String, ByteBuffer> superSlice = superRow.getSuperSlice();
      for (HSuperColumn<String, String, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
          addOrUpdateField(key, field, value.get(field.pos()));
    Query<K,T> query = new CassandraQuery<K, T>(this);
    query.setFields(getFieldsToQuery(null));
    return query;
  private void addOrUpdateField(K key, Field field, Object value) {
      case DOUBLE:
              if (memberValue instanceof Utf8) {
                memberValue = memberValue.toString();
              }
              if (keyValue instanceof Utf8) {
                keyValue = keyValue.toString();
              }
        cfDef.setSubComparatorType(ComparatorType.BYTESTYPE);
      cfDef.setComparatorType(ComparatorType.BYTESTYPE);
      cfDef.setDefaultValidationClass(ComparatorType.BYTESTYPE.getClassName());
        cfDef.setSubComparatorType(ComparatorType.BYTESTYPE);
      cfDef.setComparatorType(ComparatorType.BYTESTYPE);
      cfDef.setDefaultValidationClass(ComparatorType.BYTESTYPE.getClassName());
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;

import org.apache.avro.Schema;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.util.Utf8;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  public static final Logger LOG = LoggerFactory.getLogger(CassandraColumn.class);

  public abstract ByteBuffer getName();
  protected Object fromByteBuffer(Type type, ByteBuffer byteBuffer) {
    Object value = null;
    switch (type) {
      case STRING:
        value = new Utf8(StringSerializer.get().fromByteBuffer(byteBuffer));
        break;
      case BYTES:
        value = byteBuffer;
        break;
      case INT:
        value = IntegerSerializer.get().fromByteBuffer(byteBuffer);
        break;
      case LONG:
        value = LongSerializer.get().fromByteBuffer(byteBuffer);
        break;
      case FLOAT:
        value = FloatSerializer.get().fromByteBuffer(byteBuffer);
        break;
      case DOUBLE:
        value = DoubleSerializer.get().fromByteBuffer(byteBuffer);
        break;

      default:
        LOG.info("Type is not supported: "  type);

    }
    return value;
  }

import me.prettyprint.cassandra.serializers.StringSerializer;

      String fieldName = this.reverseMap.get(family  ":"  StringSerializer.get().fromByteBuffer(cassandraColumn.getName()));
  private HColumn<ByteBuffer, ByteBuffer> hColumn;
  public ByteBuffer getName() {
    ByteBuffer byteBuffer = hColumn.getValue();
    if (type == Type.ARRAY) {
      // convert string to array
      String valueString = StringSerializer.get().fromByteBuffer(byteBuffer);
      valueString = valueString.substring(1, valueString.length()-1);
      String[] elements = valueString.split(", ");
      Type elementType = fieldSchema.getElementType().getType();
      if (elementType == Schema.Type.STRING) {
        // the array type is String
        GenericArray<String> genericArray = new GenericData.Array<String>(elements.length, Schema.createArray(Schema.create(Schema.Type.STRING)));
        for (String element: elements) {
          genericArray.add(element);
        }

        value = genericArray;
      } else {
        LOG.info("Element type not supported: "  elementType);
      }
    }
    else {
      value = fromByteBuffer(type, byteBuffer);
    }

    return value;
  public void setValue(HColumn<ByteBuffer, ByteBuffer> hColumn) {
import org.apache.avro.generic.GenericArray;
import org.apache.gora.persistency.ListGenericArray;
  private HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn;
  public ByteBuffer getName() {
    return StringSerializer.get().toByteBuffer(hSuperColumn.getName());
      case ARRAY:
        Type elementType = fieldSchema.getElementType().getType();
        GenericArray array = new ListGenericArray(Schema.create(elementType));
        
        for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          ByteBuffer memberByteBuffer = hColumn.getValue();
          Object memberValue = fromByteBuffer(elementType, hColumn.getValue());
          // int i = IntegerSerializer().get().fromByteBuffer(hColumn.getName());
          array.add(memberValue);      
        }
        value = array;
        
        break;
        for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          Object memberValue = fromByteBuffer(valueType, hColumn.getValue());
          map.put(new Utf8(StringSerializer.get().fromByteBuffer(hColumn.getName())), memberValue);      
          for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
            String memberName = StringSerializer.get().fromByteBuffer(hColumn.getName());
            Field memberField = fieldSchema.getField(memberName);
            record.put(record.getFieldIndex(memberName), cassandraColumn.getValue());

  public void setValue(HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.IntegerSerializer;
    ByteBuffer byteBuffer = toByteBuffer(value);
    this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, StringSerializer.get(), ByteBufferSerializer.get()));
   * @param columnName the column name (the member name, or the index of array)
  public void addSubColumn(K key, String fieldName, ByteBuffer columnName, Object value) {
    ByteBuffer byteBuffer = toByteBuffer(value);
    
    String columnFamily = this.cassandraMapping.getFamily(fieldName);
    String superColumnName = this.cassandraMapping.getColumn(fieldName);
    
    this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(columnName, byteBuffer, ByteBufferSerializer.get(), ByteBufferSerializer.get())), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get()));
    
  }

  /**
   * Serialize value to ByteBuffer.
   * @param value the member value
   * @return ByteBuffer object
   */
  @SuppressWarnings("unchecked")
  public ByteBuffer toByteBuffer(Object value) {
    if (value == null) {
      return null;
    }
    
      byteBuffer = StringSerializer.get().toByteBuffer(((Utf8)value).toString());
    return byteBuffer;

  public List<Row<K, ByteBuffer, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
    ByteBuffer[] columnNameByteBuffers = new ByteBuffer[columnNames.length];
    for (int i = 0; i < columnNames.length; i) {
      columnNameByteBuffers[i] = StringSerializer.get().toByteBuffer(columnNames[i]);
    }
    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
    rangeSlicesQuery.setRange(ByteBuffer.wrap(new byte[0]), ByteBuffer.wrap(new byte[0]), false, GoraRecordReader.BUFFER_LIMIT_READ_VALUE);
    rangeSlicesQuery.setColumnNames(columnNameByteBuffers);
    QueryResult<OrderedRows<K, ByteBuffer, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
    OrderedRows<K, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
  public List<SuperRow<K, String, ByteBuffer, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
    QueryResult<OrderedSuperRows<K, String, ByteBuffer, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
    OrderedSuperRows<K, String, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
import java.util.Iterator;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.ListGenericArray;
      if (family == null) {
        continue;
      }
    List<Row<K, ByteBuffer, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
    for (Row<K, ByteBuffer, ByteBuffer> row : rows) {
      ColumnSlice<ByteBuffer, ByteBuffer> columnSlice = row.getColumnSlice();
      for (HColumn<ByteBuffer, ByteBuffer> hColumn : columnSlice.getColumns()) {
    List<SuperRow<K, String, ByteBuffer, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
    for (SuperRow<K, String, ByteBuffer, ByteBuffer> superRow: superRows) {
      SuperSlice<String, ByteBuffer, ByteBuffer> superSlice = superRow.getSuperSlice();
      for (HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
        // check if field has a nested structure (array, map, or record)
          case ARRAY:
            GenericArray array = (GenericArray) fieldValue;
            Type elementType = fieldSchema.getElementType().getType();
            GenericArray newArray = new ListGenericArray(Schema.create(elementType));
            Iterator iter = array.iterator();
            while (iter.hasNext()) {
              newArray.add(iter.next());
            }
            fieldValue = newArray;
            break;
              this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(member.name()), memberValue);
              this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(mapKey.toString()), keyValue);              
      case ARRAY:
        if (value != null) {
          if (value instanceof GenericArray<?>) {
            GenericArray<Object> array = (GenericArray<Object>) value;
            int i= 0;
            for (Object itemValue: array) {
              if (itemValue instanceof Utf8) {
                itemValue = itemValue.toString();
              }
              this.cassandraClient.addSubColumn(key, field.name(), IntegerSerializer.get().toByteBuffer(i), itemValue);              
            }
          } else {
            LOG.info("Array not supported: "  value.toString());
          }
        }
        break;
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;

import org.apache.avro.Schema;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.util.Utf8;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  public static final Logger LOG = LoggerFactory.getLogger(CassandraColumn.class);

  public abstract ByteBuffer getName();
  protected Object fromByteBuffer(Type type, ByteBuffer byteBuffer) {
    Object value = null;
    switch (type) {
      case STRING:
        value = new Utf8(StringSerializer.get().fromByteBuffer(byteBuffer));
        break;
      case BYTES:
        value = byteBuffer;
        break;
      case INT:
        value = IntegerSerializer.get().fromByteBuffer(byteBuffer);
        break;
      case LONG:
        value = LongSerializer.get().fromByteBuffer(byteBuffer);
        break;
      case FLOAT:
        value = FloatSerializer.get().fromByteBuffer(byteBuffer);
        break;
      case DOUBLE:
        value = DoubleSerializer.get().fromByteBuffer(byteBuffer);
        break;

      default:
        LOG.info("Type is not supported: "  type);

    }
    return value;
  }

import me.prettyprint.cassandra.serializers.StringSerializer;

      String fieldName = this.reverseMap.get(family  ":"  StringSerializer.get().fromByteBuffer(cassandraColumn.getName()));
  private HColumn<ByteBuffer, ByteBuffer> hColumn;
  public ByteBuffer getName() {
    ByteBuffer byteBuffer = hColumn.getValue();
    if (type == Type.ARRAY) {
      // convert string to array
      String valueString = StringSerializer.get().fromByteBuffer(byteBuffer);
      valueString = valueString.substring(1, valueString.length()-1);
      String[] elements = valueString.split(", ");
      Type elementType = fieldSchema.getElementType().getType();
      if (elementType == Schema.Type.STRING) {
        // the array type is String
        GenericArray<String> genericArray = new GenericData.Array<String>(elements.length, Schema.createArray(Schema.create(Schema.Type.STRING)));
        for (String element: elements) {
          genericArray.add(element);
        }

        value = genericArray;
      } else {
        LOG.info("Element type not supported: "  elementType);
      }
    }
    else {
      value = fromByteBuffer(type, byteBuffer);
    }

    return value;
  public void setValue(HColumn<ByteBuffer, ByteBuffer> hColumn) {
import org.apache.avro.generic.GenericArray;
import org.apache.gora.persistency.ListGenericArray;
  private HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn;
  public ByteBuffer getName() {
    return StringSerializer.get().toByteBuffer(hSuperColumn.getName());
      case ARRAY:
        Type elementType = fieldSchema.getElementType().getType();
        GenericArray array = new ListGenericArray(Schema.create(elementType));
        
        for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          ByteBuffer memberByteBuffer = hColumn.getValue();
          Object memberValue = fromByteBuffer(elementType, hColumn.getValue());
          // int i = IntegerSerializer().get().fromByteBuffer(hColumn.getName());
          array.add(memberValue);      
        }
        value = array;
        
        break;
        for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          Object memberValue = fromByteBuffer(valueType, hColumn.getValue());
          map.put(new Utf8(StringSerializer.get().fromByteBuffer(hColumn.getName())), memberValue);      
          for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
            String memberName = StringSerializer.get().fromByteBuffer(hColumn.getName());
            Field memberField = fieldSchema.getField(memberName);
            record.put(record.getFieldIndex(memberName), cassandraColumn.getValue());

  public void setValue(HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.IntegerSerializer;
    ByteBuffer byteBuffer = toByteBuffer(value);
    this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, StringSerializer.get(), ByteBufferSerializer.get()));
   * @param columnName the column name (the member name, or the index of array)
  public void addSubColumn(K key, String fieldName, ByteBuffer columnName, Object value) {
    ByteBuffer byteBuffer = toByteBuffer(value);
    
    String columnFamily = this.cassandraMapping.getFamily(fieldName);
    String superColumnName = this.cassandraMapping.getColumn(fieldName);
    
    this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(columnName, byteBuffer, ByteBufferSerializer.get(), ByteBufferSerializer.get())), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get()));
    
  }

  /**
   * Serialize value to ByteBuffer.
   * @param value the member value
   * @return ByteBuffer object
   */
  @SuppressWarnings("unchecked")
  public ByteBuffer toByteBuffer(Object value) {
    if (value == null) {
      return null;
    }
    
      byteBuffer = StringSerializer.get().toByteBuffer(((Utf8)value).toString());
    return byteBuffer;

  public List<Row<K, ByteBuffer, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
    ByteBuffer[] columnNameByteBuffers = new ByteBuffer[columnNames.length];
    for (int i = 0; i < columnNames.length; i) {
      columnNameByteBuffers[i] = StringSerializer.get().toByteBuffer(columnNames[i]);
    }
    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
    rangeSlicesQuery.setRange(ByteBuffer.wrap(new byte[0]), ByteBuffer.wrap(new byte[0]), false, GoraRecordReader.BUFFER_LIMIT_READ_VALUE);
    rangeSlicesQuery.setColumnNames(columnNameByteBuffers);
    QueryResult<OrderedRows<K, ByteBuffer, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
    OrderedRows<K, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
  public List<SuperRow<K, String, ByteBuffer, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
    QueryResult<OrderedSuperRows<K, String, ByteBuffer, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
    OrderedSuperRows<K, String, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
import java.util.Iterator;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.ListGenericArray;
      if (family == null) {
        continue;
      }
    List<Row<K, ByteBuffer, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
    for (Row<K, ByteBuffer, ByteBuffer> row : rows) {
      ColumnSlice<ByteBuffer, ByteBuffer> columnSlice = row.getColumnSlice();
      for (HColumn<ByteBuffer, ByteBuffer> hColumn : columnSlice.getColumns()) {
    List<SuperRow<K, String, ByteBuffer, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
    for (SuperRow<K, String, ByteBuffer, ByteBuffer> superRow: superRows) {
      SuperSlice<String, ByteBuffer, ByteBuffer> superSlice = superRow.getSuperSlice();
      for (HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
        // check if field has a nested structure (array, map, or record)
          case ARRAY:
            GenericArray array = (GenericArray) fieldValue;
            Type elementType = fieldSchema.getElementType().getType();
            GenericArray newArray = new ListGenericArray(Schema.create(elementType));
            Iterator iter = array.iterator();
            while (iter.hasNext()) {
              newArray.add(iter.next());
            }
            fieldValue = newArray;
            break;
              this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(member.name()), memberValue);
              this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(mapKey.toString()), keyValue);              
      case ARRAY:
        if (value != null) {
          if (value instanceof GenericArray<?>) {
            GenericArray<Object> array = (GenericArray<Object>) value;
            int i= 0;
            for (Object itemValue: array) {
              if (itemValue instanceof Utf8) {
                itemValue = itemValue.toString();
              }
              this.cassandraClient.addSubColumn(key, field.name(), IntegerSerializer.get().toByteBuffer(i), itemValue);              
            }
          } else {
            LOG.info("Array not supported: "  value.toString());
          }
        }
        break;
    line(0, "import org.apache.avro.specific.FixedSize;");
          case FIXED:
    line(0, "import org.apache.avro.specific.FixedSize;");
          case FIXED:
        byte[] splitStart = startRow.length == 0 || 
            Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ? 
        byte[] splitStop = (stopRow.length == 0 || 
            Bytes.compareTo(keys.getSecond()[i], stopRow) <= 0) && 
            keys.getSecond()[i].length > 0 ? keys.getSecond()[i] : stopRow;
        byte[] splitStart = startRow.length == 0 || 
            Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ? 
        byte[] splitStop = (stopRow.length == 0 || 
            Bytes.compareTo(keys.getSecond()[i], stopRow) <= 0) && 
            keys.getSecond()[i].length > 0 ? keys.getSecond()[i] : stopRow;
      table = new HTable(conf, tableName) {
        @Override
        public synchronized void flushCommits() throws IOException {
          super.flushCommits();
        }
      };
      table = new HTable(conf, tableName) {
        @Override
        public synchronized void flushCommits() throws IOException {
          super.flushCommits();
        }
      };
  private Class<T> persistentClass;
  private CassandraMapping cassandraMapping = null;
  public void initialize(Class<K> keyClass, Class<T> persistentClass) throws Exception {

    // get cassandra mapping with persistent class
    this.persistentClass = persistentClass;
    this.cassandraMapping = CassandraMappingManager.getManager().get(persistentClass);

  public CassandraMapping(Element keyspace, Element mapping) {
    	// LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	// LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
    	// LOG.info("Located Cassandra Keyspace cluster: '"  CLUSTER_ATTRIBUTE  "'");
    	// LOG.info("Located Cassandra Keyspace host: '"  HOST_ATTRIBUTE  "'");
      	// LOG.info("Located column family name: '"  NAME_ATTRIBUTE  "'");
    	// LOG.info("Located super column family");
      this.cassandraClient.initialize(keyClass, persistent);
  private Class<T> persistentClass;
  private CassandraMapping cassandraMapping = null;
  public void initialize(Class<K> keyClass, Class<T> persistentClass) throws Exception {

    // get cassandra mapping with persistent class
    this.persistentClass = persistentClass;
    this.cassandraMapping = CassandraMappingManager.getManager().get(persistentClass);

  public CassandraMapping(Element keyspace, Element mapping) {
    	// LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	// LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
    	// LOG.info("Located Cassandra Keyspace cluster: '"  CLUSTER_ATTRIBUTE  "'");
    	// LOG.info("Located Cassandra Keyspace host: '"  HOST_ATTRIBUTE  "'");
      	// LOG.info("Located column family name: '"  NAME_ATTRIBUTE  "'");
    	// LOG.info("Located super column family");
      this.cassandraClient.initialize(keyClass, persistent);
import me.prettyprint.hector.api.Serializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
  protected Object fromByteBuffer(Schema schema, ByteBuffer byteBuffer) {
    Serializer serializer = GoraSerializerTypeInferer.getSerializer(schema);
    if (serializer == null) {
      LOG.info("Schema is not supported: "  schema.toString());
    } else {
      value = serializer.fromByteBuffer(byteBuffer);
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.TypeUtils;
    if (byteBuffer == null) {
      return null;
    }
      GenericArraySerializer serializer = GenericArraySerializer.get(fieldSchema.getElementType());
      GenericArray genericArray = serializer.fromByteBuffer(byteBuffer);
      value = genericArray;
    } else {
      value = fromByteBuffer(fieldSchema, byteBuffer);
import org.apache.gora.cassandra.serializers.Utf8Serializer;
        ListGenericArray array = new ListGenericArray(fieldSchema.getElementType());
          Object memberValue = fromByteBuffer(fieldSchema.getElementType(), hColumn.getValue());
          Object memberValue = null;
          memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());
          map.put(Utf8Serializer.get().fromByteBuffer(hColumn.getName()), memberValue);      
            if (memberName == null || memberName.length() == 0) {
              LOG.warn("member name is null or empty.");
              continue;
            }
import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
import org.apache.gora.cassandra.serializers.TypeUtils;
    this.keySerializer = GoraSerializerTypeInferer.getSerializer(keyClass);
    if (columnName == null) {
      LOG.warn("Column name is null for field="  fieldName  " with value="  value.toString());
      return;
    }
    HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);

    HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
  }

  public void addSubColumn(K key, String fieldName, String columnName, Object value) {
    addSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName), value);
  }

  public void addSubColumn(K key, String fieldName, Integer columnName, Object value) {
    addSubColumn(key, fieldName, IntegerSerializer.get().toByteBuffer(columnName), value);
  }


  @SuppressWarnings("unchecked")
  public void addGenericArray(K key, String fieldName, GenericArray array) {
    if (isSuper( cassandraMapping.getFamily(fieldName) )) {
      int i= 0;
      for (Object itemValue: array) {

        // TODO: hack, do not store empty arrays
        if (itemValue instanceof GenericArray<?>) {
          if (((GenericArray)itemValue).size() == 0) {
            continue;
          }
        }

        addSubColumn(key, fieldName, i, itemValue);
      }
    }
    else {
      addColumn(key, fieldName, array);
    }
    Serializer serializer = GoraSerializerTypeInferer.getSerializer(value);
    if (serializer == null) {
      LOG.info("Serializer not found for: "  value.toString());
      byteBuffer = serializer.toByteBuffer(value);
    }

    if (byteBuffer == null) {
      LOG.info("value class="  value.getClass().getName()  " value="  value  " -> null");
import org.apache.avro.specific.SpecificFixed;
            ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
      case BOOLEAN:
      case FIXED:
                if (((GenericArray)memberValue).size() == 0) {

              this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
                if (((GenericArray)keyValue).size() == 0) {

              this.cassandraClient.addSubColumn(key, field.name(), mapKey.toString(), keyValue);
            this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
import me.prettyprint.hector.api.Serializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
  protected Object fromByteBuffer(Schema schema, ByteBuffer byteBuffer) {
    Serializer serializer = GoraSerializerTypeInferer.getSerializer(schema);
    if (serializer == null) {
      LOG.info("Schema is not supported: "  schema.toString());
    } else {
      value = serializer.fromByteBuffer(byteBuffer);
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.TypeUtils;
    if (byteBuffer == null) {
      return null;
    }
      GenericArraySerializer serializer = GenericArraySerializer.get(fieldSchema.getElementType());
      GenericArray genericArray = serializer.fromByteBuffer(byteBuffer);
      value = genericArray;
    } else {
      value = fromByteBuffer(fieldSchema, byteBuffer);
import org.apache.gora.cassandra.serializers.Utf8Serializer;
        ListGenericArray array = new ListGenericArray(fieldSchema.getElementType());
          Object memberValue = fromByteBuffer(fieldSchema.getElementType(), hColumn.getValue());
          Object memberValue = null;
          memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());
          map.put(Utf8Serializer.get().fromByteBuffer(hColumn.getName()), memberValue);      
            if (memberName == null || memberName.length() == 0) {
              LOG.warn("member name is null or empty.");
              continue;
            }
import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
import org.apache.gora.cassandra.serializers.TypeUtils;
    this.keySerializer = GoraSerializerTypeInferer.getSerializer(keyClass);
    if (columnName == null) {
      LOG.warn("Column name is null for field="  fieldName  " with value="  value.toString());
      return;
    }
    HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);

    HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
  }

  public void addSubColumn(K key, String fieldName, String columnName, Object value) {
    addSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName), value);
  }

  public void addSubColumn(K key, String fieldName, Integer columnName, Object value) {
    addSubColumn(key, fieldName, IntegerSerializer.get().toByteBuffer(columnName), value);
  }


  @SuppressWarnings("unchecked")
  public void addGenericArray(K key, String fieldName, GenericArray array) {
    if (isSuper( cassandraMapping.getFamily(fieldName) )) {
      int i= 0;
      for (Object itemValue: array) {

        // TODO: hack, do not store empty arrays
        if (itemValue instanceof GenericArray<?>) {
          if (((GenericArray)itemValue).size() == 0) {
            continue;
          }
        }

        addSubColumn(key, fieldName, i, itemValue);
      }
    }
    else {
      addColumn(key, fieldName, array);
    }
    Serializer serializer = GoraSerializerTypeInferer.getSerializer(value);
    if (serializer == null) {
      LOG.info("Serializer not found for: "  value.toString());
      byteBuffer = serializer.toByteBuffer(value);
    }

    if (byteBuffer == null) {
      LOG.info("value class="  value.getClass().getName()  " value="  value  " -> null");
import org.apache.avro.specific.SpecificFixed;
            ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
      case BOOLEAN:
      case FIXED:
                if (((GenericArray)memberValue).size() == 0) {

              this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
                if (((GenericArray)keyValue).size() == 0) {

              this.cassandraClient.addSubColumn(key, field.name(), mapKey.toString(), keyValue);
            this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
      DataStoreBase that = (DataStoreBase) obj;
   * First the schema name in the {@link Configuration} is used. If null,
   * the schema name in the defined properties is returned. If null then
    String confSchemaName = getOrCreateConf().get("preferred.schema.name");
    if (confSchemaName != null) {
      return confSchemaName;
    }
    
      DataStoreBase that = (DataStoreBase) obj;
   * First the schema name in the {@link Configuration} is used. If null,
   * the schema name in the defined properties is returned. If null then
    String confSchemaName = getOrCreateConf().get("preferred.schema.name");
    if (confSchemaName != null) {
      return confSchemaName;
    }
    
import org.apache.gora.cassandra.serializers.StatefulHashMapSerializer;
import org.apache.gora.persistency.StatefulHashMap;
    } else if (type == Type.MAP) {
      StatefulHashMapSerializer serializer = StatefulHashMapSerializer.get(fieldSchema.getValueType());
      StatefulHashMap map = serializer.fromByteBuffer(byteBuffer);
      value = map;
import org.apache.gora.persistency.StatefulHashMap;

    } else if (value instanceof StatefulHashMap) {
      StatefulHashMap map = (StatefulHashMap)value;
      if (map.size() == 0) {
        serializer = ByteBufferSerializer.get();
      }
      else {
        Object value0 = map.values().iterator().next();
        Schema schema = TypeUtils.getSchema(value0);
        serializer = StatefulHashMapSerializer.get(schema);
      }
    } else if (type == Type.MAP) {
      serializer = StatefulHashMapSerializer.get(schema.getValueType());
    } else if (type == Type.MAP) {
      serializer = StatefulHashMapSerializer.get(elementType);
import org.apache.gora.persistency.StatefulHashMap;
      // LOG.info("Keyspace '"  this.cassandraMapping.getKeyspaceName()  "' in cluster '"  this.cassandraMapping.getClusterName()  "' was created on host '"  this.cassandraMapping.getHostName()  "'");
        } else if (itemValue instanceof StatefulHashMap<?,?>) {
          if (((StatefulHashMap)itemValue).size() == 0) {
            continue;
          }
  @SuppressWarnings("unchecked")
  public void addStatefulHashMap(K key, String fieldName, StatefulHashMap<Utf8,Object> map) {
    if (isSuper( cassandraMapping.getFamily(fieldName) )) {
      int i= 0;
      for (Utf8 mapKey: map.keySet()) {

        // TODO: hack, do not store empty arrays
        Object mapValue = map.get(mapKey);
        if (mapValue instanceof GenericArray<?>) {
          if (((GenericArray)mapValue).size() == 0) {
            continue;
          }
        } else if (mapValue instanceof StatefulHashMap<?,?>) {
          if (((StatefulHashMap)mapValue).size() == 0) {
            continue;
          }
        }

        addSubColumn(key, fieldName, mapKey.toString(), mapValue);
      }
    }
    else {
      addColumn(key, fieldName, map);
    }
  }

        // LOG.info("Added super column family: '"  familyName  "'");
      // LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	// LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
      // LOG.info("Located Cassandra Mapping: '"  MAPPING_ELEMENT  "'");
    	// LOG.info("Located Cassandra Mapping class name: '"  NAME_ATTRIBUTE  "'");
      int fieldPos = field.pos();
      if (value.isDirty(fieldPos)) {
        Object fieldValue = value.get(fieldPos);
        p.put(fieldPos, fieldValue);
              } else if (memberValue instanceof StatefulHashMap<?,?>) {
                if (((StatefulHashMap)memberValue).size() == 0) {
                  continue;
                }
            this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
import org.apache.gora.cassandra.serializers.StatefulHashMapSerializer;
import org.apache.gora.persistency.StatefulHashMap;
    } else if (type == Type.MAP) {
      StatefulHashMapSerializer serializer = StatefulHashMapSerializer.get(fieldSchema.getValueType());
      StatefulHashMap map = serializer.fromByteBuffer(byteBuffer);
      value = map;
import org.apache.gora.persistency.StatefulHashMap;

    } else if (value instanceof StatefulHashMap) {
      StatefulHashMap map = (StatefulHashMap)value;
      if (map.size() == 0) {
        serializer = ByteBufferSerializer.get();
      }
      else {
        Object value0 = map.values().iterator().next();
        Schema schema = TypeUtils.getSchema(value0);
        serializer = StatefulHashMapSerializer.get(schema);
      }
    } else if (type == Type.MAP) {
      serializer = StatefulHashMapSerializer.get(schema.getValueType());
    } else if (type == Type.MAP) {
      serializer = StatefulHashMapSerializer.get(elementType);
import org.apache.gora.persistency.StatefulHashMap;
      // LOG.info("Keyspace '"  this.cassandraMapping.getKeyspaceName()  "' in cluster '"  this.cassandraMapping.getClusterName()  "' was created on host '"  this.cassandraMapping.getHostName()  "'");
        } else if (itemValue instanceof StatefulHashMap<?,?>) {
          if (((StatefulHashMap)itemValue).size() == 0) {
            continue;
          }
  @SuppressWarnings("unchecked")
  public void addStatefulHashMap(K key, String fieldName, StatefulHashMap<Utf8,Object> map) {
    if (isSuper( cassandraMapping.getFamily(fieldName) )) {
      int i= 0;
      for (Utf8 mapKey: map.keySet()) {

        // TODO: hack, do not store empty arrays
        Object mapValue = map.get(mapKey);
        if (mapValue instanceof GenericArray<?>) {
          if (((GenericArray)mapValue).size() == 0) {
            continue;
          }
        } else if (mapValue instanceof StatefulHashMap<?,?>) {
          if (((StatefulHashMap)mapValue).size() == 0) {
            continue;
          }
        }

        addSubColumn(key, fieldName, mapKey.toString(), mapValue);
      }
    }
    else {
      addColumn(key, fieldName, map);
    }
  }

        // LOG.info("Added super column family: '"  familyName  "'");
      // LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	// LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
      // LOG.info("Located Cassandra Mapping: '"  MAPPING_ELEMENT  "'");
    	// LOG.info("Located Cassandra Mapping class name: '"  NAME_ATTRIBUTE  "'");
      int fieldPos = field.pos();
      if (value.isDirty(fieldPos)) {
        Object fieldValue = value.get(fieldPos);
        p.put(fieldPos, fieldValue);
              } else if (memberValue instanceof StatefulHashMap<?,?>) {
                if (((StatefulHashMap)memberValue).size() == 0) {
                  continue;
                }
            this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
      LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
      LOG.info("Located Cassandra Mapping: '"  MAPPING_ELEMENT  "'");
    	LOG.info("Located Cassandra Mapping class name: '"  NAME_ATTRIBUTE  "'");
      LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "'");
      LOG.info("Located Cassandra Mapping: '"  MAPPING_ELEMENT  "'");
    	LOG.info("Located Cassandra Mapping class name: '"  NAME_ATTRIBUTE  "'");
import java.util.Map.Entry;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
	  DataStoreBase that = (DataStoreBase) obj;
   * First the schema name in the defined properties is returned. If null then
import java.util.Map.Entry;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
	  DataStoreBase that = (DataStoreBase) obj;
   * First the schema name in the defined properties is returned. If null then
      DataStoreBase that = (DataStoreBase) obj;
   * The schema name is prefixed with schema.prefix from {@link Configuration}.
   * The schema name in the defined properties is returned. If null then
    String prefix = getOrCreateConf().get("schema.prefix","");
    
      return prefixschemaName;
      return prefixmappingSchemaName;
    return prefixStringUtils.getClassname(persistentClass);
      DataStoreBase that = (DataStoreBase) obj;
   * The schema name is prefixed with schema.prefix from {@link Configuration}.
   * The schema name in the defined properties is returned. If null then
    String prefix = getOrCreateConf().get("schema.prefix","");
    
      return prefixschemaName;
      return prefixmappingSchemaName;
    return prefixStringUtils.getClassname(persistentClass);

  /**
   * Check if keyspace already exists.
   */
  public boolean keyspaceExists() {
    KeyspaceDefinition keyspaceDefinition = this.cluster.describeKeyspace(this.cassandraMapping.getKeyspaceName());
    return (keyspaceDefinition != null);
  }
    return cassandraClient.keyspaceExists();

  /**
   * Check if keyspace already exists.
   */
  public boolean keyspaceExists() {
    KeyspaceDefinition keyspaceDefinition = this.cluster.describeKeyspace(this.cassandraMapping.getKeyspaceName());
    return (keyspaceDefinition != null);
  }
    return cassandraClient.keyspaceExists();
import org.apache.gora.persistency.State;
    List<byte[]> list = new ArrayList<byte[]>(map.size());
    int n = 0;
      if (map.getState(key) == State.DELETED) {
        continue;
      }
      n = 4;
    List<byte[]> list = new ArrayList<byte[]>(map.size());
    int n = 0;
      if (map.getState(key) == State.DELETED) {
        continue;
      }
      n = 4;
      n = 4;
import org.apache.gora.persistency.State;
  /**
   * Delete a member in a super column. This is used for map and record Avro types.
   * @param key the row key
   * @param fieldName the field name
   * @param columnName the column name (the member name, or the index of array)
   */
  @SuppressWarnings("unchecked")
  public void deleteSubColumn(K key, String fieldName, ByteBuffer columnName) {

    String columnFamily = this.cassandraMapping.getFamily(fieldName);
    String superColumnName = this.cassandraMapping.getColumn(fieldName);
    
    HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
  }

  public void deleteSubColumn(K key, String fieldName, String columnName) {
    deleteSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName));
  }


        if (map.getState(mapKey) == State.DELETED) {
          deleteSubColumn(key, fieldName, mapKey.toString());
          continue;
        }
            // needs to keep State.DELETED.
  public static<K> void deleteSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName) {
    mutator.subDelete(key, columnFamily, superColumnName, columnName, StringSerializer.get(), ByteBufferSerializer.get());
  }


import org.apache.gora.persistency.State;
    List<byte[]> list = new ArrayList<byte[]>(map.size());
    int n = 0;
      if (map.getState(key) == State.DELETED) {
        continue;
      }
      n = 4;
    List<byte[]> list = new ArrayList<byte[]>(map.size());
    int n = 0;
      if (map.getState(key) == State.DELETED) {
        continue;
      }
      n = 4;
      n = 4;
import org.apache.gora.persistency.State;
  /**
   * Delete a member in a super column. This is used for map and record Avro types.
   * @param key the row key
   * @param fieldName the field name
   * @param columnName the column name (the member name, or the index of array)
   */
  @SuppressWarnings("unchecked")
  public void deleteSubColumn(K key, String fieldName, ByteBuffer columnName) {

    String columnFamily = this.cassandraMapping.getFamily(fieldName);
    String superColumnName = this.cassandraMapping.getColumn(fieldName);
    
    HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
  }

  public void deleteSubColumn(K key, String fieldName, String columnName) {
    deleteSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName));
  }


        if (map.getState(mapKey) == State.DELETED) {
          deleteSubColumn(key, fieldName, mapKey.toString());
          continue;
        }
            // needs to keep State.DELETED.
  public static<K> void deleteSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName) {
    mutator.subDelete(key, columnFamily, superColumnName, columnName, StringSerializer.get(), ByteBufferSerializer.get());
  }



  /**
   * Obtain Schema/Keyspace name
   * @return Keyspace
   */
  public String getKeyspaceName() {
	return this.cassandraMapping.getKeyspaceName();
  }

    LOG.debug("creating Cassandra keyspace");
  
  /**
   * In Cassandra Schemas are referred to as Keyspaces
   * @return Keyspace
   */
	return this.cassandraClient.getKeyspaceName();

  /**
   * Obtain Schema/Keyspace name
   * @return Keyspace
   */
  public String getKeyspaceName() {
	return this.cassandraMapping.getKeyspaceName();
  }

    LOG.debug("creating Cassandra keyspace");
  
  /**
   * In Cassandra Schemas are referred to as Keyspaces
   * @return Keyspace
   */
	return this.cassandraClient.getKeyspaceName();
  private static final String USAGE = "LogAnalytics <input_data_store> <output_data_store>";
  
    if(args.length < 2) {
      System.err.println(USAGE);
      System.exit(1);
    }
  private static final String USAGE = "LogAnalytics <input_data_store> <output_data_store>";
  
    if(args.length < 2) {
      System.err.println(USAGE);
      System.exit(1);
    }
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloQuery<K,T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloResult<K,T extends PersistentBase> extends ResultBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class AccumuloStore<K,T extends PersistentBase> extends DataStoreBase<K,T> {
  public static final Logger LOG = LoggerFactory.getLogger(AccumuloStore.class);
  
  public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) {
    try{
      super.initialize(keyClass, persistentClass, properties);
  
      String mock = DataStoreFactory.findProperty(properties, this, MOCK_PROPERTY, null);
      String mappingFile = DataStoreFactory.getMappingFile(properties, this, DEFAULT_MAPPING_FILE);
      String user = DataStoreFactory.findProperty(properties, this, USERNAME_PROPERTY, null);
      String password = DataStoreFactory.findProperty(properties, this, PASSWORD_PROPERTY, null);
      
      mapping = readMapping(mappingFile);
  
      if (mapping.encoder == null || mapping.encoder.equals("")) {
        encoder = new org.apache.gora.accumulo.encoders.BinaryEncoder();
        try {
          encoder = (Encoder) getClass().getClassLoader().loadClass(mapping.encoder).newInstance();
        } catch (InstantiationException e) {
          throw new IOException(e);
        } catch (IllegalAccessException e) {
          throw new IOException(e);
        } catch (ClassNotFoundException e) {
          throw new IOException(e);
        }
  
      try {
        if (mock == null || !mock.equals("true")) {
          String instance = DataStoreFactory.findProperty(properties, this, INSTANCE_NAME_PROPERTY, null);
          String zookeepers = DataStoreFactory.findProperty(properties, this, ZOOKEEPERS_NAME_PROPERTY, null);
          conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, password);
          authInfo = new AuthInfo(user, ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());
        } else {
          conn = new MockInstance().getConnector(user, password);
        }
  
        if (autoCreateSchema)
          createSchema();
      } catch (AccumuloException e) {
        throw new IOException(e);
      } catch (AccumuloSecurityException e) {
        throw new IOException(e);
      }
    }catch(IOException e){
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void createSchema() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void deleteSchema() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    } 
  public boolean schemaExists() {
  public T get(K key, String[] fields) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return null;
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void put(K key, T val) {
    try{
      Mutation m = new Mutation(new Text(toBytes(key)));
      Schema schema = val.getSchema();
      StateManager stateManager = val.getStateManager();
      
      Iterator<Field> iter = schema.getFields().iterator();
      
      int count = 0;
      for (int i = 0; iter.hasNext(); i) {
        Field field = iter.next();
        if (!stateManager.isDirty(val, i)) {
          continue;
        }
        
        Object o = val.get(i);
        Pair<Text,Text> col = mapping.fieldMap.get(field.name());
  
        switch (field.schema().getType()) {
          case MAP:
            if (o instanceof StatefulMap) {
              StatefulMap map = (StatefulMap) o;
              Set<?> es = map.states().entrySet();
              for (Object entry : es) {
                Object mapKey = ((Entry) entry).getKey();
                State state = (State) ((Entry) entry).getValue();
  
                switch (state) {
                  case NEW:
                  case DIRTY:
                    m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(map.get(mapKey))));
                    count;
                    break;
                  case DELETED:
                    m.putDelete(col.getFirst(), new Text(toBytes(mapKey)));
                    count;
                    break;
                }
                
            } else {
              Map map = (Map) o;
              Set<?> es = map.entrySet();
              for (Object entry : es) {
                Object mapKey = ((Entry) entry).getKey();
                Object mapVal = ((Entry) entry).getValue();
                m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(mapVal)));
                count;
              }
            break;
          case ARRAY:
            GenericArray array = (GenericArray) o;
            int j = 0;
            for (Object item : array) {
              m.put(col.getFirst(), new Text(toBytes(j)), new Value(toBytes(item)));
            break;
          case RECORD:
            SpecificDatumWriter writer = new SpecificDatumWriter(field.schema());
            ByteArrayOutputStream os = new ByteArrayOutputStream();
            BinaryEncoder encoder = new BinaryEncoder(os);
            writer.write(o, encoder);
            encoder.flush();
            m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
            break;
          default:
            m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
        }
  
      
      if (count > 0)
        try {
          getBatchWriter().addMutation(m);
        } catch (MutationsRejectedException e) {
          LOG.error(e.getMessage());
          LOG.error(e.getStackTrace().toString());
        }
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public boolean delete(K key) {
  public long deleteByQuery(Query<K,T> query) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return 0;
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return 0;
    } catch (IOException e){
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return 0;
  public Result<K,T> execute(Query<K,T> query) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return null;
    } 
  public void flush() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    } 
  public void close() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    } 
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraClient<K, T extends PersistentBase> {
public class CassandraStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) {
      super.initialize(keyClass, persistent, properties);
    } catch (Exception e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void close() {
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
  public void deleteSchema() {
  public Result<K, T> execute(Query<K, T> query) {
  public void flush() {
  public T get(K key, String[] fields) {
    boolean hasResult = false;
    try {
      hasResult = result.next();
    } catch (Exception e) {
      // TODO Auto-generated catch block
      e.printStackTrace();
    }
    return this.cassandraClient.getKeyspaceName();
  public void put(K key, T value) {
            PersistentBase persistent = (PersistentBase) fieldValue;
            PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
  public boolean schemaExists() {
	  try{
	    WebPage page;
	    log.info("creating web page data");
	    
	    for(int i=0; i<URLS.length; i) {
	      page = new WebPage();
	      page.setUrl(new Utf8(URLS[i]));
	      page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
	      for(String token : CONTENTS[i].split(" ")) {
	        page.addToParsedContent(new Utf8(token));  
	      }
	      
	      for(int j=0; j<LINKS[i].length; j) {
	        page.putToOutlinks(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
	      }
	      
	      Metadata metadata = new Metadata();
	      metadata.setVersion(1);
	      metadata.putToData(new Utf8("metakey"), new Utf8("metavalue"));
	      page.setMetadata(metadata);
	      
	      dataStore.put(URLS[i], page);
	    }
	    dataStore.flush();
	    log.info("finished creating web page data");
  	}
 	catch(Exception e){
 		log.info("error creating web page data");
 	}
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumReader<T extends PersistentBase>
    Persistent cloned = (PersistentBase)persistent.newInstance(new StateManagerImpl());
        case STRING : ((PersistentBase)cloned).put(pos, cloneObject(
            field.schema(), ((PersistentBase)persistent).get(pos), ((PersistentBase)cloned).get(pos))); break;
        default     : ((PersistentBase)cloned).put(pos, ((PersistentBase)persistent).get(pos)); break;
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumWriter<T extends PersistentBase>
import java.io.IOException;

import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.Result;
public class AvroQuery<K, T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class DataFileAvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
	  if(reader != null)
		  reader.close();
	  reader = null;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class AvroStore<K, T extends PersistentBase>
  
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
      Properties properties) {
      super.initialize(keyClass, persistentClass, properties);
  
      if(properties != null) {
        if(this.codecType == null) {
          String codecType = DataStoreFactory.findProperty(
              properties, this, CODEC_TYPE_KEY, "BINARY");
          this.codecType = CodecType.valueOf(codecType);
        }
  public void close() {
    try{
      super.close();
      if(encoder != null) {
        encoder.flush();
      }
      encoder = null;
      decoder = null;
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
  public void flush() {
    try{
      super.flush();
      if(encoder != null)
        encoder.flush();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public T get(K key, String[] fields) {
  public void put(K key, T obj) {
    try{
      getDatumWriter().write(obj, getEncoder());
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void write(DataOutput out) {
  public void readFields(DataInput in) {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class DataFileAvroStore<K, T extends PersistentBase> extends AvroStore<K, T> {
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
  
  public T get(K key, String[] fields) {
  public void put(K key, T obj) {
    try{
      getWriter().append(obj);
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  protected Result<K, T> executeQuery(Query<K, T> query) {
    try{
      return new DataFileAvroResult<K, T>(this, query
          , createReader(createFsInput()));
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
    }
  protected Result<K,T> executePartial(FileSplitPartitionQuery<K,T> query) {
    try{
      FsInput fsInput = createFsInput();
      DataFileReader<T> reader = createReader(fsInput);
      return new DataFileAvroResult<K, T>(this, query, reader, fsInput
          , query.getStart(), query.getLength());
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
    }
  public void flush() {
    try{
      super.flush();
      if(writer != null) {
        writer.flush();
      }
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
  public void close() {
    try{
      if(writer != null)  
        writer.close(); //hadoop 0.20.2 HDFS streams do not allow 
                        //to close twice, so close the writer first 
      writer = null;
      super.close();
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
import org.apache.gora.persistency.impl.PersistentBase;
public class GoraInputFormat<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.impl.ResultBase;
public class GoraRecordReader<K, T extends PersistentBase> extends RecordReader<K,T> {
  public void executeQuery() throws IOException, Exception {
    try{
	  return result.getProgress();
  	}
 	catch(Exception e){
 		return 0;
 	}
	  try{
	    if (counter.isModulo()) {
	      boolean firstBatch = (this.result == null);
	      if (! firstBatch) {
	        this.query.setStartKey(this.result.getKey());
	        if (this.query.getLimit() == counter.getRecordsMax()) {
	          this.query.setLimit(counter.getRecordsMax()  1);
	        }
	      }
	      if (this.result != null) {
	        this.result.close();
	      }
	      
	      executeQuery();
	      
	      if (! firstBatch) {
	        // skip first result
	        this.result.next();
	      }
	    }
	    
	    counter.increment();
	    return this.result.next();
	  }
	  catch(Exception e){
		return false;
	  }
  //@Override
	  try{
		  store.close();
	  }catch(Exception e){
		  LOG.info("Exception at GoraRecordWriter.class while closing datastore."  e.getMessage());
	  }
	  try{
	    store.put(key, (Persistent) value);
	    
	    counter.increment();
	    if (counter.isModulo()) {
	      LOG.info("Flushing the datastore after "  counter.getRecordsNumber()  " records");
	      store.flush();
	    }
	  }catch(Exception e){
		  LOG.info("Exception at GoraRecordWriter.class while writing to datastore."  e.getMessage());
	  }
import org.apache.gora.persistency.impl.PersistentBase;
   implements Deserializer<PersistentBase> {
  private Class<? extends PersistentBase> persistentClass;
  private PersistentDatumReader<PersistentBase> datumReader;
  public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
      datumReader = new PersistentDatumReader<PersistentBase>(schema, true);
  //@Override
  //@Override
  public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
  public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
  public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
  public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
  public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
  private PersistentDatumWriter<PersistentBase> datumWriter;
    this.datumWriter = new PersistentDatumWriter<PersistentBase>();
  //@Override
  //@Override
  public void serialize(PersistentBase persistent) throws IOException {   
import org.apache.gora.persistency.impl.PersistentBase;
public class MemStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  public static class MemQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
  public static class MemResult<K, T extends PersistentBase> extends ResultBase<K, T> {
    //@Override
    public void close() { }
    
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
	try{
		long deletedRows = 0;
	    Result<K,T> result = query.execute();
	
	    while(result.next()) {
	      if(delete(result.getKey()))
	        deletedRows;
	    }
	    return 0;
	  }
	catch(Exception e){
		  return 0;
	}
  public Result<K, T> execute(Query<K, T> query) {
  public T get(K key, String[] fields) {
      ((PersistentBase)newObj).put(index, ((PersistentBase)obj).get(index));
  public void put(K key, T obj) {
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
  public void close() {
  public void createSchema() { }
  public void deleteSchema() {
  public boolean schemaExists() {
  public void flush() { }
public interface BeanFactory<K, T>{
public interface Persistent extends Cloneable{

public interface StateManager{
public abstract class PersistentBase implements Persistent, SpecificRecord {
  protected static final PersistentDatumReader<PersistentBase> datumReader =
    new PersistentDatumReader<PersistentBase>();
  //@Override
  //@Override
  //@Override
    dirtyBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
    readableBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
public interface PartitionQuery<K, T> extends Query<K, T> {
public interface Query<K, T> {
  Result<K,T> execute() throws Exception, IOException;
public interface Result<K,T> {
  boolean next() throws Exception, IOException;
  float getProgress() throws IOException, InterruptedException, Exception;

import org.apache.gora.persistency.impl.PersistentBase;
public class FileSplitPartitionQuery<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
public class PartitionQueryImpl<K, T extends PersistentBase>
    this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
    this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
import org.apache.hadoop.conf.Configurable; 
import org.apache.hadoop.io.Writable;
public abstract class QueryBase<K, T extends PersistentBase>  
implements Query<K,T>, Writable, Configurable {
	
  protected DataStoreBase<K,T> dataStore;
    this.dataStore = (DataStoreBase<K, T>)dataStore;
  public Result<K,T> execute() throws Exception, IOException {
    this.dataStore = (DataStoreBase<K, T>)dataStore;
      dataStore = (DataStoreBase<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(dataStoreClass), conf);
  //@Override
  public final boolean next() throws Exception, IOException {
	  if(isLimitReached()) {
		  return false;
	  }
	    
	  clear();
	  persistent = getOrCreatePersistent(persistent);
	  boolean ret = nextInner();
	  
	  if(ret) offset;
	  return ret;
  protected T getOrCreatePersistent(T persistent) throws Exception, IOException {
	  if(persistent != null) {
			return persistent;
		}
		return dataStore.newPersistent();
  }
  
  @Override
  public void close() throws IOException{
	// TODO Auto-generated method stub
	
 * after a subsequent call to {@link #flush()}. Additionally, exception
 * handling is largely DataStore specific and is not largely dealt
 * with from within this interface.
public interface DataStore<K, T> {
      Properties properties);
   * @throws IOException
  void createSchema();
   * @throws IOException
  void deleteSchema();
   * @throws IOException
  void truncateSchema();
   * @throws IOException
  boolean schemaExists();
   * @throws IOException
  K newKey();
   * @throws IOException
  T newPersistent();
   * @throws IOException
  T get(K key);
   * @throws IOException
  T get(K key, String[] fields);
   * @throws IOException
  void put(K key, T obj);
   * @throws IOException
  boolean delete(K key);
   * @throws IOException
  long deleteByQuery(Query<K, T> query);
   * @throws IOException
  Result<K,T> execute(Query<K, T> query);
   * @throws IOException 
  List<PartitionQuery<K,T>> getPartitions(Query<K,T> query) throws IOException;
   * @throws IOException
  void flush();
   * @throws IOException
  void close();
public class DataStoreFactory{
import java.io.Closeable;
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.io.Writable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
 * A Base class for Avro persistent {@link DataStore}s.
public abstract class DataStoreBase<K, T extends PersistentBase>
implements DataStore<K, T>, Configurable, Writable, Closeable {
	
  
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
      Properties properties) {
      setKeyClass(keyClass);
      setPersistentClass(persistentClass);
      if(this.beanFactory == null)
        this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
      schema = this.beanFactory.getCachedPersistent().getSchema();
      fieldMap = AvroUtils.getFieldMap(schema);
  
      autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
      this.properties = properties;
  
      datumReader = new PersistentDatumReader<T>(schema, false);
      datumWriter = new PersistentDatumWriter<T>(schema, false);
  public K newKey() {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
  public T newPersistent() {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
  public T get(K key) {
  public void readFields(DataInput in) {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void write(DataOutput out) {
    try {
      Text.writeString(out, getKeyClass().getCanonicalName());
      Text.writeString(out, getPersistentClass().getCanonicalName());
      WritableUtils.writeProperties(out, properties);
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    }
  public void truncateSchema() {
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public abstract class FileBackedDataStoreBase<K, T extends PersistentBase>
  
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
      Properties properties) {
  protected OutputStream createOutputStream() {
    OutputStream conf = null;
    try{
      Path path = new Path(outputPath);
      FileSystem fs = path.getFileSystem(getConf());
      conf = fs.create(path);
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
    return conf;
    try{
      if(inputStream == null) {
        inputStream = createInputStream();
      }
      return inputStream;
    }catch(IOException ex){
      throw new IOException(ex);
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
    List<InputSplit> splits = null;
    List<PartitionQuery<K, T>> queries = null;
    try{
      splits = GoraMapReduceUtils.getSplits(getConf(), inputPath);
      queries = new ArrayList<PartitionQuery<K,T>>(splits.size());
  
      for(InputSplit split : splits) {
        queries.add(new FileSplitPartitionQuery<K, T>(query, (FileSplit) split));
      }
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
  public Result<K, T> execute(Query<K, T> query) {
    Result<K, T> results = null;
    try{
      if(query instanceof FileSplitPartitionQuery) {
        results = executePartial((FileSplitPartitionQuery<K, T>) query);
      } else {
        results = executeQuery(query);
      }
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    return results;
  public void flush() {
    try{
      if(outputStream != null)
        outputStream.flush();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void createSchema() {
  public void deleteSchema() {
  public boolean schemaExists() {
  public void write(DataOutput out) {
    try{
      super.write(out);
      org.apache.gora.util.IOUtils.writeNullFieldsInfo(out, inputPath, outputPath);
      if(inputPath != null)
        Text.writeString(out, inputPath);
      if(outputPath != null)
        Text.writeString(out, outputPath);
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void readFields(DataInput in) {
    try{
      super.readFields(in);
      boolean[] nullFields = org.apache.gora.util.IOUtils.readNullFieldsInfo(in);
      if(!nullFields[0])
        inputPath = Text.readString(in);
      if(!nullFields[1])
        outputPath = Text.readString(in);
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void close() {
      IOUtils.closeStream(inputStream);
      IOUtils.closeStream(outputStream);
      inputStream = null;
      outputStream = null;
import org.apache.gora.persistency.impl.PersistentBase;
  public static<T extends PersistentBase> void serialize(OutputStream os,
  public static<T extends PersistentBase> byte[] serialize(PersistentDatumWriter<T> datumWriter
  public static<K, T extends PersistentBase> K deserialize(InputStream is,
  public static<K, T extends PersistentBase> K deserialize(byte[] bytes,
  public static<T extends PersistentBase> byte[] deserialize(PersistentDatumWriter<T> datumWriter
  public void close() {
  public void createSchema() {
  public void deleteSchema() {
  public void truncateSchema() {
  public boolean schemaExists() {
  public boolean delete(String key) {
  public long deleteByQuery(Query<String, MockPersistent> query) {
  public Result<String, MockPersistent> execute(Query<String, MockPersistent> query) {
  public void flush() {
  public MockPersistent get(String key, String[] fields) {
  public void put(String key, MockPersistent obj) {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseGetResult<K, T extends PersistentBase> extends HBaseResult<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class HBaseResult<K, T extends PersistentBase> 
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseScannerResult<K, T extends PersistentBase> 
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseStore<K, T extends PersistentBase> extends DataStoreBase<K, T>
      Properties properties) {
      
      super.initialize(keyClass, persistentClass, properties);
      this.conf = HBaseConfiguration.create(getConf());
      admin = new HBaseAdmin(this.conf);
      
          LOG.error(ex1.getMessage());
          LOG.error(ex1.getStackTrace().toString());
          //throw (ex1); //throw the original exception
      } 
    
    try{
      boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
      table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
    }
  public void createSchema() {
    try{
      if(schemaExists()) {
        return;
      }
      HTableDescriptor tableDesc = mapping.getTable();
  
      admin.createTable(tableDesc);
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
  public void deleteSchema() {
    try{
      if(!schemaExists()) {
        return;
      }
      admin.disableTable(getSchemaName());
      admin.deleteTable(getSchemaName());
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
  public boolean schemaExists() {
    try{
      return admin.tableExists(mapping.getTableName());
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
      return false;
    }
  public T get(K key, String[] fields) {
    try{
      fields = getFieldsToQuery(fields);
      Get get = new Get(toBytes(key));
      addFields(get, fields);
      Result result = table.get(get);
      return newInstance(result, fields);
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
      return null;
    }
  public void put(K key, T persistent) {
    try{
      Schema schema = persistent.getSchema();
      StateManager stateManager = persistent.getStateManager();
      byte[] keyRaw = toBytes(key);
      Put put = new Put(keyRaw);
      Delete delete = new Delete(keyRaw);
      boolean hasPuts = false;
      boolean hasDeletes = false;
      Iterator<Field> iter = schema.getFields().iterator();
      for (int i = 0; iter.hasNext(); i) {
        Field field = iter.next();
        if (!stateManager.isDirty(persistent, i)) {
          continue;
        }
        Type type = field.schema().getType();
        Object o = persistent.get(i);
        HBaseColumn hcol = mapping.getColumn(field.name());
        switch(type) {
          case MAP:
            if(o instanceof StatefulMap) {
              StatefulHashMap<Utf8, ?> map = (StatefulHashMap<Utf8, ?>) o;
              for (Entry<Utf8, State> e : map.states().entrySet()) {
                Utf8 mapKey = e.getKey();
                switch (e.getValue()) {
                  case DIRTY:
                    byte[] qual = Bytes.toBytes(mapKey.toString());
                    byte[] val = toBytes(map.get(mapKey), field.schema().getValueType());
                    put.add(hcol.getFamily(), qual, val);
                    hasPuts = true;
                    break;
                  case DELETED:
                    qual = Bytes.toBytes(mapKey.toString());
                    hasDeletes = true;
                    delete.deleteColumn(hcol.getFamily(), qual);
                    break;
                }
              }
            } else {
              Set<Map.Entry> set = ((Map)o).entrySet();
              for(Entry entry: set) {
                byte[] qual = toBytes(entry.getKey());
                byte[] val = toBytes(entry.getValue());
                put.add(hcol.getFamily(), qual, val);
                hasPuts = true;
            break;
          case ARRAY:
            if(o instanceof GenericArray) {
              GenericArray arr = (GenericArray) o;
              int j=0;
              for(Object item : arr) {
                byte[] val = toBytes(item);
                put.add(hcol.getFamily(), Bytes.toBytes(j), val);
                hasPuts = true;
              }
            break;
          default:
            put.add(hcol.getFamily(), hcol.getQualifier(), toBytes(o, field.schema()));
            hasPuts = true;
            break;
        }
      if (hasPuts) {
        table.put(put);
      }
      if (hasDeletes) {
        table.delete(delete);
      }
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
  public boolean delete(K key) {
    try{
      table.delete(new Delete(toBytes(key)));
      //HBase does not return success information and executing a get for
      //success is a bit costly
      return true;
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
      return false;
  public long deleteByQuery(Query<K, T> query) {
    try {
      String[] fields = getFieldsToQuery(query.getFields());
      //find whether all fields are queried, which means that complete
      //rows will be deleted
      boolean isAllFields = Arrays.equals(fields
          , getBeanFactory().getCachedPersistent().getFields());
  
      org.apache.gora.query.Result<K, T> result = null;
      result = query.execute();
      ArrayList<Delete> deletes = new ArrayList<Delete>();
      while(result.next()) {
        Delete delete = new Delete(toBytes(result.getKey()));
        deletes.add(delete);
        if(!isAllFields) {
          addFields(delete, query);
        }
      }
      table.delete(deletes);
      return deletes.size();
    } catch (Exception e) {
      // TODO Auto-generated catch block
      e.printStackTrace();
      return -1;
    }
  }

  @Override
  public void flush() {
    try{
      table.flushCommits();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public org.apache.gora.query.Result<K, T> execute(Query<K, T> query){
    try{
      //check if query.fields is null
      query.setFields(getFieldsToQuery(query.getFields()));
  
      if(query.getStartKey() != null && query.getStartKey().equals(
          query.getEndKey())) {
        Get get = new Get(toBytes(query.getStartKey()));
        addFields(get, query.getFields());
        addTimeRange(get, query);
        Result result = table.get(get);
        return new HBaseGetResult<K,T>(this, query, result);
      } else {
        ResultScanner scanner = createScanner(query);
  
        org.apache.gora.query.Result<K,T> result
        = new HBaseScannerResult<K,T>(this,query, scanner);
  
        return result;
      }
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
  public void close() {
    try{
      table.close();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HSqlInsertUpdateStatement<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class InsertUpdateStatement<K, V extends PersistentBase> {
import org.apache.gora.persistency.impl.PersistentBase;
  public static <K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class MySqlInsertUpdateStatement<K, V extends PersistentBase> extends InsertUpdateStatement<K, V> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  public void close() {
  public void createSchema() {
  public void deleteSchema() {
  public boolean schemaExists() {
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
  public void flush() {
  public T get(K key, String[] requestFields) {
  public Result<K, T> execute(Query<K, T> query) {
  public void put(K key, T persistent) {
  private void parse(String input) throws IOException, ParseException, Exception {
  private void storePageview(long key, Pageview pageview) throws IOException, Exception {
  private void get(long key) throws IOException, Exception {
  private void query(long key) throws IOException, Exception {
  private void query(long startKey, long endKey) throws IOException, Exception {
  private void deleteByQuery(long startKey, long endKey) throws IOException, Exception {
  private void printResult(Result<Long, Pageview> result) throws IOException, Exception {
  private void close() throws IOException, Exception {
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloQuery<K,T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloResult<K,T extends PersistentBase> extends ResultBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class AccumuloStore<K,T extends PersistentBase> extends DataStoreBase<K,T> {
  public static final Logger LOG = LoggerFactory.getLogger(AccumuloStore.class);
  
  public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) {
    try{
      super.initialize(keyClass, persistentClass, properties);
  
      String mock = DataStoreFactory.findProperty(properties, this, MOCK_PROPERTY, null);
      String mappingFile = DataStoreFactory.getMappingFile(properties, this, DEFAULT_MAPPING_FILE);
      String user = DataStoreFactory.findProperty(properties, this, USERNAME_PROPERTY, null);
      String password = DataStoreFactory.findProperty(properties, this, PASSWORD_PROPERTY, null);
      
      mapping = readMapping(mappingFile);
  
      if (mapping.encoder == null || mapping.encoder.equals("")) {
        encoder = new org.apache.gora.accumulo.encoders.BinaryEncoder();
        try {
          encoder = (Encoder) getClass().getClassLoader().loadClass(mapping.encoder).newInstance();
        } catch (InstantiationException e) {
          throw new IOException(e);
        } catch (IllegalAccessException e) {
          throw new IOException(e);
        } catch (ClassNotFoundException e) {
          throw new IOException(e);
        }
  
      try {
        if (mock == null || !mock.equals("true")) {
          String instance = DataStoreFactory.findProperty(properties, this, INSTANCE_NAME_PROPERTY, null);
          String zookeepers = DataStoreFactory.findProperty(properties, this, ZOOKEEPERS_NAME_PROPERTY, null);
          conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, password);
          authInfo = new AuthInfo(user, ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());
        } else {
          conn = new MockInstance().getConnector(user, password);
        }
  
        if (autoCreateSchema)
          createSchema();
      } catch (AccumuloException e) {
        throw new IOException(e);
      } catch (AccumuloSecurityException e) {
        throw new IOException(e);
      }
    }catch(IOException e){
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void createSchema() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void deleteSchema() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    } 
  public boolean schemaExists() {
  public T get(K key, String[] fields) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return null;
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void put(K key, T val) {
    try{
      Mutation m = new Mutation(new Text(toBytes(key)));
      Schema schema = val.getSchema();
      StateManager stateManager = val.getStateManager();
      
      Iterator<Field> iter = schema.getFields().iterator();
      
      int count = 0;
      for (int i = 0; iter.hasNext(); i) {
        Field field = iter.next();
        if (!stateManager.isDirty(val, i)) {
          continue;
        }
        
        Object o = val.get(i);
        Pair<Text,Text> col = mapping.fieldMap.get(field.name());
  
        switch (field.schema().getType()) {
          case MAP:
            if (o instanceof StatefulMap) {
              StatefulMap map = (StatefulMap) o;
              Set<?> es = map.states().entrySet();
              for (Object entry : es) {
                Object mapKey = ((Entry) entry).getKey();
                State state = (State) ((Entry) entry).getValue();
  
                switch (state) {
                  case NEW:
                  case DIRTY:
                    m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(map.get(mapKey))));
                    count;
                    break;
                  case DELETED:
                    m.putDelete(col.getFirst(), new Text(toBytes(mapKey)));
                    count;
                    break;
                }
                
            } else {
              Map map = (Map) o;
              Set<?> es = map.entrySet();
              for (Object entry : es) {
                Object mapKey = ((Entry) entry).getKey();
                Object mapVal = ((Entry) entry).getValue();
                m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(mapVal)));
                count;
              }
            break;
          case ARRAY:
            GenericArray array = (GenericArray) o;
            int j = 0;
            for (Object item : array) {
              m.put(col.getFirst(), new Text(toBytes(j)), new Value(toBytes(item)));
            break;
          case RECORD:
            SpecificDatumWriter writer = new SpecificDatumWriter(field.schema());
            ByteArrayOutputStream os = new ByteArrayOutputStream();
            BinaryEncoder encoder = new BinaryEncoder(os);
            writer.write(o, encoder);
            encoder.flush();
            m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
            break;
          default:
            m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
        }
  
      
      if (count > 0)
        try {
          getBatchWriter().addMutation(m);
        } catch (MutationsRejectedException e) {
          LOG.error(e.getMessage());
          LOG.error(e.getStackTrace().toString());
        }
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public boolean delete(K key) {
  public long deleteByQuery(Query<K,T> query) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return 0;
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return 0;
    } catch (IOException e){
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return 0;
  public Result<K,T> execute(Query<K,T> query) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
      return null;
    } 
  public void flush() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    } 
  public void close() {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    } 
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraClient<K, T extends PersistentBase> {
public class CassandraStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) {
      super.initialize(keyClass, persistent, properties);
    } catch (Exception e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void close() {
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
  public void deleteSchema() {
  public Result<K, T> execute(Query<K, T> query) {
  public void flush() {
  public T get(K key, String[] fields) {
    boolean hasResult = false;
    try {
      hasResult = result.next();
    } catch (Exception e) {
      // TODO Auto-generated catch block
      e.printStackTrace();
    }
    return this.cassandraClient.getKeyspaceName();
  public void put(K key, T value) {
            PersistentBase persistent = (PersistentBase) fieldValue;
            PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
  public boolean schemaExists() {
	  try{
	    WebPage page;
	    log.info("creating web page data");
	    
	    for(int i=0; i<URLS.length; i) {
	      page = new WebPage();
	      page.setUrl(new Utf8(URLS[i]));
	      page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
	      for(String token : CONTENTS[i].split(" ")) {
	        page.addToParsedContent(new Utf8(token));  
	      }
	      
	      for(int j=0; j<LINKS[i].length; j) {
	        page.putToOutlinks(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
	      }
	      
	      Metadata metadata = new Metadata();
	      metadata.setVersion(1);
	      metadata.putToData(new Utf8("metakey"), new Utf8("metavalue"));
	      page.setMetadata(metadata);
	      
	      dataStore.put(URLS[i], page);
	    }
	    dataStore.flush();
	    log.info("finished creating web page data");
  	}
 	catch(Exception e){
 		log.info("error creating web page data");
 	}
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumReader<T extends PersistentBase>
    Persistent cloned = (PersistentBase)persistent.newInstance(new StateManagerImpl());
        case STRING : ((PersistentBase)cloned).put(pos, cloneObject(
            field.schema(), ((PersistentBase)persistent).get(pos), ((PersistentBase)cloned).get(pos))); break;
        default     : ((PersistentBase)cloned).put(pos, ((PersistentBase)persistent).get(pos)); break;
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumWriter<T extends PersistentBase>
import java.io.IOException;

import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.Result;
public class AvroQuery<K, T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class DataFileAvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
	  if(reader != null)
		  reader.close();
	  reader = null;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class AvroStore<K, T extends PersistentBase>
  
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
      Properties properties) {
      super.initialize(keyClass, persistentClass, properties);
  
      if(properties != null) {
        if(this.codecType == null) {
          String codecType = DataStoreFactory.findProperty(
              properties, this, CODEC_TYPE_KEY, "BINARY");
          this.codecType = CodecType.valueOf(codecType);
        }
  public void close() {
    try{
      super.close();
      if(encoder != null) {
        encoder.flush();
      }
      encoder = null;
      decoder = null;
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
  public void flush() {
    try{
      super.flush();
      if(encoder != null)
        encoder.flush();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public T get(K key, String[] fields) {
  public void put(K key, T obj) {
    try{
      getDatumWriter().write(obj, getEncoder());
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void write(DataOutput out) {
  public void readFields(DataInput in) {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class DataFileAvroStore<K, T extends PersistentBase> extends AvroStore<K, T> {
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
  
  public T get(K key, String[] fields) {
  public void put(K key, T obj) {
    try{
      getWriter().append(obj);
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  protected Result<K, T> executeQuery(Query<K, T> query) {
    try{
      return new DataFileAvroResult<K, T>(this, query
          , createReader(createFsInput()));
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
    }
  protected Result<K,T> executePartial(FileSplitPartitionQuery<K,T> query) {
    try{
      FsInput fsInput = createFsInput();
      DataFileReader<T> reader = createReader(fsInput);
      return new DataFileAvroResult<K, T>(this, query, reader, fsInput
          , query.getStart(), query.getLength());
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
    }
  public void flush() {
    try{
      super.flush();
      if(writer != null) {
        writer.flush();
      }
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
  public void close() {
    try{
      if(writer != null)  
        writer.close(); //hadoop 0.20.2 HDFS streams do not allow 
                        //to close twice, so close the writer first 
      writer = null;
      super.close();
    } catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
import org.apache.gora.persistency.impl.PersistentBase;
public class GoraInputFormat<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.impl.ResultBase;
public class GoraRecordReader<K, T extends PersistentBase> extends RecordReader<K,T> {
  public void executeQuery() throws IOException, Exception {
    try{
	  return result.getProgress();
  	}
 	catch(Exception e){
 		return 0;
 	}
	  try{
	    if (counter.isModulo()) {
	      boolean firstBatch = (this.result == null);
	      if (! firstBatch) {
	        this.query.setStartKey(this.result.getKey());
	        if (this.query.getLimit() == counter.getRecordsMax()) {
	          this.query.setLimit(counter.getRecordsMax()  1);
	        }
	      }
	      if (this.result != null) {
	        this.result.close();
	      }
	      
	      executeQuery();
	      
	      if (! firstBatch) {
	        // skip first result
	        this.result.next();
	      }
	    }
	    
	    counter.increment();
	    return this.result.next();
	  }
	  catch(Exception e){
		return false;
	  }
  //@Override
	  try{
		  store.close();
	  }catch(Exception e){
		  LOG.info("Exception at GoraRecordWriter.class while closing datastore."  e.getMessage());
	  }
	  try{
	    store.put(key, (Persistent) value);
	    
	    counter.increment();
	    if (counter.isModulo()) {
	      LOG.info("Flushing the datastore after "  counter.getRecordsNumber()  " records");
	      store.flush();
	    }
	  }catch(Exception e){
		  LOG.info("Exception at GoraRecordWriter.class while writing to datastore."  e.getMessage());
	  }
import org.apache.gora.persistency.impl.PersistentBase;
   implements Deserializer<PersistentBase> {
  private Class<? extends PersistentBase> persistentClass;
  private PersistentDatumReader<PersistentBase> datumReader;
  public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
      datumReader = new PersistentDatumReader<PersistentBase>(schema, true);
  //@Override
  //@Override
  public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
  public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
  public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
  public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
  public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
  private PersistentDatumWriter<PersistentBase> datumWriter;
    this.datumWriter = new PersistentDatumWriter<PersistentBase>();
  //@Override
  //@Override
  public void serialize(PersistentBase persistent) throws IOException {   
import org.apache.gora.persistency.impl.PersistentBase;
public class MemStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  public static class MemQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
  public static class MemResult<K, T extends PersistentBase> extends ResultBase<K, T> {
    //@Override
    public void close() { }
    
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
	try{
		long deletedRows = 0;
	    Result<K,T> result = query.execute();
	
	    while(result.next()) {
	      if(delete(result.getKey()))
	        deletedRows;
	    }
	    return 0;
	  }
	catch(Exception e){
		  return 0;
	}
  public Result<K, T> execute(Query<K, T> query) {
  public T get(K key, String[] fields) {
      ((PersistentBase)newObj).put(index, ((PersistentBase)obj).get(index));
  public void put(K key, T obj) {
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
  public void close() {
  public void createSchema() { }
  public void deleteSchema() {
  public boolean schemaExists() {
  public void flush() { }
public interface BeanFactory<K, T>{
public interface Persistent extends Cloneable{

public interface StateManager{
public abstract class PersistentBase implements Persistent, SpecificRecord {
  protected static final PersistentDatumReader<PersistentBase> datumReader =
    new PersistentDatumReader<PersistentBase>();
  //@Override
  //@Override
  //@Override
    dirtyBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
    readableBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
public interface PartitionQuery<K, T> extends Query<K, T> {
public interface Query<K, T> {
  Result<K,T> execute() throws Exception, IOException;
public interface Result<K,T> {
  boolean next() throws Exception, IOException;
  float getProgress() throws IOException, InterruptedException, Exception;

import org.apache.gora.persistency.impl.PersistentBase;
public class FileSplitPartitionQuery<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
public class PartitionQueryImpl<K, T extends PersistentBase>
    this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
    this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
import org.apache.hadoop.conf.Configurable; 
import org.apache.hadoop.io.Writable;
public abstract class QueryBase<K, T extends PersistentBase>  
implements Query<K,T>, Writable, Configurable {
	
  protected DataStoreBase<K,T> dataStore;
    this.dataStore = (DataStoreBase<K, T>)dataStore;
  public Result<K,T> execute() throws Exception, IOException {
    this.dataStore = (DataStoreBase<K, T>)dataStore;
      dataStore = (DataStoreBase<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(dataStoreClass), conf);
  //@Override
  public final boolean next() throws Exception, IOException {
	  if(isLimitReached()) {
		  return false;
	  }
	    
	  clear();
	  persistent = getOrCreatePersistent(persistent);
	  boolean ret = nextInner();
	  
	  if(ret) offset;
	  return ret;
  protected T getOrCreatePersistent(T persistent) throws Exception, IOException {
	  if(persistent != null) {
			return persistent;
		}
		return dataStore.newPersistent();
  }
  
  @Override
  public void close() throws IOException{
	// TODO Auto-generated method stub
	
 * after a subsequent call to {@link #flush()}. Additionally, exception
 * handling is largely DataStore specific and is not largely dealt
 * with from within this interface.
public interface DataStore<K, T> {
      Properties properties);
   * @throws IOException
  void createSchema();
   * @throws IOException
  void deleteSchema();
   * @throws IOException
  void truncateSchema();
   * @throws IOException
  boolean schemaExists();
   * @throws IOException
  K newKey();
   * @throws IOException
  T newPersistent();
   * @throws IOException
  T get(K key);
   * @throws IOException
  T get(K key, String[] fields);
   * @throws IOException
  void put(K key, T obj);
   * @throws IOException
  boolean delete(K key);
   * @throws IOException
  long deleteByQuery(Query<K, T> query);
   * @throws IOException
  Result<K,T> execute(Query<K, T> query);
   * @throws IOException 
  List<PartitionQuery<K,T>> getPartitions(Query<K,T> query) throws IOException;
   * @throws IOException
  void flush();
   * @throws IOException
  void close();
public class DataStoreFactory{
import java.io.Closeable;
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.io.Writable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
 * A Base class for Avro persistent {@link DataStore}s.
public abstract class DataStoreBase<K, T extends PersistentBase>
implements DataStore<K, T>, Configurable, Writable, Closeable {
	
  
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
      Properties properties) {
      setKeyClass(keyClass);
      setPersistentClass(persistentClass);
      if(this.beanFactory == null)
        this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
      schema = this.beanFactory.getCachedPersistent().getSchema();
      fieldMap = AvroUtils.getFieldMap(schema);
  
      autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
      this.properties = properties;
  
      datumReader = new PersistentDatumReader<T>(schema, false);
      datumWriter = new PersistentDatumWriter<T>(schema, false);
  public K newKey() {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
  public T newPersistent() {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
  public T get(K key) {
  public void readFields(DataInput in) {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
  public void write(DataOutput out) {
    try {
      Text.writeString(out, getKeyClass().getCanonicalName());
      Text.writeString(out, getPersistentClass().getCanonicalName());
      WritableUtils.writeProperties(out, properties);
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    }
  public void truncateSchema() {
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public abstract class FileBackedDataStoreBase<K, T extends PersistentBase>
  
  public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
      Properties properties) {
  protected OutputStream createOutputStream() {
    OutputStream conf = null;
    try{
      Path path = new Path(outputPath);
      FileSystem fs = path.getFileSystem(getConf());
      conf = fs.create(path);
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
    return conf;
    try{
      if(inputStream == null) {
        inputStream = createInputStream();
      }
      return inputStream;
    }catch(IOException ex){
      throw new IOException(ex);
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
    List<InputSplit> splits = null;
    List<PartitionQuery<K, T>> queries = null;
    try{
      splits = GoraMapReduceUtils.getSplits(getConf(), inputPath);
      queries = new ArrayList<PartitionQuery<K,T>>(splits.size());
  
      for(InputSplit split : splits) {
        queries.add(new FileSplitPartitionQuery<K, T>(query, (FileSplit) split));
      }
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
  public Result<K, T> execute(Query<K, T> query) {
    Result<K, T> results = null;
    try{
      if(query instanceof FileSplitPartitionQuery) {
        results = executePartial((FileSplitPartitionQuery<K, T>) query);
      } else {
        results = executeQuery(query);
      }
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    return results;
  public void flush() {
    try{
      if(outputStream != null)
        outputStream.flush();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void createSchema() {
  public void deleteSchema() {
  public boolean schemaExists() {
  public void write(DataOutput out) {
    try{
      super.write(out);
      org.apache.gora.util.IOUtils.writeNullFieldsInfo(out, inputPath, outputPath);
      if(inputPath != null)
        Text.writeString(out, inputPath);
      if(outputPath != null)
        Text.writeString(out, outputPath);
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void readFields(DataInput in) {
    try{
      super.readFields(in);
      boolean[] nullFields = org.apache.gora.util.IOUtils.readNullFieldsInfo(in);
      if(!nullFields[0])
        inputPath = Text.readString(in);
      if(!nullFields[1])
        outputPath = Text.readString(in);
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public void close() {
      IOUtils.closeStream(inputStream);
      IOUtils.closeStream(outputStream);
      inputStream = null;
      outputStream = null;
import org.apache.gora.persistency.impl.PersistentBase;
  public static<T extends PersistentBase> void serialize(OutputStream os,
  public static<T extends PersistentBase> byte[] serialize(PersistentDatumWriter<T> datumWriter
  public static<K, T extends PersistentBase> K deserialize(InputStream is,
  public static<K, T extends PersistentBase> K deserialize(byte[] bytes,
  public static<T extends PersistentBase> byte[] deserialize(PersistentDatumWriter<T> datumWriter
  public void close() {
  public void createSchema() {
  public void deleteSchema() {
  public void truncateSchema() {
  public boolean schemaExists() {
  public boolean delete(String key) {
  public long deleteByQuery(Query<String, MockPersistent> query) {
  public Result<String, MockPersistent> execute(Query<String, MockPersistent> query) {
  public void flush() {
  public MockPersistent get(String key, String[] fields) {
  public void put(String key, MockPersistent obj) {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseGetResult<K, T extends PersistentBase> extends HBaseResult<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class HBaseResult<K, T extends PersistentBase> 
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseScannerResult<K, T extends PersistentBase> 
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseStore<K, T extends PersistentBase> extends DataStoreBase<K, T>
      Properties properties) {
      
      super.initialize(keyClass, persistentClass, properties);
      this.conf = HBaseConfiguration.create(getConf());
      admin = new HBaseAdmin(this.conf);
      
          LOG.error(ex1.getMessage());
          LOG.error(ex1.getStackTrace().toString());
          //throw (ex1); //throw the original exception
      } 
    
    try{
      boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
      table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
    }
  public void createSchema() {
    try{
      if(schemaExists()) {
        return;
      }
      HTableDescriptor tableDesc = mapping.getTable();
  
      admin.createTable(tableDesc);
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
  public void deleteSchema() {
    try{
      if(!schemaExists()) {
        return;
      }
      admin.disableTable(getSchemaName());
      admin.deleteTable(getSchemaName());
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
  public boolean schemaExists() {
    try{
      return admin.tableExists(mapping.getTableName());
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
      return false;
    }
  public T get(K key, String[] fields) {
    try{
      fields = getFieldsToQuery(fields);
      Get get = new Get(toBytes(key));
      addFields(get, fields);
      Result result = table.get(get);
      return newInstance(result, fields);
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
      return null;
    }
  public void put(K key, T persistent) {
    try{
      Schema schema = persistent.getSchema();
      StateManager stateManager = persistent.getStateManager();
      byte[] keyRaw = toBytes(key);
      Put put = new Put(keyRaw);
      Delete delete = new Delete(keyRaw);
      boolean hasPuts = false;
      boolean hasDeletes = false;
      Iterator<Field> iter = schema.getFields().iterator();
      for (int i = 0; iter.hasNext(); i) {
        Field field = iter.next();
        if (!stateManager.isDirty(persistent, i)) {
          continue;
        }
        Type type = field.schema().getType();
        Object o = persistent.get(i);
        HBaseColumn hcol = mapping.getColumn(field.name());
        switch(type) {
          case MAP:
            if(o instanceof StatefulMap) {
              StatefulHashMap<Utf8, ?> map = (StatefulHashMap<Utf8, ?>) o;
              for (Entry<Utf8, State> e : map.states().entrySet()) {
                Utf8 mapKey = e.getKey();
                switch (e.getValue()) {
                  case DIRTY:
                    byte[] qual = Bytes.toBytes(mapKey.toString());
                    byte[] val = toBytes(map.get(mapKey), field.schema().getValueType());
                    put.add(hcol.getFamily(), qual, val);
                    hasPuts = true;
                    break;
                  case DELETED:
                    qual = Bytes.toBytes(mapKey.toString());
                    hasDeletes = true;
                    delete.deleteColumn(hcol.getFamily(), qual);
                    break;
                }
              }
            } else {
              Set<Map.Entry> set = ((Map)o).entrySet();
              for(Entry entry: set) {
                byte[] qual = toBytes(entry.getKey());
                byte[] val = toBytes(entry.getValue());
                put.add(hcol.getFamily(), qual, val);
                hasPuts = true;
            break;
          case ARRAY:
            if(o instanceof GenericArray) {
              GenericArray arr = (GenericArray) o;
              int j=0;
              for(Object item : arr) {
                byte[] val = toBytes(item);
                put.add(hcol.getFamily(), Bytes.toBytes(j), val);
                hasPuts = true;
              }
            break;
          default:
            put.add(hcol.getFamily(), hcol.getQualifier(), toBytes(o, field.schema()));
            hasPuts = true;
            break;
        }
      if (hasPuts) {
        table.put(put);
      }
      if (hasDeletes) {
        table.delete(delete);
      }
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
  public boolean delete(K key) {
    try{
      table.delete(new Delete(toBytes(key)));
      //HBase does not return success information and executing a get for
      //success is a bit costly
      return true;
    } catch(IOException ex2){
      LOG.error(ex2.getMessage());
      LOG.error(ex2.getStackTrace().toString());
      return false;
  public long deleteByQuery(Query<K, T> query) {
    try {
      String[] fields = getFieldsToQuery(query.getFields());
      //find whether all fields are queried, which means that complete
      //rows will be deleted
      boolean isAllFields = Arrays.equals(fields
          , getBeanFactory().getCachedPersistent().getFields());
  
      org.apache.gora.query.Result<K, T> result = null;
      result = query.execute();
      ArrayList<Delete> deletes = new ArrayList<Delete>();
      while(result.next()) {
        Delete delete = new Delete(toBytes(result.getKey()));
        deletes.add(delete);
        if(!isAllFields) {
          addFields(delete, query);
        }
      }
      table.delete(deletes);
      return deletes.size();
    } catch (Exception e) {
      // TODO Auto-generated catch block
      e.printStackTrace();
      return -1;
    }
  }

  @Override
  public void flush() {
    try{
      table.flushCommits();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
  public org.apache.gora.query.Result<K, T> execute(Query<K, T> query){
    try{
      //check if query.fields is null
      query.setFields(getFieldsToQuery(query.getFields()));
  
      if(query.getStartKey() != null && query.getStartKey().equals(
          query.getEndKey())) {
        Get get = new Get(toBytes(query.getStartKey()));
        addFields(get, query.getFields());
        addTimeRange(get, query);
        Result result = table.get(get);
        return new HBaseGetResult<K,T>(this, query, result);
      } else {
        ResultScanner scanner = createScanner(query);
  
        org.apache.gora.query.Result<K,T> result
        = new HBaseScannerResult<K,T>(this,query, scanner);
  
        return result;
      }
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      return null;
  public void close() {
    try{
      table.close();
    }catch(IOException ex){
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    }
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HSqlInsertUpdateStatement<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class InsertUpdateStatement<K, V extends PersistentBase> {
import org.apache.gora.persistency.impl.PersistentBase;
  public static <K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class MySqlInsertUpdateStatement<K, V extends PersistentBase> extends InsertUpdateStatement<K, V> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  public void close() {
  public void createSchema() {
  public void deleteSchema() {
  public boolean schemaExists() {
  public boolean delete(K key) {
  public long deleteByQuery(Query<K, T> query) {
  public void flush() {
  public T get(K key, String[] requestFields) {
  public Result<K, T> execute(Query<K, T> query) {
  public void put(K key, T persistent) {
  private void parse(String input) throws IOException, ParseException, Exception {
  private void storePageview(long key, Pageview pageview) throws IOException, Exception {
  private void get(long key) throws IOException, Exception {
  private void query(long key) throws IOException, Exception {
  private void query(long startKey, long endKey) throws IOException, Exception {
  private void deleteByQuery(long startKey, long endKey) throws IOException, Exception {
  private void printResult(Result<Long, Pageview> result) throws IOException, Exception {
  private void close() throws IOException, Exception {
    Text.writeString(out, obj.getClass().getName());
    conf.set(classKey, obj.getClass().getName());
}
    Text.writeString(out, obj.getClass().getName());
    conf.set(classKey, obj.getClass().getName());
}
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.Persistent;
public class GoraMapper<K1, V1 extends Persistent, K2, V2> extends Mapper<K1, V1, K2, V2> {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
    initMapperJob(job, dataStoreClass, inKeyClass, inValueClass, outKeyClass,
        outValueClass, mapperClass, null, reuseObjects);
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
import org.apache.gora.persistency.Persistent;

public interface PartitionQuery<K, T extends Persistent> extends Query<K, T> {
import org.apache.gora.persistency.Persistent;
public interface Query<K, T extends Persistent> {
  void setDataStore(DataStore<K, T> dataStore);
  DataStore<K, T> getDataStore();
  Result<K, T> execute();
import org.apache.gora.persistency.Persistent;
public interface Result<K, T extends Persistent> {
  DataStore<K, T> getDataStore();
  float getProgress() throws IOException, InterruptedException;
public class PartitionQueryImpl<K, T extends PersistentBase> extends QueryBase<K, T>
    implements PartitionQuery<K, T> {
public abstract class QueryBase<K, T extends PersistentBase>
    implements Query<K,T>, Writable, Configurable {
  public Result<K,T> execute() {
public abstract class QueryWSBase<K, T extends Persistent> implements Query<K,T> {
  public Result<K,T> execute() {
public interface DataStore<K, T extends Persistent> {
  void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties);
  Result<K, T> execute(Query<K, T> query);
  List<PartitionQuery<K, T>> getPartitions(Query<K, T> query) throws IOException;
  public float getProgress() throws IOException, InterruptedException {
public class GoraMapper<K1, V1 extends Persistent, K2, V2> extends Mapper<K1, V1, K2, V2> {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
    initMapperJob(job, dataStoreClass, inKeyClass, inValueClass, outKeyClass,
        outValueClass, mapperClass, null, reuseObjects);
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
  public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
      boolean reuseObjects) throws IOException {
import org.apache.gora.persistency.Persistent;

public interface PartitionQuery<K, T extends Persistent> extends Query<K, T> {
import org.apache.gora.persistency.Persistent;
public interface Query<K, T extends Persistent> {
  void setDataStore(DataStore<K, T> dataStore);
  DataStore<K, T> getDataStore();
  Result<K, T> execute();
import org.apache.gora.persistency.Persistent;
public interface Result<K, T extends Persistent> {
  DataStore<K, T> getDataStore();
  float getProgress() throws IOException, InterruptedException;
public class PartitionQueryImpl<K, T extends PersistentBase> extends QueryBase<K, T>
    implements PartitionQuery<K, T> {
public abstract class QueryBase<K, T extends PersistentBase>
    implements Query<K,T>, Writable, Configurable {
  public Result<K,T> execute() {
public abstract class QueryWSBase<K, T extends Persistent> implements Query<K,T> {
  public Result<K,T> execute() {
public interface DataStore<K, T extends Persistent> {
  void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties);
  Result<K, T> execute(Query<K, T> query);
  List<PartitionQuery<K, T>> getPartitions(Query<K, T> query) throws IOException;
  public float getProgress() throws IOException, InterruptedException {
public class HBaseScannerResult<K, T extends PersistentBase> extends HBaseResult<K, T> {
            = new HBaseScannerResult<K,T>(this,query, scanner);
  public ResultScanner createScanner(Query<K, T> query) throws IOException {
public class HBaseTableConnection implements HTableInterface {
  public HBaseTableConnection(Configuration conf, String tableName, boolean autoflush)
      throws IOException {
  public static class LogAnalyticsMapper extends GoraMapper<Long, Pageview, TextLong,
      LongWritable> {
    protected void map(Long key, Pageview pageview, Context context)
        throws IOException ,InterruptedException {
  public static class LogAnalyticsReducer extends GoraReducer<TextLong, LongWritable,
      String, MetricDatum> {
    protected void reduce(TextLong tuple, Iterable<LongWritable> values, Context context)
   * @param inStore
   * @param outStore
   * @param numReducer
  public Job createJob(DataStore<Long, Pageview> inStore,
      DataStore<String, MetricDatum> outStore, int numReducer) throws IOException {
    GoraMapper.initMapperJob(job, inStore, TextLong.class, LongWritable.class,
        LogAnalyticsMapper.class, true);
    Configuration conf = new Configuration();
          getDataStore(dataStoreClass, String.class, MetricDatum.class, conf);
	    inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, conf);
	    outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, conf);
public class HBaseScannerResult<K, T extends PersistentBase> extends HBaseResult<K, T> {
            = new HBaseScannerResult<K,T>(this,query, scanner);
  public ResultScanner createScanner(Query<K, T> query) throws IOException {
public class HBaseTableConnection implements HTableInterface {
  public HBaseTableConnection(Configuration conf, String tableName, boolean autoflush)
      throws IOException {
  public static class LogAnalyticsMapper extends GoraMapper<Long, Pageview, TextLong,
      LongWritable> {
    protected void map(Long key, Pageview pageview, Context context)
        throws IOException ,InterruptedException {
  public static class LogAnalyticsReducer extends GoraReducer<TextLong, LongWritable,
      String, MetricDatum> {
    protected void reduce(TextLong tuple, Iterable<LongWritable> values, Context context)
   * @param inStore
   * @param outStore
   * @param numReducer
  public Job createJob(DataStore<Long, Pageview> inStore,
      DataStore<String, MetricDatum> outStore, int numReducer) throws IOException {
    GoraMapper.initMapperJob(job, inStore, TextLong.class, LongWritable.class,
        LogAnalyticsMapper.class, true);
    Configuration conf = new Configuration();
          getDataStore(dataStoreClass, String.class, MetricDatum.class, conf);
	    inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, conf);
	    outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, conf);
    implements DataStore<K, T>, Configurable, Writable, Closeable {
    implements DataStore<K, T>, Configurable, Writable, Closeable {
    setKeyClass(keyClass);
    setPersistentClass(persistentClass);
    if (this.beanFactory == null) {
      this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
    }
    schema = this.beanFactory.getCachedPersistent().getSchema();
    fieldMap = AvroUtils.getFieldMap(schema);

    autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
    this.properties = properties;

    datumReader = new PersistentDatumReader<T>(schema, false);
    datumWriter = new PersistentDatumWriter<T>(schema, false);
        if (hcol == null) {
          throw new RuntimeException("HBase mapping for field [" persistent.getClass().getName() 
              "#" field.name()"] not found. Wrong gora-hbase-mapping.xml?");
        }
    } catch (Exception ex) {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
    setKeyClass(keyClass);
    setPersistentClass(persistentClass);
    if (this.beanFactory == null) {
      this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
    }
    schema = this.beanFactory.getCachedPersistent().getSchema();
    fieldMap = AvroUtils.getFieldMap(schema);

    autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
    this.properties = properties;

    datumReader = new PersistentDatumReader<T>(schema, false);
    datumWriter = new PersistentDatumWriter<T>(schema, false);
        if (hcol == null) {
          throw new RuntimeException("HBase mapping for field [" persistent.getClass().getName() 
              "#" field.name()"] not found. Wrong gora-hbase-mapping.xml?");
        }
    } catch (Exception ex) {
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      if (col == null) {
        throw new  RuntimeException("HBase mapping for field [" f "] not found. " 
            "Wrong gora-hbase-mapping.xml?");
      }
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
      LOG.error(ex.getMessage());
      LOG.error(ex.getStackTrace().toString());
import java.text.SimpleDateFormat;
import org.apache.gora.util.LicenseHeaders;
import org.apache.gora.util.TimingUtil;
/** Generate specific Java interfaces and classes for protocols and schemas. 
 *  GoraCompiler takes its inspiration from, and is largely based on Avro's {@link SpecificCompiler}.
 */
  private static LicenseHeaders licenseHeader = new LicenseHeaders(null);
      
    log.info("Compiling Protocol: "  src  " to: "  dest);
    if(licenseHeader != null) {
      log.info("The generated file will be "  licenseHeader.getLicenseName()  " licensed.");
    }
    for (Schema s : protocol.getTypes())          // enqueue types 
    log.info("Compiling Schema: "  src  " to: "  dest);
    if(licenseHeader != null) {
      log.info("The generated artifact will be "  licenseHeader.getLicenseName()  " licensed.");
    }
    if (licenseHeader != null) {
      line(0, licenseHeader.getLicense());
    }
  
  /**
   * The main method used to invoke the GoraCompiler. It accepts an input (JSON) avsc 
   * schema file, the target output directory and an optional parameter defining the
   * license header to be used when compiling the avsc into the generated class.
   * If no license is explicitely defined, an ASFv2.0 license header is attributed
   * to all generated files by default.
   */
      System.err.println("Usage: GoraCompiler <schema file> <output dir> [-license <id>]");
      System.err.println("  <schema file>     - individual avsc file to be compiled");
      System.err.println("  <output dir>      - output directory for generated Java files");
      System.err.println("  [-license <id>]   - the preferred license header to add to the\n" 
                                           "\t\t      generated Java file. Current options include; \n" 
                                              "\t\t  ASLv2   (Apache Software License v2.0) \n" 
                                              "\t\t  AGPLv3  (GNU Affero General Public License)\n" 
                                              "\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n" 
                                              "\t\t  FDLv13  (GNU Free Documentation License v1.3)\n" 
                                              "\t\t  GPLv1   (GNU General Public License v1.0)\n" 
                                              "\t\t  GPLv2   (GNU General Public License v2.0)\n" 
                                              "\t\t  GPLv3   (GNU General Public License v3.0)\n " 
                                              "\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n" 
                                              "\t\t  LGPLv3  (GNU Lesser General Public License v2.1)\n") ;
    for (int i = 1; i < args.length; i) {
      licenseHeader.setLicenseName("ASLv2");
      if ("-license".equals(args[i])) {
        licenseHeader.setLicenseName(args[i]);
      } 
    }
    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
    long start = System.currentTimeMillis();
    log.info("GoraCompiler: starting at "  sdf.format(start));
    long end = System.currentTimeMillis();
    log.info("GoraCompiler: finished at "  sdf.format(end)  ", elapsed: "  TimingUtil.elapsedTime(start, end));
    return;
import java.text.SimpleDateFormat;
import org.apache.gora.util.LicenseHeaders;
import org.apache.gora.util.TimingUtil;
/** Generate specific Java interfaces and classes for protocols and schemas. 
 *  GoraCompiler takes its inspiration from, and is largely based on Avro's {@link SpecificCompiler}.
 */
  private static LicenseHeaders licenseHeader = new LicenseHeaders(null);
      
    log.info("Compiling Protocol: "  src  " to: "  dest);
    if(licenseHeader != null) {
      log.info("The generated file will be "  licenseHeader.getLicenseName()  " licensed.");
    }
    for (Schema s : protocol.getTypes())          // enqueue types 
    log.info("Compiling Schema: "  src  " to: "  dest);
    if(licenseHeader != null) {
      log.info("The generated artifact will be "  licenseHeader.getLicenseName()  " licensed.");
    }
    if (licenseHeader != null) {
      line(0, licenseHeader.getLicense());
    }
  
  /**
   * The main method used to invoke the GoraCompiler. It accepts an input (JSON) avsc 
   * schema file, the target output directory and an optional parameter defining the
   * license header to be used when compiling the avsc into the generated class.
   * If no license is explicitely defined, an ASFv2.0 license header is attributed
   * to all generated files by default.
   */
      System.err.println("Usage: GoraCompiler <schema file> <output dir> [-license <id>]");
      System.err.println("  <schema file>     - individual avsc file to be compiled");
      System.err.println("  <output dir>      - output directory for generated Java files");
      System.err.println("  [-license <id>]   - the preferred license header to add to the\n" 
                                           "\t\t      generated Java file. Current options include; \n" 
                                              "\t\t  ASLv2   (Apache Software License v2.0) \n" 
                                              "\t\t  AGPLv3  (GNU Affero General Public License)\n" 
                                              "\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n" 
                                              "\t\t  FDLv13  (GNU Free Documentation License v1.3)\n" 
                                              "\t\t  GPLv1   (GNU General Public License v1.0)\n" 
                                              "\t\t  GPLv2   (GNU General Public License v2.0)\n" 
                                              "\t\t  GPLv3   (GNU General Public License v3.0)\n " 
                                              "\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n" 
                                              "\t\t  LGPLv3  (GNU Lesser General Public License v2.1)\n") ;
    for (int i = 1; i < args.length; i) {
      licenseHeader.setLicenseName("ASLv2");
      if ("-license".equals(args[i])) {
        licenseHeader.setLicenseName(args[i]);
      } 
    }
    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
    long start = System.currentTimeMillis();
    log.info("GoraCompiler: starting at "  sdf.format(start));
    long end = System.currentTimeMillis();
    log.info("GoraCompiler: finished at "  sdf.format(end)  ", elapsed: "  TimingUtil.elapsedTime(start, end));
    return;
    public String getField(int index) { return null; }
    public person clone() { return null; }
    public void clearNew() { }
    public boolean isReadable(int fieldIndex) { return false; }

   * @param pTableNameTable name
   * @param pKeySchemaKey schema used
   * @param pItemsList of items belonging to a specific table
   * @param pItemsThe items belonging to the table
   * @param pIdenThe number of spaces used for identation
   * @param pItemNameItem's name
   * @param pItemTypeItem's type
   * @param pIdenNumber of spaces used for indentation
   * @param pKeySchemaThe key schema for a specific table
   * @param pIdenNumber of spaces used for indentation
   * @param pKeySchemaKey schema
   * @param pIdenNumber of spaces used for indentation
   * @param sString to be camelcasified
   * @param nameClass name
   * @param pIdenNumber of spaces used for indentation
    line(pIden, "public String getField(int index) {return null; }");
    line(pIden, "public person clone() {return null; }");
    line(pIden, "public void clearNew() {}");
    line(pIden, "public boolean isReadable(int fieldIndex) {return false; }");
   * @param indentNumber of spaces used for indentation
   * @param textText to be written
   * @param nameString to be converted
   * @param argsReceives the schema file to be compiled and where this should be written
   * @param pMapFileThe schema file to be mapped into a table
      
    
    
    












  super(null);
  super(dataStore);




   * @param pHashAttrValueHash attribute value where to start scanning

   * @param pNewConditionCondition for querying
   * @param pHashAttrValueHash attribute value where to start

   * @returnAttributeValue build from query






  


  
  
  
  
      KeySchemaElement rangeKeyElement = new KeySchemaElement().withAttributeName(rangeKeyName).withAttributeType(rangeKeyType);
      kSchema.setRangeKeyElement(rangeKeyElement);
      tablesToKeySchemas.put(tableName, kSchema);
  
          kSchema = new KeySchema();
        KeySchemaElement hashKey = new KeySchemaElement().withAttributeName(keyName).withAttributeType(keyType);
        kSchema.setHashKeyElement(hashKey);
        tablesToKeySchemas.put(tableName, kSchema);
  
  
     
     
      itemAttribs = (HashMap<String, String>) items.get(itemNumber);
      if (itemAttribs == null)
        items.add(new HashMap<String, String>());
        return (HashMap<String, String>) items.get(itemNumber);
  
  
          // if there are not schemas defined
          if (tablesToKeySchemas.isEmpty()) return "";
          if (!verifyKeySchema(tableName)) return "";
  
  
  
      KeySchemaElement rangeKey = kSchema.getRangeKeyElement();
      KeySchemaElement hashKey = kSchema.getHashKeyElement();
      // A range key must have a hash key as well
      
      if (rangeKey != null){
        if (hashKey != null)	
          return true;
        else 	  
          return false;
      }
      // A hash key may exist by itself
      if (hashKey != null)  
        return true;
      return false;
  

        // verifying if key schemas have been properly defined
        String wrongTableName = verifyAllKeySchemas();  
        if (!wrongTableName.equals("")) throw new IllegalStateException("no key schemas defined for table "  wrongTableName);
        // Return the tableDescription and all the attributes needed
    
    
    
    if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
    if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
      if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
  if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
    if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
    try{


    public String getField(int index) { return null; }
    public person clone() { return null; }
    public void clearNew() { }
    public boolean isReadable(int fieldIndex) { return false; }

   * @param pTableNameTable name
   * @param pKeySchemaKey schema used
   * @param pItemsList of items belonging to a specific table
   * @param pItemsThe items belonging to the table
   * @param pIdenThe number of spaces used for identation
   * @param pItemNameItem's name
   * @param pItemTypeItem's type
   * @param pIdenNumber of spaces used for indentation
   * @param pKeySchemaThe key schema for a specific table
   * @param pIdenNumber of spaces used for indentation
   * @param pKeySchemaKey schema
   * @param pIdenNumber of spaces used for indentation
   * @param sString to be camelcasified
   * @param nameClass name
   * @param pIdenNumber of spaces used for indentation
    line(pIden, "public String getField(int index) {return null; }");
    line(pIden, "public person clone() {return null; }");
    line(pIden, "public void clearNew() {}");
    line(pIden, "public boolean isReadable(int fieldIndex) {return false; }");
   * @param indentNumber of spaces used for indentation
   * @param textText to be written
   * @param nameString to be converted
   * @param argsReceives the schema file to be compiled and where this should be written
   * @param pMapFileThe schema file to be mapped into a table
      
    
    
    












  super(null);
  super(dataStore);




   * @param pHashAttrValueHash attribute value where to start scanning

   * @param pNewConditionCondition for querying
   * @param pHashAttrValueHash attribute value where to start

   * @returnAttributeValue build from query






  


  
  
  
  
      KeySchemaElement rangeKeyElement = new KeySchemaElement().withAttributeName(rangeKeyName).withAttributeType(rangeKeyType);
      kSchema.setRangeKeyElement(rangeKeyElement);
      tablesToKeySchemas.put(tableName, kSchema);
  
          kSchema = new KeySchema();
        KeySchemaElement hashKey = new KeySchemaElement().withAttributeName(keyName).withAttributeType(keyType);
        kSchema.setHashKeyElement(hashKey);
        tablesToKeySchemas.put(tableName, kSchema);
  
  
     
     
      itemAttribs = (HashMap<String, String>) items.get(itemNumber);
      if (itemAttribs == null)
        items.add(new HashMap<String, String>());
        return (HashMap<String, String>) items.get(itemNumber);
  
  
          // if there are not schemas defined
          if (tablesToKeySchemas.isEmpty()) return "";
          if (!verifyKeySchema(tableName)) return "";
  
  
  
      KeySchemaElement rangeKey = kSchema.getRangeKeyElement();
      KeySchemaElement hashKey = kSchema.getHashKeyElement();
      // A range key must have a hash key as well
      
      if (rangeKey != null){
        if (hashKey != null)	
          return true;
        else 	  
          return false;
      }
      // A hash key may exist by itself
      if (hashKey != null)  
        return true;
      return false;
  

        // verifying if key schemas have been properly defined
        String wrongTableName = verifyAllKeySchemas();  
        if (!wrongTableName.equals("")) throw new IllegalStateException("no key schemas defined for table "  wrongTableName);
        // Return the tableDescription and all the attributes needed
    
    
    
    if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
    if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
      if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
  if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
    if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
    try{


            StatefulHashMap map = (StatefulHashMap) fieldValue;
            StatefulHashMap newMap = new StatefulHashMap();
            for (Object mapKey : map.keySet()) {
              newMap.put(mapKey, map.get(mapKey));
              newMap.putState(mapKey, map.getState(mapKey));
            }
            fieldValue = newMap;
            StatefulHashMap map = (StatefulHashMap) fieldValue;
            StatefulHashMap newMap = new StatefulHashMap();
            for (Object mapKey : map.keySet()) {
              newMap.put(mapKey, map.get(mapKey));
              newMap.putState(mapKey, map.getState(mapKey));
            }
            fieldValue = newMap;
import me.prettyprint.hector.api.ddl.ComparatorType;

      // GORA-197
      for (ColumnFamilyDefinition cfDef : columnFamilyDefinitions) {
        cfDef.setComparatorType(ComparatorType.BYTESTYPE);
      }

    else {
      List<ColumnFamilyDefinition> cfDefs = keyspaceDefinition.getCfDefs();
      if (cfDefs == null || cfDefs.size() == 0) {
        LOG.warn(keyspaceDefinition.getName()  " does not have any column family.");
      }
      else {
        for (ColumnFamilyDefinition cfDef : cfDefs) {
          ComparatorType comparatorType = cfDef.getComparatorType();
          if (! comparatorType.equals(ComparatorType.BYTESTYPE)) {
            // GORA-197
            LOG.warn("The comparator type of "  cfDef.getName()  " column family is "  comparatorType.getTypeName()
                    ", not BytesType. It may cause a fatal error on column validation later.");
          }
          else {
            // LOG.info("The comparator type of "  cfDef.getName()  " column family is "  comparatorType.getTypeName()  ".");
          }
        }
      }
    }
import me.prettyprint.hector.api.ddl.ComparatorType;

      // GORA-197
      for (ColumnFamilyDefinition cfDef : columnFamilyDefinitions) {
        cfDef.setComparatorType(ComparatorType.BYTESTYPE);
      }

    else {
      List<ColumnFamilyDefinition> cfDefs = keyspaceDefinition.getCfDefs();
      if (cfDefs == null || cfDefs.size() == 0) {
        LOG.warn(keyspaceDefinition.getName()  " does not have any column family.");
      }
      else {
        for (ColumnFamilyDefinition cfDef : cfDefs) {
          ComparatorType comparatorType = cfDef.getComparatorType();
          if (! comparatorType.equals(ComparatorType.BYTESTYPE)) {
            // GORA-197
            LOG.warn("The comparator type of "  cfDef.getName()  " column family is "  comparatorType.getTypeName()
                    ", not BytesType. It may cause a fatal error on column validation later.");
          }
          else {
            // LOG.info("The comparator type of "  cfDef.getName()  " column family is "  comparatorType.getTypeName()  ".");
          }
        }
      }
    }
    // LOG.info("persistentClass="  persistentClass.getName()  " -> cassandraMapping="  cassandraMapping);

    	LOG.error("Keyspace element should not be null!");
        return;
    	LOG.error("Error locating Cassandra Keyspace name attribute!");
    	// LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "' -> "  keyspaceName);
    	LOG.error("Error locating Cassandra Keyspace cluster attribute!");
    	// LOG.info("Located Cassandra Keyspace cluster: '"  CLUSTER_ATTRIBUTE  "' -> "  clusterName);
    	LOG.error("Error locating Cassandra Keyspace host attribute!");
    	// LOG.info("Located Cassandra Keyspace host: '"  HOST_ATTRIBUTE  "' -> "  hostName);
      	LOG.error("Error locating column family name attribute!");
      	continue;
      	// LOG.info("Located column family name: '"  NAME_ATTRIBUTE  "' -> "  familyName);
    if (mappingElement == null) {
      LOG.error("Mapping element does not exist for className="  className);
      return null;
    }
    // LOG.info("className="  className  " -> keyspaceName="  keyspaceName);
    if (keyspaceElement == null) {
      LOG.error("Keyspace element does not exist for keyspaceName="  keyspaceName);
      return null;
    }
      LOG.error("Error locating Cassandra Keyspace element!");
      // LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	    LOG.error("Error locating Cassandra Keyspace name attribute!");
    	    continue;
        else {
    	    // LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "' -> "  keyspaceName);
        }
      LOG.error("Error locating Cassandra Mapping class element!");
      // LOG.info("Located Cassandra Mapping: '"  MAPPING_ELEMENT  "'");
    	    LOG.error("Error locating Cassandra Mapping class name attribute!");
        else {
    	    // LOG.info("Located Cassandra Mapping class name: '"  NAME_ATTRIBUTE  "' -> "  className);
        }
    // LOG.info("persistentClass="  persistentClass.getName()  " -> cassandraMapping="  cassandraMapping);

    	LOG.error("Keyspace element should not be null!");
        return;
    	LOG.error("Error locating Cassandra Keyspace name attribute!");
    	// LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "' -> "  keyspaceName);
    	LOG.error("Error locating Cassandra Keyspace cluster attribute!");
    	// LOG.info("Located Cassandra Keyspace cluster: '"  CLUSTER_ATTRIBUTE  "' -> "  clusterName);
    	LOG.error("Error locating Cassandra Keyspace host attribute!");
    	// LOG.info("Located Cassandra Keyspace host: '"  HOST_ATTRIBUTE  "' -> "  hostName);
      	LOG.error("Error locating column family name attribute!");
      	continue;
      	// LOG.info("Located column family name: '"  NAME_ATTRIBUTE  "' -> "  familyName);
    if (mappingElement == null) {
      LOG.error("Mapping element does not exist for className="  className);
      return null;
    }
    // LOG.info("className="  className  " -> keyspaceName="  keyspaceName);
    if (keyspaceElement == null) {
      LOG.error("Keyspace element does not exist for keyspaceName="  keyspaceName);
      return null;
    }
      LOG.error("Error locating Cassandra Keyspace element!");
      // LOG.info("Located Cassandra Keyspace: '"  KEYSPACE_ELEMENT  "'");
    	    LOG.error("Error locating Cassandra Keyspace name attribute!");
    	    continue;
        else {
    	    // LOG.info("Located Cassandra Keyspace name: '"  NAME_ATTRIBUTE  "' -> "  keyspaceName);
        }
      LOG.error("Error locating Cassandra Mapping class element!");
      // LOG.info("Located Cassandra Mapping: '"  MAPPING_ELEMENT  "'");
    	    LOG.error("Error locating Cassandra Mapping class name attribute!");
        else {
    	    // LOG.info("Located Cassandra Mapping class name: '"  NAME_ATTRIBUTE  "' -> "  className);
        }
  @SuppressWarnings("unchecked")
      LOG.debug("Located Cassandra Keyspace");
      LOG.debug("Located Cassandra Keyspace name: '"  keyspaceName  "'");
      LOG.debug("Located Cassandra Keyspace cluster: '"  clusterName  "'");
      LOG.debug("Located Cassandra Keyspace host: '"  hostName  "'");
        LOG.debug("Located column family: '"  familyName  "'" );
      LOG.debug("Located super column family");
        LOG.debug("Added super column family: '"  familyName  "'");
  private static final String KEYCLASS_ATTRIBUTE = "keyClass";
  private static final String CLUSTER_ATTRIBUTE = "cluster";
  public CassandraMapping get(Class<?> persistentClass) {
      LOG.debug("className="  className  " -> keyspaceName="  keyspaceName);
    // get mapping file
    // find cassandra keyspace element
        // log name, cluster and host for given keyspace(s)
        String clusterName = keyspace.getAttributeValue(CLUSTER_ATTRIBUTE);
        String hostName = keyspace.getAttributeValue(HOST_ATTRIBUTE);
        LOG.debug("Located Cassandra Keyspace: '"  keyspaceName  "' in cluster '"  clusterName  
          "' on host '"  hostName  "'.");
          LOG.error("Error locating Cassandra Keyspace name attribute!");
          continue;
        // associate persistent and class names for keyspace(s)
        String keyClassName = mapping.getAttributeValue(KEYCLASS_ATTRIBUTE);
        String keyspaceName = mapping.getAttributeValue(KEYSPACE_ELEMENT);
        LOG.debug("Located Cassandra Mapping: keyClass: '"  keyClassName  "' in storage class '" 
           className  "' for Keyspace '"  keyspaceName  "'.");
          LOG.error("Error locating Cassandra Mapping class name attribute!");
          continue;
  @SuppressWarnings("unchecked")
      LOG.debug("Located Cassandra Keyspace");
      LOG.debug("Located Cassandra Keyspace name: '"  keyspaceName  "'");
      LOG.debug("Located Cassandra Keyspace cluster: '"  clusterName  "'");
      LOG.debug("Located Cassandra Keyspace host: '"  hostName  "'");
        LOG.debug("Located column family: '"  familyName  "'" );
      LOG.debug("Located super column family");
        LOG.debug("Added super column family: '"  familyName  "'");
  private static final String KEYCLASS_ATTRIBUTE = "keyClass";
  private static final String CLUSTER_ATTRIBUTE = "cluster";
  public CassandraMapping get(Class<?> persistentClass) {
      LOG.debug("className="  className  " -> keyspaceName="  keyspaceName);
    // get mapping file
    // find cassandra keyspace element
        // log name, cluster and host for given keyspace(s)
        String clusterName = keyspace.getAttributeValue(CLUSTER_ATTRIBUTE);
        String hostName = keyspace.getAttributeValue(HOST_ATTRIBUTE);
        LOG.debug("Located Cassandra Keyspace: '"  keyspaceName  "' in cluster '"  clusterName  
          "' on host '"  hostName  "'.");
          LOG.error("Error locating Cassandra Keyspace name attribute!");
          continue;
        // associate persistent and class names for keyspace(s)
        String keyClassName = mapping.getAttributeValue(KEYCLASS_ATTRIBUTE);
        String keyspaceName = mapping.getAttributeValue(KEYSPACE_ELEMENT);
        LOG.debug("Located Cassandra Mapping: keyClass: '"  keyClassName  "' in storage class '" 
           className  "' for Keyspace '"  keyspaceName  "'.");
          LOG.error("Error locating Cassandra Mapping class name attribute!");
          continue;
public class PersistentSerialization implements Serialization<PersistentBase> {
public class PersistentSerialization implements Serialization<PersistentBase> {
import java.util.Collections;
  private Map<K, T> buffer = Collections.synchronizedMap(new LinkedHashMap<K, T>());
import java.util.Collections;
  private Map<K, T> buffer = Collections.synchronizedMap(new LinkedHashMap<K, T>());
    synchronized(mutator) {
      HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);
    }
    synchronized(mutator) {
      HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
    }
    synchronized(mutator) {
      HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
    }
/**
 * This class it not thread safe.
 * According to Hector's JavaDoc a Mutator isn't thread safe, too.
 * Take a look at {@CassandraClient} for safe usage.
 */
    synchronized(mutator) {
      HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);
    }
    synchronized(mutator) {
      HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
    }
    synchronized(mutator) {
      HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
    }
/**
 * This class it not thread safe.
 * According to Hector's JavaDoc a Mutator isn't thread safe, too.
 * Take a look at {@CassandraClient} for safe usage.
 */
    log.info("Creating Hadoop Job: "  job.getJobName());
    log.info("Creating Hadoop Job: "  job.getJobName());
   * It should be noted that should the "qualifier" attribute and its associated
   * value be absent from class field definition, it will automatically be set to 
   * the field name value.
   * 
      LOG.error("Keyspace element should not be null!");
      return;
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace");
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace name: '"  keyspaceName  "'");
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace cluster: '"  clusterName  "'");
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace host: '"  hostName  "'");
      }  
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located column family: '"  familyName  "'" );
        }
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located super column family");
        }
        if (LOG.isDebugEnabled()) {
          LOG.debug("Added super column family: '"  familyName  "'");
        }
      if (fieldName == null) {
       LOG.error("Field name is not declared.");
        continue;
      }
      if (familyName == null) {
        LOG.error("Family name is not declared for \""  fieldName  "\" field.");
        continue;
      }
      if (columnName == null) {
        LOG.warn("Column name (qualifier) is not declared for \""  fieldName  "\" field.");
        columnName = fieldName;
      }

    if (LOG.isDebugEnabled()) {
    }
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located Cassandra Keyspace: '"  keyspaceName  "' in cluster '"  clusterName  
        }
        if (LOG.isDebugEnabled()) {
        }
   * It should be noted that should the "qualifier" attribute and its associated
   * value be absent from class field definition, it will automatically be set to 
   * the field name value.
   * 
      LOG.error("Keyspace element should not be null!");
      return;
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace");
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace name: '"  keyspaceName  "'");
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace cluster: '"  clusterName  "'");
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Located Cassandra Keyspace host: '"  hostName  "'");
      }  
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located column family: '"  familyName  "'" );
        }
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located super column family");
        }
        if (LOG.isDebugEnabled()) {
          LOG.debug("Added super column family: '"  familyName  "'");
        }
      if (fieldName == null) {
       LOG.error("Field name is not declared.");
        continue;
      }
      if (familyName == null) {
        LOG.error("Family name is not declared for \""  fieldName  "\" field.");
        continue;
      }
      if (columnName == null) {
        LOG.warn("Column name (qualifier) is not declared for \""  fieldName  "\" field.");
        columnName = fieldName;
      }

    if (LOG.isDebugEnabled()) {
    }
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located Cassandra Keyspace: '"  keyspaceName  "' in cluster '"  clusterName  
        }
        if (LOG.isDebugEnabled()) {
        }
      extends GoraMapper<K, T, NullWritable, NullWritable> {
    assert(job.isComplete() == true);
    assert(job.isComplete() == true);
      extends GoraMapper<K, T, NullWritable, NullWritable> {
    assert(job.isComplete() == true);
    assert(job.isComplete() == true);
      /*
      htu.startMiniMapReduceCluster(numServers);
      */
      // add a shutdown hook for shutting down the minicluster.
            //htu.shutdownMiniMapReduceCluster();
      /*
      htu.startMiniMapReduceCluster(numServers);
      */
      // add a shutdown hook for shutting down the minicluster.
            //htu.shutdownMiniMapReduceCluster();
import java.util.ArrayList;
  private final static String SCHEMA_EXTENTION = ".avsc";
  
  /** Generates Java classes for a number of schema files. */
  public static void compileSchema(File[] srcFiles, File dest) throws IOException {
  if(licenseHeader != null) {
  log.info("The generated artifact will be "  licenseHeader.getLicenseName()  " licensed.");
   }
       for (File src : srcFiles) {
        log.info("Compiling Schema: "  src  " to: "  dest);
        GoraCompiler compiler = new GoraCompiler(dest);
        compiler.enqueue(Schema.parse(src));          // enqueue types
        compiler.compile();                           // generate classes for types
  	  }
  	}
      System.err.println("  <schema file>     - individual avsc file to be compiled or a directory path containing avsc files");
    File inputFile = new File(args[0]);
    File output = new File(args[1]);
    if(!inputFile.exists() || !output.exists()){
    	System.err.println("input file path or output file path doesn't exists.");
    	System.exit(1);
    }
    if(inputFile.isDirectory()) {
    	ArrayList<File> inputSchemas = new ArrayList<File>();
    	File[] listOfFiles= inputFile.listFiles();
    	for (File file : listOfFiles) {
    	    if (file.isFile() && file.exists() && file.getName().endsWith(SCHEMA_EXTENTION)) {
    	    	inputSchemas.add(file);
    	    }
    	}
    compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
    }
    else if (inputFile.isFile()) {
    	compileSchema(inputFile, output);	
    }
import java.util.ArrayList;
  private final static String SCHEMA_EXTENTION = ".avsc";
  
  /** Generates Java classes for a number of schema files. */
  public static void compileSchema(File[] srcFiles, File dest) throws IOException {
  if(licenseHeader != null) {
  log.info("The generated artifact will be "  licenseHeader.getLicenseName()  " licensed.");
   }
       for (File src : srcFiles) {
        log.info("Compiling Schema: "  src  " to: "  dest);
        GoraCompiler compiler = new GoraCompiler(dest);
        compiler.enqueue(Schema.parse(src));          // enqueue types
        compiler.compile();                           // generate classes for types
  	  }
  	}
      System.err.println("  <schema file>     - individual avsc file to be compiled or a directory path containing avsc files");
    File inputFile = new File(args[0]);
    File output = new File(args[1]);
    if(!inputFile.exists() || !output.exists()){
    	System.err.println("input file path or output file path doesn't exists.");
    	System.exit(1);
    }
    if(inputFile.isDirectory()) {
    	ArrayList<File> inputSchemas = new ArrayList<File>();
    	File[] listOfFiles= inputFile.listFiles();
    	for (File file : listOfFiles) {
    	    if (file.isFile() && file.exists() && file.getName().endsWith(SCHEMA_EXTENTION)) {
    	    	inputSchemas.add(file);
    	    }
    	}
    compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
    }
    else if (inputFile.isFile()) {
    	compileSchema(inputFile, output);	
    }
import org.apache.gora.util.GoraException;

      if (mapping.tableName == null) {
        throw new GoraException("Please define the gora to accumulo mapping in "  filename  " for "  persistentClass.getCanonicalName());
      }

        if (col == null) {
          throw new GoraException("Please define the gora to accumulo mapping for field "  field.name());
        }

import org.apache.gora.util.GoraException;

      if (mapping.tableName == null) {
        throw new GoraException("Please define the gora to accumulo mapping in "  filename  " for "  persistentClass.getCanonicalName());
      }

        if (col == null) {
          throw new GoraException("Please define the gora to accumulo mapping for field "  field.name());
        }

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
        case UNION:
          case UNION:
  private int unionType;

  public void setUnionType(int pUnionType){
    this.unionType = pUnionType;
  }

  public int getUnionType(){
    return unionType;
  }
    Serializer<?> serializer = GoraSerializerTypeInferer.getSerializer(schema);
import org.apache.avro.Schema.Type;
import org.apache.gora.cassandra.store.CassandraStore;
  
  /**
   * Gets the column containing the type of the union type element stored.
   * TODO: This might seem too much of a overhead if we consider that N rows have M columns,
   *       this might have to be reviewed to get the specific column in O(1)
   * @param pFieldName
   * @param pCassandraRow
   * @return
   */
  private CassandraColumn getUnionTypeColumn(String pFieldName, Object[] pCassandraRow){
    
    for (int iCnt = 0; iCnt < pCassandraRow.length; iCnt){
      CassandraColumn cColumn = (CassandraColumn)pCassandraRow[iCnt];
      String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName());
      if (pFieldName.equals(columnName))
        return cColumn;
    }
    return null;
  }
      if (fieldName != null ){
        // get field
        int pos = this.persistent.getFieldIndex(fieldName);
        Field field = fields.get(pos);
        Type fieldType = field.schema().getType();
        System.out.println(StringSerializer.get().fromByteBuffer(cassandraColumn.getName())  fieldName  " "  fieldType.name());
        if (fieldType == Type.UNION){
          // TODO getting UNION stored type
          // TODO get value of UNION stored type. This field does not need to be written back to the store
          cassandraColumn.setUnionType(getNonNullTypePos(field.schema().getTypes()));
        }

        // get value
        cassandraColumn.setField(field);
        Object value = cassandraColumn.getValue();

        this.persistent.put(pos, value);
        // this field does not need to be written back to the store
        this.persistent.clearDirty(pos);
      }
      else
        LOG.debug("FieldName was null while iterating CassandraRow and using Avro Union type");
  private int getNonNullTypePos(List<Schema> pTypes){
    int iCnt = 0;
    for (Schema sch :  pTypes)
      if (!sch.getName().equals("null"))
        return iCnt;
      else 
        iCnt;
    return CassandraStore.DEFAULT_UNION_SCHEMA;
  }

import me.prettyprint.cassandra.serializers.StringSerializer;


  
  /**
   * Gets a specific CassandraColumn within a row using its name
   * @param pCassandraColumnName
   * @return CassandraColumn
   */
  public CassandraColumn getCassandraColumn(String pCassandraColumnName){
    for (CassandraColumn cColumn: this)
      if ( pCassandraColumnName.equals(StringSerializer.get().fromByteBuffer(cColumn.getName())) )
        return cColumn;
    
    return null;
  }
import org.apache.gora.cassandra.store.CassandraStore;
    } else if (type == Type.UNION){
      // the selected union schema is obtained
      Schema unionFieldSchema = getUnionSchema(super.getUnionType(), field.schema());
      // we use the selected union schema to deserialize our actual value
      value = fromByteBuffer(unionFieldSchema, byteBuffer);
  
  /**
   * Gets the specific schema for a union data type
   * @param pSchemaPos
   * @param pSchema
   * @return
   */
  private Schema getUnionSchema (int pSchemaPos, Schema pSchema){
    Schema unionSchema = pSchema.getTypes().get(pSchemaPos);
    // default union element
    if ( unionSchema == null )
      pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
    return unionSchema;
  }
    } else if (type == Type.UNION){
      serializer = ByteBufferSerializer.get();
  /**
   * Adds an subColumn inside the cassandraMapping file when a String is serialized
   * @param key
   * @param fieldName
   * @param columnName
   * @param value
   */
  /**
   * Adds an subColumn inside the cassandraMapping file when an Integer is serialized
   * @param key
   * @param fieldName
   * @param columnName
   * @param value
   */
  
  private String getMappingFamily(String pField){
    String family = null;
    // TODO checking if it was a UNION field the one we are retrieving
      family = this.cassandraMapping.getFamily(pField);
    return family;
  }
  
  private String getMappingColumn(String pField){
    String column = null;
    // TODO checking if it was a UNION field the one we are retrieving e.g. column = pField;
      column = this.cassandraMapping.getColumn(pField);
    return column;
  }
      String family = this.getMappingFamily(field);
      String column = this.getMappingColumn(field);

  /**
   * Retrieves the cassandraMapping which holds whatever was mapped from the gora-cassandra-mapping.xml
   * @return
   */
  public CassandraMapping getCassandraMapping(){
    return this.cassandraMapping;
  }
      String family = this.getMappingFamily(field);
      String column = this.getMappingColumn(field);
  
  /**
   * Add new column to CassandraMapping using the self-explanatory parameters
   * @param pFamilyName
   * @param pFieldName
   * @param pColumnName
   */
  public void addColumn(String pFamilyName, String pFieldName, String pColumnName){
    this.familyMap.put(pFieldName, pFamilyName);
    this.columnMap.put(pFieldName, pColumnName);
  }

import java.io.InputStream;
  /**
  * Objects to maintain mapped keyspaces
  */
    InputStream inputStream = getClass().getClassLoader().getResourceAsStream(MAPPING_FILE);
    if (inputStream == null){
      LOG.warn("Mapping file '"  MAPPING_FILE  "' could not be found!");
      throw new IOException("Mapping file '"  MAPPING_FILE  "' could not be found!");
    }
    Document document = saxBuilder.build(inputStream);
   * Default schema index used when AVRO Union data types are stored
   */
  public static int DEFAULT_UNION_SCHEMA = 0;

  /**
    CassandraResultSet<K> cassandraResultSet = new CassandraResultSet<K>();
          case UNION:
            // storing the union selected schema, the actual value will be stored as soon as getting out of here
            // TODO determine which schema we are using: int schemaPos = getUnionSchema(fieldValue,fieldSchema);
            // and save it p.put( p.getFieldIndex(field.name()  CassandraStore.UNION_COL_SUFIX), schemaPos);
            break;
      switch (type) {
        case STRING:
        case BOOLEAN:
        case INT:
        case LONG:
        case BYTES:
        case FLOAT:
        case DOUBLE:
        case FIXED:
          this.cassandraClient.addColumn(key, field.name(), value);
          break;
        case RECORD:
          if (value != null) {
            if (value instanceof PersistentBase) {
              PersistentBase persistentBase = (PersistentBase) value;
              for (Field member: schema.getFields()) {
                
                // TODO: hack, do not store empty arrays
                Object memberValue = persistentBase.get(member.pos());
                if (memberValue instanceof GenericArray<?>) {
                  if (((GenericArray)memberValue).size() == 0) {
                    continue;
                  }
                this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
       case UNION:
         if(value != null) {
           LOG.info("Union being supported with value: "  value.toString());
           // TODO add union schema index used
           // adding union value
           this.cassandraClient.addColumn(key, field.name(), value);
         } else {
           LOG.info("Union not supported: "  value.toString());
         }
  /**
   * Gets the position within the schema of the type used
   * @param pValue
   * @param pUnionSchema
   * @return
   */
  private int getUnionSchema(Object pValue, Schema pUnionSchema){
    int unionSchemaPos = 0;
    String valueType = pValue.getClass().getSimpleName();
    Iterator<Schema> it = pUnionSchema.getTypes().iterator();
    while ( it.hasNext() ){
      String schemaName = it.next().getName();
      if (valueType.equals("Utf8") && schemaName.equals(Type.STRING.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("HeapByteBuffer") && schemaName.equals(Type.STRING.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Integer") && schemaName.equals(Type.INT.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Long") && schemaName.equals(Type.LONG.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Double") && schemaName.equals(Type.DOUBLE.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Float") && schemaName.equals(Type.FLOAT.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Boolean") && schemaName.equals(Type.BOOLEAN.name().toLowerCase()))
        return unionSchemaPos;
      unionSchemaPos ;
    }
    // if we weren't able to determine which data type it is, then we return the default
    return 0;
  }

    null,
	      if (CONTENTS[i]!=null){
	        page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
	        for(String token : CONTENTS[i].split(" ")) {
	    	  page.addToParsedContent(new Utf8(token));  
	        }
          }
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
  public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"dateOfBirth\",\"type\":\"long\"},{\"name\":\"ssn\",\"type\":\"string\"},{\"name\":\"salary\",\"type\":\"int\"},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"]},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}]}]}");
    BOSS(4,"boss"),
    WEBPAGE(5,"webpage"),
  public static final String[] _ALL_FIELDS = {"name","dateOfBirth","ssn","salary","boss","webpage",};
  private Object boss;
  private WebPage webpage;
    case 4: return boss;
    case 5: return webpage;
    case 4:boss = (Object)_value; break;
    case 5:webpage = (WebPage)_value; break;
  public Object getBoss() {
    return (Object) get(4);
  }
  public void setBoss(Employee value) {
    put(4, value);
  }
  public void setBoss(Utf8 value) {
    put(4, value);
  }
  public WebPage getWebpage() {
    return (WebPage) get(5);
  }
  public void setWebpage(WebPage value) {
    put(5, value);
  }
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
  public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}");
      if (page.getContent() != null) {
        // Get the content from a WebPage as obtained from the DataStore
        String content = new String(page.getContent().array());

        StringTokenizer itr = new StringTokenizer(content);
        while (itr.hasMoreTokens()) {
          word.set(itr.nextToken());
          context.write(word, one);
        }
            break;
          case UNION:
            fieldType = type(fieldSchema);
            //Create get method: public <unbox(field.schema())> get<camelKey>()
            line(1, "public "unbox(field.schema())" get" camelKey"() {");
            line(2, "return ("unbox(field.schema())") get("i");");
            line(1, "}");
            
            //Create set methods: public void set<camelKey>(<subschema.fieldType> value)
            for (Schema s : fieldSchema.getTypes()) {
              if (s.getType().equals(Schema.Type.NULL)) continue ;
              String unionFieldType = type(s);
              line(1, "public void set"camelKey"("unionFieldType" value) {");
              line(2, "put("i", value);");
              line(1, "}");
            }
            break;
          case NULL:
            throw new RuntimeException("Unexpected NULL field: "field);
          default:
            throw new RuntimeException("Unknown field: "field);
import org.apache.avro.generic.GenericData;
  /**
   * Computes a (record's) field's hash code.
   * @param i Index of the field in the actual
   * @param field
   * @return
   */
    // XXX Union special case: in a field being union we have to check the
    // inner schemas for Type.BYTES special case, but because it is not a
    // field we check it this way. Too simple case to create another
    // private method
    boolean isUnionField = false ;
    int unionIndex = -1 ;
    
    if (field.schema().getType() == Type.UNION) {
      isUnionField = true ;
      unionIndex = GenericData.get().resolveUnion(field.schema(), o);
    }
    
    if(field.schema().getType() == Type.BYTES
       || (isUnionField
           && field.schema().getTypes().get(unionIndex).getType() == Type.BYTES)) {
      // ByteBuffer.hashCode() depends on internal 'position' index, but we must ignore that.
      return newInstance(result, fields);      
  /**
   * {@inheritDoc}
   * Serializes the Persistent data and saves in HBase.
   * Topmost fields of the record are persisted in "raw" format (not avro serialized). This behavior happens
   * in maps and arrays too.
   * 
   * ["null","type"] type (a.k.a. optional field) is persisted like as if it is ["type"], but the column get
   * deleted if value==null (so value read after will be null).
   * 
   * @param persistent Record to be persisted in HBase
   */
                    // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
                    if (val == null) { // value == null => must delete the column
                      delete.deleteColumn(hcol.getFamily(), qual);
                      hasDeletes = true;
                    } else {
                      put.add(hcol.getFamily(), qual, val);
                      hasPuts = true;
                    }
                byte[] val = toBytes(entry.getValue(), field.schema().getValueType());
                // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
                if (val == null) { // value == null => must delete the column
                  delete.deleteColumn(hcol.getFamily(), qual);
                  hasDeletes = true;
                } else {
                  put.add(hcol.getFamily(), qual, val);
                  hasPuts = true;
                }
                byte[] val = toBytes(item, field.schema().getElementType());
                // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
                if (val == null) { // value == null => must delete the column
                  delete.deleteColumn(hcol.getFamily(), Bytes.toBytes(j));
                  hasDeletes = true;
                } else {
                  put.add(hcol.getFamily(), Bytes.toBytes(j), val);
                  hasPuts = true;
                }
            // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
            byte[] serializedBytes = toBytes(o, field.schema()) ;
            if (serializedBytes == null) { // value == null => must delete the column
              delete.deleteColumn(hcol.getFamily(), hcol.getQualifier());
              hasDeletes = true;
            } else {
              put.add(hcol.getFamily(), hcol.getQualifier(), serializedBytes);
              hasPuts = true;
            }
  /**
   * Creates a new Persistent instance with the values in 'result' for the fields listed.
   * @param result result form a HTable#get()
   * @param fields List of fields queried, or null for all
   * @return A new instance with default values for not listed fields
   *         null if 'result' is null.
   * @throws IOException
   */
          byte[] val = result.getValue(col.getFamily(), col.getQualifier());
import org.apache.avro.generic.GenericData;
import org.apache.gora.avro.PersistentDatumReader;
import org.apache.gora.avro.PersistentDatumWriter;
  /**
   * Deserializes an array of bytes matching the given schema to the proper basic (enum, Utf8,...) or
   * complex type (Persistent/Record).
   * 
   * Does not handle <code>arrays/maps</code> if not inside a <code>record</code> type.
   * 
   * @param schema Avro schema describing the expected data
   * @param val array of bytes with the data serialized
   * @return Enum|Utf8|ByteBuffer|Integer|Long|Float|Double|Boolean|Persistent|Null
   * @throws IOException
   */
    case UNION:
      // XXX Special case: When reading the top-level field of a record we must handle the
      // special case ["null","type"] definitions: this will be written as if it was ["type"]
      // if not in a special case, will execute "case RECORD".
      
      // if 'val' is empty we ignore the special case (will match Null in "case RECORD")  
      if (schema.getTypes().size() == 2) {
        
        // schema [type0, type1]
        Type type0 = schema.getTypes().get(0).getType() ;
        Type type1 = schema.getTypes().get(1).getType() ;
        
        // Check if types are different and there's a "null", like ["null","type"] or ["type","null"]
        if (!type0.equals(type1)
            && (   type0.equals(Schema.Type.NULL)
                || type1.equals(Schema.Type.NULL))) {

          if (type0.equals(Schema.Type.NULL))
            schema = schema.getTypes().get(1) ;
          else 
            schema = schema.getTypes().get(0) ;
          
          return fromBytes(schema, val) ; // Deserialize as if schema was ["type"] 
        }
        
      }
      // else
      //   type = [type0,type1] where type0=type1
      //   or val == null
      // => deserialize like "case RECORD"

      PersistentDatumReader<?> reader = null ;
            
      // For UNION schemas, must use a specific PersistentDatumReader
      // from the readerMap since unions don't have own name
      // (key name in map will be "UNION-type-type-...")
      if (schema.getType().equals(Schema.Type.UNION)) {
        reader = (PersistentDatumReader<?>)readerMap.get(String.valueOf(schema.hashCode()));
        if (reader == null) {
          reader = new PersistentDatumReader(schema, false);// ignore dirty bits
          readerMap.put(String.valueOf(schema.hashCode()), reader);
        }
      } else {
        // ELSE use reader for Record
        reader = (PersistentDatumReader<?>)readerMap.get(schema.getFullName());
        if (reader == null) {
          reader = new PersistentDatumReader(schema, false);// ignore dirty bits
          readerMap.put(schema.getFullName(), reader);
        }
      return reader.read((Object)null, schema, decoder);
  /**
   * Converts an array of bytes to the target <em>basic class</em>.
   * @param clazz (Byte|Boolean|Short|Integer|Long|Float|Double|String|Utf8).class
   * @param val array of bytes with the value
   * @return an instance of <code>clazz</code> with the bytes in <code>val</code>
   *         deserialized with org.apache.hadoop.hbase.util.Bytes
   */
  /**
   * Converts an instance of a <em>basic class</em> to an array of bytes.
   * @param o Instance of Enum|Byte|Boolean|Short|Integer|Long|Float|Double|String|Utf8
   * @return array of bytes with <code>o</code> serialized with org.apache.hadoop.hbase.util.Bytes
   */
  /**
   * Serializes an object following the given schema.
   * Does not handle <code>array/map</code> if it is not inside a <code>record</code>
   * @param o Utf8|ByteBuffer|Integer|Long|Float|Double|Boolean|Enum|Persistent
   * @param schema The schema describing the object (or a compatible description)
   * @return array of bytes of the serialized object
   * @throws IOException
   */
    case UNION:
      // XXX Special case: When writing the top-level field of a record we must handle the
      // special case ["null","type"] definitions: this will be written as if it was ["type"]
      // if not in a special case, will execute "case RECORD".
      
      if (schema.getTypes().size() == 2) {
        
        // schema [type0, type1]
        Type type0 = schema.getTypes().get(0).getType() ;
        Type type1 = schema.getTypes().get(1).getType() ;
        
        // Check if types are different and there's a "null", like ["null","type"] or ["type","null"]
        if (!type0.equals(type1)
            && (   type0.equals(Schema.Type.NULL)
                || type1.equals(Schema.Type.NULL))) {

          if (o == null) return null ;
          
          int index = GenericData.get().resolveUnion(schema, o);
          schema = schema.getTypes().get(index) ;
          
          return toBytes(o, schema) ; // Serialize as if schema was ["type"] 
        }
        
      }
      // else
      //   type = [type0,type1] where type0=type1
      // => Serialize like "case RECORD" with Avro
      
      PersistentDatumWriter writer = null ;
      // For UNION schemas, must use a specific PersistentDatumReader
      // from the readerMap since unions don't have own name
      // (key name in map will be "UNION-type-type-...")
      if (schema.getType().equals(Schema.Type.UNION)) {
        writer = (PersistentDatumWriter<?>) writerMap.get(String.valueOf(schema.hashCode()));
        if (writer == null) {
          writer = new PersistentDatumWriter(schema,false);// ignore dirty bits
          writerMap.put(String.valueOf(schema.hashCode()),writer);
        }
      } else {
        // ELSE use writer for Record
        writer = (PersistentDatumWriter<?>) writerMap.get(schema.getFullName());
        if (writer == null) {
          writer = new PersistentDatumWriter(schema,false);// ignore dirty bits
          writerMap.put(schema.getFullName(),writer);
        }
      writer.write(schema,o, encoder);
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.

import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.

import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
        case UNION:
          case UNION:
  private int unionType;

  public void setUnionType(int pUnionType){
    this.unionType = pUnionType;
  }

  public int getUnionType(){
    return unionType;
  }
    Serializer<?> serializer = GoraSerializerTypeInferer.getSerializer(schema);
import org.apache.avro.Schema.Type;
import org.apache.gora.cassandra.store.CassandraStore;
  
  /**
   * Gets the column containing the type of the union type element stored.
   * TODO: This might seem too much of a overhead if we consider that N rows have M columns,
   *       this might have to be reviewed to get the specific column in O(1)
   * @param pFieldName
   * @param pCassandraRow
   * @return
   */
  private CassandraColumn getUnionTypeColumn(String pFieldName, Object[] pCassandraRow){
    
    for (int iCnt = 0; iCnt < pCassandraRow.length; iCnt){
      CassandraColumn cColumn = (CassandraColumn)pCassandraRow[iCnt];
      String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName());
      if (pFieldName.equals(columnName))
        return cColumn;
    }
    return null;
  }
      if (fieldName != null ){
        // get field
        int pos = this.persistent.getFieldIndex(fieldName);
        Field field = fields.get(pos);
        Type fieldType = field.schema().getType();
        System.out.println(StringSerializer.get().fromByteBuffer(cassandraColumn.getName())  fieldName  " "  fieldType.name());
        if (fieldType == Type.UNION){
          // TODO getting UNION stored type
          // TODO get value of UNION stored type. This field does not need to be written back to the store
          cassandraColumn.setUnionType(getNonNullTypePos(field.schema().getTypes()));
        }

        // get value
        cassandraColumn.setField(field);
        Object value = cassandraColumn.getValue();

        this.persistent.put(pos, value);
        // this field does not need to be written back to the store
        this.persistent.clearDirty(pos);
      }
      else
        LOG.debug("FieldName was null while iterating CassandraRow and using Avro Union type");
  private int getNonNullTypePos(List<Schema> pTypes){
    int iCnt = 0;
    for (Schema sch :  pTypes)
      if (!sch.getName().equals("null"))
        return iCnt;
      else 
        iCnt;
    return CassandraStore.DEFAULT_UNION_SCHEMA;
  }

import me.prettyprint.cassandra.serializers.StringSerializer;


  
  /**
   * Gets a specific CassandraColumn within a row using its name
   * @param pCassandraColumnName
   * @return CassandraColumn
   */
  public CassandraColumn getCassandraColumn(String pCassandraColumnName){
    for (CassandraColumn cColumn: this)
      if ( pCassandraColumnName.equals(StringSerializer.get().fromByteBuffer(cColumn.getName())) )
        return cColumn;
    
    return null;
  }
import org.apache.gora.cassandra.store.CassandraStore;
    } else if (type == Type.UNION){
      // the selected union schema is obtained
      Schema unionFieldSchema = getUnionSchema(super.getUnionType(), field.schema());
      // we use the selected union schema to deserialize our actual value
      value = fromByteBuffer(unionFieldSchema, byteBuffer);
  
  /**
   * Gets the specific schema for a union data type
   * @param pSchemaPos
   * @param pSchema
   * @return
   */
  private Schema getUnionSchema (int pSchemaPos, Schema pSchema){
    Schema unionSchema = pSchema.getTypes().get(pSchemaPos);
    // default union element
    if ( unionSchema == null )
      pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
    return unionSchema;
  }
    } else if (type == Type.UNION){
      serializer = ByteBufferSerializer.get();
  /**
   * Adds an subColumn inside the cassandraMapping file when a String is serialized
   * @param key
   * @param fieldName
   * @param columnName
   * @param value
   */
  /**
   * Adds an subColumn inside the cassandraMapping file when an Integer is serialized
   * @param key
   * @param fieldName
   * @param columnName
   * @param value
   */
  
  private String getMappingFamily(String pField){
    String family = null;
    // TODO checking if it was a UNION field the one we are retrieving
      family = this.cassandraMapping.getFamily(pField);
    return family;
  }
  
  private String getMappingColumn(String pField){
    String column = null;
    // TODO checking if it was a UNION field the one we are retrieving e.g. column = pField;
      column = this.cassandraMapping.getColumn(pField);
    return column;
  }
      String family = this.getMappingFamily(field);
      String column = this.getMappingColumn(field);

  /**
   * Retrieves the cassandraMapping which holds whatever was mapped from the gora-cassandra-mapping.xml
   * @return
   */
  public CassandraMapping getCassandraMapping(){
    return this.cassandraMapping;
  }
      String family = this.getMappingFamily(field);
      String column = this.getMappingColumn(field);
  
  /**
   * Add new column to CassandraMapping using the self-explanatory parameters
   * @param pFamilyName
   * @param pFieldName
   * @param pColumnName
   */
  public void addColumn(String pFamilyName, String pFieldName, String pColumnName){
    this.familyMap.put(pFieldName, pFamilyName);
    this.columnMap.put(pFieldName, pColumnName);
  }

import java.io.InputStream;
  /**
  * Objects to maintain mapped keyspaces
  */
    InputStream inputStream = getClass().getClassLoader().getResourceAsStream(MAPPING_FILE);
    if (inputStream == null){
      LOG.warn("Mapping file '"  MAPPING_FILE  "' could not be found!");
      throw new IOException("Mapping file '"  MAPPING_FILE  "' could not be found!");
    }
    Document document = saxBuilder.build(inputStream);
   * Default schema index used when AVRO Union data types are stored
   */
  public static int DEFAULT_UNION_SCHEMA = 0;

  /**
    CassandraResultSet<K> cassandraResultSet = new CassandraResultSet<K>();
          case UNION:
            // storing the union selected schema, the actual value will be stored as soon as getting out of here
            // TODO determine which schema we are using: int schemaPos = getUnionSchema(fieldValue,fieldSchema);
            // and save it p.put( p.getFieldIndex(field.name()  CassandraStore.UNION_COL_SUFIX), schemaPos);
            break;
      switch (type) {
        case STRING:
        case BOOLEAN:
        case INT:
        case LONG:
        case BYTES:
        case FLOAT:
        case DOUBLE:
        case FIXED:
          this.cassandraClient.addColumn(key, field.name(), value);
          break;
        case RECORD:
          if (value != null) {
            if (value instanceof PersistentBase) {
              PersistentBase persistentBase = (PersistentBase) value;
              for (Field member: schema.getFields()) {
                
                // TODO: hack, do not store empty arrays
                Object memberValue = persistentBase.get(member.pos());
                if (memberValue instanceof GenericArray<?>) {
                  if (((GenericArray)memberValue).size() == 0) {
                    continue;
                  }
                this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
       case UNION:
         if(value != null) {
           LOG.info("Union being supported with value: "  value.toString());
           // TODO add union schema index used
           // adding union value
           this.cassandraClient.addColumn(key, field.name(), value);
         } else {
           LOG.info("Union not supported: "  value.toString());
         }
  /**
   * Gets the position within the schema of the type used
   * @param pValue
   * @param pUnionSchema
   * @return
   */
  private int getUnionSchema(Object pValue, Schema pUnionSchema){
    int unionSchemaPos = 0;
    String valueType = pValue.getClass().getSimpleName();
    Iterator<Schema> it = pUnionSchema.getTypes().iterator();
    while ( it.hasNext() ){
      String schemaName = it.next().getName();
      if (valueType.equals("Utf8") && schemaName.equals(Type.STRING.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("HeapByteBuffer") && schemaName.equals(Type.STRING.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Integer") && schemaName.equals(Type.INT.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Long") && schemaName.equals(Type.LONG.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Double") && schemaName.equals(Type.DOUBLE.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Float") && schemaName.equals(Type.FLOAT.name().toLowerCase()))
        return unionSchemaPos;
      else if (valueType.equals("Boolean") && schemaName.equals(Type.BOOLEAN.name().toLowerCase()))
        return unionSchemaPos;
      unionSchemaPos ;
    }
    // if we weren't able to determine which data type it is, then we return the default
    return 0;
  }

    null,
	      if (CONTENTS[i]!=null){
	        page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
	        for(String token : CONTENTS[i].split(" ")) {
	    	  page.addToParsedContent(new Utf8(token));  
	        }
          }
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
  public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"dateOfBirth\",\"type\":\"long\"},{\"name\":\"ssn\",\"type\":\"string\"},{\"name\":\"salary\",\"type\":\"int\"},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"]},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}]}]}");
    BOSS(4,"boss"),
    WEBPAGE(5,"webpage"),
  public static final String[] _ALL_FIELDS = {"name","dateOfBirth","ssn","salary","boss","webpage",};
  private Object boss;
  private WebPage webpage;
    case 4: return boss;
    case 5: return webpage;
    case 4:boss = (Object)_value; break;
    case 5:webpage = (WebPage)_value; break;
  public Object getBoss() {
    return (Object) get(4);
  }
  public void setBoss(Employee value) {
    put(4, value);
  }
  public void setBoss(Utf8 value) {
    put(4, value);
  }
  public WebPage getWebpage() {
    return (WebPage) get(5);
  }
  public void setWebpage(WebPage value) {
    put(5, value);
  }
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
/**
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
 *
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.
 */

import org.apache.avro.specific.FixedSize;
  public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}");
      if (page.getContent() != null) {
        // Get the content from a WebPage as obtained from the DataStore
        String content = new String(page.getContent().array());

        StringTokenizer itr = new StringTokenizer(content);
        while (itr.hasMoreTokens()) {
          word.set(itr.nextToken());
          context.write(word, one);
        }
            break;
          case UNION:
            fieldType = type(fieldSchema);
            //Create get method: public <unbox(field.schema())> get<camelKey>()
            line(1, "public "unbox(field.schema())" get" camelKey"() {");
            line(2, "return ("unbox(field.schema())") get("i");");
            line(1, "}");
            
            //Create set methods: public void set<camelKey>(<subschema.fieldType> value)
            for (Schema s : fieldSchema.getTypes()) {
              if (s.getType().equals(Schema.Type.NULL)) continue ;
              String unionFieldType = type(s);
              line(1, "public void set"camelKey"("unionFieldType" value) {");
              line(2, "put("i", value);");
              line(1, "}");
            }
            break;
          case NULL:
            throw new RuntimeException("Unexpected NULL field: "field);
          default:
            throw new RuntimeException("Unknown field: "field);
import org.apache.avro.generic.GenericData;
  /**
   * Computes a (record's) field's hash code.
   * @param i Index of the field in the actual
   * @param field
   * @return
   */
    // XXX Union special case: in a field being union we have to check the
    // inner schemas for Type.BYTES special case, but because it is not a
    // field we check it this way. Too simple case to create another
    // private method
    boolean isUnionField = false ;
    int unionIndex = -1 ;
    
    if (field.schema().getType() == Type.UNION) {
      isUnionField = true ;
      unionIndex = GenericData.get().resolveUnion(field.schema(), o);
    }
    
    if(field.schema().getType() == Type.BYTES
       || (isUnionField
           && field.schema().getTypes().get(unionIndex).getType() == Type.BYTES)) {
      // ByteBuffer.hashCode() depends on internal 'position' index, but we must ignore that.
      return newInstance(result, fields);      
  /**
   * {@inheritDoc}
   * Serializes the Persistent data and saves in HBase.
   * Topmost fields of the record are persisted in "raw" format (not avro serialized). This behavior happens
   * in maps and arrays too.
   * 
   * ["null","type"] type (a.k.a. optional field) is persisted like as if it is ["type"], but the column get
   * deleted if value==null (so value read after will be null).
   * 
   * @param persistent Record to be persisted in HBase
   */
                    // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
                    if (val == null) { // value == null => must delete the column
                      delete.deleteColumn(hcol.getFamily(), qual);
                      hasDeletes = true;
                    } else {
                      put.add(hcol.getFamily(), qual, val);
                      hasPuts = true;
                    }
                byte[] val = toBytes(entry.getValue(), field.schema().getValueType());
                // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
                if (val == null) { // value == null => must delete the column
                  delete.deleteColumn(hcol.getFamily(), qual);
                  hasDeletes = true;
                } else {
                  put.add(hcol.getFamily(), qual, val);
                  hasPuts = true;
                }
                byte[] val = toBytes(item, field.schema().getElementType());
                // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
                if (val == null) { // value == null => must delete the column
                  delete.deleteColumn(hcol.getFamily(), Bytes.toBytes(j));
                  hasDeletes = true;
                } else {
                  put.add(hcol.getFamily(), Bytes.toBytes(j), val);
                  hasPuts = true;
                }
            // XXX - Gora 207: Top-most record level ["null","type"] must be saved raw. "null"=>delete
            byte[] serializedBytes = toBytes(o, field.schema()) ;
            if (serializedBytes == null) { // value == null => must delete the column
              delete.deleteColumn(hcol.getFamily(), hcol.getQualifier());
              hasDeletes = true;
            } else {
              put.add(hcol.getFamily(), hcol.getQualifier(), serializedBytes);
              hasPuts = true;
            }
  /**
   * Creates a new Persistent instance with the values in 'result' for the fields listed.
   * @param result result form a HTable#get()
   * @param fields List of fields queried, or null for all
   * @return A new instance with default values for not listed fields
   *         null if 'result' is null.
   * @throws IOException
   */
          byte[] val = result.getValue(col.getFamily(), col.getQualifier());
import org.apache.avro.generic.GenericData;
import org.apache.gora.avro.PersistentDatumReader;
import org.apache.gora.avro.PersistentDatumWriter;
  /**
   * Deserializes an array of bytes matching the given schema to the proper basic (enum, Utf8,...) or
   * complex type (Persistent/Record).
   * 
   * Does not handle <code>arrays/maps</code> if not inside a <code>record</code> type.
   * 
   * @param schema Avro schema describing the expected data
   * @param val array of bytes with the data serialized
   * @return Enum|Utf8|ByteBuffer|Integer|Long|Float|Double|Boolean|Persistent|Null
   * @throws IOException
   */
    case UNION:
      // XXX Special case: When reading the top-level field of a record we must handle the
      // special case ["null","type"] definitions: this will be written as if it was ["type"]
      // if not in a special case, will execute "case RECORD".
      
      // if 'val' is empty we ignore the special case (will match Null in "case RECORD")  
      if (schema.getTypes().size() == 2) {
        
        // schema [type0, type1]
        Type type0 = schema.getTypes().get(0).getType() ;
        Type type1 = schema.getTypes().get(1).getType() ;
        
        // Check if types are different and there's a "null", like ["null","type"] or ["type","null"]
        if (!type0.equals(type1)
            && (   type0.equals(Schema.Type.NULL)
                || type1.equals(Schema.Type.NULL))) {

          if (type0.equals(Schema.Type.NULL))
            schema = schema.getTypes().get(1) ;
          else 
            schema = schema.getTypes().get(0) ;
          
          return fromBytes(schema, val) ; // Deserialize as if schema was ["type"] 
        }
        
      }
      // else
      //   type = [type0,type1] where type0=type1
      //   or val == null
      // => deserialize like "case RECORD"

      PersistentDatumReader<?> reader = null ;
            
      // For UNION schemas, must use a specific PersistentDatumReader
      // from the readerMap since unions don't have own name
      // (key name in map will be "UNION-type-type-...")
      if (schema.getType().equals(Schema.Type.UNION)) {
        reader = (PersistentDatumReader<?>)readerMap.get(String.valueOf(schema.hashCode()));
        if (reader == null) {
          reader = new PersistentDatumReader(schema, false);// ignore dirty bits
          readerMap.put(String.valueOf(schema.hashCode()), reader);
        }
      } else {
        // ELSE use reader for Record
        reader = (PersistentDatumReader<?>)readerMap.get(schema.getFullName());
        if (reader == null) {
          reader = new PersistentDatumReader(schema, false);// ignore dirty bits
          readerMap.put(schema.getFullName(), reader);
        }
      return reader.read((Object)null, schema, decoder);
  /**
   * Converts an array of bytes to the target <em>basic class</em>.
   * @param clazz (Byte|Boolean|Short|Integer|Long|Float|Double|String|Utf8).class
   * @param val array of bytes with the value
   * @return an instance of <code>clazz</code> with the bytes in <code>val</code>
   *         deserialized with org.apache.hadoop.hbase.util.Bytes
   */
  /**
   * Converts an instance of a <em>basic class</em> to an array of bytes.
   * @param o Instance of Enum|Byte|Boolean|Short|Integer|Long|Float|Double|String|Utf8
   * @return array of bytes with <code>o</code> serialized with org.apache.hadoop.hbase.util.Bytes
   */
  /**
   * Serializes an object following the given schema.
   * Does not handle <code>array/map</code> if it is not inside a <code>record</code>
   * @param o Utf8|ByteBuffer|Integer|Long|Float|Double|Boolean|Enum|Persistent
   * @param schema The schema describing the object (or a compatible description)
   * @return array of bytes of the serialized object
   * @throws IOException
   */
    case UNION:
      // XXX Special case: When writing the top-level field of a record we must handle the
      // special case ["null","type"] definitions: this will be written as if it was ["type"]
      // if not in a special case, will execute "case RECORD".
      
      if (schema.getTypes().size() == 2) {
        
        // schema [type0, type1]
        Type type0 = schema.getTypes().get(0).getType() ;
        Type type1 = schema.getTypes().get(1).getType() ;
        
        // Check if types are different and there's a "null", like ["null","type"] or ["type","null"]
        if (!type0.equals(type1)
            && (   type0.equals(Schema.Type.NULL)
                || type1.equals(Schema.Type.NULL))) {

          if (o == null) return null ;
          
          int index = GenericData.get().resolveUnion(schema, o);
          schema = schema.getTypes().get(index) ;
          
          return toBytes(o, schema) ; // Serialize as if schema was ["type"] 
        }
        
      }
      // else
      //   type = [type0,type1] where type0=type1
      // => Serialize like "case RECORD" with Avro
      
      PersistentDatumWriter writer = null ;
      // For UNION schemas, must use a specific PersistentDatumReader
      // from the readerMap since unions don't have own name
      // (key name in map will be "UNION-type-type-...")
      if (schema.getType().equals(Schema.Type.UNION)) {
        writer = (PersistentDatumWriter<?>) writerMap.get(String.valueOf(schema.hashCode()));
        if (writer == null) {
          writer = new PersistentDatumWriter(schema,false);// ignore dirty bits
          writerMap.put(String.valueOf(schema.hashCode()),writer);
        }
      } else {
        // ELSE use writer for Record
        writer = (PersistentDatumWriter<?>) writerMap.get(schema.getFullName());
        if (writer == null) {
          writer = new PersistentDatumWriter(schema,false);// ignore dirty bits
          writerMap.put(schema.getFullName(),writer);
        }
      writer.write(schema,o, encoder);
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.

import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
 *Licensed to the Apache Software Foundation (ASF) under one
 *or more contributor license agreements.  See the NOTICE file
 *distributed with this work for additional information
 *regarding copyright ownership.  The ASF licenses this file
 *to you under the Apache License, Version 2.0 (the"
 *License"); you may not use this file except in compliance
 *with the License.  You may obtain a copy of the License at
  * http://www.apache.org/licenses/LICENSE-2.0
 * 
 *Unless required by applicable law or agreed to in writing, software
 *distributed under the License is distributed on an "AS IS" BASIS,
 *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *See the License for the specific language governing permissions and
 *limitations under the License.

import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
      LOG.error(e.getMessage(), e);
      throw new IOException("Unable to read "  filename, ex);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
          LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      throw new IOException("Unable to read "  filename, ex);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
          LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
                                              "\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;
                                              "\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;
    if (query.getFields() != null) {
    }
    if (schemaExists()) {
        return;
    }

        mapping.getCollectionName(), new BasicDBObject()); //  send a DBObject to force creation
      // otherwise creation is deferred
      MongoDBQuery<K, T> query = new MongoDBQuery<K, T>(this);
      query.setFields(getFieldsToQuery(null));
      return query;
      case UNION:
          if (fieldSchema.getTypes().size() == 2) {

          // schema [type0, type1]
          Type type0 = fieldSchema.getTypes().get(0).getType() ;
          Type type1 = fieldSchema.getTypes().get(1).getType() ;

          // Check if types are different and there's a "null", like ["null","type"] or ["type","null"]
          if (!type0.equals(type1)
                  && (   type0.equals(Schema.Type.NULL)
                  || type1.equals(Schema.Type.NULL))) {
              persistent.put(field.pos(), easybson.getBytes(docf)); // Deserialize as if schema was ["type"]
          }

          }
        break;
                      .getValueType().getType()));
              break;
import org.apache.gora.mongodb.store.MongoMapping;
  public static DBObject toProjection(Query<?, ?> query, MongoMapping mapping) {
      for (String k : query.getFields()) {
        proj.put(mapping.getDocumentField(k), true);
      }
    DBObject p = MongoDBQuery.toProjection(query, mapping);
        long writeCapacUnits = Long.parseLong(tableElement.getAttributeValue("writecunit"));
        long writeCapacUnits = Long.parseLong(tableElement.getAttributeValue("writecunit"));
/**
 * {@link org.apache.gora.cassandra.store.CassandraStore} is the primary class 
 * responsible for directing Gora CRUD operations into Cassandra. We (delegate) rely 
 * heavily on {@ link org.apache.gora.cassandra.store.CassandraClient} for many operations
 * such as initialization, creating and deleting schemas (Cassandra Keyspaces), etc.  
 */
  
  /** Logging implementation */
  /** The default constructor for CassandraStore */
 
  /** 
   * Initialize is called when then the call to 
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore(Class<D> dataStoreClass, Class<K> keyClass, Class<T> persistent, org.apache.hadoop.conf.Configuration conf)}
   * is made. In this case, we merely delegate the store initialization to the 
   * {@link org.apache.gora.cassandra.store.CassandraClient#initialize(Class<K> keyClass, Class<T> persistentClass)}. 
   */
  /**
   * When executing Gora Queries in Cassandra we query the Cassandra keyspace by families.
   * When add sub/supercolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
  
  /**
   * When we add subcolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
  /**
   * When we add supercolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
/**
 * {@link org.apache.gora.cassandra.store.CassandraStore} is the primary class 
 * responsible for directing Gora CRUD operations into Cassandra. We (delegate) rely 
 * heavily on {@ link org.apache.gora.cassandra.store.CassandraClient} for many operations
 * such as initialization, creating and deleting schemas (Cassandra Keyspaces), etc.  
 */
  
  /** Logging implementation */
  /** The default constructor for CassandraStore */
 
  /** 
   * Initialize is called when then the call to 
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore(Class<D> dataStoreClass, Class<K> keyClass, Class<T> persistent, org.apache.hadoop.conf.Configuration conf)}
   * is made. In this case, we merely delegate the store initialization to the 
   * {@link org.apache.gora.cassandra.store.CassandraClient#initialize(Class<K> keyClass, Class<T> persistentClass)}. 
   */
  /**
   * When executing Gora Queries in Cassandra we query the Cassandra keyspace by families.
   * When add sub/supercolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
  
  /**
   * When we add subcolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
  /**
   * When we add supercolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
}
}
public class SolrStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  private static final Logger LOG = LoggerFactory.getLogger( SolrStore.class );
  protected static final String DEFAULT_MAPPING_FILE = "gora-solr-mapping.xml";
  protected static final String SOLR_URL_PROPERTY = "solr.url";
  protected static final String SOLR_CONFIG_PROPERTY = "solr.config";
  protected static final String SOLR_SCHEMA_PROPERTY = "solr.schema";
  protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batchSize";
  //protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";
  protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commitWithin";
  protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.resultsSize";
    
  protected static final int DEFAULT_BATCH_SIZE = 100;
  protected static final int DEFAULT_COMMIT_WITHIN = 1000;
  protected static final int DEFAULT_RESULTS_SIZE = 100;
  private SolrMapping mapping;
  private String solrServerUrl, solrConfig, solrSchema;
  private SolrServer server, adminServer;
  private ArrayList<SolrInputDocument> batch;
  private int batchSize = DEFAULT_BATCH_SIZE;
  private int commitWithin = DEFAULT_COMMIT_WITHIN;
  private int resultsSize = DEFAULT_RESULTS_SIZE;

  @Override
  public void initialize( Class<K> keyClass, Class<T> persistentClass, Properties properties ) {
    super.initialize( keyClass, persistentClass, properties );
    String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
    try {
      mapping = readMapping( mappingFile );
    }
    catch ( IOException e ) {
      LOG.error( e.getMessage() );
      LOG.error( e.getStackTrace().toString() );
    solrServerUrl = DataStoreFactory.findProperty( properties, this, SOLR_URL_PROPERTY, null );
    solrConfig = DataStoreFactory.findProperty( properties, this, SOLR_CONFIG_PROPERTY, null );
    solrSchema = DataStoreFactory.findProperty( properties, this, SOLR_SCHEMA_PROPERTY, null );
    LOG.info( "Using Solr server at "  solrServerUrl );
    adminServer = new HttpSolrServer( solrServerUrl );
    server = new HttpSolrServer( solrServerUrl  "/"  mapping.getCoreName() );
    if ( autoCreateSchema ) {
      createSchema();
    }
    String batchSizeString = DataStoreFactory.findProperty( properties, this, SOLR_BATCH_SIZE_PROPERTY, null );
    if ( batchSizeString != null ) {
      try {
        batchSize = Integer.parseInt( batchSizeString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid batch size '"  batchSizeString  "', using default "  DEFAULT_BATCH_SIZE );
      }
    }
    batch = new ArrayList<SolrInputDocument>( batchSize );
    String commitWithinString = DataStoreFactory.findProperty( properties, this, SOLR_COMMIT_WITHIN_PROPERTY, null );
    if ( commitWithinString != null ) {
      try {
        commitWithin = Integer.parseInt( commitWithinString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid commit within '"  commitWithinString  "', using default "  DEFAULT_COMMIT_WITHIN );
      }
    }
    String resultsSizeString = DataStoreFactory.findProperty( properties, this, SOLR_RESULTS_SIZE_PROPERTY, null );
    if ( resultsSizeString != null ) {
      try {
        resultsSize = Integer.parseInt( resultsSizeString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid results size '"  resultsSizeString  "', using default "  DEFAULT_RESULTS_SIZE );
      }
    }
  }
  @SuppressWarnings("unchecked")
  private SolrMapping readMapping( String filename ) throws IOException {
    SolrMapping map = new SolrMapping();
    try {
      SAXBuilder builder = new SAXBuilder();
      Document doc = builder.build( getClass().getClassLoader().getResourceAsStream( filename ) );
      List<Element> classes = doc.getRootElement().getChildren( "class" );
      for ( Element classElement : classes ) {
        if ( classElement.getAttributeValue( "keyClass" ).equals( keyClass.getCanonicalName() )
            && classElement.getAttributeValue( "name" ).equals( persistentClass.getCanonicalName() ) ) {
          String tableName = getSchemaName( classElement.getAttributeValue( "table" ), persistentClass );
          map.setCoreName( tableName );
          Element primaryKeyEl = classElement.getChild( "primarykey" );
          map.setPrimaryKey( primaryKeyEl.getAttributeValue( "column" ) );
          List<Element> fields = classElement.getChildren( "field" );

          for ( Element field : fields ) {
            String fieldName = field.getAttributeValue( "name" );
            String columnName = field.getAttributeValue( "column" );
            map.addField( fieldName, columnName );
          }
          break;
      }
    } catch ( Exception ex ) {
      throw new IOException( ex );
    return map;
  }
  public SolrMapping getMapping() {
    return mapping;
  }
  @Override
  public String getSchemaName() {
    return mapping.getCoreName();
  }
  @Override
  public void createSchema() {
    try {
      if ( !schemaExists() )
          CoreAdminRequest.createCore( mapping.getCoreName(), mapping.getCoreName(), adminServer, solrConfig,
          solrSchema );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
  }
  @Override
  /** Default implementation deletes and recreates the schema*/
  public void truncateSchema() {
    try {
      server.deleteByQuery( "*:*" );
      server.commit();
    } catch ( Exception e ) {
      // ignore?
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
  }
  @Override
  public void deleteSchema() {
    // XXX should this be only in truncateSchema ???
    try {
      server.deleteByQuery( "*:*" );
      server.commit();
    } catch ( Exception e ) {
    // ignore?
    // LOG.error(e.getMessage());
    // LOG.error(e.getStackTrace().toString());
    try {
      CoreAdminRequest.unloadCore( mapping.getCoreName(), adminServer );
    } catch ( Exception e ) {
      if ( e.getMessage().contains( "No such core" ) ) {
        return; // it's ok, the core is not there
      } else {
        LOG.error( e.getMessage(), e.getStackTrace().toString() );
      }
  }
  @Override
  public boolean schemaExists() {
    boolean exists = false;
    try {
      CoreAdminResponse rsp = CoreAdminRequest.getStatus( mapping.getCoreName(), adminServer );
      exists = rsp.getUptime( mapping.getCoreName() ) != null;
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return exists;
  }
  private static final String toDelimitedString( String[] arr, String sep ) {
    if ( arr == null || arr.length == 0 ) {
      return "";
    }
    StringBuilder sb = new StringBuilder();
    for ( int i = 0; i < arr.length; i ) {
      if ( i > 0 )
        sb.append( sep );
        sb.append( arr[i] );
    }
    return sb.toString();
  }

  public static String escapeQueryKey( String key ) {
    if ( key == null ) {
      return null;
    }
    StringBuilder sb = new StringBuilder();
    for ( int i = 0; i < key.length(); i ) {
      char c = key.charAt( i );
      switch ( c ) {
        case ':':
        case '*':
          sb.append( "\\"  c );
          break;
        default:
        sb.append( c );
      }
    }
    return sb.toString();
  }

  @Override
  public T get( K key, String[] fields ) {
    ModifiableSolrParams params = new ModifiableSolrParams();
    params.set( CommonParams.QT, "/get" );
    params.set( CommonParams.FL, toDelimitedString( fields, "," ) );
    params.set( "id",  key.toString() );
    try {
      QueryResponse rsp = server.query( params );
      Object o = rsp.getResponse().get( "doc" );
      if ( o == null ) {
      }
      return newInstance( (SolrDocument)o, fields );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return null;
  }
  public T newInstance( SolrDocument doc, String[] fields )
      throws IOException {
    T persistent = newPersistent();
    if ( fields == null ) {
      fields = fieldMap.keySet().toArray( new String[fieldMap.size()] );
    String pk = mapping.getPrimaryKey();
    for ( String f : fields ) {
      Field field = fieldMap.get( f );
      Schema fieldSchema = field.schema();
      String sf = null;
      if ( pk.equals( f ) ) {
        sf = f;
      } else {
        sf = mapping.getSolrField( f );                
      }
      Object sv = doc.get( sf );
      Object v;
      if ( sv == null ) {
        continue;
      }
      switch ( fieldSchema.getType() ) {
        case MAP:
        case ARRAY:
        case RECORD:
          v = IOUtils.deserialize( (byte[]) sv, datumReader, fieldSchema, persistent.get( field.pos() ) );
          persistent.put( field.pos(), v );
          break;
        case ENUM:
          v = AvroUtils.getEnumValue( fieldSchema, (String) sv );
          persistent.put( field.pos(), v );
          break;
        case FIXED:
          throw new IOException( "???" );
          // break;
        case BYTES:
          persistent.put( field.pos(), ByteBuffer.wrap( (byte[]) sv ) );
          break;
        case BOOLEAN:
        case DOUBLE:
        case FLOAT:
        case INT:
        case LONG:
          persistent.put( field.pos(), sv );
          break;
        case STRING:
          persistent.put( field.pos(), new Utf8( sv.toString() ) );
          break;
        case UNION:
          LOG.error( "Union is not supported yet" );
          break;
        default:
          LOG.error( "Unknown field type: "  fieldSchema.getType() );
      }
      persistent.setDirty( field.pos() );
    persistent.clearDirty();
    return persistent;
  }
  @Override
  public void put( K key, T persistent ) {
    Schema schema = persistent.getSchema();
    StateManager stateManager = persistent.getStateManager();
    if ( !stateManager.isDirty( persistent ) ) {
      // nothing to do
      return;
    }
    SolrInputDocument doc = new SolrInputDocument();
    // add primary key
    doc.addField( mapping.getPrimaryKey(), key );
    // populate the doc
    List<Field> fields = schema.getFields();
    for ( Field field : fields ) {
      String sf = mapping.getSolrField( field.name() );
      // Solr will append values to fields in a SolrInputDocument, even the key
      // mapping won't find the primary
      if ( sf == null ) {
        continue;
      }
      Schema fieldSchema = field.schema();
      Object v = persistent.get( field.pos() );
      if ( v == null ) {
        continue;
      }
      switch ( fieldSchema.getType() ) {
        case MAP:
        case ARRAY:
        case RECORD:
          byte[] data = null;
          try {
            data = IOUtils.serialize( datumWriter, fieldSchema, v );
          } catch ( IOException e ) {
          }
          doc.addField( sf, data );
          break;
        case BYTES:
          doc.addField( sf, ( (ByteBuffer) v ).array() );
          break;
        case ENUM:
        case STRING:
          doc.addField( sf, v.toString() );
          break;
        case BOOLEAN:
        case DOUBLE:
        case FLOAT:
        case INT:
        case LONG:
          doc.addField( sf, v );
          break;
        case UNION:
          LOG.error( "Union is not supported yet" );
          break;
        default:
          LOG.error( "Unknown field type: "  fieldSchema.getType() );
      }
    LOG.info( "DOCUMENT: "  doc );
    batch.add( doc );
    if ( batch.size() >= batchSize ) {
      try {
        add( batch, commitWithin );
        batch.clear();
      } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
      }
  }
  @Override
  public boolean delete( K key ) {
    String keyField = mapping.getPrimaryKey();
    try {
      UpdateResponse rsp = server.deleteByQuery( keyField  ":"  escapeQueryKey( key.toString() ) );
      server.commit();
      LOG.info( rsp.toString() );
      return true;
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return false;
  }
  @Override
  public long deleteByQuery( Query<K, T> query ) {
    String q = ( (SolrQuery<K, T>) query ).toSolrQuery();
    try {
      UpdateResponse rsp = server.deleteByQuery( q );
      server.commit();
      LOG.info( rsp.toString() );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return 0;
  }
  @Override
  public Result<K, T> execute( Query<K, T> query ) {
    try {
      return new SolrResult<K, T>( this, query, server, resultsSize );
    } catch ( IOException e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return null;
  }
  @Override
  public Query<K, T> newQuery() {
    return new SolrQuery<K, T>( this );
  }
  @Override
  public List<PartitionQuery<K, T>> getPartitions( Query<K, T> query )
      throws IOException {
    // TODO: implement this using Hadoop DB support

    ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<PartitionQuery<K, T>>();
    partitions.add( new PartitionQueryImpl<K, T>( query ) );

    return partitions;
  }

  @Override
  public void flush() {
    try {
      if ( batch.size() > 0 ) {
        add( batch, commitWithin );
        batch.clear();
      }
    } catch ( Exception e ) {
      LOG.error(e.getMessage(), e.getStackTrace());
  }

  @Override
  public void close() {
    // In testing, the index gets closed before the commit in flush() can happen
    // so an exception gets thrown
    //flush();
  }
  private void add(ArrayList<SolrInputDocument> batch, int commitWithin) throws SolrServerException, IOException {
    if (commitWithin == 0) {
      server.add( batch );
      server.commit( false, true, true );
    } else {
      server.add( batch, commitWithin );            
  }  
public class SolrStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  private static final Logger LOG = LoggerFactory.getLogger( SolrStore.class );
  protected static final String DEFAULT_MAPPING_FILE = "gora-solr-mapping.xml";
  protected static final String SOLR_URL_PROPERTY = "solr.url";
  protected static final String SOLR_CONFIG_PROPERTY = "solr.config";
  protected static final String SOLR_SCHEMA_PROPERTY = "solr.schema";
  protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batchSize";
  //protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";
  protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commitWithin";
  protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.resultsSize";
    
  protected static final int DEFAULT_BATCH_SIZE = 100;
  protected static final int DEFAULT_COMMIT_WITHIN = 1000;
  protected static final int DEFAULT_RESULTS_SIZE = 100;
  private SolrMapping mapping;
  private String solrServerUrl, solrConfig, solrSchema;
  private SolrServer server, adminServer;
  private ArrayList<SolrInputDocument> batch;
  private int batchSize = DEFAULT_BATCH_SIZE;
  private int commitWithin = DEFAULT_COMMIT_WITHIN;
  private int resultsSize = DEFAULT_RESULTS_SIZE;

  @Override
  public void initialize( Class<K> keyClass, Class<T> persistentClass, Properties properties ) {
    super.initialize( keyClass, persistentClass, properties );
    String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
    try {
      mapping = readMapping( mappingFile );
    }
    catch ( IOException e ) {
      LOG.error( e.getMessage() );
      LOG.error( e.getStackTrace().toString() );
    solrServerUrl = DataStoreFactory.findProperty( properties, this, SOLR_URL_PROPERTY, null );
    solrConfig = DataStoreFactory.findProperty( properties, this, SOLR_CONFIG_PROPERTY, null );
    solrSchema = DataStoreFactory.findProperty( properties, this, SOLR_SCHEMA_PROPERTY, null );
    LOG.info( "Using Solr server at "  solrServerUrl );
    adminServer = new HttpSolrServer( solrServerUrl );
    server = new HttpSolrServer( solrServerUrl  "/"  mapping.getCoreName() );
    if ( autoCreateSchema ) {
      createSchema();
    }
    String batchSizeString = DataStoreFactory.findProperty( properties, this, SOLR_BATCH_SIZE_PROPERTY, null );
    if ( batchSizeString != null ) {
      try {
        batchSize = Integer.parseInt( batchSizeString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid batch size '"  batchSizeString  "', using default "  DEFAULT_BATCH_SIZE );
      }
    }
    batch = new ArrayList<SolrInputDocument>( batchSize );
    String commitWithinString = DataStoreFactory.findProperty( properties, this, SOLR_COMMIT_WITHIN_PROPERTY, null );
    if ( commitWithinString != null ) {
      try {
        commitWithin = Integer.parseInt( commitWithinString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid commit within '"  commitWithinString  "', using default "  DEFAULT_COMMIT_WITHIN );
      }
    }
    String resultsSizeString = DataStoreFactory.findProperty( properties, this, SOLR_RESULTS_SIZE_PROPERTY, null );
    if ( resultsSizeString != null ) {
      try {
        resultsSize = Integer.parseInt( resultsSizeString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid results size '"  resultsSizeString  "', using default "  DEFAULT_RESULTS_SIZE );
      }
    }
  }
  @SuppressWarnings("unchecked")
  private SolrMapping readMapping( String filename ) throws IOException {
    SolrMapping map = new SolrMapping();
    try {
      SAXBuilder builder = new SAXBuilder();
      Document doc = builder.build( getClass().getClassLoader().getResourceAsStream( filename ) );
      List<Element> classes = doc.getRootElement().getChildren( "class" );
      for ( Element classElement : classes ) {
        if ( classElement.getAttributeValue( "keyClass" ).equals( keyClass.getCanonicalName() )
            && classElement.getAttributeValue( "name" ).equals( persistentClass.getCanonicalName() ) ) {
          String tableName = getSchemaName( classElement.getAttributeValue( "table" ), persistentClass );
          map.setCoreName( tableName );
          Element primaryKeyEl = classElement.getChild( "primarykey" );
          map.setPrimaryKey( primaryKeyEl.getAttributeValue( "column" ) );
          List<Element> fields = classElement.getChildren( "field" );

          for ( Element field : fields ) {
            String fieldName = field.getAttributeValue( "name" );
            String columnName = field.getAttributeValue( "column" );
            map.addField( fieldName, columnName );
          }
          break;
      }
    } catch ( Exception ex ) {
      throw new IOException( ex );
    return map;
  }
  public SolrMapping getMapping() {
    return mapping;
  }
  @Override
  public String getSchemaName() {
    return mapping.getCoreName();
  }
  @Override
  public void createSchema() {
    try {
      if ( !schemaExists() )
          CoreAdminRequest.createCore( mapping.getCoreName(), mapping.getCoreName(), adminServer, solrConfig,
          solrSchema );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
  }
  @Override
  /** Default implementation deletes and recreates the schema*/
  public void truncateSchema() {
    try {
      server.deleteByQuery( "*:*" );
      server.commit();
    } catch ( Exception e ) {
      // ignore?
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
  }
  @Override
  public void deleteSchema() {
    // XXX should this be only in truncateSchema ???
    try {
      server.deleteByQuery( "*:*" );
      server.commit();
    } catch ( Exception e ) {
    // ignore?
    // LOG.error(e.getMessage());
    // LOG.error(e.getStackTrace().toString());
    try {
      CoreAdminRequest.unloadCore( mapping.getCoreName(), adminServer );
    } catch ( Exception e ) {
      if ( e.getMessage().contains( "No such core" ) ) {
        return; // it's ok, the core is not there
      } else {
        LOG.error( e.getMessage(), e.getStackTrace().toString() );
      }
  }
  @Override
  public boolean schemaExists() {
    boolean exists = false;
    try {
      CoreAdminResponse rsp = CoreAdminRequest.getStatus( mapping.getCoreName(), adminServer );
      exists = rsp.getUptime( mapping.getCoreName() ) != null;
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return exists;
  }
  private static final String toDelimitedString( String[] arr, String sep ) {
    if ( arr == null || arr.length == 0 ) {
      return "";
    }
    StringBuilder sb = new StringBuilder();
    for ( int i = 0; i < arr.length; i ) {
      if ( i > 0 )
        sb.append( sep );
        sb.append( arr[i] );
    }
    return sb.toString();
  }

  public static String escapeQueryKey( String key ) {
    if ( key == null ) {
      return null;
    }
    StringBuilder sb = new StringBuilder();
    for ( int i = 0; i < key.length(); i ) {
      char c = key.charAt( i );
      switch ( c ) {
        case ':':
        case '*':
          sb.append( "\\"  c );
          break;
        default:
        sb.append( c );
      }
    }
    return sb.toString();
  }

  @Override
  public T get( K key, String[] fields ) {
    ModifiableSolrParams params = new ModifiableSolrParams();
    params.set( CommonParams.QT, "/get" );
    params.set( CommonParams.FL, toDelimitedString( fields, "," ) );
    params.set( "id",  key.toString() );
    try {
      QueryResponse rsp = server.query( params );
      Object o = rsp.getResponse().get( "doc" );
      if ( o == null ) {
      }
      return newInstance( (SolrDocument)o, fields );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return null;
  }
  public T newInstance( SolrDocument doc, String[] fields )
      throws IOException {
    T persistent = newPersistent();
    if ( fields == null ) {
      fields = fieldMap.keySet().toArray( new String[fieldMap.size()] );
    String pk = mapping.getPrimaryKey();
    for ( String f : fields ) {
      Field field = fieldMap.get( f );
      Schema fieldSchema = field.schema();
      String sf = null;
      if ( pk.equals( f ) ) {
        sf = f;
      } else {
        sf = mapping.getSolrField( f );                
      }
      Object sv = doc.get( sf );
      Object v;
      if ( sv == null ) {
        continue;
      }
      switch ( fieldSchema.getType() ) {
        case MAP:
        case ARRAY:
        case RECORD:
          v = IOUtils.deserialize( (byte[]) sv, datumReader, fieldSchema, persistent.get( field.pos() ) );
          persistent.put( field.pos(), v );
          break;
        case ENUM:
          v = AvroUtils.getEnumValue( fieldSchema, (String) sv );
          persistent.put( field.pos(), v );
          break;
        case FIXED:
          throw new IOException( "???" );
          // break;
        case BYTES:
          persistent.put( field.pos(), ByteBuffer.wrap( (byte[]) sv ) );
          break;
        case BOOLEAN:
        case DOUBLE:
        case FLOAT:
        case INT:
        case LONG:
          persistent.put( field.pos(), sv );
          break;
        case STRING:
          persistent.put( field.pos(), new Utf8( sv.toString() ) );
          break;
        case UNION:
          LOG.error( "Union is not supported yet" );
          break;
        default:
          LOG.error( "Unknown field type: "  fieldSchema.getType() );
      }
      persistent.setDirty( field.pos() );
    persistent.clearDirty();
    return persistent;
  }
  @Override
  public void put( K key, T persistent ) {
    Schema schema = persistent.getSchema();
    StateManager stateManager = persistent.getStateManager();
    if ( !stateManager.isDirty( persistent ) ) {
      // nothing to do
      return;
    }
    SolrInputDocument doc = new SolrInputDocument();
    // add primary key
    doc.addField( mapping.getPrimaryKey(), key );
    // populate the doc
    List<Field> fields = schema.getFields();
    for ( Field field : fields ) {
      String sf = mapping.getSolrField( field.name() );
      // Solr will append values to fields in a SolrInputDocument, even the key
      // mapping won't find the primary
      if ( sf == null ) {
        continue;
      }
      Schema fieldSchema = field.schema();
      Object v = persistent.get( field.pos() );
      if ( v == null ) {
        continue;
      }
      switch ( fieldSchema.getType() ) {
        case MAP:
        case ARRAY:
        case RECORD:
          byte[] data = null;
          try {
            data = IOUtils.serialize( datumWriter, fieldSchema, v );
          } catch ( IOException e ) {
          }
          doc.addField( sf, data );
          break;
        case BYTES:
          doc.addField( sf, ( (ByteBuffer) v ).array() );
          break;
        case ENUM:
        case STRING:
          doc.addField( sf, v.toString() );
          break;
        case BOOLEAN:
        case DOUBLE:
        case FLOAT:
        case INT:
        case LONG:
          doc.addField( sf, v );
          break;
        case UNION:
          LOG.error( "Union is not supported yet" );
          break;
        default:
          LOG.error( "Unknown field type: "  fieldSchema.getType() );
      }
    LOG.info( "DOCUMENT: "  doc );
    batch.add( doc );
    if ( batch.size() >= batchSize ) {
      try {
        add( batch, commitWithin );
        batch.clear();
      } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
      }
  }
  @Override
  public boolean delete( K key ) {
    String keyField = mapping.getPrimaryKey();
    try {
      UpdateResponse rsp = server.deleteByQuery( keyField  ":"  escapeQueryKey( key.toString() ) );
      server.commit();
      LOG.info( rsp.toString() );
      return true;
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return false;
  }
  @Override
  public long deleteByQuery( Query<K, T> query ) {
    String q = ( (SolrQuery<K, T>) query ).toSolrQuery();
    try {
      UpdateResponse rsp = server.deleteByQuery( q );
      server.commit();
      LOG.info( rsp.toString() );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return 0;
  }
  @Override
  public Result<K, T> execute( Query<K, T> query ) {
    try {
      return new SolrResult<K, T>( this, query, server, resultsSize );
    } catch ( IOException e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return null;
  }
  @Override
  public Query<K, T> newQuery() {
    return new SolrQuery<K, T>( this );
  }
  @Override
  public List<PartitionQuery<K, T>> getPartitions( Query<K, T> query )
      throws IOException {
    // TODO: implement this using Hadoop DB support

    ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<PartitionQuery<K, T>>();
    partitions.add( new PartitionQueryImpl<K, T>( query ) );

    return partitions;
  }

  @Override
  public void flush() {
    try {
      if ( batch.size() > 0 ) {
        add( batch, commitWithin );
        batch.clear();
      }
    } catch ( Exception e ) {
      LOG.error(e.getMessage(), e.getStackTrace());
  }

  @Override
  public void close() {
    // In testing, the index gets closed before the commit in flush() can happen
    // so an exception gets thrown
    //flush();
  }
  private void add(ArrayList<SolrInputDocument> batch, int commitWithin) throws SolrServerException, IOException {
    if (commitWithin == 0) {
      server.add( batch );
      server.commit( false, true, true );
    } else {
      server.add( batch, commitWithin );            
  }  
  public static DBObject toProjection(String[] fields, MongoMapping mapping) {
      for (String k : fields) {
import org.apache.gora.persistency.impl.BeanFactoryImpl;
import org.apache.gora.util.ClassLoadingUtils;
   *

    String[] fields = getFieldsToQuery(query.getFields());
    DBObject p = MongoDBQuery.toProjection(fields, mapping);
    BSONDecorator easybson = new BSONDecorator(obj);
    fields = getFieldsToQuery(fields);
      Object result = fromDBObject(fieldSchema, storeType, field, docf, easybson);
      persistent.put(field.pos(), result);
    private Object fromDBObject(final Schema fieldSchema, final DocumentFieldType storeType, final Field field, final String docf, final BSONDecorator easybson) {
        Object result = null;
        switch (fieldSchema.getType()) {
            case MAP:
                BasicDBObject map = easybson.getDBObject(docf);
                StatefulHashMap<Utf8, Object> rmap = new StatefulHashMap<Utf8, Object>();
                for (Entry<String, Object> e : map.entrySet()) {
                    // ensure Key decoding -> middle dots replaced with dots
                    // FIXME: better approach ?
                    String oKey = e.getKey().replace("\u00B7", ".");

                    switch (fieldSchema.getValueType().getType()) {
                        case STRING:
                            rmap.put(new Utf8(oKey), new Utf8((String) e.getValue()));
                            break;
                        case BYTES:
                            rmap.put(new Utf8(oKey), ByteBuffer.wrap((byte[]) e.getValue()));
                            break;
                        default:
                            rmap.put(new Utf8(oKey), e.getValue());
                            break;
                    }
                }
                rmap.clearStates();
                result = rmap;
                break;
            case ARRAY:
                List<Object> list = easybson.getDBList(docf);
                switch (fieldSchema.getElementType().getType()) {
                    case STRING:
                        ListGenericArray<Utf8> arrS = new ListGenericArray<Utf8>(fieldSchema);
                        for (Object o : list)
                            arrS.add(new Utf8((String) o));
                        result = arrS;
                        break;
                    case BYTES:
                        ListGenericArray<ByteBuffer> arrB = new ListGenericArray<ByteBuffer>(
                                fieldSchema);
                        for (Object o : list)
                            arrB.add(ByteBuffer.wrap((byte[]) o));
                        result = arrB;
                        break;
                    default:
                        ListGenericArray<Object> arrT = new ListGenericArray<Object>(
                                fieldSchema);
                        for (Object o : list)
                            arrT.add(o);
                        result = arrT;
                        break;
                }
                break;
            case RECORD:
                DBObject rec = easybson.getDBObject(docf);
                BSONDecorator innerBson = new BSONDecorator(rec);
                Class<?> clazz = null;
                try {
                    clazz = ClassLoadingUtils.loadClass(fieldSchema.getFullName());
                } catch (ClassNotFoundException e) {
                }
                Persistent record = new BeanFactoryImpl(keyClass, clazz).newPersistent();
                for (Field recField : fieldSchema.getFields()) {
                    Schema innerSchema = recField.schema();
                    DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerSchema.getName());
                    String recDocField = mapping.getDocumentField(recField.name()) != null ? mapping.getDocumentField(recField.name()) : recField.name();
                    ((PersistentBase) record).put(recField.pos(), fromDBObject(innerSchema, innerStoreType, recField, recDocField, innerBson));
                }
                result = record;
                break;
            case BOOLEAN:
                result = easybson.getBoolean(docf);
                break;
            case DOUBLE:
                result = easybson.getDouble(docf);
                break;
            case FLOAT:
                result = easybson.getDouble(docf).floatValue();
                break;
            case INT:
                result = easybson.getInt(docf);
                break;
            case LONG:
                result = easybson.getLong(docf);
                break;
            case STRING:
                if (storeType == DocumentFieldType.OBJECTID) {
                    // Try auto-conversion of BSON data to ObjectId
                    // It will work if data is stored as String or as ObjectId
                    final Object bin = easybson.get(docf);
                    final ObjectId id = ObjectId.massageToObjectId(bin);
                    result = new Utf8(id.toString());
                } else if (storeType == DocumentFieldType.DATE) {
                    final Object bin = easybson.get(docf);
                    if (bin instanceof Date) {
                        Calendar calendar = Calendar.getInstance(TimeZone
                                .getTimeZone("UTC"));
                        calendar.setTime((Date) bin);
                        result = new Utf8(DatatypeConverter.printDateTime(calendar));
                    } else {
                        result = new Utf8(bin.toString());
                    }
                } else {
                    result = easybson.getUtf8String(docf);
                }
                break;
            case ENUM:
                result = AvroUtils.getEnumValue(fieldSchema,
                        easybson.getUtf8String(docf).toString());
                break;
            case BYTES:
            case FIXED:
                result = easybson.getBytes(docf);
                break;
            case NULL:
                result = null;
                break;
            case UNION:
                // schema [type0, type1]
                Type type0 = fieldSchema.getTypes().get(0).getType();
                Type type1 = fieldSchema.getTypes().get(1).getType();

                // Check if types are different and there's a "null", like ["null","type"] or ["type","null"]
                if (!type0.equals(type1)
                        && (type0.equals(Type.NULL)
                        || type1.equals(Type.NULL))) {
                    Schema innerSchema = fieldSchema.getTypes().get(1);
                    DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerSchema.getName());
                    result = fromDBObject(innerSchema, innerStoreType, field, docf, easybson); // Deserialize as if schema was ["type"]
                }
                break;
            default:
                LOG.warn("Unable to read {}", docf);
                break;
        }
        return result;
    }

    // ////////////////////////////////////////////////////////// SERIALIZATION
   * {@link MongoStore#newInstance(org.apache.gora.persistency.impl.PersistentBase)} one from two points:
   *         have to be updated... and formatted to be passed in parameter of a
        DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
        result.put(docf, toDBObject(f.schema(), f.schema().getType(), storeType, value));
   * {@link MongoStore#newInstance(org.apache.gora.persistency.impl.PersistentBase)} one from two points:
        DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
        Object o = toDBObject(f.schema(), f.schema().getType(), storeType, value);
        result.put(docf, o);
    private Object toDBObject(Schema fieldSchema, Type fieldType, DocumentFieldType storeType, Object value) {
        Object result = null;
        switch (fieldType) {
            case MAP:
                if (storeType != null && storeType != DocumentFieldType.DOCUMENT) {
                    throw new IllegalStateException(
                            "Field "
                                     fieldSchema.getType()
                                     ": to store a Gora 'map', target Mongo mapping have to be of 'document' type");
                }
                Schema valueSchema = fieldSchema.getValueType();
                result = toMongoMap((Map<Utf8, ?>) value, valueSchema.getType());
                break;
            case ARRAY:
                if (storeType != null && storeType != DocumentFieldType.LIST) {
                    throw new IllegalStateException(
                            "Field "
                                     fieldSchema.getType()
                                     ": To store a Gora 'array', target Mongo mapping have to be of 'list' type");
                }
                Schema elementSchema = fieldSchema.getElementType();
                result = toMongoList((GenericArray<?>) value, elementSchema.getType());
                break;
            case BYTES:
                // Beware of ByteBuffer not being safely serialized
                if (value != null) {
                    result = ((ByteBuffer) value).array();
                }
                break;
            case INT:
            case LONG:
            case FLOAT:
            case DOUBLE:
            case BOOLEAN:
                result = value;
                break;
            case STRING:
                if (storeType == DocumentFieldType.OBJECTID) {
                    if (value != null) {
                        ObjectId id;
                        try {
                            id = new ObjectId(value.toString());
                        } catch (IllegalArgumentException e1) {
                            // Unable to parse anything from Utf8 value, throw error
                            throw new IllegalStateException("Field "  fieldSchema.getType()
                                     ": Invalid string: unable to convert to ObjectId");
                        }
                        result = id;
                    }
                } else if (storeType == DocumentFieldType.DATE) {
                    if (value != null) {
                        // Try to parse date from Utf8 value
                        Calendar calendar = null;
                        try {
                            // Parse as date  time
                            calendar = DatatypeConverter.parseDateTime(value.toString());
                        } catch (IllegalArgumentException e1) {
                            try {
                                // Parse as date only
                                calendar = DatatypeConverter.parseDate(value.toString());
                            } catch (IllegalArgumentException e2) {
                                // No-op
                            }
                        }
                        if (calendar == null) {
                            // Unable to parse anything from Utf8 value, throw error
                            throw new IllegalStateException("Field "  fieldSchema.getType()
                                     ": Invalid date format '"  value  "'");
                        }
                        result = calendar.getTime();
                    }
                } else {
                    // Beware of Utf8 not being safely serialized
                    if (value != null) {
                        result = value.toString();
                    }
                }
                break;
            case ENUM:
                // Beware of Utf8 not being safely serialized
                if (value != null)
                    result = value.toString();
                break;
            case RECORD:
                if (value == null)
                    break;
                BasicDBObject record = new BasicDBObject();
                for (Field member : fieldSchema.getFields()) {
                    Object innerValue = ((PersistentBase) value).get(member.pos());
                    String innerDoc = mapping.getDocumentField(member.name());
                    Type innerType = member.schema().getType();
                    DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerDoc);
                    record.put(member.name(), toDBObject(member.schema(), innerType, innerStoreType, innerValue));
                }
                result = record;
                break;
            case UNION:
                // schema [type0, type1]
                Type type0 = fieldSchema.getTypes().get(0).getType();
                Type type1 = fieldSchema.getTypes().get(1).getType();

                // Check if types are different and there's a "null", like ["null","type"] or ["type","null"]
                if (!type0.equals(type1)
                        && (type0.equals(Schema.Type.NULL)
                        || type1.equals(Schema.Type.NULL))) {
                    Schema innerSchema = fieldSchema.getTypes().get(1);
                    DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerSchema.getName());
                    result = toDBObject(innerSchema, type1, innerStoreType, value); // Deserialize as if schema was ["type"]
                }
                break;
            case FIXED:
                result = value;
                break;

            default:
                LOG.error("Unknown field type: "  fieldSchema.getType());
                break;
        }

        return result;
    }

    /**
    case LONG:
    case INT:
        easybson.put(key, value);
        break;
        case LONG:
        case INT:
            easybson.put(key, value);
            break;
          LOG.error("A record in a record! Seriously? Fuck it, it's not supported yet.");
          break;
        case UNION:
          LOG.error("Union is not supported");
          break;
          LOG.error("Unknown field type: "  member.schema().getType());
    case UNION:
      LOG.error("Union is not supported");
      break;
      LOG.error("Unknown field type: "  field.schema().getType());
      LOG.debug("Load from DBObject (MAIN), field:{}, schemaType:{}, docField:{}, storeType:{}", new Object[]{field.name(), fieldSchema.getType(), docf, storeType});
                if (rec == null) {
                    return result;
                }
                    String innerDocField = mapping.getDocumentField(recField.name()) != null ? mapping.getDocumentField(recField.name()) : recField.name();
                    String fieldPath = docf  "."  innerDocField;
                    LOG.debug("Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}", new Object[]{recField.name(), innerSchema.getType(), fieldPath, innerStoreType});
                    ((PersistentBase) record).put(recField.pos(), fromDBObject(innerSchema, innerStoreType, recField, innerDocField, innerBson));
                    LOG.debug("Load from DBObject (UNION), schemaType:{}, docField:{}, storeType:{}", new Object[]{innerSchema.getType(), docf, innerStoreType});
        LOG.debug("Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}", new Object[]{docf, f.schema().getType(), storeType});
        LOG.debug("Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}", new Object[]{docf, f.schema().getType(), storeType});
                    LOG.debug("Transform value to DBObject (RECORD), docField:{}, schemaType:{}, storeType:{}", new Object[]{member.name(), member.schema().getType(), innerStoreType});
                    LOG.debug("Transform value to DBObject (UNION), schemaType:{}, type1:{}, storeType:{}", new Object[]{innerSchema.getType(), type1, innerStoreType});
    PartitionQueryImpl<K, T> partitionQuery = new PartitionQueryImpl<K, T>(query);
    partitionQuery.setConf(getConf());
    partitions.add(partitionQuery);
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return Employee created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Name.
   * @return Utf8 representing Employee Name.
   */
  
  /**
   * Sets the Name.
   * @param value containing Employee Name.
   */
  
  /**
   * Gets the DateOfBirth.
   * @return long representing Employee DateOfBirth.
   */
  
  /**
   * Sets the DateOfBirth.
   * @param value containing Employee DateOfBirth.
   */
  
  /**
   * Gets the Ssn.
   * @return Utf8 representing Employee Ssn.
   */
  
  /**
   * Sets the Ssn.
   * @param value containing Employee Ssn.
   */
  
  /**
   * Gets the Salary.
   * @return int representing Employee Salary.
   */
  
  /**
   * Sets the Salary.
   * @param value containing Employee Salary.
   */
  
  /**
   * Gets Boss.
   * @return the Object value.
   */
  
  /**
   * Sets the Boss.
   * @param the Boss value to be set.
   */
  
  /**
   * Sets the Boss.
   * @param the Boss value to be set.
   */
  
  /**
   * Gets Webpage.
   * @return the WebPage value.
   */
  
  /**
   * Sets the Webpage.
   * @param the Webpage value to be set.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return Metadata created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Version.
   * @return int representing Metadata Version.
   */
  
  /**
   * Sets the Version.
   * @param value containing Metadata Version.
   */
  
  /**
   * Gets Data.
   * @return Map containing Data value.
   */
  
  /**
   * Gets the Data's value using a key.
   * @param key gets a specific Data using a MetadataID.
   * @return Utf8 containing Data value.
   */
  
  /**
   * Adds a Data into a Metadata.
   * @param Map containing Data value.
   */
  
  /**
   * Removes Data from a Metadata.
   * @return key Metadata ID to be removed.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return TokenDatum created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Count.
   * @return int representing TokenDatum Count.
   */
  
  /**
   * Sets the Count.
   * @param value containing TokenDatum Count.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return WebPage created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Url.
   * @return Utf8 representing WebPage Url.
   */
  
  /**
   * Sets the Url.
   * @param value containing WebPage Url.
   */
  
  /**
   * Gets Content.
   * @return the ByteBuffer value.
   */
  
  /**
   * Sets the Content.
   * @param the Content value to be set.
   */
  
  /**
   * Gets the ParsedContent array.
   * @return GenericArray<Utf8> containing Utf8 elements.
   */
  
  /**
   * Adds a Utf8 element into the array.
   * @param the Utf8 element to be added.
   */
  
  /**
   * Gets Outlinks.
   * @return Map containing Outlinks value.
   */
  
  /**
   * Gets the Outlinks's value using a key.
   * @param key gets a specific Outlinks using a WebPageID.
   * @return Utf8 containing Outlinks value.
   */
  
  /**
   * Adds a Outlinks into a WebPage.
   * @param Map containing Outlinks value.
   */
  
  /**
   * Removes Outlinks from a WebPage.
   * @return key WebPage ID to be removed.
   */
  
  /**
   * Gets the Metadata.
   * @return Metadata representing WebPage Metadata.
   */
  
  /**
   * Sets the Metadata.
   * @param value containing WebPage Metadata.
   */
  /**
   * Method that adds javadoc to the generated data beans.
   * @param indent  Specifies the indentation for the javadoc
   * @param javadoc The javadoc to be added. Use \n to span the javadoc
   *                to multiple lines.
   * @throws IOException
   */
  private void addJavaDoc(int indent, String javadoc) throws IOException {

    if (javadoc==null)
      return;

    if (indent<0)
      return;

    line(indent, "");
    line(indent, "/**");

    if (javadoc.contains("\n")){
      String javadocLines[] = javadoc.split("\n");

      for(String line : javadocLines)
        line(indent, " * "line);
    }
    else
      line(indent," * "javadoc);

    line(indent, " */");
  }

        addJavaDoc(0,schema.getDoc());
        line(0, "@SuppressWarnings(\"all\")");

        addJavaDoc(1,"Variable holding the data bean schema.");
        addJavaDoc(1,"Enum containing all data bean's fields.");

        addJavaDoc(2,"Field's index.");

        addJavaDoc(2,"Field's name.");

        addJavaDoc(2,"Field's constructor\n"
                     "@param index field's index.\n"
                     "@param name field's name.");

        addJavaDoc(2,"Gets field's index.\n"
                     "@return int field's index.");

        addJavaDoc(2,"Gets field's name.\n"
                "@return String field's name.");

        addJavaDoc(2,"Gets field's attributes to string.\n"
                "@return String field's attributes to string.");
        addJavaDoc(2,"Contains all field's names.");
          addJavaDoc(1,field.doc());
        addJavaDoc(1,"Default Constructor");

        addJavaDoc(1,"Constructor\n@param stateManager for the data bean.");
        addJavaDoc(1,"Returns a new instance by using a state manager.\n"
                     "@param stateManager for the data bean.\n"
                     "@return "schema.getName()" created.");
        addJavaDoc(1,"Returns the schema of the data bean.\n"
                     "@return Schema for the data bean.");

        addJavaDoc(1,"Gets a specific field.\n"
                     "@param field index of a field for the data bean.\n"
                     "@return Object representing a data bean's field.");

        addJavaDoc(1,"Puts a value for a specific field.\n"
                     "@param field index of a field for the data bean.\n"
                     "@param value value of a field for the data bean.");
            addJavaDoc(1,"Gets the "camelKey".\n"
                         "@return "unboxed" representing "schema.getName()" "camelKey".");

            addJavaDoc(1,"Sets the "camelKey".\n"
                         "@param value containing "schema.getName()" "camelKey".");

            addJavaDoc(1,"Gets the "camelKey" array.\n"
                    "@return GenericArray<"fieldType"> containing "fieldType" elements.");

            addJavaDoc(1,"Adds a "unboxed" element into the array.\n"
                    "@param the "unboxed" element to be added.");

            addJavaDoc(1,"Gets "camelKey".\n"
                    "@return Map containing "camelKey" value.");

            addJavaDoc(1,"Gets the "camelKey"'s value using a key.\n"
                    "@param key gets a specific "camelKey" using a "schema.getName()"ID.\n"
                    "@return "fieldType" containing "camelKey" value.");

            addJavaDoc(1,"Adds a "camelKey" into a "schema.getName()".\n"
                    "@param Map containing "camelKey" value.");

            addJavaDoc(1,"Removes "camelKey" from a "schema.getName()".\n"
                    "@return key "schema.getName()" ID to be removed.");

            addJavaDoc(1,"Gets "camelKey".\n"
                    "@return the "unbox(field.schema())" value.");

              addJavaDoc(1,"Sets the "camelKey".\n"
                      "@param the "camelKey" value to be set.");
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return Employee created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Name.
   * @return Utf8 representing Employee Name.
   */
  
  /**
   * Sets the Name.
   * @param value containing Employee Name.
   */
  
  /**
   * Gets the DateOfBirth.
   * @return long representing Employee DateOfBirth.
   */
  
  /**
   * Sets the DateOfBirth.
   * @param value containing Employee DateOfBirth.
   */
  
  /**
   * Gets the Ssn.
   * @return Utf8 representing Employee Ssn.
   */
  
  /**
   * Sets the Ssn.
   * @param value containing Employee Ssn.
   */
  
  /**
   * Gets the Salary.
   * @return int representing Employee Salary.
   */
  
  /**
   * Sets the Salary.
   * @param value containing Employee Salary.
   */
  
  /**
   * Gets Boss.
   * @return the Object value.
   */
  
  /**
   * Sets the Boss.
   * @param the Boss value to be set.
   */
  
  /**
   * Sets the Boss.
   * @param the Boss value to be set.
   */
  
  /**
   * Gets Webpage.
   * @return the WebPage value.
   */
  
  /**
   * Sets the Webpage.
   * @param the Webpage value to be set.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return Metadata created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Version.
   * @return int representing Metadata Version.
   */
  
  /**
   * Sets the Version.
   * @param value containing Metadata Version.
   */
  
  /**
   * Gets Data.
   * @return Map containing Data value.
   */
  
  /**
   * Gets the Data's value using a key.
   * @param key gets a specific Data using a MetadataID.
   * @return Utf8 containing Data value.
   */
  
  /**
   * Adds a Data into a Metadata.
   * @param Map containing Data value.
   */
  
  /**
   * Removes Data from a Metadata.
   * @return key Metadata ID to be removed.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return TokenDatum created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Count.
   * @return int representing TokenDatum Count.
   */
  
  /**
   * Sets the Count.
   * @param value containing TokenDatum Count.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return WebPage created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Url.
   * @return Utf8 representing WebPage Url.
   */
  
  /**
   * Sets the Url.
   * @param value containing WebPage Url.
   */
  
  /**
   * Gets Content.
   * @return the ByteBuffer value.
   */
  
  /**
   * Sets the Content.
   * @param the Content value to be set.
   */
  
  /**
   * Gets the ParsedContent array.
   * @return GenericArray<Utf8> containing Utf8 elements.
   */
  
  /**
   * Adds a Utf8 element into the array.
   * @param the Utf8 element to be added.
   */
  
  /**
   * Gets Outlinks.
   * @return Map containing Outlinks value.
   */
  
  /**
   * Gets the Outlinks's value using a key.
   * @param key gets a specific Outlinks using a WebPageID.
   * @return Utf8 containing Outlinks value.
   */
  
  /**
   * Adds a Outlinks into a WebPage.
   * @param Map containing Outlinks value.
   */
  
  /**
   * Removes Outlinks from a WebPage.
   * @return key WebPage ID to be removed.
   */
  
  /**
   * Gets the Metadata.
   * @return Metadata representing WebPage Metadata.
   */
  
  /**
   * Sets the Metadata.
   * @param value containing WebPage Metadata.
   */
  /**
   * Method that adds javadoc to the generated data beans.
   * @param indent  Specifies the indentation for the javadoc
   * @param javadoc The javadoc to be added. Use \n to span the javadoc
   *                to multiple lines.
   * @throws IOException
   */
  private void addJavaDoc(int indent, String javadoc) throws IOException {

    if (javadoc==null)
      return;

    if (indent<0)
      return;

    line(indent, "");
    line(indent, "/**");

    if (javadoc.contains("\n")){
      String javadocLines[] = javadoc.split("\n");

      for(String line : javadocLines)
        line(indent, " * "line);
    }
    else
      line(indent," * "javadoc);

    line(indent, " */");
  }

        addJavaDoc(0,schema.getDoc());
        line(0, "@SuppressWarnings(\"all\")");

        addJavaDoc(1,"Variable holding the data bean schema.");
        addJavaDoc(1,"Enum containing all data bean's fields.");

        addJavaDoc(2,"Field's index.");

        addJavaDoc(2,"Field's name.");

        addJavaDoc(2,"Field's constructor\n"
                     "@param index field's index.\n"
                     "@param name field's name.");

        addJavaDoc(2,"Gets field's index.\n"
                     "@return int field's index.");

        addJavaDoc(2,"Gets field's name.\n"
                "@return String field's name.");

        addJavaDoc(2,"Gets field's attributes to string.\n"
                "@return String field's attributes to string.");
        addJavaDoc(2,"Contains all field's names.");
          addJavaDoc(1,field.doc());
        addJavaDoc(1,"Default Constructor");

        addJavaDoc(1,"Constructor\n@param stateManager for the data bean.");
        addJavaDoc(1,"Returns a new instance by using a state manager.\n"
                     "@param stateManager for the data bean.\n"
                     "@return "schema.getName()" created.");
        addJavaDoc(1,"Returns the schema of the data bean.\n"
                     "@return Schema for the data bean.");

        addJavaDoc(1,"Gets a specific field.\n"
                     "@param field index of a field for the data bean.\n"
                     "@return Object representing a data bean's field.");

        addJavaDoc(1,"Puts a value for a specific field.\n"
                     "@param field index of a field for the data bean.\n"
                     "@param value value of a field for the data bean.");
            addJavaDoc(1,"Gets the "camelKey".\n"
                         "@return "unboxed" representing "schema.getName()" "camelKey".");

            addJavaDoc(1,"Sets the "camelKey".\n"
                         "@param value containing "schema.getName()" "camelKey".");

            addJavaDoc(1,"Gets the "camelKey" array.\n"
                    "@return GenericArray<"fieldType"> containing "fieldType" elements.");

            addJavaDoc(1,"Adds a "unboxed" element into the array.\n"
                    "@param the "unboxed" element to be added.");

            addJavaDoc(1,"Gets "camelKey".\n"
                    "@return Map containing "camelKey" value.");

            addJavaDoc(1,"Gets the "camelKey"'s value using a key.\n"
                    "@param key gets a specific "camelKey" using a "schema.getName()"ID.\n"
                    "@return "fieldType" containing "camelKey" value.");

            addJavaDoc(1,"Adds a "camelKey" into a "schema.getName()".\n"
                    "@param Map containing "camelKey" value.");

            addJavaDoc(1,"Removes "camelKey" from a "schema.getName()".\n"
                    "@return key "schema.getName()" ID to be removed.");

            addJavaDoc(1,"Gets "camelKey".\n"
                    "@return the "unbox(field.schema())" value.");

              addJavaDoc(1,"Sets the "camelKey".\n"
                      "@param the "camelKey" value to be set.");
import java.util.Arrays;
  private final static String DEFAULT_SCHEMA_EXTENTION = ".avsc";
              "\t\t      generated Java file. Current options include; \n" 
              "\t\t  ASLv2   (Apache Software License v2.0) \n" 
              "\t\t  AGPLv3  (GNU Affero General Public License)\n" 
              "\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n" 
              "\t\t  FDLv13  (GNU Free Documentation License v1.3)\n" 
              "\t\t  GPLv1   (GNU General Public License v1.0)\n" 
              "\t\t  GPLv2   (GNU General Public License v2.0)\n" 
              "\t\t  GPLv3   (GNU General Public License v3.0)\n " 
              "\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n" 
              "\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;

    SimpleDateFormat sdf;
    File inputFile;
    File output;
    long start;

      }

    if (args.length==2){ //case of single file or single directory
      inputFile = new File(args[0]);
      output = new File(args[1]);

      if(!inputFile.exists() || !output.exists()){
        log.error("input file path or output file path doesn't exist.");
        System.exit(1);
      }

      sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      start = System.currentTimeMillis();
      log.info("GoraCompiler: starting at "  sdf.format(start));
      if(inputFile.isDirectory()) {
        ArrayList<File> inputSchemas = new ArrayList<File>();
        File[] listOfFiles= inputFile.listFiles();

        if ( (listOfFiles!=null) && (listOfFiles.length>0)){
          for (File file : listOfFiles) {
            if (file.isFile() && file.exists() && file.getName().endsWith(DEFAULT_SCHEMA_EXTENTION)) {
              inputSchemas.add(file);
            }
          }

          compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
        }
        else{
          log.info("Path contains no files. Nothing to compile.");
        }
      }
      else if (inputFile.isFile()) {
        compileSchema(inputFile, output);
      }
    else{ //case of dynamic filename extension (such as *.* or *.json)
      List<String> files = new ArrayList<String>(Arrays.asList(args));
      output = new File(files.get(files.size()-1));
      files.remove(files.size()-1); //remove the last one, as this is the output directory

      if(!output.exists()){
        log.error("output path doesn't exist");
        System.exit(1);
      }

      sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      start = System.currentTimeMillis();
      log.info("GoraCompiler: starting at "  sdf.format(start));

      for(String filename : files){ //loop for all the retrieved files
        inputFile = new File(filename);
        if(!inputFile.exists()){
          log.error("input file: "filename" doesn't exist.");
          continue; //in case the file does not exist, continue to the next file
        }

        compileSchema(inputFile, output);
      }

import java.util.Arrays;
  private final static String DEFAULT_SCHEMA_EXTENTION = ".avsc";
              "\t\t      generated Java file. Current options include; \n" 
              "\t\t  ASLv2   (Apache Software License v2.0) \n" 
              "\t\t  AGPLv3  (GNU Affero General Public License)\n" 
              "\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n" 
              "\t\t  FDLv13  (GNU Free Documentation License v1.3)\n" 
              "\t\t  GPLv1   (GNU General Public License v1.0)\n" 
              "\t\t  GPLv2   (GNU General Public License v2.0)\n" 
              "\t\t  GPLv3   (GNU General Public License v3.0)\n " 
              "\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n" 
              "\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;

    SimpleDateFormat sdf;
    File inputFile;
    File output;
    long start;

      }

    if (args.length==2){ //case of single file or single directory
      inputFile = new File(args[0]);
      output = new File(args[1]);

      if(!inputFile.exists() || !output.exists()){
        log.error("input file path or output file path doesn't exist.");
        System.exit(1);
      }

      sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      start = System.currentTimeMillis();
      log.info("GoraCompiler: starting at "  sdf.format(start));
      if(inputFile.isDirectory()) {
        ArrayList<File> inputSchemas = new ArrayList<File>();
        File[] listOfFiles= inputFile.listFiles();

        if ( (listOfFiles!=null) && (listOfFiles.length>0)){
          for (File file : listOfFiles) {
            if (file.isFile() && file.exists() && file.getName().endsWith(DEFAULT_SCHEMA_EXTENTION)) {
              inputSchemas.add(file);
            }
          }

          compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
        }
        else{
          log.info("Path contains no files. Nothing to compile.");
        }
      }
      else if (inputFile.isFile()) {
        compileSchema(inputFile, output);
      }
    else{ //case of dynamic filename extension (such as *.* or *.json)
      List<String> files = new ArrayList<String>(Arrays.asList(args));
      output = new File(files.get(files.size()-1));
      files.remove(files.size()-1); //remove the last one, as this is the output directory

      if(!output.exists()){
        log.error("output path doesn't exist");
        System.exit(1);
      }

      sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      start = System.currentTimeMillis();
      log.info("GoraCompiler: starting at "  sdf.format(start));

      for(String filename : files){ //loop for all the retrieved files
        inputFile = new File(filename);
        if(!inputFile.exists()){
          log.error("input file: "filename" doesn't exist.");
          continue; //in case the file does not exist, continue to the next file
        }

        compileSchema(inputFile, output);
      }

/**
 * {@link org.apache.gora.cassandra.store.CassandraStore} is the primary class 
 * responsible for directing Gora CRUD operations into Cassandra. We (delegate) rely 
 * heavily on {@ link org.apache.gora.cassandra.store.CassandraClient} for many operations
 * such as initialization, creating and deleting schemas (Cassandra Keyspaces), etc.  
 */
  
  /** Logging implementation */
  /** The default constructor for CassandraStore */
 
  /** 
   * Initialize is called when then the call to 
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore(Class<D> dataStoreClass, Class<K> keyClass, Class<T> persistent, org.apache.hadoop.conf.Configuration conf)}
   * is made. In this case, we merely delegate the store initialization to the 
   * {@link org.apache.gora.cassandra.store.CassandraClient#initialize(Class<K> keyClass, Class<T> persistentClass)}. 
   */
  /**
   * When executing Gora Queries in Cassandra we query the Cassandra keyspace by families.
   * When add sub/supercolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
  
  /**
   * When we add subcolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
  /**
   * When we add supercolumns, Gora keys are mapped to Cassandra partition keys only. 
   * This is because we follow the Cassandra logic where column family data is 
   * partitioned across nodes based on row Key.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return Employee created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Name.
   * @return Utf8 representing Employee Name.
   */
  
  /**
   * Sets the Name.
   * @param value containing Employee Name.
   */
  
  /**
   * Gets the DateOfBirth.
   * @return long representing Employee DateOfBirth.
   */
  
  /**
   * Sets the DateOfBirth.
   * @param value containing Employee DateOfBirth.
   */
  
  /**
   * Gets the Ssn.
   * @return Utf8 representing Employee Ssn.
   */
  
  /**
   * Sets the Ssn.
   * @param value containing Employee Ssn.
   */
  
  /**
   * Gets the Salary.
   * @return int representing Employee Salary.
   */
  
  /**
   * Sets the Salary.
   * @param value containing Employee Salary.
   */
  
  /**
   * Gets Boss.
   * @return the Object value.
   */
  
  /**
   * Sets the Boss.
   * @param the Boss value to be set.
   */
  
  /**
   * Sets the Boss.
   * @param the Boss value to be set.
   */
  
  /**
   * Gets Webpage.
   * @return the WebPage value.
   */
  
  /**
   * Sets the Webpage.
   * @param the Webpage value to be set.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return Metadata created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Version.
   * @return int representing Metadata Version.
   */
  
  /**
   * Sets the Version.
   * @param value containing Metadata Version.
   */
  
  /**
   * Gets Data.
   * @return Map containing Data value.
   */
  
  /**
   * Gets the Data's value using a key.
   * @param key gets a specific Data using a MetadataID.
   * @return Utf8 containing Data value.
   */
  
  /**
   * Adds a Data into a Metadata.
   * @param Map containing Data value.
   */
  
  /**
   * Removes Data from a Metadata.
   * @return key Metadata ID to be removed.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return TokenDatum created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Count.
   * @return int representing TokenDatum Count.
   */
  
  /**
   * Sets the Count.
   * @param value containing TokenDatum Count.
   */
  
  /**
   * Variable holding the data bean schema.
   */
  
  /**
   * Enum containing all data bean's fields.
   */
    
    /**
     * Field's index.
     */
    
    /**
     * Field's name.
     */
    
    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    
    /**
     * Gets field's index.
     * @return int field's index.
     */
    
    /**
     * Gets field's name.
     * @return String field's name.
     */
    
    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    
    /**
     * Contains all field's names.
     */
  
  /**
   * Default Constructor
   */
  
  /**
   * Constructor
   * @param stateManager for the data bean.
   */
  
  /**
   * Returns a new instance by using a state manager.
   * @param stateManager for the data bean.
   * @return WebPage created.
   */
  
  /**
   * Returns the schema of the data bean.
   * @return Schema for the data bean.
   */
  
  /**
   * Gets a specific field.
   * @param field index of a field for the data bean.
   * @return Object representing a data bean's field.
   */
  
  /**
   * Puts a value for a specific field.
   * @param field index of a field for the data bean.
   * @param value value of a field for the data bean.
   */
  
  /**
   * Gets the Url.
   * @return Utf8 representing WebPage Url.
   */
  
  /**
   * Sets the Url.
   * @param value containing WebPage Url.
   */
  
  /**
   * Gets Content.
   * @return the ByteBuffer value.
   */
  
  /**
   * Sets the Content.
   * @param the Content value to be set.
   */
  
  /**
   * Gets the ParsedContent array.
   * @return GenericArray<Utf8> containing Utf8 elements.
   */
  
  /**
   * Adds a Utf8 element into the array.
   * @param the Utf8 element to be added.
   */
  
  /**
   * Gets Outlinks.
   * @return Map containing Outlinks value.
   */
  
  /**
   * Gets the Outlinks's value using a key.
   * @param key gets a specific Outlinks using a WebPageID.
   * @return Utf8 containing Outlinks value.
   */
  
  /**
   * Adds a Outlinks into a WebPage.
   * @param Map containing Outlinks value.
   */
  
  /**
   * Removes Outlinks from a WebPage.
   * @return key WebPage ID to be removed.
   */
  
  /**
   * Gets the Metadata.
   * @return Metadata representing WebPage Metadata.
   */
  
  /**
   * Sets the Metadata.
   * @param value containing WebPage Metadata.
   */
import java.util.Arrays;
  private final static String DEFAULT_SCHEMA_EXTENTION = ".avsc";
  /**
   * Method that adds javadoc to the generated data beans.
   * @param indent  Specifies the indentation for the javadoc
   * @param javadoc The javadoc to be added. Use \n to span the javadoc
   *                to multiple lines.
   * @throws IOException
   */
  private void addJavaDoc(int indent, String javadoc) throws IOException {

    if (javadoc==null)
      return;

    if (indent<0)
      return;

    line(indent, "");
    line(indent, "/**");

    if (javadoc.contains("\n")){
      String javadocLines[] = javadoc.split("\n");

      for(String line : javadocLines)
        line(indent, " * "line);
    }
    else
      line(indent," * "javadoc);

    line(indent, " */");
  }

        addJavaDoc(0,schema.getDoc());
        line(0, "@SuppressWarnings(\"all\")");

        addJavaDoc(1,"Variable holding the data bean schema.");
        addJavaDoc(1,"Enum containing all data bean's fields.");

        addJavaDoc(2,"Field's index.");

        addJavaDoc(2,"Field's name.");

        addJavaDoc(2,"Field's constructor\n"
                     "@param index field's index.\n"
                     "@param name field's name.");

        addJavaDoc(2,"Gets field's index.\n"
                     "@return int field's index.");

        addJavaDoc(2,"Gets field's name.\n"
                "@return String field's name.");

        addJavaDoc(2,"Gets field's attributes to string.\n"
                "@return String field's attributes to string.");
        addJavaDoc(2,"Contains all field's names.");
          addJavaDoc(1,field.doc());
        addJavaDoc(1,"Default Constructor");

        addJavaDoc(1,"Constructor\n@param stateManager for the data bean.");
        addJavaDoc(1,"Returns a new instance by using a state manager.\n"
                     "@param stateManager for the data bean.\n"
                     "@return "schema.getName()" created.");
        addJavaDoc(1,"Returns the schema of the data bean.\n"
                     "@return Schema for the data bean.");

        addJavaDoc(1,"Gets a specific field.\n"
                     "@param field index of a field for the data bean.\n"
                     "@return Object representing a data bean's field.");

        addJavaDoc(1,"Puts a value for a specific field.\n"
                     "@param field index of a field for the data bean.\n"
                     "@param value value of a field for the data bean.");
            addJavaDoc(1,"Gets the "camelKey".\n"
                         "@return "unboxed" representing "schema.getName()" "camelKey".");

            addJavaDoc(1,"Sets the "camelKey".\n"
                         "@param value containing "schema.getName()" "camelKey".");

            addJavaDoc(1,"Gets the "camelKey" array.\n"
                    "@return GenericArray<"fieldType"> containing "fieldType" elements.");

            addJavaDoc(1,"Adds a "unboxed" element into the array.\n"
                    "@param the "unboxed" element to be added.");

            addJavaDoc(1,"Gets "camelKey".\n"
                    "@return Map containing "camelKey" value.");

            addJavaDoc(1,"Gets the "camelKey"'s value using a key.\n"
                    "@param key gets a specific "camelKey" using a "schema.getName()"ID.\n"
                    "@return "fieldType" containing "camelKey" value.");

            addJavaDoc(1,"Adds a "camelKey" into a "schema.getName()".\n"
                    "@param Map containing "camelKey" value.");

            addJavaDoc(1,"Removes "camelKey" from a "schema.getName()".\n"
                    "@return key "schema.getName()" ID to be removed.");

            addJavaDoc(1,"Gets "camelKey".\n"
                    "@return the "unbox(field.schema())" value.");

              addJavaDoc(1,"Sets the "camelKey".\n"
                      "@param the "camelKey" value to be set.");
              "\t\t      generated Java file. Current options include; \n" 
              "\t\t  ASLv2   (Apache Software License v2.0) \n" 
              "\t\t  AGPLv3  (GNU Affero General Public License)\n" 
              "\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n" 
              "\t\t  FDLv13  (GNU Free Documentation License v1.3)\n" 
              "\t\t  GPLv1   (GNU General Public License v1.0)\n" 
              "\t\t  GPLv2   (GNU General Public License v2.0)\n" 
              "\t\t  GPLv3   (GNU General Public License v3.0)\n " 
              "\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n" 
              "\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;

    SimpleDateFormat sdf;
    File inputFile;
    File output;
    long start;

      }

    if (args.length==2){ //case of single file or single directory
      inputFile = new File(args[0]);
      output = new File(args[1]);

      if(!inputFile.exists() || !output.exists()){
        log.error("input file path or output file path doesn't exist.");
        System.exit(1);
      }

      sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      start = System.currentTimeMillis();
      log.info("GoraCompiler: starting at "  sdf.format(start));
      if(inputFile.isDirectory()) {
        ArrayList<File> inputSchemas = new ArrayList<File>();
        File[] listOfFiles= inputFile.listFiles();

        if ( (listOfFiles!=null) && (listOfFiles.length>0)){
          for (File file : listOfFiles) {
            if (file.isFile() && file.exists() && file.getName().endsWith(DEFAULT_SCHEMA_EXTENTION)) {
              inputSchemas.add(file);
            }
          }

          compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
        }
        else{
          log.info("Path contains no files. Nothing to compile.");
        }
      }
      else if (inputFile.isFile()) {
        compileSchema(inputFile, output);
      }
    else{ //case of dynamic filename extension (such as *.* or *.json)
      List<String> files = new ArrayList<String>(Arrays.asList(args));
      output = new File(files.get(files.size()-1));
      files.remove(files.size()-1); //remove the last one, as this is the output directory

      if(!output.exists()){
        log.error("output path doesn't exist");
        System.exit(1);
      }

      sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      start = System.currentTimeMillis();
      log.info("GoraCompiler: starting at "  sdf.format(start));

      for(String filename : files){ //loop for all the retrieved files
        inputFile = new File(filename);
        if(!inputFile.exists()){
          log.error("input file: "filename" doesn't exist.");
          continue; //in case the file does not exist, continue to the next file
        }

        compileSchema(inputFile, output);
      }

}
        long writeCapacUnits = Long.parseLong(tableElement.getAttributeValue("writecunit"));
public class SolrStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  private static final Logger LOG = LoggerFactory.getLogger( SolrStore.class );
  protected static final String DEFAULT_MAPPING_FILE = "gora-solr-mapping.xml";
  protected static final String SOLR_URL_PROPERTY = "solr.url";
  protected static final String SOLR_CONFIG_PROPERTY = "solr.config";
  protected static final String SOLR_SCHEMA_PROPERTY = "solr.schema";
  protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batchSize";
  //protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";
  protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commitWithin";
  protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.resultsSize";
    
  protected static final int DEFAULT_BATCH_SIZE = 100;
  protected static final int DEFAULT_COMMIT_WITHIN = 1000;
  protected static final int DEFAULT_RESULTS_SIZE = 100;
  private SolrMapping mapping;
  private String solrServerUrl, solrConfig, solrSchema;
  private SolrServer server, adminServer;
  private ArrayList<SolrInputDocument> batch;
  private int batchSize = DEFAULT_BATCH_SIZE;
  private int commitWithin = DEFAULT_COMMIT_WITHIN;
  private int resultsSize = DEFAULT_RESULTS_SIZE;

  @Override
  public void initialize( Class<K> keyClass, Class<T> persistentClass, Properties properties ) {
    super.initialize( keyClass, persistentClass, properties );
    String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
    try {
      mapping = readMapping( mappingFile );
    }
    catch ( IOException e ) {
      LOG.error( e.getMessage() );
      LOG.error( e.getStackTrace().toString() );
    solrServerUrl = DataStoreFactory.findProperty( properties, this, SOLR_URL_PROPERTY, null );
    solrConfig = DataStoreFactory.findProperty( properties, this, SOLR_CONFIG_PROPERTY, null );
    solrSchema = DataStoreFactory.findProperty( properties, this, SOLR_SCHEMA_PROPERTY, null );
    LOG.info( "Using Solr server at "  solrServerUrl );
    adminServer = new HttpSolrServer( solrServerUrl );
    server = new HttpSolrServer( solrServerUrl  "/"  mapping.getCoreName() );
    if ( autoCreateSchema ) {
      createSchema();
    }
    String batchSizeString = DataStoreFactory.findProperty( properties, this, SOLR_BATCH_SIZE_PROPERTY, null );
    if ( batchSizeString != null ) {
      try {
        batchSize = Integer.parseInt( batchSizeString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid batch size '"  batchSizeString  "', using default "  DEFAULT_BATCH_SIZE );
      }
    }
    batch = new ArrayList<SolrInputDocument>( batchSize );
    String commitWithinString = DataStoreFactory.findProperty( properties, this, SOLR_COMMIT_WITHIN_PROPERTY, null );
    if ( commitWithinString != null ) {
      try {
        commitWithin = Integer.parseInt( commitWithinString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid commit within '"  commitWithinString  "', using default "  DEFAULT_COMMIT_WITHIN );
      }
    }
    String resultsSizeString = DataStoreFactory.findProperty( properties, this, SOLR_RESULTS_SIZE_PROPERTY, null );
    if ( resultsSizeString != null ) {
      try {
        resultsSize = Integer.parseInt( resultsSizeString );
      } catch ( NumberFormatException nfe ) {
        LOG.warn( "Invalid results size '"  resultsSizeString  "', using default "  DEFAULT_RESULTS_SIZE );
      }
    }
  }
  @SuppressWarnings("unchecked")
  private SolrMapping readMapping( String filename ) throws IOException {
    SolrMapping map = new SolrMapping();
    try {
      SAXBuilder builder = new SAXBuilder();
      Document doc = builder.build( getClass().getClassLoader().getResourceAsStream( filename ) );
      List<Element> classes = doc.getRootElement().getChildren( "class" );
      for ( Element classElement : classes ) {
        if ( classElement.getAttributeValue( "keyClass" ).equals( keyClass.getCanonicalName() )
            && classElement.getAttributeValue( "name" ).equals( persistentClass.getCanonicalName() ) ) {
          String tableName = getSchemaName( classElement.getAttributeValue( "table" ), persistentClass );
          map.setCoreName( tableName );
          Element primaryKeyEl = classElement.getChild( "primarykey" );
          map.setPrimaryKey( primaryKeyEl.getAttributeValue( "column" ) );
          List<Element> fields = classElement.getChildren( "field" );

          for ( Element field : fields ) {
            String fieldName = field.getAttributeValue( "name" );
            String columnName = field.getAttributeValue( "column" );
            map.addField( fieldName, columnName );
          }
          break;
      }
    } catch ( Exception ex ) {
      throw new IOException( ex );
    return map;
  }
  public SolrMapping getMapping() {
    return mapping;
  }
  @Override
  public String getSchemaName() {
    return mapping.getCoreName();
  }
  @Override
  public void createSchema() {
    try {
      if ( !schemaExists() )
          CoreAdminRequest.createCore( mapping.getCoreName(), mapping.getCoreName(), adminServer, solrConfig,
          solrSchema );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
  }
  @Override
  /** Default implementation deletes and recreates the schema*/
  public void truncateSchema() {
    try {
      server.deleteByQuery( "*:*" );
      server.commit();
    } catch ( Exception e ) {
      // ignore?
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
  }
  @Override
  public void deleteSchema() {
    // XXX should this be only in truncateSchema ???
    try {
      server.deleteByQuery( "*:*" );
      server.commit();
    } catch ( Exception e ) {
    // ignore?
    // LOG.error(e.getMessage());
    // LOG.error(e.getStackTrace().toString());
    try {
      CoreAdminRequest.unloadCore( mapping.getCoreName(), adminServer );
    } catch ( Exception e ) {
      if ( e.getMessage().contains( "No such core" ) ) {
        return; // it's ok, the core is not there
      } else {
        LOG.error( e.getMessage(), e.getStackTrace().toString() );
      }
  }
  @Override
  public boolean schemaExists() {
    boolean exists = false;
    try {
      CoreAdminResponse rsp = CoreAdminRequest.getStatus( mapping.getCoreName(), adminServer );
      exists = rsp.getUptime( mapping.getCoreName() ) != null;
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return exists;
  }
  private static final String toDelimitedString( String[] arr, String sep ) {
    if ( arr == null || arr.length == 0 ) {
      return "";
    }
    StringBuilder sb = new StringBuilder();
    for ( int i = 0; i < arr.length; i ) {
      if ( i > 0 )
        sb.append( sep );
        sb.append( arr[i] );
    }
    return sb.toString();
  }

  public static String escapeQueryKey( String key ) {
    if ( key == null ) {
      return null;
    }
    StringBuilder sb = new StringBuilder();
    for ( int i = 0; i < key.length(); i ) {
      char c = key.charAt( i );
      switch ( c ) {
        case ':':
        case '*':
          sb.append( "\\"  c );
          break;
        default:
        sb.append( c );
      }
    }
    return sb.toString();
  }

  @Override
  public T get( K key, String[] fields ) {
    ModifiableSolrParams params = new ModifiableSolrParams();
    params.set( CommonParams.QT, "/get" );
    params.set( CommonParams.FL, toDelimitedString( fields, "," ) );
    params.set( "id",  key.toString() );
    try {
      QueryResponse rsp = server.query( params );
      Object o = rsp.getResponse().get( "doc" );
      if ( o == null ) {
      }
      return newInstance( (SolrDocument)o, fields );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return null;
  }
  public T newInstance( SolrDocument doc, String[] fields )
      throws IOException {
    T persistent = newPersistent();
    if ( fields == null ) {
      fields = fieldMap.keySet().toArray( new String[fieldMap.size()] );
    String pk = mapping.getPrimaryKey();
    for ( String f : fields ) {
      Field field = fieldMap.get( f );
      Schema fieldSchema = field.schema();
      String sf = null;
      if ( pk.equals( f ) ) {
        sf = f;
      } else {
        sf = mapping.getSolrField( f );                
      }
      Object sv = doc.get( sf );
      Object v;
      if ( sv == null ) {
        continue;
      }
      switch ( fieldSchema.getType() ) {
        case MAP:
        case ARRAY:
        case RECORD:
          v = IOUtils.deserialize( (byte[]) sv, datumReader, fieldSchema, persistent.get( field.pos() ) );
          persistent.put( field.pos(), v );
          break;
        case ENUM:
          v = AvroUtils.getEnumValue( fieldSchema, (String) sv );
          persistent.put( field.pos(), v );
          break;
        case FIXED:
          throw new IOException( "???" );
          // break;
        case BYTES:
          persistent.put( field.pos(), ByteBuffer.wrap( (byte[]) sv ) );
          break;
        case BOOLEAN:
        case DOUBLE:
        case FLOAT:
        case INT:
        case LONG:
          persistent.put( field.pos(), sv );
          break;
        case STRING:
          persistent.put( field.pos(), new Utf8( sv.toString() ) );
          break;
        case UNION:
          LOG.error( "Union is not supported yet" );
          break;
        default:
          LOG.error( "Unknown field type: "  fieldSchema.getType() );
      }
      persistent.setDirty( field.pos() );
    persistent.clearDirty();
    return persistent;
  }
  @Override
  public void put( K key, T persistent ) {
    Schema schema = persistent.getSchema();
    StateManager stateManager = persistent.getStateManager();
    if ( !stateManager.isDirty( persistent ) ) {
      // nothing to do
      return;
    }
    SolrInputDocument doc = new SolrInputDocument();
    // add primary key
    doc.addField( mapping.getPrimaryKey(), key );
    // populate the doc
    List<Field> fields = schema.getFields();
    for ( Field field : fields ) {
      String sf = mapping.getSolrField( field.name() );
      // Solr will append values to fields in a SolrInputDocument, even the key
      // mapping won't find the primary
      if ( sf == null ) {
        continue;
      }
      Schema fieldSchema = field.schema();
      Object v = persistent.get( field.pos() );
      if ( v == null ) {
        continue;
      }
      switch ( fieldSchema.getType() ) {
        case MAP:
        case ARRAY:
        case RECORD:
          byte[] data = null;
          try {
            data = IOUtils.serialize( datumWriter, fieldSchema, v );
          } catch ( IOException e ) {
          }
          doc.addField( sf, data );
          break;
        case BYTES:
          doc.addField( sf, ( (ByteBuffer) v ).array() );
          break;
        case ENUM:
        case STRING:
          doc.addField( sf, v.toString() );
          break;
        case BOOLEAN:
        case DOUBLE:
        case FLOAT:
        case INT:
        case LONG:
          doc.addField( sf, v );
          break;
        case UNION:
          LOG.error( "Union is not supported yet" );
          break;
        default:
          LOG.error( "Unknown field type: "  fieldSchema.getType() );
      }
    LOG.info( "DOCUMENT: "  doc );
    batch.add( doc );
    if ( batch.size() >= batchSize ) {
      try {
        add( batch, commitWithin );
        batch.clear();
      } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
      }
  }
  @Override
  public boolean delete( K key ) {
    String keyField = mapping.getPrimaryKey();
    try {
      UpdateResponse rsp = server.deleteByQuery( keyField  ":"  escapeQueryKey( key.toString() ) );
      server.commit();
      LOG.info( rsp.toString() );
      return true;
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return false;
  }
  @Override
  public long deleteByQuery( Query<K, T> query ) {
    String q = ( (SolrQuery<K, T>) query ).toSolrQuery();
    try {
      UpdateResponse rsp = server.deleteByQuery( q );
      server.commit();
      LOG.info( rsp.toString() );
    } catch ( Exception e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return 0;
  }
  @Override
  public Result<K, T> execute( Query<K, T> query ) {
    try {
      return new SolrResult<K, T>( this, query, server, resultsSize );
    } catch ( IOException e ) {
      LOG.error( e.getMessage(), e.getStackTrace().toString() );
    return null;
  }
  @Override
  public Query<K, T> newQuery() {
    return new SolrQuery<K, T>( this );
  }
  @Override
  public List<PartitionQuery<K, T>> getPartitions( Query<K, T> query )
      throws IOException {
    // TODO: implement this using Hadoop DB support

    ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<PartitionQuery<K, T>>();
    partitions.add( new PartitionQueryImpl<K, T>( query ) );

    return partitions;
  }

  @Override
  public void flush() {
    try {
      if ( batch.size() > 0 ) {
        add( batch, commitWithin );
        batch.clear();
      }
    } catch ( Exception e ) {
      LOG.error(e.getMessage(), e.getStackTrace());
  }

  @Override
  public void close() {
    // In testing, the index gets closed before the commit in flush() can happen
    // so an exception gets thrown
    //flush();
  }
  private void add(ArrayList<SolrInputDocument> batch, int commitWithin) throws SolrServerException, IOException {
    if (commitWithin == 0) {
      server.add( batch );
      server.commit( false, true, true );
    } else {
      server.add( batch, commitWithin );            
  }  
          pqi.setConf(getConf());
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    partitions.add(pqi);
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    list.add(pqi);
    IOUtils.serialize(getConf(), out, baseQuery);
      baseQuery = IOUtils.deserialize(getConf(), in, null);
      startKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
      endKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
    return conf != null ? conf : new Configuration();
    SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
    SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
        PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<K, T>(
        partition.setConf(getConf());
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    partitions.add(pqi);
          pqi.setConf(getConf());
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    partitions.add(pqi);
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    list.add(pqi);
    IOUtils.serialize(getConf(), out, baseQuery);
      baseQuery = IOUtils.deserialize(getConf(), in, null);
      startKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
      endKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
    return conf != null ? conf : new Configuration();
    SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
    SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
        PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<K, T>(
        partition.setConf(getConf());
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    partitions.add(pqi);
          pqi.setConf(getConf());
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    partitions.add(pqi);
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    list.add(pqi);
    IOUtils.serialize(getConf(), out, baseQuery);
      baseQuery = IOUtils.deserialize(getConf(), in, null);
      startKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
      endKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
    return conf != null ? conf : new Configuration();
    SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
    SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
        PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<K, T>(
        partition.setConf(getConf());
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
    pqi.setConf(getConf());
    partitions.add(pqi);
                } else {
                    throw new IllegalStateException("MongoStore doesn't support 3 types union field yet. Please update your mapping");
                } else {
                    throw new IllegalStateException("MongoStore doesn't support 3 types union field yet. Please update your mapping");
    if (obj.isNew() || obj.isDirty()) {
      // TODO: why is Nutch marking objects as new ?
      // putInsert(key, obj);
      // else if ( obj.isDirty() )
      // putUpdate(key, obj);
    } else {
      // Clear its state
      obj.clearNew();
      obj.clearDirty();
    }
    if (qUpdateSet.size() > 0) {
    }
    if (qUpdateUnset.size() > 0) {
    }
    // Execute the update (if there is at least one $set ot $unset
    if (!qUpdate.isEmpty()) {
      mongoClientColl.update(qSel, qUpdate, true, false);
    } else {
      LOG.debug("No update to perform, skip {}", key);
    }
import org.apache.gora.store.DataStoreFactory;
  private static final String SCANNER_CACHING_PROPERTIES_KEY = "scanner.caching" ;
  private static final int SCANNER_CACHING_PROPERTIES_DEFAULT = 0 ;
  
  private int scannerCaching = SCANNER_CACHING_PROPERTIES_DEFAULT ;
  
    // Set scanner caching option
    try {
      this.setScannerCaching(
          Integer.valueOf(DataStoreFactory.findProperty(this.properties, this,
              SCANNER_CACHING_PROPERTIES_KEY,
              String.valueOf(SCANNER_CACHING_PROPERTIES_DEFAULT)))) ;
    }catch(Exception e){
      LOG.error("Can not load "  SCANNER_CACHING_PROPERTIES_KEY  " from gora.properties. Setting to default value: "  SCANNER_CACHING_PROPERTIES_DEFAULT, e) ;
      this.setScannerCaching(SCANNER_CACHING_PROPERTIES_DEFAULT) ; // Default value if something is wrong
    }
    
    
    scan.setCaching(this.getScannerCaching()) ; 
    
  /**
   * Gets the Scanner Caching optimization value
   * @return The value used internally in {@link Scan#setCaching(int)}
   */
  public int getScannerCaching() {
    return this.scannerCaching ;
  }
  
  /**
   * Sets the value for Scanner Caching optimization
   * 
   * @see Scan#setCaching(int)
   * 
   * @param numRows the number of rows for caching >= 0
   * @return &lt;&lt;Fluent interface&gt;&gt;
   */
  public HBaseStore<K, T> setScannerCaching(int numRows) {
    if (numRows < 0) {
      LOG.warn("Invalid Scanner Caching optimization value. Cannot set to: "  numRows  ".") ;
      return this ;
    }
    this.scannerCaching = numRows ;
    return this ;
  }
  
import org.apache.gora.store.DataStoreFactory;
  private static final String SCANNER_CACHING_PROPERTIES_KEY = "scanner.caching" ;
  private static final int SCANNER_CACHING_PROPERTIES_DEFAULT = 0 ;
  
  private int scannerCaching = SCANNER_CACHING_PROPERTIES_DEFAULT ;
  
    // Set scanner caching option
    try {
      this.setScannerCaching(
          Integer.valueOf(DataStoreFactory.findProperty(this.properties, this,
              SCANNER_CACHING_PROPERTIES_KEY,
              String.valueOf(SCANNER_CACHING_PROPERTIES_DEFAULT)))) ;
    }catch(Exception e){
      LOG.error("Can not load "  SCANNER_CACHING_PROPERTIES_KEY  " from gora.properties. Setting to default value: "  SCANNER_CACHING_PROPERTIES_DEFAULT, e) ;
      this.setScannerCaching(SCANNER_CACHING_PROPERTIES_DEFAULT) ; // Default value if something is wrong
    }
    
    
    scan.setCaching(this.getScannerCaching()) ; 
    
  /**
   * Gets the Scanner Caching optimization value
   * @return The value used internally in {@link Scan#setCaching(int)}
   */
  public int getScannerCaching() {
    return this.scannerCaching ;
  }
  
  /**
   * Sets the value for Scanner Caching optimization
   * 
   * @see Scan#setCaching(int)
   * 
   * @param numRows the number of rows for caching >= 0
   * @return &lt;&lt;Fluent interface&gt;&gt;
   */
  public HBaseStore<K, T> setScannerCaching(int numRows) {
    if (numRows < 0) {
      LOG.warn("Invalid Scanner Caching optimization value. Cannot set to: "  numRows  ".") ;
      return this ;
    }
    this.scannerCaching = numRows ;
    return this ;
  }
  
    try{
      store.close();
    }catch(Exception e){
      LOG.warn("Exception at GoraRecordWriter.class while closing datastore."  e.getMessage());
    }
    try{
      store.put(key, (Persistent) value);
      counter.increment();
      if (counter.isModulo()) {
        LOG.info("Flushing the datastore after "  counter.getRecordsNumber()  " records");
        store.flush();
      }
    }catch(Exception e){
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore."  e.getMessage());
    }
    try{
      store.close();
    }catch(Exception e){
      LOG.warn("Exception at GoraRecordWriter.class while closing datastore."  e.getMessage());
    }
    try{
      store.put(key, (Persistent) value);
      counter.increment();
      if (counter.isModulo()) {
        LOG.info("Flushing the datastore after "  counter.getRecordsNumber()  " records");
        store.flush();
      }
    }catch(Exception e){
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore."  e.getMessage());
    }





      LOG.warn("Schema: "  schema.getName()  " is not supported. No serializer "
           "could be found. Please report this to dev@gora.apache.org");
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name());
   * Serialize value to ByteBuffer using 
   * {@link org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer#getSerializer(Object)}.
   * @param value the member value {@link java.lang.Object}.
    Serializer<Object> serializer = GoraSerializerTypeInferer.getSerializer(value);
      LOG.warn("Serializer not found for: "  value.toString());
      LOG.debug(serializer.getClass()  " selected as appropriate Serializer.");
      LOG.warn("Serialization value for: "  value.getClass().getName()  " = null");
























        case RECORD:
          PersistentBase persistent = (PersistentBase) fieldValue;
          PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
          for (Field member: fieldSchema.getFields()) {
            newRecord.put(member.pos(), persistent.get(member.pos()));
          }
          fieldValue = newRecord;
          break;
        case MAP:
          StatefulHashMap map = (StatefulHashMap) fieldValue;
          StatefulHashMap newMap = new StatefulHashMap();
          for (Object mapKey : map.keySet()) {
            newMap.put(mapKey, map.get(mapKey));
            newMap.putState(mapKey, map.getState(mapKey));
          }
          fieldValue = newMap;
          break;
        case ARRAY:
          GenericArray array = (GenericArray) fieldValue;
          ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
          Iterator iter = array.iterator();
          while (iter.hasNext()) {
            newArray.add(iter.next());
          }
          fieldValue = newArray;
          break;
        case UNION:
          // storing the union selected schema, the actual value will be stored as soon as getting out of here
          // TODO determine which schema we are using: int schemaPos = getUnionSchema(fieldValue,fieldSchema);
          // and save it p.put( p.getFieldIndex(field.name()  CassandraStore.UNION_COL_SUFIX), schemaPos);
          break;


  }
    switch (type) {
    case STRING:
    case BOOLEAN:
    case INT:
    case LONG:
    case BYTES:
    case FLOAT:
    case DOUBLE:
    case FIXED:
      this.cassandraClient.addColumn(key, field.name(), value);
      break;
    case RECORD:
      if (value != null) {
        if (value instanceof PersistentBase) {
          PersistentBase persistentBase = (PersistentBase) value;
          for (Field member: schema.getFields()) {

            // TODO: hack, do not store empty arrays
            Object memberValue = persistentBase.get(member.pos());
            if (memberValue instanceof GenericArray<?>) {
              if (((GenericArray)memberValue).size() == 0) {
                continue;
            }
            this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
        } else {
          LOG.warn("Record with value: "  value.toString()  " not supported for field: "  field.name()); 
      }
      break;
    case MAP:
      if (value != null) {
        if (value instanceof StatefulHashMap<?, ?>) {
          this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
        } else {
          LOG.warn("Map with value: "  value.toString()  " not supported for field: "  field.name());
      }
      break;
    case ARRAY:
      if (value != null) {
        if (value instanceof GenericArray<?>) {
          this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
        } else {
          LOG.warn("Array with value: "  value.toString()  " not supported for field: "  field.name());
      }
      break;
    case UNION:
      if(value != null) {
        LOG.debug("Union with value: "  value.toString()  " at index: "  getUnionSchema(value, schema)  " supported for field: "  field.name());
        this.cassandraClient.addColumn(key, field.name(), value);
      } else {
        LOG.warn("Union with value: "  value.toString()  " at index: "  getUnionSchema(value, schema)  " not supported for field: "  field.name());
      }
    default:
      LOG.warn("Type: "  type.name()  " with value: "  value.toString()  
          " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
  @SuppressWarnings("unchecked")
  @SuppressWarnings("unchecked")
  @SuppressWarnings("unchecked")
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore. "  e.getMessage());





      LOG.warn("Schema: "  schema.getName()  " is not supported. No serializer "
           "could be found. Please report this to dev@gora.apache.org");
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name());
   * Serialize value to ByteBuffer using 
   * {@link org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer#getSerializer(Object)}.
   * @param value the member value {@link java.lang.Object}.
    Serializer<Object> serializer = GoraSerializerTypeInferer.getSerializer(value);
      LOG.warn("Serializer not found for: "  value.toString());
      LOG.debug(serializer.getClass()  " selected as appropriate Serializer.");
      LOG.warn("Serialization value for: "  value.getClass().getName()  " = null");
























        case RECORD:
          PersistentBase persistent = (PersistentBase) fieldValue;
          PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
          for (Field member: fieldSchema.getFields()) {
            newRecord.put(member.pos(), persistent.get(member.pos()));
          }
          fieldValue = newRecord;
          break;
        case MAP:
          StatefulHashMap map = (StatefulHashMap) fieldValue;
          StatefulHashMap newMap = new StatefulHashMap();
          for (Object mapKey : map.keySet()) {
            newMap.put(mapKey, map.get(mapKey));
            newMap.putState(mapKey, map.getState(mapKey));
          }
          fieldValue = newMap;
          break;
        case ARRAY:
          GenericArray array = (GenericArray) fieldValue;
          ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
          Iterator iter = array.iterator();
          while (iter.hasNext()) {
            newArray.add(iter.next());
          }
          fieldValue = newArray;
          break;
        case UNION:
          // storing the union selected schema, the actual value will be stored as soon as getting out of here
          // TODO determine which schema we are using: int schemaPos = getUnionSchema(fieldValue,fieldSchema);
          // and save it p.put( p.getFieldIndex(field.name()  CassandraStore.UNION_COL_SUFIX), schemaPos);
          break;


  }
    switch (type) {
    case STRING:
    case BOOLEAN:
    case INT:
    case LONG:
    case BYTES:
    case FLOAT:
    case DOUBLE:
    case FIXED:
      this.cassandraClient.addColumn(key, field.name(), value);
      break;
    case RECORD:
      if (value != null) {
        if (value instanceof PersistentBase) {
          PersistentBase persistentBase = (PersistentBase) value;
          for (Field member: schema.getFields()) {

            // TODO: hack, do not store empty arrays
            Object memberValue = persistentBase.get(member.pos());
            if (memberValue instanceof GenericArray<?>) {
              if (((GenericArray)memberValue).size() == 0) {
                continue;
            }
            this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
        } else {
          LOG.warn("Record with value: "  value.toString()  " not supported for field: "  field.name()); 
      }
      break;
    case MAP:
      if (value != null) {
        if (value instanceof StatefulHashMap<?, ?>) {
          this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
        } else {
          LOG.warn("Map with value: "  value.toString()  " not supported for field: "  field.name());
      }
      break;
    case ARRAY:
      if (value != null) {
        if (value instanceof GenericArray<?>) {
          this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
        } else {
          LOG.warn("Array with value: "  value.toString()  " not supported for field: "  field.name());
      }
      break;
    case UNION:
      if(value != null) {
        LOG.debug("Union with value: "  value.toString()  " at index: "  getUnionSchema(value, schema)  " supported for field: "  field.name());
        this.cassandraClient.addColumn(key, field.name(), value);
      } else {
        LOG.warn("Union with value: "  value.toString()  " at index: "  getUnionSchema(value, schema)  " not supported for field: "  field.name());
      }
    default:
      LOG.warn("Type: "  type.name()  " with value: "  value.toString()  
          " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
  @SuppressWarnings("unchecked")
  @SuppressWarnings("unchecked")
  @SuppressWarnings("unchecked")
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore. "  e.getMessage());
        LOG.warn("Union with 'null' value not supported for field: "  field.name());
      LOG.warn("Type: "  type.name()  " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
        LOG.warn("Union with 'null' value not supported for field: "  field.name());
      LOG.warn("Type: "  type.name()  " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
      , String defaultValue) throws IOException {

    String mappingFilename = findProperty(properties, store, MAPPING_FILE, defaultValue);

    InputStream mappingFile = store.getClass().getClassLoader().getResourceAsStream(mappingFilename);

    if (mappingFile == null)
      throw new IOException("Unable to open mapping file: "mappingFilename);

    return mappingFilename;

      String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
      , String defaultValue) throws IOException {

    String mappingFilename = findProperty(properties, store, MAPPING_FILE, defaultValue);

    InputStream mappingFile = store.getClass().getClassLoader().getResourceAsStream(mappingFilename);

    if (mappingFile == null)
      throw new IOException("Unable to open mapping file: "mappingFilename);

    return mappingFilename;

      String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
      keyStates.remove(key);
      keyStates.remove(key);
import org.apache.gora.filter.Filter;
  
  /**
   * @param Set a filter on this query.
   */
  public void setFilter(Filter<K, T> filter);
  
  /**
   * @return The filter on this query, or <code>null</code> if none.
   */
  public Filter<K, T> getFilter();
  
  /**
   * Set whether the local filter is enabled. This is usually called by
   * data store implementations that install the filter remotely
   * (for efficiency reasons) and therefore disable the local filter.
   * @param enable
   */
  void setLocalFilterEnabled(boolean enable);
  
  /**
   * @return Whether the local filter is enabled.
   * See {@link #setLocalFilterEnabled(boolean)}.
   */
  boolean isLocalFilterEnabled();
import org.apache.gora.filter.Filter;
  
  @Override
  public Filter<K, T> getFilter() {
    return baseQuery.getFilter();
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    baseQuery.setFilter(filter);
  }
import org.apache.gora.filter.Filter;
  protected Filter<K, T> filter;
  protected boolean localFilterEnabled=true;
  
  @Override
  public Filter<K, T> getFilter() {
    return filter;
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    this.filter=filter;
  }
  
  @Override
  public boolean isLocalFilterEnabled() {
    return localFilterEnabled;
  }
  
  @Override
  public void setLocalFilterEnabled(boolean enable) {
    this.localFilterEnabled=enable;
  }
  
    if(!nullFields[4]) {
      String filterClass = Text.readString(in);
      try {
        filter = (Filter<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(filterClass), conf);
        filter.readFields(in);
      } catch (ClassNotFoundException e) {
        throw new IOException(e);
      }
    }
    localFilterEnabled = in.readBoolean(); 
    if(filter != null) {
      Text.writeString(out, filter.getClass().getCanonicalName());
      filter.write(out);
    }
    out.writeBoolean(localFilterEnabled);
      builder.append(localFilterEnabled, that.localFilterEnabled);
    builder.append(localFilterEnabled);
    builder.append("localFilterEnabled", localFilterEnabled);
import org.apache.gora.filter.Filter;
import java.io.IOException;

    if(isLimitReached()) {
      return false;
    }
      
    boolean ret;
    do {
      clear();
      persistent = getOrCreatePersistent(persistent);
      ret = nextInner();
      if (ret == false) {
        //this is the end
        break;
      }
      //we keep looping until we get a row that is not filtered out
    } while (filter(key, persistent));
    if(ret) offset;
    return ret;
  }
  
  protected boolean filter(K key, T persistent) {
    if (!query.isLocalFilterEnabled()) {
      return false;
    }
    
    Filter<K, T> filter = query.getFilter();
    if (filter == null) {
      return false;
    }
    
    return filter.filter(key, persistent);
import org.apache.gora.filter.Filter;
import java.util.Arrays;

 * Webservices implementation for {@link PartitionQuery}.
  
  @Override
  public Filter<K, T> getFilter() {
    return filter;
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    this.filter=filter;
  }
  
  @Override
  public boolean isLocalFilterEnabled() {
    return localFilterEnabled;
  }
  
  @Override
  public void setLocalFilterEnabled(boolean enable) {
    this.localFilterEnabled=enable;
  }
  
import org.apache.gora.filter.Filter;
  protected Filter<K, T> filter;
  protected boolean localFilterEnabled=true;
import org.apache.gora.filter.Filter;

  @Override
  public void setFilter(Filter<K, T> filter) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public Filter<K, T> getFilter() {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public void setLocalFilterEnabled(boolean enable) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public boolean isLocalFilterEnabled() {
    // TODO Auto-generated method stub
    return false;
  }
public class HBaseColumn {
import org.apache.gora.hbase.util.HBaseFilterUtil;
  
  private HBaseFilterUtil<K, T> filterUtil;
      filterUtil = new HBaseFilterUtil<K, T>(this.conf);
  
  public HBaseMapping getMapping() {
    return mapping;
  }
                  default :
                    break;
      String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getServerAddress().getHostname();
            = new HBaseScannerResult<K,T>(this, query, scanner);
    if (query.getFilter() != null) {
      boolean succeeded = filterUtil.setFilter(scan, query.getFilter(), this);
      if (succeeded) {
        // don't need local filter
        query.setLocalFilterEnabled(false);
      }
    }
    } else if (clazz.isArray() && clazz.getComponentType().equals(Byte.TYPE)) {
      return (byte[])o;
import org.apache.gora.filter.Filter;
  
  /**
   * @param Set a filter on this query.
   */
  public void setFilter(Filter<K, T> filter);
  
  /**
   * @return The filter on this query, or <code>null</code> if none.
   */
  public Filter<K, T> getFilter();
  
  /**
   * Set whether the local filter is enabled. This is usually called by
   * data store implementations that install the filter remotely
   * (for efficiency reasons) and therefore disable the local filter.
   * @param enable
   */
  void setLocalFilterEnabled(boolean enable);
  
  /**
   * @return Whether the local filter is enabled.
   * See {@link #setLocalFilterEnabled(boolean)}.
   */
  boolean isLocalFilterEnabled();
import org.apache.gora.filter.Filter;
  
  @Override
  public Filter<K, T> getFilter() {
    return baseQuery.getFilter();
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    baseQuery.setFilter(filter);
  }
import org.apache.gora.filter.Filter;
  protected Filter<K, T> filter;
  protected boolean localFilterEnabled=true;
  
  @Override
  public Filter<K, T> getFilter() {
    return filter;
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    this.filter=filter;
  }
  
  @Override
  public boolean isLocalFilterEnabled() {
    return localFilterEnabled;
  }
  
  @Override
  public void setLocalFilterEnabled(boolean enable) {
    this.localFilterEnabled=enable;
  }
  
    if(!nullFields[4]) {
      String filterClass = Text.readString(in);
      try {
        filter = (Filter<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(filterClass), conf);
        filter.readFields(in);
      } catch (ClassNotFoundException e) {
        throw new IOException(e);
      }
    }
    localFilterEnabled = in.readBoolean(); 
    if(filter != null) {
      Text.writeString(out, filter.getClass().getCanonicalName());
      filter.write(out);
    }
    out.writeBoolean(localFilterEnabled);
      builder.append(localFilterEnabled, that.localFilterEnabled);
    builder.append(localFilterEnabled);
    builder.append("localFilterEnabled", localFilterEnabled);
import org.apache.gora.filter.Filter;
import java.io.IOException;

    if(isLimitReached()) {
      return false;
    }
      
    boolean ret;
    do {
      clear();
      persistent = getOrCreatePersistent(persistent);
      ret = nextInner();
      if (ret == false) {
        //this is the end
        break;
      }
      //we keep looping until we get a row that is not filtered out
    } while (filter(key, persistent));
    if(ret) offset;
    return ret;
  }
  
  protected boolean filter(K key, T persistent) {
    if (!query.isLocalFilterEnabled()) {
      return false;
    }
    
    Filter<K, T> filter = query.getFilter();
    if (filter == null) {
      return false;
    }
    
    return filter.filter(key, persistent);
import org.apache.gora.filter.Filter;
import java.util.Arrays;

 * Webservices implementation for {@link PartitionQuery}.
  
  @Override
  public Filter<K, T> getFilter() {
    return filter;
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    this.filter=filter;
  }
  
  @Override
  public boolean isLocalFilterEnabled() {
    return localFilterEnabled;
  }
  
  @Override
  public void setLocalFilterEnabled(boolean enable) {
    this.localFilterEnabled=enable;
  }
  
import org.apache.gora.filter.Filter;
  protected Filter<K, T> filter;
  protected boolean localFilterEnabled=true;
import org.apache.gora.filter.Filter;

  @Override
  public void setFilter(Filter<K, T> filter) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public Filter<K, T> getFilter() {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public void setLocalFilterEnabled(boolean enable) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public boolean isLocalFilterEnabled() {
    // TODO Auto-generated method stub
    return false;
  }
public class HBaseColumn {
import org.apache.gora.hbase.util.HBaseFilterUtil;
  
  private HBaseFilterUtil<K, T> filterUtil;
      filterUtil = new HBaseFilterUtil<K, T>(this.conf);
  
  public HBaseMapping getMapping() {
    return mapping;
  }
                  default :
                    break;
      String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getServerAddress().getHostname();
            = new HBaseScannerResult<K,T>(this, query, scanner);
    if (query.getFilter() != null) {
      boolean succeeded = filterUtil.setFilter(scan, query.getFilter(), this);
      if (succeeded) {
        // don't need local filter
        query.setLocalFilterEnabled(false);
      }
    }
    } else if (clazz.isArray() && clazz.getComponentType().equals(Byte.TYPE)) {
      return (byte[])o;
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name()  ". Please report this to dev@gora.apache.org");
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name()  ". Please report this to dev@gora.apache.org");
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;

import java.util.Properties;
/**
 * CassandraClient is where all of the primary datastore functionality is 
 * executed. Typically CassandraClient is invoked by calling 
 * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}.
 * CassandraClient deals with Cassandra data model definition, mutation, 
 * and general/specific mappings.
 * @see {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)} 
 *
 * @param <K>
 * @param <T>
 */

  /** The logging implementation */


  /** Object containing the XML mapping for Cassandra. */
  /** Hector client default column family consistency level. */
  public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";

  /** Cassandra serializer to be used for serializing Gora's keys. */

  /**
   * Given our key, persistentClass from 
   * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}
   * we make best efforts to dictate our data model. 
   * We make a quick check within {@link org.apache.gora.cassandra.store.CassandraClient#checkKeyspace(String)
   * to see if our keyspace has already been invented, this simple check prevents us from 
   * recreating the keyspace if it already exists. 
   * We then simple specify (based on the input keyclass) an appropriate serializer
   * via {@link org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer} before
   * defining a mutator from and by which we can mutate this object.
   * @param keyClass the Key by which we wish o assign a record object
   * @param persistentClass the generated {@link org.apache.org.gora.persistency.Peristent} bean representing the data.
   * @throws Exception
   */
    this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), 
        new CassandraHostConfigurator(this.cassandraMapping.getHostName()));


    // Just create a Keyspace object on the client side, corresponding to an 
    // already existing keyspace with already created column families.


   * In this method, we also utilize Hector's 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel}
   * logic. 
   * It is set by passing a 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel} object right 
   * when the {@link me.prettyprint.hector.api.Keyspace} is created. 
   * If we cannot find a consistency level within <code>gora.properites</code>, 
   * then column family consistency level is set to QUORUM (by default) which permits 
   * consistency to wait for a quorum of replicas to respond regardless of data center.
   * QUORUM is Hector Client's default setting and we respect that here as well.
   * 
   * @see http://hector-client.github.io/hector/build/html/content/consistency_level.html
      List<ColumnFamilyDefinition> columnFamilyDefinitions = this.cassandraMapping.getColumnFamilyDefinitions();
      keyspaceDefinition = HFactory.createKeyspaceDefinition(this.cassandraMapping.getKeyspaceName(), 
          "org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
      // GORA-167 Create a customized Consistency Level
      ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
      // Column family consistency levels
      ccl.setReadCfConsistencyLevels(clmap);
      ccl.setWriteCfConsistencyLevels(clmap);
      // Operations consistency levels
      String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector read consistency configured to '"  opConsisLvl  "'.");
      opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector write consistency configured to '"  opConsisLvl  "'.");
      HFactory.createKeyspace("Keyspace", this.cluster, ccl);
                 ", not BytesType. It may cause a fatal error on column validation later.");

  /**
   * Method in charge of setting the consistency level for defined column families.
   * @param pColFams  Column families
   * @return Map<String, HConsistencyLevel> with the mapping between colFams and consistency level.
   */
  private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
    Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
    // Get columnFamily consistency level.
    String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
    LOG.debug("ColumnFamily consistency level configured to '"  colFamConsisLvl  "'.");
    // Define consistency for ColumnFamily "ColumnFamily"
    for (ColumnFamilyDefinition colFamDef : pColFams)
      clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
    return clMap;
  }





  public void addGenericArray(K key, String fieldName, GenericArray<?> array) {


    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = 
        HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());




    family = this.cassandraMapping.getFamily(pField);

    column = this.cassandraMapping.getColumn(pField);
   * @return a map which keys are the family names and values the corresponding column 
   * names required to get all the query fields.









    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = 
        HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), 
            ByteBufferSerializer.get(), ByteBufferSerializer.get());


    return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
  /** Consistency property level for Cassandra column families */
  private static final String COL_FAM_CL = "cf.consistency.level";

  /** Consistency property level for Cassandra read operations. */
  private static final String READ_OP_CL = "read.consistency.level";

  /** Consistency property level for Cassandra write operations. */
  private static final String WRITE_OP_CL = "write.consistency.level";

  /** Variables to hold different consistency levels defined by the properties. */
  public static String colFamConsLvl;
  public static String readOpConsLvl;
  public static String writeOpConsLvl;

  private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
  
      if (autoCreateSchema) {
        // If this is not set, then each Cassandra client should set its default
        // column family
        colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
        // operations
        readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
        writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
      }
      CassandraResultSet<K> cassandraResultSet) {
  /**
   * Checks to see if a Cassandra Keyspace actually exists.
   * Returns true if it does.
   */
          "sure to include this property in gora.properties file");
import org.apache.gora.store.DataStoreFactory;
      preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
      dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
      dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
      consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;

import java.util.Properties;
/**
 * CassandraClient is where all of the primary datastore functionality is 
 * executed. Typically CassandraClient is invoked by calling 
 * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}.
 * CassandraClient deals with Cassandra data model definition, mutation, 
 * and general/specific mappings.
 * @see {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)} 
 *
 * @param <K>
 * @param <T>
 */

  /** The logging implementation */


  /** Object containing the XML mapping for Cassandra. */
  /** Hector client default column family consistency level. */
  public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";

  /** Cassandra serializer to be used for serializing Gora's keys. */

  /**
   * Given our key, persistentClass from 
   * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}
   * we make best efforts to dictate our data model. 
   * We make a quick check within {@link org.apache.gora.cassandra.store.CassandraClient#checkKeyspace(String)
   * to see if our keyspace has already been invented, this simple check prevents us from 
   * recreating the keyspace if it already exists. 
   * We then simple specify (based on the input keyclass) an appropriate serializer
   * via {@link org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer} before
   * defining a mutator from and by which we can mutate this object.
   * @param keyClass the Key by which we wish o assign a record object
   * @param persistentClass the generated {@link org.apache.org.gora.persistency.Peristent} bean representing the data.
   * @throws Exception
   */
    this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), 
        new CassandraHostConfigurator(this.cassandraMapping.getHostName()));


    // Just create a Keyspace object on the client side, corresponding to an 
    // already existing keyspace with already created column families.


   * In this method, we also utilize Hector's 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel}
   * logic. 
   * It is set by passing a 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel} object right 
   * when the {@link me.prettyprint.hector.api.Keyspace} is created. 
   * If we cannot find a consistency level within <code>gora.properites</code>, 
   * then column family consistency level is set to QUORUM (by default) which permits 
   * consistency to wait for a quorum of replicas to respond regardless of data center.
   * QUORUM is Hector Client's default setting and we respect that here as well.
   * 
   * @see http://hector-client.github.io/hector/build/html/content/consistency_level.html
      List<ColumnFamilyDefinition> columnFamilyDefinitions = this.cassandraMapping.getColumnFamilyDefinitions();
      keyspaceDefinition = HFactory.createKeyspaceDefinition(this.cassandraMapping.getKeyspaceName(), 
          "org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
      // GORA-167 Create a customized Consistency Level
      ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
      // Column family consistency levels
      ccl.setReadCfConsistencyLevels(clmap);
      ccl.setWriteCfConsistencyLevels(clmap);
      // Operations consistency levels
      String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector read consistency configured to '"  opConsisLvl  "'.");
      opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector write consistency configured to '"  opConsisLvl  "'.");
      HFactory.createKeyspace("Keyspace", this.cluster, ccl);
                 ", not BytesType. It may cause a fatal error on column validation later.");

  /**
   * Method in charge of setting the consistency level for defined column families.
   * @param pColFams  Column families
   * @return Map<String, HConsistencyLevel> with the mapping between colFams and consistency level.
   */
  private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
    Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
    // Get columnFamily consistency level.
    String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
    LOG.debug("ColumnFamily consistency level configured to '"  colFamConsisLvl  "'.");
    // Define consistency for ColumnFamily "ColumnFamily"
    for (ColumnFamilyDefinition colFamDef : pColFams)
      clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
    return clMap;
  }





  public void addGenericArray(K key, String fieldName, GenericArray<?> array) {


    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = 
        HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());




    family = this.cassandraMapping.getFamily(pField);

    column = this.cassandraMapping.getColumn(pField);
   * @return a map which keys are the family names and values the corresponding column 
   * names required to get all the query fields.









    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = 
        HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), 
            ByteBufferSerializer.get(), ByteBufferSerializer.get());


    return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
  /** Consistency property level for Cassandra column families */
  private static final String COL_FAM_CL = "cf.consistency.level";

  /** Consistency property level for Cassandra read operations. */
  private static final String READ_OP_CL = "read.consistency.level";

  /** Consistency property level for Cassandra write operations. */
  private static final String WRITE_OP_CL = "write.consistency.level";

  /** Variables to hold different consistency levels defined by the properties. */
  public static String colFamConsLvl;
  public static String readOpConsLvl;
  public static String writeOpConsLvl;

  private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
  
      if (autoCreateSchema) {
        // If this is not set, then each Cassandra client should set its default
        // column family
        colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
        // operations
        readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
        writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
      }
      CassandraResultSet<K> cassandraResultSet) {
  /**
   * Checks to see if a Cassandra Keyspace actually exists.
   * Returns true if it does.
   */
          "sure to include this property in gora.properties file");
import org.apache.gora.store.DataStoreFactory;
      preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
      dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
      dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
      consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
    if (ttlAttr == null) {
      ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
    }
      HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer, ttlAttr);
    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
    if (ttlAttr  == null) {
      ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
    }
      HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer, ttlAttr);
  /**
   * Deletes a subColumn which is a field inside a column.
   * @param key Identifying the row.
   * @param fieldName The field's name.
   * @param columnName The column's name.
   */
  /**
   * Delete a row within the keyspace.
    * @param key
    * @param fieldName
    * @param columnName
    */
  public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
    synchronized(mutator) {
      HectorUtils.deleteColumn(mutator, key, familyName, columnName);
      }
    }

  /**
   * Delete all content related to a key.
   * @param key
   */
  public void deleteByKey(K key) {
    Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
    deleteColumn(key, familyMap.values().iterator().next().toString(), null);
  }
  private static final String GCGRACE_SECONDS_ATTRIBUTE = "gc_grace_seconds";
  private static final String COLUMNS_TTL_ATTRIBUTE = "ttl";
  public static final String DEFAULT_COLUMNS_TTL = "60";
  public static final int DEFAULT_GCGRACE_SECONDS = 30;
   * Helps storing attributes defined for each field.
   */
  private Map<String, String> columnAttrMap = new HashMap<String, String>();
  
  /**
      LOG.error("Error locating Cassandra Keyspace name attribute!");
      String gcgrace_scs = element.getAttributeValue(GCGRACE_SECONDS_ATTRIBUTE);
      if (gcgrace_scs == null) {
        LOG.warn("Error locating gc_grace_seconds attribute for '"  familyName  "' column family");
        LOG.warn("Using default set to: "  DEFAULT_GCGRACE_SECONDS);
      } else {
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located gc_grace_seconds: '"  gcgrace_scs  "'" );
        }
      }
      cfDef.setGcGraceSeconds(gcgrace_scs!=null?Integer.parseInt(gcgrace_scs):DEFAULT_GCGRACE_SECONDS);
      String ttlValue = element.getAttributeValue(COLUMNS_TTL_ATTRIBUTE);

      if (ttlValue == null) {
        LOG.warn("TTL value is not defined for \""  fieldName  "\" field. \n Using default value: "  DEFAULT_COLUMNS_TTL);
      }
      // TODO we should find a way of storing more values into this map i.e. more column attributes
      this.columnAttrMap.put(columnName, ttlValue!=null?ttlValue:DEFAULT_COLUMNS_TTL);
    }
  /**
   * Gets the columnFamily related to the column name.
   * @param name
   * @return
   */
  /**
   * Gets the column related to a field.
   * @param name
   * @return
   */
   * Gets all the columnFamilies defined.
   * @return
   */
  public Map<String,String> getFamilyMap(){
    return this.familyMap;
  }

  /**
   * Gets all attributes related to a column.
   * @return
   */
  public Map<String, String> getColumnsAttribs(){
    return this.columnAttrMap;
  }

  /**
    this.cassandraClient.deleteByKey(key);
    return true;
import me.prettyprint.hector.api.mutation.MutationResult;
  /** Methods to insert columns. */
  public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
  public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, String columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
  /** Methods to create columns. */
  public static<K> HColumn<ByteBuffer,ByteBuffer> createColumn(ByteBuffer name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  public static<K> HColumn<String,ByteBuffer> createColumn(String name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  public static<K> HColumn<Integer,ByteBuffer> createColumn(Integer name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  /** Methods to create subColumns. */
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  /** Methods to delete subColumns. */
  /** Methods do delete columns. */
  public static<K> void deleteColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName){
    MutationResult mr = mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
    System.out.println(mr.toString());
  }
  /** Methods to create superColumns. */
  public static<K> HSuperColumn<String,ByteBuffer,ByteBuffer> createSuperColumn(String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
  public static<K> HSuperColumn<String,String,ByteBuffer> createSuperColumn(String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), StringSerializer.get(), ByteBufferSerializer.get());
  public static<K> HSuperColumn<String,Integer,ByteBuffer> createSuperColumn(String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), IntegerSerializer.get(), ByteBufferSerializer.get());
    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
    if (ttlAttr == null) {
      ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
    }
      HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer, ttlAttr);
    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
    if (ttlAttr  == null) {
      ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
    }
      HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer, ttlAttr);
  /**
   * Deletes a subColumn which is a field inside a column.
   * @param key Identifying the row.
   * @param fieldName The field's name.
   * @param columnName The column's name.
   */
  /**
   * Delete a row within the keyspace.
    * @param key
    * @param fieldName
    * @param columnName
    */
  public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
    synchronized(mutator) {
      HectorUtils.deleteColumn(mutator, key, familyName, columnName);
      }
    }

  /**
   * Delete all content related to a key.
   * @param key
   */
  public void deleteByKey(K key) {
    Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
    deleteColumn(key, familyMap.values().iterator().next().toString(), null);
  }
  private static final String GCGRACE_SECONDS_ATTRIBUTE = "gc_grace_seconds";
  private static final String COLUMNS_TTL_ATTRIBUTE = "ttl";
  public static final String DEFAULT_COLUMNS_TTL = "60";
  public static final int DEFAULT_GCGRACE_SECONDS = 30;
   * Helps storing attributes defined for each field.
   */
  private Map<String, String> columnAttrMap = new HashMap<String, String>();
  
  /**
      LOG.error("Error locating Cassandra Keyspace name attribute!");
      String gcgrace_scs = element.getAttributeValue(GCGRACE_SECONDS_ATTRIBUTE);
      if (gcgrace_scs == null) {
        LOG.warn("Error locating gc_grace_seconds attribute for '"  familyName  "' column family");
        LOG.warn("Using default set to: "  DEFAULT_GCGRACE_SECONDS);
      } else {
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located gc_grace_seconds: '"  gcgrace_scs  "'" );
        }
      }
      cfDef.setGcGraceSeconds(gcgrace_scs!=null?Integer.parseInt(gcgrace_scs):DEFAULT_GCGRACE_SECONDS);
      String ttlValue = element.getAttributeValue(COLUMNS_TTL_ATTRIBUTE);

      if (ttlValue == null) {
        LOG.warn("TTL value is not defined for \""  fieldName  "\" field. \n Using default value: "  DEFAULT_COLUMNS_TTL);
      }
      // TODO we should find a way of storing more values into this map i.e. more column attributes
      this.columnAttrMap.put(columnName, ttlValue!=null?ttlValue:DEFAULT_COLUMNS_TTL);
    }
  /**
   * Gets the columnFamily related to the column name.
   * @param name
   * @return
   */
  /**
   * Gets the column related to a field.
   * @param name
   * @return
   */
   * Gets all the columnFamilies defined.
   * @return
   */
  public Map<String,String> getFamilyMap(){
    return this.familyMap;
  }

  /**
   * Gets all attributes related to a column.
   * @return
   */
  public Map<String, String> getColumnsAttribs(){
    return this.columnAttrMap;
  }

  /**
    this.cassandraClient.deleteByKey(key);
    return true;
import me.prettyprint.hector.api.mutation.MutationResult;
  /** Methods to insert columns. */
  public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
  public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, String columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
  /** Methods to create columns. */
  public static<K> HColumn<ByteBuffer,ByteBuffer> createColumn(ByteBuffer name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  public static<K> HColumn<String,ByteBuffer> createColumn(String name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  public static<K> HColumn<Integer,ByteBuffer> createColumn(Integer name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  /** Methods to create subColumns. */
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  /** Methods to delete subColumns. */
  /** Methods do delete columns. */
  public static<K> void deleteColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName){
    MutationResult mr = mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
    System.out.println(mr.toString());
  }
  /** Methods to create superColumns. */
  public static<K> HSuperColumn<String,ByteBuffer,ByteBuffer> createSuperColumn(String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
  public static<K> HSuperColumn<String,String,ByteBuffer> createSuperColumn(String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), StringSerializer.get(), ByteBufferSerializer.get());
  public static<K> HSuperColumn<String,Integer,ByteBuffer> createSuperColumn(String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), IntegerSerializer.get(), ByteBufferSerializer.get());
          LOG.debug("Keyclass and nameclass match.");
          //TODO this might not be the desired behavior as the user might have actually made a mistake.
            LOG.warn("Mismatching schema's names. Mappingfile schema: '"  tableNameFromMapping 
                 "'. PersistentClass schema's name: '"  tableName  "'"
                 "Assuming they are the same.");
}
          LOG.debug("Keyclass and nameclass match.");
          //TODO this might not be the desired behavior as the user might have actually made a mistake.
            LOG.warn("Mismatching schema's names. Mappingfile schema: '"  tableNameFromMapping 
                 "'. PersistentClass schema's name: '"  tableName  "'"
                 "Assuming they are the same.");
}
        } else {
          LOG.error("KeyClass in gora-hbase-mapping is not the same as the one in the databean.");
        } else {
          LOG.error("KeyClass in gora-hbase-mapping is not the same as the one in the databean.");
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
    key = (K) ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
import java.util.concurrent.TimeUnit;
import org.apache.accumulo.core.client.BatchWriterConfig;
import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;
import org.apache.accumulo.core.client.security.tokens.PasswordToken;
import org.apache.accumulo.core.security.thrift.TCredentials;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericData;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.EncoderFactory;
import org.apache.gora.accumulo.encoders.BinaryEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;

  private TCredentials credentials;


  public Object fromBytes(Schema schema, byte data[]) throws GoraException {
    Schema fromSchema = null;
    if (schema.getType() == Type.UNION) {
      try {
        Decoder decoder = DecoderFactory.get().binaryDecoder(data, null);
        int unionIndex = decoder.readIndex();
        List<Schema> possibleTypes = schema.getTypes();
        fromSchema = possibleTypes.get(unionIndex);
        Schema effectiveSchema = possibleTypes.get(unionIndex);
        if (effectiveSchema.getType() == Type.NULL) {
          decoder.readNull();
          return null;
        } else {
          data = decoder.readBytes(null).array();
        }
      } catch (IOException e) {
        e.printStackTrace();
        throw new GoraException("Error decoding union type: ", e);
      }
    } else {
      fromSchema = schema;
    }
    return fromBytes(encoder, fromSchema, data);
    case BOOLEAN:
      return encoder.decodeBoolean(data);
    case DOUBLE:
      return encoder.decodeDouble(data);
    case FLOAT:
      return encoder.decodeFloat(data);
    case INT:
      return encoder.decodeInt(data);
    case LONG:
      return encoder.decodeLong(data);
    case STRING:
      return new Utf8(data);
    case BYTES:
      return ByteBuffer.wrap(data);
    case ENUM:
      return AvroUtils.getEnumValue(schema, encoder.decodeInt(data));
    case ARRAY:
      break;
    case FIXED:
      break;
    case MAP:
      break;
    case NULL:
      break;
    case RECORD:
      break;
    case UNION:
      break;
    default:
      break;


  public byte[] toBytes(Schema toSchema, Object o) {
    if (toSchema != null && toSchema.getType() == Type.UNION) {
      ByteArrayOutputStream baos = new ByteArrayOutputStream();
      org.apache.avro.io.BinaryEncoder avroEncoder = EncoderFactory.get().binaryEncoder(baos, null);
      int unionIndex = 0;
      try {
        if (o == null) {
          unionIndex = firstNullSchemaTypeIndex(toSchema);
          avroEncoder.writeIndex(unionIndex);
          avroEncoder.writeNull();
        } else {
          unionIndex = firstNotNullSchemaTypeIndex(toSchema);
          avroEncoder.writeIndex(unionIndex);
          avroEncoder.writeBytes(toBytes(o));
        }
        avroEncoder.flush();
        return baos.toByteArray();
      } catch (IOException e) {
        e.printStackTrace();
        return toBytes(o);
      }
    } else {     
      return toBytes(o);
    }
  }

  private int firstNullSchemaTypeIndex(Schema toSchema) {
    List<Schema> possibleTypes = toSchema.getTypes();
    int unionIndex = 0;
    for (int i = 0; i < possibleTypes.size(); i ) {
      Type pType = possibleTypes.get(i).getType();
      if (pType == Type.NULL) { // FIXME HUGE kludge to pass tests
        unionIndex = i; break;
      }
    }
    return unionIndex;
  }

  private int firstNotNullSchemaTypeIndex(Schema toSchema) {
    List<Schema> possibleTypes = toSchema.getTypes();
    int unionIndex = 0;
    for (int i = 0; i < possibleTypes.size(); i ) {
      Type pType = possibleTypes.get(i).getType();
      if (pType != Type.NULL) { // FIXME HUGE kludge to pass tests
        unionIndex = i; break;
      }
    }
    return unionIndex;
  }



        return copyIfNeeded(((Utf8) o).getBytes(), 0, ((Utf8) o).getByteLength());
        return encoder.encodeInt(((Enum<?>) o).ordinal());

        BatchWriterConfig batchWriterConfig = new BatchWriterConfig();
        batchWriterConfig.setMaxMemory(10000000);
        batchWriterConfig.setMaxLatency(60000l, TimeUnit.MILLISECONDS);
        batchWriterConfig.setMaxWriteThreads(4);
        batchWriter = conn.createBatchWriter(mapping.tableName, batchWriterConfig);



        encoder = new BinaryEncoder();

        AuthenticationToken token =  new PasswordToken(password);
          credentials = new TCredentials(user, 
              "org.apache.accumulo.core.client.security.tokens.PasswordToken", 
              ByteBuffer.wrap(password.getBytes()), instance);
          conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, token);
          conn = new MockInstance().getConnector(user, new PasswordToken(password));
          credentials = new TCredentials(user, 
              "org.apache.accumulo.core.client.security.tokens.PasswordToken", 
              ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());







        throw new GoraException("Please define the accumulo 'table' name mapping in "  filename  " for "  persistentClass.getCanonicalName());




    Map<Utf8, Object> currentMap = null;
    List currentArray = null;
    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(new byte[0], null);


      if (row == null) {
        row = entry.getKey().getRowData();
      }
      byte[] val = entry.getValue().get();

      Field field = fieldMap.get(getFieldName(entry));

          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
              fromBytes(currentSchema, entry.getValue().get()));
          persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
      case MAP:  // first entry only. Next are handled above on the next loop
        currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
        currentPos = field.pos();
        currentFam = entry.getKey().getColumnFamily();
        currentSchema = field.schema().getValueType();
        currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
            fromBytes(currentSchema, entry.getValue().get()));
        break;
      case ARRAY:
        currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
        currentPos = field.pos();
        currentFam = entry.getKey().getColumnFamily();
        currentSchema = field.schema().getElementType();
        currentField = field;

        currentArray.add(fromBytes(currentSchema, entry.getValue().get()));

        break;
      case UNION:// default value of null acts like union with null
        Schema effectiveSchema = field.schema().getTypes()
        .get(firstNotNullSchemaTypeIndex(field.schema()));
        // map and array were coded without union index so need to be read the same way
        if (effectiveSchema.getType() == Type.ARRAY) {
          currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
          currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
        }
        else if (effectiveSchema.getType() == Type.MAP) {
          currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
          currentPos = field.pos();
          currentFam = entry.getKey().getColumnFamily();
          currentSchema = effectiveSchema.getValueType();

          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
              fromBytes(currentSchema, entry.getValue().get()));
        }
        // continue like a regular top-level union
      case RECORD:
        SpecificDatumReader<?> reader = new SpecificDatumReader<Schema>(field.schema());
        persistent.put(field.pos(), reader.read(null, DecoderFactory.get().binaryDecoder(val, decoder)));
        break;
      default:
        persistent.put(field.pos(), fromBytes(field.schema(), entry.getValue().get()));

      persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
  /**
   * Retrieve field name from entry.
   * @param entry The Key-Value entry
   * @return String The field name
   */
  private String getFieldName(Entry<Key, Value> entry) {
    String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), 
        entry.getKey().getColumnQualifier()));
    if (fieldName == null) {
      fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), null));
    }
    return fieldName;
  }

      if (col != null) {
        if (col.getSecond() == null) {
          scanner.fetchColumnFamily(col.getFirst());
        } else {
          scanner.fetchColumn(col.getFirst(), col.getSecond());
        }
        LOG.error("Mapping not found for field: "  field);




      List<Field> fields = schema.getFields();

      for (int i = 1; i < fields.size(); i) {
        if (!val.isDirty(i)) {
        Field field = fields.get(i);

        Object o = val.get(field.pos());       

        case MAP:
          count = putMap(m, count, field.schema().getValueType(), o, col);
          break;
        case ARRAY:
          count = putArray(m, count, o, col);
          break;
        case UNION: // default value of null acts like union with null
          Schema effectiveSchema = field.schema().getTypes()
          .get(firstNotNullSchemaTypeIndex(field.schema()));
          // map and array need to compute qualifier
          if (effectiveSchema.getType() == Type.ARRAY) {
            count = putArray(m, count, o, col);
          }
          else if (effectiveSchema.getType() == Type.MAP) {
            count = putMap(m, count, effectiveSchema.getValueType(), o, col);
          }
          // continue like a regular top-level union
        case RECORD:
          SpecificDatumWriter<Object> writer = new SpecificDatumWriter<Object>(field.schema());
          ByteArrayOutputStream os = new ByteArrayOutputStream();
          org.apache.avro.io.BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
          writer.write(o, encoder);
          encoder.flush();
          m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
          count;
          break;
        default:
          m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
          count;



  private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col) throws GoraException {

    // First of all we delete map field on accumulo store
    Text rowKey = new Text(m.getRow());
    Query<K, T> query = newQuery();
    query.setFields(col.getFirst().toString());
    query.setStartKey((K)rowKey.toString());
    query.setEndKey((K)rowKey.toString());
    deleteByQuery(query);
    flush();
    if (o == null){
      return 0;
    }
    
    Set<?> es = ((Map<?, ?>)o).entrySet();
    for (Object entry : es) {
      Object mapKey = ((Entry<?, ?>) entry).getKey();
      Object mapVal = ((Entry<?, ?>) entry).getValue();                  
      if ((o instanceof DirtyMapWrapper && ((DirtyMapWrapper<?, ?>)o).isDirty())
          || !(o instanceof DirtyMapWrapper)) { //mapVal instanceof Dirtyable && ((Dirtyable)mapVal).isDirty()) {
        m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(valueType, mapVal)));
        count;
      }
      // TODO map value deletion
    }
    return count;
  }

  private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col) {

    // First of all we delete array field on accumulo store
    Text rowKey = new Text(m.getRow());
    Query<K, T> query = newQuery();
    query.setFields(col.getFirst().toString());
    query.setStartKey((K)rowKey.toString());
    query.setEndKey((K)rowKey.toString());
    deleteByQuery(query);
    flush();
    if (o == null){
      return 0;
    }
    
    List<?> array = (List<?>) o;  // both GenericArray and DirtyListWrapper
    int j = 0;
    for (Object item : array) {
      m.put(col.getFirst(), new Text(toBytes(j)), new Value(toBytes(item)));
      count;
    }
    return count;
  }
















        tl = TabletLocator.getInstance(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));


      while (tl.binRanges(Collections.singletonList(createRange(query)), binnedRanges, credentials).size() > 0) {







          PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, new String[] {location});






  @SuppressWarnings("unchecked")

import java.io.IOException;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
  
  
      if (schema.getType().equals(Type.RECORD) || schema.getType().equals(Type.MAP) ){
        try {
          value = AvroSerializerUtil.deserializer(value, schema);
        } catch (IOException e) {
          LOG.warn(field.name()  " named field could not be deserialized.");
        }
      }
      String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName().duplicate());
      String family = cassandraColumn.getFamily();  
      String fieldName = this.reverseMap.get(family  ":"  StringSerializer.get().fromByteBuffer(cassandraColumn.getName().duplicate()));
      
      if (fieldName != null) {
        if (fieldName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {

          int pos = this.persistent.getSchema().getField(fieldName).pos();
          Field field = fields.get(pos);
          Type fieldType = field.schema().getType();
          // LOG.info(StringSerializer.get().fromByteBuffer(cassandraColumn.getName())
          //  fieldName  " "  fieldType.name());
          if (fieldType.equals(Type.UNION)) {
            //getting UNION stored type
            CassandraColumn cc = getUnionTypeColumn(fieldName
                 CassandraStore.UNION_COL_SUFIX, cassandraRow.toArray());
            //creating temporary UNION Field
            Field unionField = new Field(fieldName
                 CassandraStore.UNION_COL_SUFIX, Schema.create(Type.INT),
                null, null);
            // get value of UNION stored type
            cc.setField(unionField);
            Object val = cc.getValue();
            cassandraColumn.setUnionType(Integer.parseInt(val.toString()));
          }

          // get value
          cassandraColumn.setField(field);
          Object value = cassandraColumn.getValue();

          this.persistent.put(pos, value);
          // this field does not need to be written back to the store
          this.persistent.clearDirty(pos);
      } else
  //TODO Should we remove this method?
  @SuppressWarnings("unused")
import java.util.List;
import java.util.Map;
import org.apache.gora.cassandra.serializers.ListSerializer;
import org.apache.gora.cassandra.serializers.MapSerializer;
  private Object getFieldValue(Type type, Schema fieldSchema, ByteBuffer byteBuffer){
    Object value = null;
    if (type.equals(Type.ARRAY)) {
      ListSerializer<?> serializer = ListSerializer.get(fieldSchema.getElementType());
      List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
      value = genericArray;
    } else if (type.equals(Type.MAP)) {
//      MapSerializer<?> serializer = MapSerializer.get(fieldSchema.getValueType());
//      Map<?, ?> map = serializer.fromByteBuffer(byteBuffer);
//      value = map;
      value = fromByteBuffer(fieldSchema, byteBuffer);
    } else if (type.equals(Type.RECORD)){
      value = fromByteBuffer(fieldSchema, byteBuffer);
    } else if (type.equals(Type.UNION)){
      // the selected union schema is obtained
      Schema unionFieldSchema = getUnionSchema(super.getUnionType(), fieldSchema);
      Type unionFieldType = unionFieldSchema.getType();
      // we use the selected union schema to deserialize our actual value
      //value = fromByteBuffer(unionFieldSchema, byteBuffer);
      value = getFieldValue(unionFieldType, unionFieldSchema, byteBuffer);
    } else {
      value = fromByteBuffer(fieldSchema, byteBuffer);
    }
    return value;
  }

    Object value = getFieldValue(type, fieldSchema, byteBuffer);
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import org.apache.gora.cassandra.serializers.CharSequenceSerializer;
import org.apache.gora.cassandra.store.CassandraStore;
 private Object getSuperValue(Field field, Schema fieldSchema, Type type){
        List<Object> array = new ArrayList<Object>();
        Map<CharSequence, Object> map = new HashMap<CharSequence, Object>();

          CharSequence mapKey = CharSequenceSerializer.get().fromByteBuffer(hColumn.getName());
          if (mapKey.toString().indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
            Object memberValue = null;
            // We need detect real type for UNION Fields
            if (fieldSchema.getValueType().getType().equals(Type.UNION)){
              
              HColumn<ByteBuffer, ByteBuffer> cc = getUnionTypeColumn(mapKey
                   CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns());
              Integer unionIndex = getUnionIndex(mapKey.toString(), cc);
              Schema realSchema = fieldSchema.getValueType().getTypes().get(unionIndex);
              memberValue = fromByteBuffer(realSchema, hColumn.getValue());
              
            }else{
              memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());            
            }            
            map.put(mapKey, memberValue);      
          }
            if (memberName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
              
            Schema memberSchema = memberField.schema();
            Type memberType = memberSchema.getType();
            
            
            if (memberType.equals(Type.UNION)){
              HColumn<ByteBuffer, ByteBuffer> hc = getUnionTypeColumn(memberField.name()
                   CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns().toArray());
              Integer unionIndex = getUnionIndex(memberField.name(),hc);
              cassandraColumn.setUnionType(unionIndex);
            }
            
            record.put(record.getSchema().getField(memberName).pos(), cassandraColumn.getValue());
          }
      case UNION:
        int schemaPos = this.getUnionType();
        Schema unioSchema = fieldSchema.getTypes().get(schemaPos);
        Type unionType = unioSchema.getType();
        value = getSuperValue(field, unioSchema, unionType);
        break;
        Object memberValue = null;
        // Using for UnionIndex of Union type field get value. UnionIndex always Integer.  
        for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          memberValue = fromByteBuffer(fieldSchema, hColumn.getValue());      
        }
        value = memberValue;
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name());
    return value;
  }

 private Integer getUnionIndex(String fieldName, HColumn<ByteBuffer, ByteBuffer> uc){
   Integer val = IntegerSerializer.get().fromByteBuffer(uc.getValue());
   return Integer.parseInt(val.toString());
 }
 
  private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName,
    List<HColumn<ByteBuffer, ByteBuffer>> columns) {
    return getUnionTypeColumn(fieldName, columns.toArray());
}

  private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName, Object[] hColumns) {
    for (int iCnt = 0; iCnt < hColumns.length; iCnt){
      @SuppressWarnings("unchecked")
      HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>)hColumns[iCnt];
      String columnName = StringSerializer.get().fromByteBuffer(hColumn.getNameBytes().duplicate());
      if (fieldName.equals(columnName))
        return hColumn;
    }
    return null;
}

  public Object getValue() {
    Field field = getField();
    Schema fieldSchema = field.schema();
    Type type = fieldSchema.getType();
    
    Object value = getSuperValue(field, fieldSchema, type);
import java.util.Map;
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.BytesArraySerializer;
import me.prettyprint.cassandra.serializers.ObjectSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.Persistent;
      serializer = CharSequenceSerializer.get();
      serializer = ListSerializer.get(schema);
    } else if (value instanceof Map) {
      Map map = (Map)value;
        serializer = MapSerializer.get(schema);
    } else if (value instanceof Persistent){
      serializer = ObjectSerializer.get();
    }
    else {
      serializer = CharSequenceSerializer.get();
    if (type.equals(Type.STRING)) {
      serializer = CharSequenceSerializer.get();
    } else if (type.equals(Type.BOOLEAN)) {
    } else if (type.equals(Type.BYTES)) {
    } else if (type.equals(Type.DOUBLE)) {
    } else if (type.equals(Type.FLOAT)) {
    } else if (type.equals(Type.INT)) {
    } else if (type.equals(Type.LONG)) {
    } else if (type.equals(Type.FIXED)) {
    } else if (type.equals(Type.ARRAY)) {
      serializer = ListSerializer.get(schema.getElementType());
    } else if (type.equals(Type.MAP)) {
    	serializer = MapSerializer.get(schema.getValueType());
    } else if (type.equals(Type.UNION)){
    } else if (type.equals(Type.RECORD)){
      serializer = BytesArraySerializer.get();
      serializer = CharSequenceSerializer.get();
      serializer = ListSerializer.get(elementType);
      serializer = MapSerializer.get(elementType);
import java.util.List;
import java.util.Map;
  public static Class<? extends Object> getClass(Object value) {
      return Schema.createArray( getElementSchema((GenericArray<?>)value) );
    } else if (clazz.isAssignableFrom(List.class)) {
    } else if (clazz.isAssignableFrom(Map.class)) {
  public static Class<?> getClass(Type type) {
      return List.class;
      return Map.class;
  public static Schema getSchema(Class<?> clazz) {
  public static Class<?> getClass(Schema schema) {
  public static int getFixedSize(Class<?> clazz) {
  public static Schema getElementSchema(GenericArray<?> array) {
  
  
  
    this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
    
    
    // Just create a Keyspace object on the client side, corresponding to an already existing keyspace with already created column families.
    
  
   * In this method, we also utilise Hector's {@ConfigurableConsistencyLevel}
   * logic. It is set by passing a ConfigurableConsistencyLevel object right 
   * when the Keyspace is created. Currently consistency level is .ONE which 
   * permits consistency to wait until one replica has responded. 
          "org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);      
      // LOG.info("Keyspace '"  this.cassandraMapping.getKeyspaceName()  "' in cluster '"  this.cassandraMapping.getClusterName()  "' was created on host '"  this.cassandraMapping.getHostName()  "'");
      
      // Create a customized Consistency Level
      ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
      // Define CL.ONE for ColumnFamily "ColumnFamily"
      clmap.put("ColumnFamily", HConsistencyLevel.ONE);

      // In this we use CL.ONE for read and writes. But you can use different CLs if needed.
      configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
      configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
      HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);

               ", not BytesType. It may cause a fatal error on column validation later.");
            LOG.debug("The comparator type of "  cfDef.getName()  " column family is " 
               comparatorType.getTypeName()  ".");
    if (ttlAttr == null)
   * Delete a row within the keyspace.
   * @param key
   * @param fieldName
   * @param columnName
   */
  public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
    synchronized(mutator) {
      HectorUtils.deleteColumn(mutator, key, familyName, columnName);
    }
  }

  /**
   * Deletes an entry based on its key.
   * @param key
   */
  public void deleteByKey(K key) {
    Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
    deleteColumn(key, familyMap.values().iterator().next().toString(), null);
  }

  /**
    
    if (ttlAttr == null)
    
   * Deletes a subColumn 
   * @param key
   * @param fieldName
   * @param columnName
   * Deletes all subcolumns from a super column.
   * @param key the row key.
   * @param fieldName the field name.
  public void deleteSubColumn(K key, String fieldName) {
    String columnFamily = this.cassandraMapping.getFamily(fieldName);
    String superColumnName = this.cassandraMapping.getColumn(fieldName);
    synchronized(mutator) {
      HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, null);
    }
  public void deleteGenericArray(K key, String fieldName) {
    //TODO Verify this. Everything that goes inside a genericArray will go inside a column so let's just delete that.
    deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
  }
          if (((List<?>)itemValue).size() == 0) {
        } else if (itemValue instanceof Map<?,?>) {
          if (((Map<?, ?>)itemValue).size() == 0) {

  public void deleteStatefulHashMap(K key, String fieldName) {
      deleteSubColumn(key, fieldName);
    } else {
      deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
    }
  }
  public void addStatefulHashMap(K key, String fieldName, Map<CharSequence,Object> map) {
    if (isSuper( cassandraMapping.getFamily(fieldName) )) {
      // as we don't know what has changed inside the map or If it's an empty map, then delete its content.
      deleteSubColumn(key, fieldName);
      // update if there is anything to update.
      if (!map.isEmpty()) {
        // If it's not empty, then update its content.
        for (CharSequence mapKey: map.keySet()) {
          // TODO: hack, do not store empty arrays
          Object mapValue = map.get(mapKey);
          if (mapValue instanceof GenericArray<?>) {
            if (((List<?>)mapValue).size() == 0) {
              continue;
            }
          } else if (mapValue instanceof Map<?,?>) {
            if (((Map<?, ?>)mapValue).size() == 0) {
              continue;
            }
          addSubColumn(key, fieldName, mapKey.toString(), mapValue);
    
    
    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
    
    
  
    // checking if it was a UNION field the one we are retrieving
    if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
      family = this.cassandraMapping.getFamily(pField.substring(0,pField.indexOf(CassandraStore.UNION_COL_SUFIX)));
    else
      family = this.cassandraMapping.getFamily(pField);
     return family;
   }
 
    if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
      column = pField;
    else
      column = this.cassandraMapping.getColumn(pField);
      return column;
    }
   * @return a map which keys are the family names and values the 
   * corresponding column names required to get all the query fields.
      
    
   * Retrieves the cassandraMapping which holds whatever was mapped 
   * from the gora-cassandra-mapping.xml
   * @return 
  
   * Select the field names according to the column names, which format 
   * if fully qualified: "family:column"
   * @return a map which keys are the fully qualified column 
   * names and values the query fields
      
    
  /**
   * Determines if a column is a superColumn or not.
   * @param family
   * @return boolean
   */
    
    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
    
    


	return this.cassandraMapping.getKeyspaceName();
  
        LOG.debug("Located gc_grace_seconds: '"  gcgrace_scs  "'" );




      // TODO we should find a way of storing more values into this map
   * Add new column to the CassandraMapping using the the below parameters
   * @param pFamilyName the column family name
   * @param pFieldName the Avro field from the Schema
   * @param pColumnName the column name within the column family.
 /**
      LOG.debug("persistentClassName="  className  " -> keyspaceName="  keyspaceName);
import java.util.HashMap;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.generic.GenericData.Array;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.commons.lang.ArrayUtils;
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
 * heavily on {@link org.apache.gora.cassandra.store.CassandraClient} for many operations
  private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
   * Fixed string with value "UnionIndex" used to generate an extra column based on 
   * the original field's name
   */
  public static String UNION_COL_SUFIX = "_UnionIndex";

  /**
   * Default schema index with value "0" used when AVRO Union data types are stored

   * We don't want to lock the entire collection before iterating over the keys, 
   * since in the meantime other threads are adding entries to the map.
  public static final ThreadLocal<BinaryEncoder> encoders =
      new ThreadLocal<BinaryEncoder>();
  
  /**
   * Create a {@link java.util.concurrent.ConcurrentHashMap} for the 
   * datum readers and writers. 
   * This is necessary because they are not thread safe, at least not before 
   * Avro 1.4.0 (See AVRO-650).
   * When they are thread safe, it is possible to maintain a single reader and
   * writer pair for every schema, instead of one for every thread.
   * @see <a href="https://issues.apache.org/jira/browse/AVRO-650">AVRO-650</a>
   */
  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = 
      new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
  
   * When we add sub/supercolumns, Gora keys are mapped to Cassandra partition keys only. 
      CassandraResultSet<K> cassandraResultSet) {
        cassandraRow = new CassandraRow<K>();
   * Flush the buffer which is a synchronized {@link java.util.LinkedHashMap}
   * storing fields pending to be stored by 
   * {@link org.apache.gora.cassandra.store.CassandraStore#put(Object, PersistentBase)}
   * operations. Invoking this method therefore writes the buffered rows
   * into Cassandra.
    @SuppressWarnings("unchecked")
    // iterating over the key set directly would throw 
    //ConcurrentModificationException with java.util.HashMap and subclasses
        LOG.info("Value to update is null for key: "  key);

          addOrUpdateField(key, field, field.schema(), value.get(field.pos()));
    // remove flushed rows from the buffer as all 
    // added or updated fields should now have been written.
    
    if (fields == null){
      fields = this.getFields();
    }
    // Generating UnionFields
    ArrayList<String> unionFields = new ArrayList<String>();
    for (String field: fields){
      Field schemaField =this.fieldMap.get(field);
      Type type = schemaField.schema().getType();
      if (type.getName().equals("UNION".toLowerCase())){
        unionFields.add(fieldUNION_COL_SUFIX);
      }
    }
    
    String[] arr = unionFields.toArray(new String[unionFields.size()]);
    String[] both = (String[]) ArrayUtils.addAll(fields, arr);
    
    query.setFields(both);

    // TODO GORA-298 Implement CassandraStore#getPartitions
   * 
   * When doing the 
   * {@link org.apache.gora.cassandra.store.CassandraStore#put(Object, PersistentBase)}
   * operation, the logic is as follows:
   * <ol>
   * <li>Obtain the Avro {@link org.apache.avro.Schema} for the object.</li>
   * <li>Create a new duplicate instance of the object (explained in more detail below) **.</li>
   * <li>Obtain a {@link java.util.List} of the {@link org.apache.avro.Schema} 
   * {@link org.apache.avro.Schema.Field}'s.</li>
   * <li>Iterate through the field {@link java.util.List}. This allows us to 
   * consequently process each item.</li>
   * <li>Check to see if the {@link org.apache.avro.Schema.Field} is NOT dirty. 
   * If this condition is true then we DO NOT process this field.</li>
   * <li>Obtain the element at the specified position in this list so we can 
   * directly operate on it.</li>
   * <li>Obtain the {@link org.apache.avro.Schema.Type} of the element obtained 
   * above and process it accordingly. N.B. For nested type ARRAY, MAP
   * RECORD or UNION, we shadow the checks in bullet point 5 above to infer that the 
   * {@link org.apache.avro.Schema.Field} is either at 
   * position 0 OR it is NOT dirty. If one of these conditions is true then we DO NOT
   * process this field. This is carried out in 
   * {@link org.apache.gora.cassandra.store.CassandraStore#getFieldValue(Schema, Type, Object)}</li>
   * <li>We then insert the Key and Object into the {@link java.util.LinkedHashMap} buffer 
   * before being flushed. This performs a structural modification of the map.</li>
   * </ol>
   * ** We create a duplicate instance of the object to be persisted and insert processed
   * objects into a synchronized {@link java.util.LinkedHashMap}. This allows 
   * us to keep all the objects in memory till flushing.
   * @see org.apache.gora.store.DataStore#put(java.lang.Object, 
   * org.apache.gora.persistency.Persistent).
   * @param key for the Avro Record (object).
   * @param value Record object to be persisted in Cassandra
    @SuppressWarnings("unchecked")
    T p = (T) SpecificData.get().newRecord(value, schema);
    List<Field> fields = schema.getFields();
    for (int i = 1; i < fields.size(); i) {
      if (!value.isDirty(i)) {
        continue;
      Field field = fields.get(i);
      Type type = field.schema().getType();
      Object fieldValue = value.get(field.pos());
      Schema fieldSchema = field.schema();
      // check if field has a nested structure (array, map, record or union)
      fieldValue = getFieldValue(fieldSchema, type, fieldValue);
      p.put(field.pos(), fieldValue);
   * For every field within an object, we pass in a field schema, Type and value.
   * This enables us to process fields (based on their characteristics) 
   * preparing them for persistence.
   * @param fieldSchema the associated field schema
   * @param type the field type
   * @param fieldValue the field value.
   * @return
   */
  private Object getFieldValue(Schema fieldSchema, Type type, Object fieldValue ){
    switch(type) {
    case RECORD:
      Persistent persistent = (Persistent) fieldValue;
      Persistent newRecord = (Persistent) SpecificData.get().newRecord(persistent, persistent.getSchema());
      for (Field member: fieldSchema.getFields()) {
        if (member.pos() == 0 || !persistent.isDirty()) {
          continue;
        }
        Schema memberSchema = member.schema();
        Type memberType = memberSchema.getType();
        Object memberValue = persistent.get(member.pos());
        newRecord.put(member.pos(), getFieldValue(memberSchema, memberType, memberValue));
      }
      fieldValue = newRecord;
      break;
    case MAP:
      Map<?, ?> map = (Map<?, ?>) fieldValue;
      fieldValue = map;
      break;
    case ARRAY:
      fieldValue = (List<?>) fieldValue;
      break;
    case UNION:
      // storing the union selected schema, the actual value will 
      // be stored as soon as we get break out.
      if (fieldValue != null){
        int schemaPos = getUnionSchema(fieldValue,fieldSchema);
        Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
        Type unionType = unionSchema.getType();
        fieldValue = getFieldValue(unionSchema, unionType, fieldValue);
      }
      //p.put( schemaPos, p.getSchema().getField(field.name()  CassandraStore.UNION_COL_SUFIX));
      //p.put(fieldPos, fieldValue);
      break;
    default:
      break;
    }    
    return fieldValue;
  }
  
  /**
   * @param schema  the schema belonging to the particular Avro field
  @SuppressWarnings({ "unchecked", "rawtypes" })
  private void addOrUpdateField(K key, Field field, Schema schema, Object value) {
    // checking if the value to be updated is used for saving union schema
    if (field.name().indexOf(CassandraStore.UNION_COL_SUFIX) < 0){
      switch (type) {
      case STRING:
      case BOOLEAN:
      case INT:
      case LONG:
      case BYTES:
      case FLOAT:
      case DOUBLE:
      case FIXED:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case RECORD:
        if (value != null) {
          if (value instanceof PersistentBase) {
            PersistentBase persistentBase = (PersistentBase) value;            
            try {
              byte[] byteValue = AvroSerializerUtil.serializer(persistentBase, schema);
              this.cassandraClient.addColumn(key, field.name(), byteValue);
            } catch (IOException e) {
              LOG.warn(field.name()  " named record could not be serialized.");
          } else {
            LOG.warn("Record with value: "  value.toString()  " not supported for field: "  field.name());
          LOG.warn("Setting content of: "  field.name()  " to null.");
          String familyName =  this.cassandraClient.getCassandraMapping().getFamily(field.name());
          this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
        break;
      case MAP:
        if (value != null) {
          if (value instanceof Map<?, ?>) {            
            Map<CharSequence,Object> map = (Map<CharSequence,Object>)value;
            Schema valueSchema = schema.getValueType();
            Type valueType = valueSchema.getType();
            if (Type.UNION.equals(valueType)){
              Map<CharSequence,Object> valueMap = new HashMap<CharSequence, Object>();
              for (CharSequence mapKey: map.keySet()) {
                Object mapValue = map.get(mapKey);
                int valueUnionIndex = getUnionSchema(mapValue, valueSchema);
                valueMap.put((mapKeyUNION_COL_SUFIX), valueUnionIndex);
                valueMap.put(mapKey, mapValue);
              }
              map = valueMap;
            }
            
            String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
            
            // If map is not super column. We using Avro serializer. 
            if (!this.cassandraClient.isSuper( familyName )){
              try {
                byte[] byteValue = AvroSerializerUtil.serializer(map, schema);
                this.cassandraClient.addColumn(key, field.name(), byteValue);
              } catch (IOException e) {
                LOG.warn(field.name()  " named map could not be serialized.");
              }
            }else{
              this.cassandraClient.addStatefulHashMap(key, field.name(), map);              
            }
          } else {
            LOG.warn("Map with value: "  value.toString()  " not supported for field: "  field.name());
          }
          // delete map
          LOG.warn("Setting content of: "  field.name()  " to null.");
          this.cassandraClient.deleteStatefulHashMap(key, field.name());
        break;
      case ARRAY:
        if (value != null) {
          if (value instanceof DirtyListWrapper<?>) {
            DirtyListWrapper fieldValue = (DirtyListWrapper<?>)value;
            GenericArray valueArray = new Array(fieldValue.size(), schema);
            for (int i = 0; i < fieldValue.size(); i) {
              valueArray.add(i, fieldValue.get(i));
            }
            this.cassandraClient.addGenericArray(key, field.name(), (GenericArray<?>)valueArray);
          } else {
            LOG.warn("Array with value: "  value.toString()  " not supported for field: "  field.name());
          }
          LOG.warn("Setting content of: "  field.name()  " to null.");
          this.cassandraClient.deleteGenericArray(key, field.name());
        break;
      case UNION:
     // adding union schema index
        String columnName = field.name()  UNION_COL_SUFIX;
        String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
        if(value != null) {
          int schemaPos = getUnionSchema(value, schema);
          LOG.debug("Union with value: "  value.toString()  " at index: "  schemaPos  " supported for field: "  field.name());
          this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
          if (this.cassandraClient.isSuper( familyName )){
            this.cassandraClient.addSubColumn(key, columnName, columnName, schemaPos);
          }else{
            this.cassandraClient.addColumn(key, columnName, schemaPos);
            
          }
          //this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
          // adding union value
          Schema unionSchema = schema.getTypes().get(schemaPos);
          addOrUpdateField(key, field, unionSchema, value);
          //this.cassandraClient.addColumn(key, field.name(), value);
        } else {
          LOG.warn("Setting content of: "  field.name()  " to null.");
          if (this.cassandraClient.isSuper( familyName )){
            this.cassandraClient.deleteSubColumn(key, field.name());
          } else {
            this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
          }
        }
        break;
      default:
        LOG.warn("Type: "  type.name()  " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
   * Given an object and the object schema this function obtains,
   * from within the UNION schema, the position of the type used.
   * If no data type can be inferred then we return a default value
   * of position 0.
   * @return the unionSchemaPosition.
//    String valueType = pValue.getClass().getSimpleName();
      Type schemaType = it.next().getType();
      if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
      else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
      else if (pValue instanceof Integer && schemaType.equals(Type.INT))
      else if (pValue instanceof Long && schemaType.equals(Type.LONG))
      else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
      else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
      else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
        return unionSchemaPos;
      else if (pValue instanceof Map && schemaType.equals(Type.MAP))
        return unionSchemaPos;
      else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
        return unionSchemaPos;
      else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
    return DEFAULT_UNION_SCHEMA;
   * Simple method to check if a Cassandra Keyspace exists.
   * @return true if a Keyspace exists.



    mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());

import java.util.ArrayList;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
    //"http://example.com",
    //"fck fck dck",
      throws IOException {
    try{
      WebPage page;
      log.info("creating web page data");
      
      for(int i=0; i<URLS.length; i) {
        page = WebPage.newBuilder().build();
        page.setUrl(new Utf8(URLS[i]));
        page.setParsedContent(new ArrayList<CharSequence>());
        if (CONTENTS[i]!=null){
          page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
          for(String token : CONTENTS[i].split(" ")) {
            page.getParsedContent().add(new Utf8(token));  
        }
        for(int j=0; j<LINKS[i].length; j) {
          page.getOutlinks().put(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
        }
        
        Metadata metadata = Metadata.newBuilder().build();
        metadata.setVersion(1);
        metadata.getData().put(new Utf8("metakey"), new Utf8("metavalue"));
        page.setMetadata(metadata);

        dataStore.put(URLS[i], page);
      }
      dataStore.flush();
      log.info("finished creating web page data");
    }
    catch(Exception e){
      log.info("error creating web page data");
    } 
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class Employee extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    NAME(1, "name"),
    DATE_OF_BIRTH(2, "dateOfBirth"),
    SSN(3, "ssn"),
    SALARY(4, "salary"),
    BOSS(5, "boss"),
    WEBPAGE(6, "webpage"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "name",
  "dateOfBirth",
  "ssn",
  "salary",
  "boss",
  "webpage",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence name;
  private java.lang.CharSequence ssn;
  private java.lang.Object boss;
  private org.apache.gora.examples.generated.WebPage webpage;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return name;
    case 2: return dateOfBirth;
    case 3: return ssn;
    case 4: return salary;
    case 5: return boss;
    case 6: return webpage;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: name = (java.lang.CharSequence)(value); break;
    case 2: dateOfBirth = (java.lang.Long)(value); break;
    case 3: ssn = (java.lang.CharSequence)(value); break;
    case 4: salary = (java.lang.Integer)(value); break;
    case 5: boss = (java.lang.Object)(value); break;
    case 6: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

   * Gets the value of the 'name' field.
  public java.lang.CharSequence getName() {
    return name;
  }

  /**
   * Sets the value of the 'name' field.
   * @param value the value to set.
   */
  public void setName(java.lang.CharSequence value) {
    this.name = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isNameDirty(java.lang.CharSequence value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'dateOfBirth' field.
   */
  public java.lang.Long getDateOfBirth() {
    return dateOfBirth;
  }

  /**
   * Sets the value of the 'dateOfBirth' field.
   * @param value the value to set.
   */
  public void setDateOfBirth(java.lang.Long value) {
    this.dateOfBirth = value;
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isDateOfBirthDirty(java.lang.Long value) {
    return isDirty(2);
  }

  /**
   * Gets the value of the 'ssn' field.
   */
  public java.lang.CharSequence getSsn() {
    return ssn;
  }

  /**
   * Sets the value of the 'ssn' field.
   * @param value the value to set.
   */
  public void setSsn(java.lang.CharSequence value) {
    this.ssn = value;
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isSsnDirty(java.lang.CharSequence value) {
    return isDirty(3);
  }

  /**
   * Gets the value of the 'salary' field.
   */
  public java.lang.Integer getSalary() {
    return salary;
  }

  /**
   * Sets the value of the 'salary' field.
   * @param value the value to set.
   */
  public void setSalary(java.lang.Integer value) {
    this.salary = value;
    setDirty(4);
  }
  
  /**
   * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isSalaryDirty(java.lang.Integer value) {
    return isDirty(4);
  }

  /**
   * Gets the value of the 'boss' field.
   */
  public java.lang.Object getBoss() {
    return boss;
  }

  /**
   * Sets the value of the 'boss' field.
   * @param value the value to set.
   */
  public void setBoss(java.lang.Object value) {
    this.boss = value;
    setDirty(5);
  }
  
  /**
   * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isBossDirty(java.lang.Object value) {
    return isDirty(5);
  }

  /**
   * Gets the value of the 'webpage' field.
   */
  public org.apache.gora.examples.generated.WebPage getWebpage() {
    return webpage;
  }

  /**
   * Sets the value of the 'webpage' field.
   * @param value the value to set.
   */
  public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
    this.webpage = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
    return isDirty(6);
  }

  /** Creates a new Employee RecordBuilder */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder() {
    return new org.apache.gora.examples.generated.Employee.Builder();
  }
  
  /** Creates a new Employee RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee.Builder other) {
    return new org.apache.gora.examples.generated.Employee.Builder(other);
  }
  
  /** Creates a new Employee RecordBuilder by copying an existing Employee instance */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee other) {
    return new org.apache.gora.examples.generated.Employee.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Employee instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Employee>
    implements org.apache.avro.data.RecordBuilder<Employee> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence name;
    private long dateOfBirth;
    private java.lang.CharSequence ssn;
    private int salary;
    private java.lang.Object boss;
    private org.apache.gora.examples.generated.WebPage webpage;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.Employee.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.Employee.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Employee instance */
    private Builder(org.apache.gora.examples.generated.Employee other) {
            super(org.apache.gora.examples.generated.Employee.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.name)) {
        this.name = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.name);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.dateOfBirth)) {
        this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.dateOfBirth);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.ssn)) {
        this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ssn);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.salary)) {
        this.salary = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.salary);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.boss)) {
        this.boss = (java.lang.Object) data().deepCopy(fields()[5].schema(), other.boss);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.webpage)) {
        this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[6].schema(), other.webpage);
        fieldSetFlags()[6] = true;
      }
    }

    /** Gets the value of the 'name' field */
    public java.lang.CharSequence getName() {
      return name;
    }
    
    /** Sets the value of the 'name' field */
    public org.apache.gora.examples.generated.Employee.Builder setName(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.name = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'name' field has been set */
    public boolean hasName() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'name' field */
    public org.apache.gora.examples.generated.Employee.Builder clearName() {
      name = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'dateOfBirth' field */
    public java.lang.Long getDateOfBirth() {
      return dateOfBirth;
    }
    
    /** Sets the value of the 'dateOfBirth' field */
    public org.apache.gora.examples.generated.Employee.Builder setDateOfBirth(long value) {
      validate(fields()[2], value);
      this.dateOfBirth = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'dateOfBirth' field has been set */
    public boolean hasDateOfBirth() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'dateOfBirth' field */
    public org.apache.gora.examples.generated.Employee.Builder clearDateOfBirth() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'ssn' field */
    public java.lang.CharSequence getSsn() {
      return ssn;
    }
    
    /** Sets the value of the 'ssn' field */
    public org.apache.gora.examples.generated.Employee.Builder setSsn(java.lang.CharSequence value) {
      validate(fields()[3], value);
      this.ssn = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'ssn' field has been set */
    public boolean hasSsn() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'ssn' field */
    public org.apache.gora.examples.generated.Employee.Builder clearSsn() {
      ssn = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'salary' field */
    public java.lang.Integer getSalary() {
      return salary;
    }
    
    /** Sets the value of the 'salary' field */
    public org.apache.gora.examples.generated.Employee.Builder setSalary(int value) {
      validate(fields()[4], value);
      this.salary = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'salary' field has been set */
    public boolean hasSalary() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'salary' field */
    public org.apache.gora.examples.generated.Employee.Builder clearSalary() {
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'boss' field */
    public java.lang.Object getBoss() {
      return boss;
    }
    
    /** Sets the value of the 'boss' field */
    public org.apache.gora.examples.generated.Employee.Builder setBoss(java.lang.Object value) {
      validate(fields()[5], value);
      this.boss = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'boss' field has been set */
    public boolean hasBoss() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'boss' field */
    public org.apache.gora.examples.generated.Employee.Builder clearBoss() {
      boss = null;
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'webpage' field */
    public org.apache.gora.examples.generated.WebPage getWebpage() {
      return webpage;
    }
    
    /** Sets the value of the 'webpage' field */
    public org.apache.gora.examples.generated.Employee.Builder setWebpage(org.apache.gora.examples.generated.WebPage value) {
      validate(fields()[6], value);
      this.webpage = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'webpage' field has been set */
    public boolean hasWebpage() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'webpage' field */
    public org.apache.gora.examples.generated.Employee.Builder clearWebpage() {
      webpage = null;
      fieldSetFlags()[6] = false;
      return this;
    }
    
    @Override
    public Employee build() {
      try {
        Employee record = new Employee();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.name = fieldSetFlags()[1] ? this.name : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.dateOfBirth = fieldSetFlags()[2] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[2]);
        record.ssn = fieldSetFlags()[3] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[3]);
        record.salary = fieldSetFlags()[4] ? this.salary : (java.lang.Integer) defaultValue(fields()[4]);
        record.boss = fieldSetFlags()[5] ? this.boss : (java.lang.Object) defaultValue(fields()[5]);
        record.webpage = fieldSetFlags()[6] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[6]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public Employee.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Employee newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'name' field.
		   */
	  public java.lang.CharSequence getName() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'name' field.
		   * @param value the value to set.
	   */
	  public void setName(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isNameDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'dateOfBirth' field.
		   */
	  public java.lang.Long getDateOfBirth() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'dateOfBirth' field.
		   * @param value the value to set.
	   */
	  public void setDateOfBirth(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isDateOfBirthDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'ssn' field.
		   */
	  public java.lang.CharSequence getSsn() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'ssn' field.
		   * @param value the value to set.
	   */
	  public void setSsn(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isSsnDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'salary' field.
		   */
	  public java.lang.Integer getSalary() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'salary' field.
		   * @param value the value to set.
	   */
	  public void setSalary(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isSalaryDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'boss' field.
		   */
	  public java.lang.Object getBoss() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'boss' field.
		   * @param value the value to set.
	   */
	  public void setBoss(java.lang.Object value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isBossDirty(java.lang.Object value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'webpage' field.
		   */
	  public org.apache.gora.examples.generated.WebPage getWebpage() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'webpage' field.
		   * @param value the value to set.
	   */
	  public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class Metadata extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    VERSION(1, "version"),
    DATA(2, "data"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "version",
  "data",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return version;
    case 2: return data;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: version = (java.lang.Integer)(value); break;
    case 2: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'version' field.
   */
  public java.lang.Integer getVersion() {
    return version;
  }

  /**
   * Sets the value of the 'version' field.
   * @param value the value to set.
   */
  public void setVersion(java.lang.Integer value) {
    this.version = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isVersionDirty(java.lang.Integer value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'data' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
    return data;
  }

  /**
   * Sets the value of the 'data' field.
   * @param value the value to set.
   */
  public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.data = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(2);
  }

  /** Creates a new Metadata RecordBuilder */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder() {
    return new org.apache.gora.examples.generated.Metadata.Builder();
  }
  
  /** Creates a new Metadata RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata.Builder other) {
    return new org.apache.gora.examples.generated.Metadata.Builder(other);
  }
  
  /** Creates a new Metadata RecordBuilder by copying an existing Metadata instance */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata other) {
    return new org.apache.gora.examples.generated.Metadata.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Metadata instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Metadata>
    implements org.apache.avro.data.RecordBuilder<Metadata> {

    private java.nio.ByteBuffer __g__dirty;
    private int version;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.Metadata.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Metadata instance */
    private Builder(org.apache.gora.examples.generated.Metadata other) {
            super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.version)) {
        this.version = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.version);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.data)) {
        this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.data);
        fieldSetFlags()[2] = true;
      }
    }

    /** Gets the value of the 'version' field */
    public java.lang.Integer getVersion() {
      return version;
    }
    
    /** Sets the value of the 'version' field */
    public org.apache.gora.examples.generated.Metadata.Builder setVersion(int value) {
      validate(fields()[1], value);
      this.version = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'version' field has been set */
    public boolean hasVersion() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'version' field */
    public org.apache.gora.examples.generated.Metadata.Builder clearVersion() {
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'data' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
      return data;
    }
    
    /** Sets the value of the 'data' field */
    public org.apache.gora.examples.generated.Metadata.Builder setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[2], value);
      this.data = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'data' field has been set */
    public boolean hasData() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'data' field */
    public org.apache.gora.examples.generated.Metadata.Builder clearData() {
      data = null;
      fieldSetFlags()[2] = false;
      return this;
    }
    
    @Override
    public Metadata build() {
      try {
        Metadata record = new Metadata();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.version = fieldSetFlags()[1] ? this.version : (java.lang.Integer) defaultValue(fields()[1]);
        record.data = fieldSetFlags()[2] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[2]));
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public Metadata.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Metadata newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'version' field.
		   */
	  public java.lang.Integer getVersion() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'version' field.
		   * @param value the value to set.
	   */
	  public void setVersion(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isVersionDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'data' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'data' field.
		   * @param value the value to set.
	   */
	  public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class TokenDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    COUNT(1, "count"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "count",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return count;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: count = (java.lang.Integer)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'count' field.
   */
  public java.lang.Integer getCount() {
    return count;
  }

  /**
   * Sets the value of the 'count' field.
   * @param value the value to set.
   */
  public void setCount(java.lang.Integer value) {
    this.count = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isCountDirty(java.lang.Integer value) {
    return isDirty(1);
  }

  /** Creates a new TokenDatum RecordBuilder */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder() {
    return new org.apache.gora.examples.generated.TokenDatum.Builder();
  }
  
  /** Creates a new TokenDatum RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
    return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
  }
  
  /** Creates a new TokenDatum RecordBuilder by copying an existing TokenDatum instance */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum other) {
    return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for TokenDatum instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<TokenDatum>
    implements org.apache.avro.data.RecordBuilder<TokenDatum> {

    private java.nio.ByteBuffer __g__dirty;
    private int count;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing TokenDatum instance */
    private Builder(org.apache.gora.examples.generated.TokenDatum other) {
            super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.count)) {
        this.count = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.count);
        fieldSetFlags()[1] = true;
      }
    }

    /** Gets the value of the 'count' field */
    public java.lang.Integer getCount() {
      return count;
    }
    
    /** Sets the value of the 'count' field */
    public org.apache.gora.examples.generated.TokenDatum.Builder setCount(int value) {
      validate(fields()[1], value);
      this.count = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'count' field has been set */
    public boolean hasCount() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'count' field */
    public org.apache.gora.examples.generated.TokenDatum.Builder clearCount() {
      fieldSetFlags()[1] = false;
      return this;
    }
    
    @Override
    public TokenDatum build() {
      try {
        TokenDatum record = new TokenDatum();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.count = fieldSetFlags()[1] ? this.count : (java.lang.Integer) defaultValue(fields()[1]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public TokenDatum.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public TokenDatum newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'count' field.
		   */
	  public java.lang.Integer getCount() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'count' field.
		   * @param value the value to set.
	   */
	  public void setCount(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isCountDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class WebPage extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    URL(1, "url"),
    CONTENT(2, "content"),
    PARSED_CONTENT(3, "parsedContent"),
    OUTLINKS(4, "outlinks"),
    HEADERS(5, "headers"),
    METADATA(6, "metadata"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "url",
  "content",
  "parsedContent",
  "outlinks",
  "headers",
  "metadata",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence url;
  private java.nio.ByteBuffer content;
  private java.util.List<java.lang.CharSequence> parsedContent;
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
  private org.apache.gora.examples.generated.Metadata metadata;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return url;
    case 2: return content;
    case 3: return parsedContent;
    case 4: return outlinks;
    case 5: return headers;
    case 6: return metadata;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: url = (java.lang.CharSequence)(value); break;
    case 2: content = (java.nio.ByteBuffer)(value); break;
    case 3: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
    case 4: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    case 5: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
    case 6: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'url' field.
   */
  public java.lang.CharSequence getUrl() {
    return url;
  }

  /**
   * Sets the value of the 'url' field.
   * @param value the value to set.
   */
  public void setUrl(java.lang.CharSequence value) {
    this.url = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUrlDirty(java.lang.CharSequence value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'content' field.
   */
  public java.nio.ByteBuffer getContent() {
    return content;
  }

  /**
   * Sets the value of the 'content' field.
   * @param value the value to set.
   */
  public void setContent(java.nio.ByteBuffer value) {
    this.content = value;
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isContentDirty(java.nio.ByteBuffer value) {
    return isDirty(2);
  }

  /**
   * Gets the value of the 'parsedContent' field.
   */
  public java.util.List<java.lang.CharSequence> getParsedContent() {
    return parsedContent;
  }

  /**
   * Sets the value of the 'parsedContent' field.
   * @param value the value to set.
   */
  public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
    this.parsedContent = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper(value);
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
    return isDirty(3);
  }

  /**
   * Gets the value of the 'outlinks' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
    return outlinks;
  }

  /**
   * Sets the value of the 'outlinks' field.
   * @param value the value to set.
   */
  public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.outlinks = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(4);
  }
  
  /**
   * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(4);
  }

  /**
   * Gets the value of the 'headers' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
    return headers;
  }

  /**
   * Sets the value of the 'headers' field.
   * @param value the value to set.
   */
  public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.headers = value;
    setDirty(5);
  }
  
  /**
   * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(5);
  }

  /**
   * Gets the value of the 'metadata' field.
   */
  public org.apache.gora.examples.generated.Metadata getMetadata() {
    return metadata;
  }

  /**
   * Sets the value of the 'metadata' field.
   * @param value the value to set.
   */
  public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
    this.metadata = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
    return isDirty(6);
  }

  /** Creates a new WebPage RecordBuilder */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder() {
    return new org.apache.gora.examples.generated.WebPage.Builder();
  }
  
  /** Creates a new WebPage RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage.Builder other) {
    return new org.apache.gora.examples.generated.WebPage.Builder(other);
  }
  
  /** Creates a new WebPage RecordBuilder by copying an existing WebPage instance */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage other) {
    return new org.apache.gora.examples.generated.WebPage.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for WebPage instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<WebPage>
    implements org.apache.avro.data.RecordBuilder<WebPage> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence url;
    private java.nio.ByteBuffer content;
    private java.util.List<java.lang.CharSequence> parsedContent;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
    private org.apache.gora.examples.generated.Metadata metadata;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.WebPage.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing WebPage instance */
    private Builder(org.apache.gora.examples.generated.WebPage other) {
            super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.content)) {
        this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[2].schema(), other.content);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.parsedContent)) {
        this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.parsedContent);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.outlinks)) {
        this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.outlinks);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.headers)) {
        this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[5].schema(), other.headers);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.metadata)) {
        this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[6].schema(), other.metadata);
        fieldSetFlags()[6] = true;
      }
    }

    /** Gets the value of the 'url' field */
    public java.lang.CharSequence getUrl() {
      return url;
    }
    
    /** Sets the value of the 'url' field */
    public org.apache.gora.examples.generated.WebPage.Builder setUrl(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.url = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'url' field has been set */
    public boolean hasUrl() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'url' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearUrl() {
      url = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'content' field */
    public java.nio.ByteBuffer getContent() {
      return content;
    }
    
    /** Sets the value of the 'content' field */
    public org.apache.gora.examples.generated.WebPage.Builder setContent(java.nio.ByteBuffer value) {
      validate(fields()[2], value);
      this.content = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'content' field has been set */
    public boolean hasContent() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'content' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearContent() {
      content = null;
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'parsedContent' field */
    public java.util.List<java.lang.CharSequence> getParsedContent() {
      return parsedContent;
    }
    
    /** Sets the value of the 'parsedContent' field */
    public org.apache.gora.examples.generated.WebPage.Builder setParsedContent(java.util.List<java.lang.CharSequence> value) {
      validate(fields()[3], value);
      this.parsedContent = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'parsedContent' field has been set */
    public boolean hasParsedContent() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'parsedContent' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearParsedContent() {
      parsedContent = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'outlinks' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
      return outlinks;
    }
    
    /** Sets the value of the 'outlinks' field */
    public org.apache.gora.examples.generated.WebPage.Builder setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[4], value);
      this.outlinks = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'outlinks' field has been set */
    public boolean hasOutlinks() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'outlinks' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearOutlinks() {
      outlinks = null;
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'headers' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
      return headers;
    }
    
    /** Sets the value of the 'headers' field */
    public org.apache.gora.examples.generated.WebPage.Builder setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[5], value);
      this.headers = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'headers' field has been set */
    public boolean hasHeaders() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'headers' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearHeaders() {
      headers = null;
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'metadata' field */
    public org.apache.gora.examples.generated.Metadata getMetadata() {
      return metadata;
    }
    
    /** Sets the value of the 'metadata' field */
    public org.apache.gora.examples.generated.WebPage.Builder setMetadata(org.apache.gora.examples.generated.Metadata value) {
      validate(fields()[6], value);
      this.metadata = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'metadata' field has been set */
    public boolean hasMetadata() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'metadata' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearMetadata() {
      metadata = null;
      fieldSetFlags()[6] = false;
      return this;
    }
    
    @Override
    public WebPage build() {
      try {
        WebPage record = new WebPage();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.content = fieldSetFlags()[2] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[2]);
        record.parsedContent = fieldSetFlags()[3] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[3]));
        record.outlinks = fieldSetFlags()[4] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[4]));
        record.headers = fieldSetFlags()[5] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[5]);
        record.metadata = fieldSetFlags()[6] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public WebPage.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public WebPage newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'url' field.
		   */
	  public java.lang.CharSequence getUrl() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'url' field.
		   * @param value the value to set.
	   */
	  public void setUrl(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUrlDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'content' field.
		   */
	  public java.nio.ByteBuffer getContent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'content' field.
		   * @param value the value to set.
	   */
	  public void setContent(java.nio.ByteBuffer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isContentDirty(java.nio.ByteBuffer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'parsedContent' field.
		   */
	  public java.util.List<java.lang.CharSequence> getParsedContent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'parsedContent' field.
		   * @param value the value to set.
	   */
	  public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'outlinks' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'outlinks' field.
		   * @param value the value to set.
	   */
	  public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'headers' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'headers' field.
		   * @param value the value to set.
	   */
	  public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'metadata' field.
		   */
	  public org.apache.gora.examples.generated.Metadata getMetadata() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metadata' field.
		   * @param value the value to set.
	   */
	  public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
        return EncoderFactory.get().binaryEncoder(getOrCreateOutputStream(), null);
        return EncoderFactory.get().jsonEncoder(schema, getOrCreateOutputStream());
        return DecoderFactory.get().binaryDecoder(getOrCreateInputStream(), null);
        return DecoderFactory.get().jsonDecoder(schema, getOrCreateInputStream());
    int fieldIndex = persistent.getSchema().getField(fieldName).pos();
    @SuppressWarnings("unchecked")
    int fieldIndex = persistent.getSchema().getField(fieldName).pos(); //.getIndexNamed(fieldName); throws org.apache.avro.AvroRuntimeException: Not a union:
  /**
   * Add our own serializer (obtained via the {@link PersistentSerialization} 
   * wrapper) to any other <code>io.serializations</code> which may be specified 
   * within existing Hadoop configuration.
   * 
   * @param conf the Hadoop configuration object
   * @param reuseObjects boolean parameter to reuse objects
   */
import org.apache.avro.specific.SpecificDatumReader;
* Hadoop deserializer using {@link SpecificDatumReader}
   implements Deserializer<Persistent> {
  private Class<? extends Persistent> persistentClass;
  private SpecificDatumReader<Persistent> datumReader;
  public PersistentDeserializer(Class<? extends Persistent> c, boolean reuseObjects) {
      datumReader = new SpecificDatumReader<Persistent>(schema);
  @Override
* supplies an input stream that is only valid until the end of one
* record serialization. Each time deserialize() is called, the IS
* is advanced to point to the right location, so we should not
* buffer the whole input stream at once.
*/
    decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);
  @Override
  public Persistent deserialize(Persistent persistent) throws IOException {
public class PersistentSerialization
implements Serialization<Persistent> {
  public Deserializer<Persistent> getDeserializer(Class<Persistent> c) {
  public Serializer<Persistent> getSerializer(Class<Persistent> c) {
 * or more contributor license agreements. See the NOTICE file
 * regarding copyright ownership. The ASF licenses this file
 * with the License. You may obtain a copy of the License at
 * http://www.apache.org/licenses/LICENSE-2.0
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
 * Hadoop serializer using Avro's {@link SpecificDatumWriter}
 * with {@link BinaryEncoder}.
public class PersistentSerializer implements Serializer<Persistent> {
  private SpecificDatumWriter<Persistent> datumWriter;
  private BinaryEncoder encoder;
    this.datumWriter = new SpecificDatumWriter<Persistent>();
  @Override
  /**
   * Open a connection for the {@link OutputStream}; should be
   * called before serialization occurs. N.B. the {@link PersistentSerializer#close()}
   * should be called 'finally' after serialization is complete.
   */
  @Override
    encoder = EncoderFactory.get().directBinaryEncoder(out, null);
  /**
   * Do the serialization of the {@link Persistent} object
   */
  public void serialize(Persistent persistent) throws IOException {

import org.apache.avro.Schema.Field;

import org.apache.gora.util.AvroUtils;
  try{
    long deletedRows = 0;
      Result<K,T> result = query.execute();

      while(result.next()) {
        if(delete(result.getKey()))
          deletedRows;
      }
      return 0;
    }
    catch(Exception e){
      return 0;
    }
    List<Field> otherFields = obj.getSchema().getFields();
    String[] otherFieldStrings = new String[otherFields.size()];
    for(int i = 0; i<otherFields.size(); i ){
      otherFieldStrings[i] = otherFields.get(i).name();
    }
    if(Arrays.equals(fields, otherFieldStrings)) { 
    T newObj = (T) AvroUtils.deepClonePersistent(obj); 
      for(int i = 0; i<otherFields.size(); i) {
      int index = otherFields.get(i).pos(); 
      newObj.put(index, obj.get(index));
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import java.util.List;

import org.apache.avro.Schema.Field;
import org.apache.avro.specific.SpecificRecord;

import org.apache.gora.persistency.Dirtyable;

* Objects that are persisted by Gora implements this interface.
*/
public interface Persistent extends SpecificRecord, Dirtyable {

  public static String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
* Clears the inner state of the object without any modification to the actual
* data on the data store. This method should be called before re-using the
* object to hold the data for another result.
*/

* Returns whether the field has been modified.
*
* @param fieldIndex
* the offset of the field in the object
* @return whether the field has been modified.
*/
* Returns whether the field has been modified.
*
* @param field
* the name of the field
* @return whether the field has been modified.
*/

* Sets all the fields of the object as dirty.
*/

* Sets the field as dirty.
*
* @param fieldIndex
* the offset of the field in the object
*/

* Sets the field as dirty.
*
* @param field
* the name of the field
*/

* Clears the field as dirty.
*
* @param fieldIndex
* the offset of the field in the object
*/

* Clears the field as dirty.
*
* @param field
* the name of the field
*/
* Get an object which can be used to mark this field as deleted (rather than
* state unknown, which is indicated by null).
*
* @return a tombstone.
*/
  public abstract Tombstone getTombstone();
* Get a list of fields from this persistent object's schema that are not
* managed by Gora.
*
* @return the unmanaged fields
*/
  public List<Field> getUnmanagedFields();
   * Constructs a new instance of the object by using appropriate builder. This
   * method is intended to be used by Gora framework.
   * 
   * @return a new instance of the object
  Persistent newInstance();
  /** Class of the key to be used */
  
  /** Class of the persistent objects to be stored */
  /** Constructor of the key */
  /** Object's key */
  
  /** Persistent object of class T */
  /** Flag to be used to determine if a key is persistent or not */
  /**
   * Default constructor for this class.
   * @param keyClass.
   * @param persistentClass
   */
    return keyClass.newInstance();
    return (T) persistent.newInstance();
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import java.util.Collection;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.gora.persistency.Dirtyable;
* Base classs implementing common functionality for Persistent classes.
*/
public abstract class PersistentBase extends SpecificRecordBase implements
    Persistent {
  public static class PersistentData extends SpecificData {
    private static final PersistentData INSTANCE = new PersistentData();
    public static PersistentData get() {
      return INSTANCE;
    public boolean equals(SpecificRecord obj1, SpecificRecord that) {
      if (that == obj1)
        return true; // identical object
      if (!(that instanceof SpecificRecord))
        return false; // not a record
      if (obj1.getClass() != that.getClass())
        return false; // not same schema
      return PersistentData.get().compare(obj1, that, obj1.getSchema(), true) == 0;
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirtyBytes.put(i, (byte) 0);
    for (Field field : getSchema().getFields()) {
      clearDirynessIfFieldIsDirtyable(field.pos());
  }

  private void clearDirynessIfFieldIsDirtyable(int fieldIndex) {
    if (fieldIndex == 0)
      return;
    Object value = get(fieldIndex);
    if (value instanceof Dirtyable) {
      ((Dirtyable) value).clearDirty();
  public void clearDirty(int fieldIndex) {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    currentByte = (byte) ((~(1 << bitOffset)) & currentByte);
    dirtyBytes.put(byteOffset, currentByte);
    clearDirynessIfFieldIsDirtyable(fieldIndex);

  @Override
  public void clearDirty(String field) {
    clearDirty(getSchema().getField(field).pos());
  }

  @Override
  public boolean isDirty() {
    boolean isSubRecordDirty = false;
    for (Field field : fields) {
      isSubRecordDirty = isSubRecordDirty || checkIfMutableFieldAndDirty(field);
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    boolean dirty = false;
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirty = dirty || dirtyBytes.get(i) != 0;
    }
    return isSubRecordDirty || dirty;
  }

  private boolean checkIfMutableFieldAndDirty(Field field) {
    if (field.pos() == 0)
      return false;
    switch (field.schema().getType()) {
    case RECORD:
    case MAP:
    case ARRAY:
      Object value = get(field.pos());
      return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
    case UNION:
      value = get(field.pos());
      return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
    default:
      break;
    }
    return false;
  }

  @Override
  public boolean isDirty(int fieldIndex) {
    Field field = getSchema().getFields().get(fieldIndex);
    boolean isSubRecordDirty = checkIfMutableFieldAndDirty(field);
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    return isSubRecordDirty || 0 != ((1 << bitOffset) & currentByte);
  }

  @Override
  public boolean isDirty(String fieldName) {
    Field field = getSchema().getField(fieldName);
    if(field == null){
      throw new IndexOutOfBoundsException
      ("Field " fieldName  " does not exist in this schema.");
    }
    return isDirty(field.pos());
  }

  @Override
  public void setDirty() {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirtyBytes.put(i, (byte) -128);
    }
  }

  @Override
  public void setDirty(int fieldIndex) {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    currentByte = (byte) ((1 << bitOffset) | currentByte);
    dirtyBytes.put(byteOffset, currentByte);
  }

  @Override
  public void setDirty(String field) {
    setDirty(getSchema().getField(field).pos());
  }

  private ByteBuffer getDirtyBytes() {
    return (ByteBuffer) get(0);
  }

  @Override
  public void clear() {
    Collection<Field> unmanagedFields = getUnmanagedFields();
    for (Field field : getSchema().getFields()) {
      if (!unmanagedFields.contains(field))
        continue;
      put(field.pos(), PersistentData.get().deepCopy(field.schema(), PersistentData.get().getDefaultValue(field)));
    }
    clearDirty();
  }

  @Override
  public boolean equals(Object that) {
    if (that == this) {
      return true;
    } else if (that instanceof Persistent) {
      return PersistentData.get().equals(this, (SpecificRecord) that);
    } else {
      return false;
    }
  public List<Field> getUnmanagedFields(){
    List<Field> fields = getSchema().getFields();
    return fields.subList(1, fields.size());
  
      return keyClass.newInstance();
    try {
      return (T) persistentClass.newInstance();
    } catch (InstantiationException e) {
      throw new RuntimeException(e);
    } catch (IllegalAccessException e) {
      e.printStackTrace();
      throw new RuntimeException(e);
    }
  private void clearReadable() {
	  // TODO Auto-generated method stub
	  
	@Override
    return true;
    return true;
  public static final String SCHEMA_NAME = "schema.name";

      Properties properties = new Properties();
          .getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
          throws GoraException {
          ReflectionUtils.newInstance(dataStoreClass);
          throws GoraException {
          throws GoraException {
          = (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
          throws GoraException {
  /**
   * Looks for the <code>gora-&lt;classname&gt;-mapping.xml</code> as a resource 
   * on the classpath. This can however also be specified within the 
   * <code>gora.properties</code> file with the key 
   * <code>gora.&lt;classname&gt;.mapping.file=</code>.
   * @param properties which hold keys from which we can obtain values for datastore mappings.
   * @param store {@link org.apache.gora.store.DataStore} object to get the mapping for.
   * @param defaultValue default value for the <code>gora-&lt;classname&gt;-mapping.xml</code>
   * @return mappingFilename if one is located.
   * @throws IOException if there is a problem reading or obtaining the mapping file.
   */
import java.util.ArrayList;
import java.util.List;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter; 
import org.apache.gora.persistency.Persistent;
  protected SpecificDatumReader<T> datumReader;
  protected SpecificDatumWriter<T> datumWriter;
    datumReader = new SpecificDatumReader<T>(schema);
    datumWriter = new SpecificDatumWriter<T>(schema);
    return getFields();
  }
  
  protected String[] getFields() {
    List<Field> schemaFields = beanFactory.getCachedPersistent().getSchema().getFields();
    
    List<Field> list = new ArrayList<Field>();
    for (Field field : schemaFields) {
      if (!Persistent.DIRTY_BYTES_FIELD_NAME.equalsIgnoreCase(field.name())) {
        list.add(field);
      }
    }
    schemaFields = list;
    
    String[] fieldNames = new String[schemaFields.size()];
    for(int i = 0; i<fieldNames.length; i ){
      fieldNames[i] = schemaFields.get(i).name();
    }
    
    return fieldNames;
   * First the schema name in the {@link Configuration} is used. If null,
   * the schema name in the defined properties is returned. If null then
    String confSchemaName = getOrCreateConf().get("preferred.schema.name");
    if (confSchemaName != null) {
      return confSchemaName;
    }
      return schemaName;
      return mappingSchemaName;
    return StringUtils.getClassname(persistentClass);
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
 * An utility class for Avro related tasks
    for (Field field : fields) {



  public static Schema getSchema(Class<? extends Persistent> clazz)
      throws SecurityException, NoSuchFieldException, IllegalArgumentException,
      IllegalAccessException {

    java.lang.reflect.Field field = clazz.getDeclaredField("SCHEMA$");

  /**
   * Return the field names from a persistent object
   * 
   * @param persistent
   *          the persistent object to get the fields names from
   * @return the field names
   */
  public static String[] getPersistentFieldNames(Persistent persistent) {
    return getSchemaFieldNames(persistent.getSchema());
  }

  /**
   * Return the field names from a schema object
   * 
   * @param persistent
   *          the persistent object to get the fields names from
   * @return the field names
   */
  public static String[] getSchemaFieldNames(Schema schema) {
    List<Field> fields = schema.getFields();
    String[] fieldNames = new String[fields.size() - 1];
    for (int i = 0; i < fieldNames.length; i) {
      fieldNames[i] = fields.get(i  1).name();
    }
    return fieldNames;
  }

  public static <T extends Persistent> T deepClonePersistent(T persistent) {
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    BinaryEncoder enc = EncoderFactory.get().binaryEncoder(bos, null);
    SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<Persistent>(
        persistent.getSchema());
    try {
      writer.write(persistent, enc);
    } catch (IOException e) {
      throw new RuntimeException(
          "Unable to serialize avro object to byte buffer - "
               "please report this issue to the Gora bugtracker "
               "or your administrator.");
    }
    byte[] value = bos.toByteArray();
    Decoder dec = DecoderFactory.get().binaryDecoder(value, null);
    @SuppressWarnings("unchecked")
    SpecificDatumReader<T> reader = new SpecificDatumReader<T>(
        (Class<T>) persistent.getClass());
    try {
      return reader.read(null, dec);
    } catch (IOException e) {
      throw new RuntimeException(
          "Unable to deserialize avro object from byte buffer - "
               "please report this issue to the Gora bugtracker "
               "or your administrator.");
    }

  }

import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
  public static <T> T fromBytes( byte[] val, Schema schema
      , SpecificDatumReader<T> datumReader, T object)
      return (T)Enum.valueOf(ReflectData.get().getClass(schema), symbol);
    case STRING:  return (T)new Utf8(toString(val));
    case BYTES:   return (T)ByteBuffer.wrap(val);
    case INT:     return (T)Integer.valueOf(bytesToVint(val));
    case LONG:    return (T)Long.valueOf(bytesToVlong(val));
    case FLOAT:   return (T)Float.valueOf(toFloat(val));
    case DOUBLE:  return (T)Double.valueOf(toDouble(val));
    case BOOLEAN: return (T)Boolean.valueOf(val[0] != 0);
    case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, schema, (SpecificRecord)object);
  @SuppressWarnings("unchecked")
  public static <T> byte[] toBytes(T o, Schema schema
      , SpecificDatumWriter<T> datumWriter)
    case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, schema, (SpecificRecord)o);
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter; 
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.util.ByteBufferInputStream;
import org.apache.avro.util.ByteBufferOutputStream;
  public static<T extends SpecificRecord> void serialize(OutputStream os,
      SpecificDatumWriter<T> datumWriter, Schema schema, T object)
    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
    datumWriter.write(object, encoder);
  public static<T> void serialize(OutputStream os,
      SpecificDatumWriter<T> datumWriter, Schema schema, T object)
      throws IOException {

    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
    datumWriter.write(object, encoder);
    encoder.flush();
  }
  
  /**
   * Serializes the field object using the datumWriter.
   */
  public static<T extends SpecificRecord> byte[] serialize(SpecificDatumWriter<T> datumWriter
      , Schema schema, T object) throws IOException {
  /**
   * Serializes the field object using the datumWriter.
   */
  public static<T> byte[] serialize(SpecificDatumWriter<T> datumWriter
      , Schema schema, T object) throws IOException {
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    serialize(os, datumWriter, schema, object);
    return os.toByteArray();
  }
  
  public static<K, T extends SpecificRecord> T deserialize(InputStream is,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
    decoder = DecoderFactory.get().binaryDecoder(is, decoder);
    return (T)datumReader.read(object, decoder);
  public static<K, T extends SpecificRecord> T deserialize(byte[] bytes,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
    decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
    return (T)datumReader.read(object, decoder);
   * Deserializes the field object using the datumReader.
  public static<K, T> T deserialize(byte[] bytes,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
      throws IOException {
    decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
    return (T)datumReader.read(object, decoder);
  
import java.lang.reflect.Method;

import org.apache.avro.specific.SpecificRecordBuilderBase;
import org.apache.gora.persistency.Persistent;
  
  public static <T extends Persistent> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) throws SecurityException
    , NoSuchMethodException, IllegalArgumentException, IllegalAccessException, InvocationTargetException {
    return (SpecificRecordBuilderBase<T>) clazz.getMethod("newBuilder").invoke(null);
  }
  
import org.apache.gora.persistency.Tombstone;
  public Tombstone getTombstone() {
    return new Tombstone(){};
  public Persistent newInstance() {
    return new MockPersistent();
      preferredSchema = properties.getProperty(PREF_SCH_NAME);
      dynamoDBClient = getClient(properties.getProperty(CLI_TYP_PROP),(AWSCredentials)getConf());
      dynamoDBClient.setEndpoint(properties.getProperty(ENDPOINT_PROP));
      consistency = properties.getProperty(CONSISTENCY_READS);
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


   * {@inheritDoc} Serializes the Persistent data and saves in HBase. Topmost
   * fields of the record are persisted in "raw" format (not avro serialized).
   * This behavior happens in maps and arrays too.
   * ["null","type"] type (a.k.a. optional field) is persisted like as if it is
   * ["type"], but the column get deleted if value==null (so value read after
   * will be null).
   * @param persistent
   *          Record to be persisted in HBase
    try {
      List<Field> fields = schema.getFields();
      for (int i = 1; i < fields.size(); i) {
        if (!persistent.isDirty(i)) {
        Field field = fields.get(i);
          throw new RuntimeException("HBase mapping for field ["
               persistent.getClass().getName()  "#"  field.name()
               "] not found. Wrong gora-hbase-mapping.xml?");
        addPutsAndDeletes(put, delete, o, field.schema().getType(),
            field.schema(), hcol, hcol.getQualifier());
      if (put.size() > 0) {
      if (delete.size() > 0) {
        table.delete(delete);
        table.delete(delete); // HBase sometimes does not delete arbitrarily
    } catch (IOException ex2) {
  private void addPutsAndDeletes(Put put, Delete delete, Object o, Type type,
      Schema schema, HBaseColumn hcol, byte[] qualifier) throws IOException {
    switch (type) {
    case UNION:
      if (isNullable(schema) && o == null) {
        if (qualifier == null) {
          delete.deleteFamily(hcol.getFamily());
        } else {
          delete.deleteColumn(hcol.getFamily(), qualifier);
        }
      } else {
//        int index = GenericData.get().resolveUnion(schema, o);
        int index = getResolvedUnionIndex(schema);
        if (index > 1) {  //if more than 2 type in union, serialize directly for now
          byte[] serializedBytes = toBytes(o, schema);
          put.add(hcol.getFamily(), qualifier, serializedBytes);
        } else {
          Schema resolvedSchema = schema.getTypes().get(index);
          addPutsAndDeletes(put, delete, o, resolvedSchema.getType(),
              resolvedSchema, hcol, qualifier);
        }
      }
      break;
    case MAP:
      // if it's a map that has been modified, then the content should be replaced by the new one
      // This is because we don't know if the content has changed or not.
      if (qualifier == null) {
        delete.deleteFamily(hcol.getFamily());
      } else {
        delete.deleteColumn(hcol.getFamily(), qualifier);
      }
      @SuppressWarnings({ "rawtypes", "unchecked" })
      Set<Entry> set = ((Map) o).entrySet();
      for (@SuppressWarnings("rawtypes") Entry entry : set) {
        byte[] qual = toBytes(entry.getKey());
        addPutsAndDeletes(put, delete, entry.getValue(), schema.getValueType()
            .getType(), schema.getValueType(), hcol, qual);
      }
      break;
    case ARRAY:
      List<?> array = (List<?>) o;
      int j = 0;
      for (Object item : array) {
        addPutsAndDeletes(put, delete, item, schema.getElementType().getType(),
            schema.getElementType(), hcol, Bytes.toBytes(j));
      }
      break;
    default:
      byte[] serializedBytes = toBytes(o, schema);
      put.add(hcol.getFamily(), qualifier, serializedBytes);
      break;
    }
  }

  private boolean isNullable(Schema unionSchema) {
    for (Schema innerSchema : unionSchema.getTypes()) {
      if (innerSchema.getType().equals(Schema.Type.NULL)) {
        return true;
      }
    }
    return false;
  }

      boolean isAllFields = Arrays.equals(fields, getFields());
      addFamilyOrColumn(get, col, fieldSchema);
  private void addFamilyOrColumn(Get get, HBaseColumn col, Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(get, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      get.addFamily(col.family);
      break;
    default:
      get.addColumn(col.family, col.qualifier);
      break;
    }
  }

  private void addFields(Scan scan, Query<K, T> query) throws IOException {
      addFamilyOrColumn(scan, col, fieldSchema);
  private void addFamilyOrColumn(Scan scan, HBaseColumn col, Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(scan, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      scan.addFamily(col.family);
      break;
    default:
      scan.addColumn(col.family, col.qualifier);
      break;
    }
  }

  // TODO: HBase Get, Scan, Delete should extend some common interface with
  // addFamily, etc
  private void addFields(Delete delete, Query<K, T> query)    throws IOException {
      addFamilyOrColumn(delete, col, fieldSchema);
    }
  }

  private void addFamilyOrColumn(Delete delete, HBaseColumn col,
      Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(delete, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      delete.deleteFamily(col.family);
      break;
    default:
      delete.deleteColumn(col.family, col.qualifier);
      break;
      setField(result,persistent, col, field, fieldSchema);
    }
    persistent.clearDirty();
    return persistent;
  }

  private void setField(Result result, T persistent, HBaseColumn col,
      Field field, Schema fieldSchema) throws IOException {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      if (index > 1) { //if more than 2 type in union, deserialize directly for now
        byte[] val = result.getValue(col.getFamily(), col.getQualifier());
        if (val == null) {
          return;
        }
        setField(persistent, field, val);
      } else {
        Schema resolvedSchema = fieldSchema.getTypes().get(index);
        setField(result, persistent, col, field, resolvedSchema);
      }
      break;
    case MAP:
      NavigableMap<byte[], byte[]> qualMap = result.getNoVersionMap().get(
          col.getFamily());
      if (qualMap == null) {
        return;
      }
      Schema valueSchema = fieldSchema.getValueType();
      Map<Utf8, Object> map = new HashMap<Utf8, Object>();
      for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
        map.put(new Utf8(Bytes.toString(e.getKey())),
            fromBytes(valueSchema, e.getValue()));
      }
      setField(persistent, field, map);
      break;
    case ARRAY:
      qualMap = result.getFamilyMap(col.getFamily());
      if (qualMap == null) {
        return;
      }
      valueSchema = fieldSchema.getElementType();
      ArrayList<Object> arrayList = new ArrayList<Object>();
      DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<Object>(arrayList);
      for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
        dirtyListWrapper.add(fromBytes(valueSchema, e.getValue()));
      }
      setField(persistent, field, arrayList);
      break;
    default:
      byte[] val = result.getValue(col.getFamily(), col.getQualifier());
      if (val == null) {
        return;
      }
      setField(persistent, field, val);
      break;
    }
  }

  //TODO temporary solution, has to be changed after implementation of saving the index of union type
  private int getResolvedUnionIndex(Schema unionScema) {
    if (unionScema.getTypes().size() == 2) {

      // schema [type0, type1]
      Type type0 = unionScema.getTypes().get(0).getType();
      Type type1 = unionScema.getTypes().get(1).getType();

      // Check if types are different and there's a "null", like ["null","type"]
      // or ["type","null"]
      if (!type0.equals(type1)
          && (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {

        if (type0.equals(Schema.Type.NULL))
          return 1;
        else
          return 0;
    return 2;
    persistent.put(field.pos(), new DirtyMapWrapper(map));
  @SuppressWarnings({ "rawtypes", "unchecked" })
  private void setField(T persistent, Field field, List list) {
    persistent.put(field.pos(), new DirtyListWrapper(list));
}
import java.util.Map;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.RowMutations;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;

  @Override
  public void batch(List<? extends Row> actions, Object[] results)
      throws IOException, InterruptedException {
    // TODO Auto-generated method stub
    getTable().batch(actions, results);
    
  }

  @Override
  public Object[] batch(List<? extends Row> actions) throws IOException,
      InterruptedException {
    // TODO Auto-generated method stub
    return getTable().batch(actions);
  }

  @Override
  public void mutateRow(RowMutations rm) throws IOException {
    // TODO Auto-generated method stub
    
  }

  @Override
  public Result append(Append append) throws IOException {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol> T coprocessorProxy(Class<T> protocol,
      byte[] row) {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol, R> Map<byte[], R> coprocessorExec(
      Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable)
      throws IOException, Throwable {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol, R> void coprocessorExec(
      Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable,
      Callback<R> callback) throws IOException, Throwable {
    // TODO Auto-generated method stub
    
  }

  @Override
  public void setAutoFlush(boolean autoFlush) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public long getWriteBufferSize() {
    // TODO Auto-generated method stub
    return 0;
  }

  @Override
  public void setWriteBufferSize(long writeBufferSize) throws IOException {
    // TODO Auto-generated method stub
    
  }
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.io.EncoderFactory;


  private static ThreadLocal<ByteArrayOutputStream> outputStream =
      new ThreadLocal<ByteArrayOutputStream>();
  
  public static final ThreadLocal<BinaryEncoder> encoders =
      new ThreadLocal<BinaryEncoder>();
  public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = 
      new ConcurrentHashMap<String, SpecificDatumReader<?>>();
     
  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = 
      new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
   * Deserializes an array of bytes matching the given schema to the proper basic 
   * (enum, Utf8,...) or complex type (Persistent/Record).
  @SuppressWarnings({ "rawtypes" })
      // For UNION schemas, must use a specific SpecificDatumReader
      String schemaId = schema.getType().equals(Schema.Type.UNION) ? String.valueOf(schema.hashCode()) : schema.getFullName();      
      
      SpecificDatumReader<?> reader = (SpecificDatumReader<?>)readerMap.get(schemaId);
      if (reader == null) {
        reader = new SpecificDatumReader(schema);// ignore dirty bits
        SpecificDatumReader localReader=null;
        if((localReader=readerMap.putIfAbsent(schemaId, reader))!=null) {
          reader = localReader;
      BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(val, null);
      return reader.read(null, decoder);
    case STRING:  return Bytes.toBytes(((CharSequence)o).toString()); // TODO: maybe ((Utf8)o).getBytes(); ?
      SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap.get(schema.getFullName());
      if (writer == null) {
        writer = new SpecificDatumWriter(schema);// ignore dirty bits
        writerMap.put(schema.getFullName(),writer);

      BinaryEncoder encoderFromCache = encoders.get();
      ByteArrayOutputStream bos = new ByteArrayOutputStream();
      outputStream.set(bos);
      BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(bos, null);
      if (encoderFromCache == null) {

      ByteArrayOutputStream os = outputStream.get();

      writer.write(o, encoder);
    //htu.getConfiguration().set("hbase.zookeeper.quorum", "localhost");
    //htu.getConfiguration().setInt("hbase.zookeeper.property.clientPort", 2181);
    
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;

  private static final Logger LOG = LoggerFactory.getLogger(SolrStore.class);


  // protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";

  /**
   * Default schema index with value "0" used when AVRO Union data types are
   * stored
   */
  public static int DEFAULT_UNION_SCHEMA = 0;

  /*
   * Create a threadlocal map for the datum readers and writers, because they
   * are not thread safe, at least not before Avro 1.4.0 (See AVRO-650). When
   * they are thread safe, it is possible to maintain a single reader and writer
   * pair for every schema, instead of one for every thread.
   */

  public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<String, SpecificDatumReader<?>>();

  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<String, SpecificDatumWriter<?>>();

  public void initialize(Class<K> keyClass, Class<T> persistentClass,
      Properties properties) {
    super.initialize(keyClass, persistentClass, properties);
      String mappingFile = DataStoreFactory.getMappingFile(properties, this,
          DEFAULT_MAPPING_FILE);
      mapping = readMapping(mappingFile);
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    solrServerUrl = DataStoreFactory.findProperty(properties, this,
        SOLR_URL_PROPERTY, null);
    solrConfig = DataStoreFactory.findProperty(properties, this,
        SOLR_CONFIG_PROPERTY, null);
    solrSchema = DataStoreFactory.findProperty(properties, this,
        SOLR_SCHEMA_PROPERTY, null);
    LOG.info("Using Solr server at "  solrServerUrl);
    adminServer = new HttpSolrServer(solrServerUrl);
    server = new HttpSolrServer(solrServerUrl  "/"  mapping.getCoreName());
    if (autoCreateSchema) {
    String batchSizeString = DataStoreFactory.findProperty(properties, this,
        SOLR_BATCH_SIZE_PROPERTY, null);
    if (batchSizeString != null) {
        batchSize = Integer.parseInt(batchSizeString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid batch size '"  batchSizeString  "', using default "
             DEFAULT_BATCH_SIZE);
    batch = new ArrayList<SolrInputDocument>(batchSize);
    String commitWithinString = DataStoreFactory.findProperty(properties, this,
        SOLR_COMMIT_WITHIN_PROPERTY, null);
    if (commitWithinString != null) {
        commitWithin = Integer.parseInt(commitWithinString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid commit within '"  commitWithinString
             "', using default "  DEFAULT_COMMIT_WITHIN);
    String resultsSizeString = DataStoreFactory.findProperty(properties, this,
        SOLR_RESULTS_SIZE_PROPERTY, null);
    if (resultsSizeString != null) {
        resultsSize = Integer.parseInt(resultsSizeString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid results size '"  resultsSizeString
             "', using default "  DEFAULT_RESULTS_SIZE);
  private SolrMapping readMapping(String filename) throws IOException {
      Document doc = builder.build(getClass().getClassLoader()
          .getResourceAsStream(filename));
      List<Element> classes = doc.getRootElement().getChildren("class");
      for (Element classElement : classes) {
        if (classElement.getAttributeValue("keyClass").equals(
            keyClass.getCanonicalName())
            && classElement.getAttributeValue("name").equals(
                persistentClass.getCanonicalName())) {
          String tableName = getSchemaName(
              classElement.getAttributeValue("table"), persistentClass);
          map.setCoreName(tableName);
          Element primaryKeyEl = classElement.getChild("primarykey");
          map.setPrimaryKey(primaryKeyEl.getAttributeValue("column"));
          List<Element> fields = classElement.getChildren("field");
          for (Element field : fields) {
            String fieldName = field.getAttributeValue("name");
            String columnName = field.getAttributeValue("column");
            map.addField(fieldName, columnName);
    } catch (Exception ex) {
      throw new IOException(ex);
      if (!schemaExists())
        CoreAdminRequest.createCore(mapping.getCoreName(),
            mapping.getCoreName(), adminServer, solrConfig, solrSchema);
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
      server.deleteByQuery("*:*");
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
      server.deleteByQuery("*:*");
    } catch (Exception e) {
      // ignore?
      // LOG.error(e.getMessage());
      // LOG.error(e.getStackTrace().toString());
      CoreAdminRequest.unloadCore(mapping.getCoreName(), adminServer);
    } catch (Exception e) {
      if (e.getMessage().contains("No such core")) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
      CoreAdminResponse rsp = CoreAdminRequest.getStatus(mapping.getCoreName(),
          adminServer);
      exists = rsp.getUptime(mapping.getCoreName()) != null;
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  private static final String toDelimitedString(String[] arr, String sep) {
    if (arr == null || arr.length == 0) {
    for (int i = 0; i < arr.length; i) {
      if (i > 0)
        sb.append(sep);
      sb.append(arr[i]);
  public static String escapeQueryKey(String key) {
    if (key == null) {
    for (int i = 0; i < key.length(); i) {
      char c = key.charAt(i);
      switch (c) {
      case ':':
      case '*':
        sb.append("\\"  c);
        break;
      default:
        sb.append(c);
  public T get(K key, String[] fields) {
    params.set(CommonParams.QT, "/get");
    params.set(CommonParams.FL, toDelimitedString(fields, ","));
    params.set("id", key.toString());
      QueryResponse rsp = server.query(params);
      Object o = rsp.getResponse().get("doc");
      if (o == null) {
      return newInstance((SolrDocument) o, fields);
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public T newInstance(SolrDocument doc, String[] fields) throws IOException {
    if (fields == null) {
      fields = fieldMap.keySet().toArray(new String[fieldMap.size()]);
    for (String f : fields) {
      Field field = fieldMap.get(f);
      if (pk.equals(f)) {
        sf = mapping.getSolrField(f);
      Object sv = doc.get(sf);
      if (sv == null) {

      Object v = deserializeFieldValue(field, fieldSchema, sv, persistent);
      persistent.put(field.pos(), v);
      persistent.setDirty(field.pos());

  @SuppressWarnings("rawtypes")
  private SpecificDatumReader getDatumReader(String schemaId, Schema fieldSchema) {
    SpecificDatumReader<?> reader = (SpecificDatumReader<?>) readerMap
        .get(schemaId);
    if (reader == null) {
      reader = new SpecificDatumReader(fieldSchema);// ignore dirty bits
      SpecificDatumReader localReader = null;
      if ((localReader = readerMap.putIfAbsent(schemaId, reader)) != null) {
        reader = localReader;
      }
    }
    return reader;
  }

  @SuppressWarnings("rawtypes")
  private SpecificDatumWriter getDatumWriter(String schemaId, Schema fieldSchema) {
    SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap
        .get(schemaId);
    if (writer == null) {
      writer = new SpecificDatumWriter(fieldSchema);// ignore dirty bits
      writerMap.put(schemaId, writer);
    }

    return writer;
  }

  @SuppressWarnings("unchecked")
  private Object deserializeFieldValue(Field field, Schema fieldSchema,
      Object solrValue, T persistent) throws IOException {
    Object fieldValue = null;
    switch (fieldSchema.getType()) {
    case MAP:
    case ARRAY:
    case RECORD:
      @SuppressWarnings("rawtypes")
      SpecificDatumReader reader = getDatumReader(fieldSchema.getFullName(),
          fieldSchema);
      fieldValue = IOUtils.deserialize((byte[]) solrValue, reader, fieldSchema,
          persistent.get(field.pos()));
      break;
    case ENUM:
      fieldValue = AvroUtils.getEnumValue(fieldSchema, (String) solrValue);
      break;
    case FIXED:
      throw new IOException("???");
      // break;
    case BYTES:
      fieldValue = ByteBuffer.wrap((byte[]) solrValue);
      break;
    case STRING:
      fieldValue = new Utf8(solrValue.toString());
      break;
    case UNION:
      if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
        // schema [type0, type1]
        Type type0 = fieldSchema.getTypes().get(0).getType();
        Type type1 = fieldSchema.getTypes().get(1).getType();

        // Check if types are different and there's a "null", like
        // ["null","type"] or ["type","null"]
        if (!type0.equals(type1)) {
          if (type0.equals(Schema.Type.NULL))
            fieldSchema = fieldSchema.getTypes().get(1);
          else
            fieldSchema = fieldSchema.getTypes().get(0);
        } else {
          fieldSchema = fieldSchema.getTypes().get(0);
        }
        fieldValue = deserializeFieldValue(field, fieldSchema, solrValue,
            persistent);
      } else {
        @SuppressWarnings("rawtypes")
        SpecificDatumReader unionReader = getDatumReader(
            String.valueOf(fieldSchema.hashCode()), fieldSchema);
        fieldValue = IOUtils.deserialize((byte[]) solrValue, unionReader,
            fieldSchema, persistent.get(field.pos()));
        break;
      }
      break;
    default:
      fieldValue = solrValue;
    }
    return fieldValue;
  }

  public void put(K key, T persistent) {
    if (!persistent.isDirty()) {
    doc.addField(mapping.getPrimaryKey(), key);
    for (Field field : fields) {
      String sf = mapping.getSolrField(field.name());
      if (sf == null) {
      Object v = persistent.get(field.pos());
      if (v == null) {
      v = serializeFieldValue(fieldSchema, v);
      doc.addField(sf, v);

    LOG.info("DOCUMENT: "  doc);
    batch.add(doc);
    if (batch.size() >= batchSize) {
        add(batch, commitWithin);
      } catch (Exception e) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
  @SuppressWarnings("unchecked")
  private Object serializeFieldValue(Schema fieldSchema, Object fieldValue) {
    switch (fieldSchema.getType()) {
    case MAP:
    case ARRAY:
    case RECORD:
      byte[] data = null;
      try {
        @SuppressWarnings("rawtypes")
        SpecificDatumWriter writer = getDatumWriter(fieldSchema.getFullName(),
            fieldSchema);
        data = IOUtils.serialize(writer, fieldSchema, fieldValue);
      } catch (IOException e) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
      }
      fieldValue = data;
      break;
    case BYTES:
      fieldValue = ((ByteBuffer) fieldValue).array();
      break;
    case ENUM:
    case STRING:
      fieldValue = fieldValue.toString();
      break;
    case UNION:
      // If field's schema is null and one type, we do undertake serialization.
      // All other types are serialized.
      if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
        int schemaPos = getUnionSchema(fieldValue, fieldSchema);
        Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
        fieldValue = serializeFieldValue(unionSchema, fieldValue);
      } else {
        byte[] serilazeData = null;
        try {
          @SuppressWarnings("rawtypes")
          SpecificDatumWriter writer = getDatumWriter(
              String.valueOf(fieldSchema.hashCode()), fieldSchema);
          serilazeData = IOUtils.serialize(writer, fieldSchema, fieldValue);
        } catch (IOException e) {
          LOG.error(e.getMessage(), e.getStackTrace().toString());
        }
        fieldValue = serilazeData;
      }
      break;
    default:
      // LOG.error("Unknown field type: "  fieldSchema.getType());
      break;
    }
    return fieldValue;
  }

  private boolean isNullable(Schema unionSchema) {
    for (Schema innerSchema : unionSchema.getTypes()) {
      if (innerSchema.getType().equals(Schema.Type.NULL)) {
        return true;
      }
    }
    return false;
  }

  /**
   * Given an object and the object schema this function obtains, from within
   * the UNION schema, the position of the type used. If no data type can be
   * inferred then we return a default value of position 0.
   * 
   * @param pValue
   * @param pUnionSchema
   * @return the unionSchemaPosition.
   */
  private int getUnionSchema(Object pValue, Schema pUnionSchema) {
    int unionSchemaPos = 0;
    Iterator<Schema> it = pUnionSchema.getTypes().iterator();
    while (it.hasNext()) {
      Type schemaType = it.next().getType();
      if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
        return unionSchemaPos;
      else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
        return unionSchemaPos;
      else if (pValue instanceof Integer && schemaType.equals(Type.INT))
        return unionSchemaPos;
      else if (pValue instanceof Long && schemaType.equals(Type.LONG))
        return unionSchemaPos;
      else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
        return unionSchemaPos;
      else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
        return unionSchemaPos;
      else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
        return unionSchemaPos;
      else if (pValue instanceof Map && schemaType.equals(Type.MAP))
        return unionSchemaPos;
      else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
        return unionSchemaPos;
      else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
        return unionSchemaPos;
      unionSchemaPos;
    }
    // if we weren't able to determine which data type it is, then we return the
    // default
    return DEFAULT_UNION_SCHEMA;
  }

  public boolean delete(K key) {
      UpdateResponse rsp = server.deleteByQuery(keyField  ":"
           escapeQueryKey(key.toString()));
      LOG.info(rsp.toString());
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public long deleteByQuery(Query<K, T> query) {
    String q = ((SolrQuery<K, T>) query).toSolrQuery();
      UpdateResponse rsp = server.deleteByQuery(q);
      LOG.info(rsp.toString());
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public Result<K, T> execute(Query<K, T> query) {
      return new SolrResult<K, T>(this, query, server, resultsSize);
    } catch (IOException e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
    return new SolrQuery<K, T>(this);
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
      if (batch.size() > 0) {
        add(batch, commitWithin);
    } catch (Exception e) {
    // flush();

  private void add(ArrayList<SolrInputDocument> batch, int commitWithin)
      throws SolrServerException, IOException {
      server.add(batch);
      server.commit(false, true, true);
      server.add(batch, commitWithin);
  }
      CharSequence url = pageview.getUrl();
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.tutorial.log.generated;  
public class MetricDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}]}");
  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence metricDimension;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return metricDimension;
    case 2: return timestamp;
    case 3: return metric;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: metricDimension = (java.lang.CharSequence)(value); break;
    case 2: timestamp = (java.lang.Long)(value); break;
    case 3: metric = (java.lang.Long)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  /**
   * Gets the value of the 'metricDimension' field.
   */
  public java.lang.CharSequence getMetricDimension() {
    return metricDimension;

  /**
   * Sets the value of the 'metricDimension' field.
   * @param value the value to set.
   */
  public void setMetricDimension(java.lang.CharSequence value) {
    this.metricDimension = value;
    setDirty(1);
  
  /**
   * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
    return isDirty(1);

  /**
   * Gets the value of the 'timestamp' field.
   */
  public java.lang.Long getTimestamp() {
    return timestamp;

  /**
   * Sets the value of the 'timestamp' field.
   * @param value the value to set.
   */
  public void setTimestamp(java.lang.Long value) {
    this.timestamp = value;
    setDirty(2);
  
  /**
   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isTimestampDirty(java.lang.Long value) {
    return isDirty(2);

  /**
   * Gets the value of the 'metric' field.
   */
  public java.lang.Long getMetric() {
    return metric;
  }

  /**
   * Sets the value of the 'metric' field.
   * @param value the value to set.
   */
  public void setMetric(java.lang.Long value) {
    this.metric = value;
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetricDirty(java.lang.Long value) {
    return isDirty(3);
  }

  /** Creates a new MetricDatum RecordBuilder */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder() {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder();
  }
  
  /** Creates a new MetricDatum RecordBuilder by copying an existing Builder */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
  }
  
  /** Creates a new MetricDatum RecordBuilder by copying an existing MetricDatum instance */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for MetricDatum instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MetricDatum>
    implements org.apache.avro.data.RecordBuilder<MetricDatum> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence metricDimension;
    private long timestamp;
    private long metric;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing MetricDatum instance */
    private Builder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
            super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.metricDimension)) {
        this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.metricDimension);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.metric)) {
        this.metric = (java.lang.Long) data().deepCopy(fields()[3].schema(), other.metric);
        fieldSetFlags()[3] = true;
      }
    }

    /** Gets the value of the 'metricDimension' field */
    public java.lang.CharSequence getMetricDimension() {
      return metricDimension;
    }
    
    /** Sets the value of the 'metricDimension' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetricDimension(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.metricDimension = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'metricDimension' field has been set */
    public boolean hasMetricDimension() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'metricDimension' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetricDimension() {
      metricDimension = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'timestamp' field */
    public java.lang.Long getTimestamp() {
      return timestamp;
    }
    
    /** Sets the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setTimestamp(long value) {
      validate(fields()[2], value);
      this.timestamp = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'timestamp' field has been set */
    public boolean hasTimestamp() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearTimestamp() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'metric' field */
    public java.lang.Long getMetric() {
      return metric;
    }
    
    /** Sets the value of the 'metric' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetric(long value) {
      validate(fields()[3], value);
      this.metric = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'metric' field has been set */
    public boolean hasMetric() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'metric' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetric() {
      fieldSetFlags()[3] = false;
      return this;
    }
    
    @Override
    public MetricDatum build() {
      try {
        MetricDatum record = new MetricDatum();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.metricDimension = fieldSetFlags()[1] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
        record.metric = fieldSetFlags()[3] ? this.metric : (java.lang.Long) defaultValue(fields()[3]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }
  
  public MetricDatum.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public MetricDatum newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'metricDimension' field.
		   */
	  public java.lang.CharSequence getMetricDimension() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metricDimension' field.
		   * @param value the value to set.
	   */
	  public void setMetricDimension(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'timestamp' field.
		   */
	  public java.lang.Long getTimestamp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'timestamp' field.
		   * @param value the value to set.
	   */
	  public void setTimestamp(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isTimestampDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'metric' field.
		   */
	  public java.lang.Long getMetric() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metric' field.
		   * @param value the value to set.
	   */
	  public void setMetric(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetricDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
  }
  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.tutorial.log.generated;  
public class Pageview extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AAA=\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}]}");
  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[2]);
  private java.lang.CharSequence url;
  private java.lang.CharSequence ip;
  private java.lang.CharSequence httpMethod;
  private java.lang.CharSequence referrer;
  private java.lang.CharSequence userAgent;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return url;
    case 2: return timestamp;
    case 3: return ip;
    case 4: return httpMethod;
    case 5: return httpStatusCode;
    case 6: return responseSize;
    case 7: return referrer;
    case 8: return userAgent;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: url = (java.lang.CharSequence)(value); break;
    case 2: timestamp = (java.lang.Long)(value); break;
    case 3: ip = (java.lang.CharSequence)(value); break;
    case 4: httpMethod = (java.lang.CharSequence)(value); break;
    case 5: httpStatusCode = (java.lang.Integer)(value); break;
    case 6: responseSize = (java.lang.Integer)(value); break;
    case 7: referrer = (java.lang.CharSequence)(value); break;
    case 8: userAgent = (java.lang.CharSequence)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  /**
   * Gets the value of the 'url' field.
   */
  public java.lang.CharSequence getUrl() {
    return url;

  /**
   * Sets the value of the 'url' field.
   * @param value the value to set.
   */
  public void setUrl(java.lang.CharSequence value) {
    this.url = value;
    setDirty(1);
  
  /**
   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUrlDirty(java.lang.CharSequence value) {
    return isDirty(1);

  /**
   * Gets the value of the 'timestamp' field.
   */
  public java.lang.Long getTimestamp() {
    return timestamp;

  /**
   * Sets the value of the 'timestamp' field.
   * @param value the value to set.
   */
  public void setTimestamp(java.lang.Long value) {
    this.timestamp = value;
    setDirty(2);
  
  /**
   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isTimestampDirty(java.lang.Long value) {
    return isDirty(2);

  /**
   * Gets the value of the 'ip' field.
   */
  public java.lang.CharSequence getIp() {
    return ip;

  /**
   * Sets the value of the 'ip' field.
   * @param value the value to set.
   */
  public void setIp(java.lang.CharSequence value) {
    this.ip = value;
    setDirty(3);
  
  /**
   * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isIpDirty(java.lang.CharSequence value) {
    return isDirty(3);

  /**
   * Gets the value of the 'httpMethod' field.
   */
  public java.lang.CharSequence getHttpMethod() {
    return httpMethod;

  /**
   * Sets the value of the 'httpMethod' field.
   * @param value the value to set.
   */
  public void setHttpMethod(java.lang.CharSequence value) {
    this.httpMethod = value;
    setDirty(4);
  
  /**
   * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHttpMethodDirty(java.lang.CharSequence value) {
    return isDirty(4);

  /**
   * Gets the value of the 'httpStatusCode' field.
   */
  public java.lang.Integer getHttpStatusCode() {
    return httpStatusCode;

  /**
   * Sets the value of the 'httpStatusCode' field.
   * @param value the value to set.
   */
  public void setHttpStatusCode(java.lang.Integer value) {
    this.httpStatusCode = value;
    setDirty(5);
  
  /**
   * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
    return isDirty(5);

  /**
   * Gets the value of the 'responseSize' field.
   */
  public java.lang.Integer getResponseSize() {
    return responseSize;

  /**
   * Sets the value of the 'responseSize' field.
   * @param value the value to set.
   */
  public void setResponseSize(java.lang.Integer value) {
    this.responseSize = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isResponseSizeDirty(java.lang.Integer value) {
    return isDirty(6);
  }

  /**
   * Gets the value of the 'referrer' field.
   */
  public java.lang.CharSequence getReferrer() {
    return referrer;
  }

  /**
   * Sets the value of the 'referrer' field.
   * @param value the value to set.
   */
  public void setReferrer(java.lang.CharSequence value) {
    this.referrer = value;
    setDirty(7);
  }
  
  /**
   * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isReferrerDirty(java.lang.CharSequence value) {
    return isDirty(7);
  }

  /**
   * Gets the value of the 'userAgent' field.
   */
  public java.lang.CharSequence getUserAgent() {
    return userAgent;
  }

  /**
   * Sets the value of the 'userAgent' field.
   * @param value the value to set.
   */
  public void setUserAgent(java.lang.CharSequence value) {
    this.userAgent = value;
    setDirty(8);
  }
  
  /**
   * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUserAgentDirty(java.lang.CharSequence value) {
    return isDirty(8);
  }

  /** Creates a new Pageview RecordBuilder */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder() {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder();
  }
  
  /** Creates a new Pageview RecordBuilder by copying an existing Builder */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
  }
  
  /** Creates a new Pageview RecordBuilder by copying an existing Pageview instance */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview other) {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Pageview instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Pageview>
    implements org.apache.avro.data.RecordBuilder<Pageview> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence url;
    private long timestamp;
    private java.lang.CharSequence ip;
    private java.lang.CharSequence httpMethod;
    private int httpStatusCode;
    private int responseSize;
    private java.lang.CharSequence referrer;
    private java.lang.CharSequence userAgent;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Pageview instance */
    private Builder(org.apache.gora.tutorial.log.generated.Pageview other) {
            super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.ip)) {
        this.ip = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ip);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.httpMethod)) {
        this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[4].schema(), other.httpMethod);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.httpStatusCode)) {
        this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.httpStatusCode);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.responseSize)) {
        this.responseSize = (java.lang.Integer) data().deepCopy(fields()[6].schema(), other.responseSize);
        fieldSetFlags()[6] = true;
      }
      if (isValidValue(fields()[7], other.referrer)) {
        this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.referrer);
        fieldSetFlags()[7] = true;
      }
      if (isValidValue(fields()[8], other.userAgent)) {
        this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[8].schema(), other.userAgent);
        fieldSetFlags()[8] = true;
      }
    }

    /** Gets the value of the 'url' field */
    public java.lang.CharSequence getUrl() {
      return url;
    }
    
    /** Sets the value of the 'url' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setUrl(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.url = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'url' field has been set */
    public boolean hasUrl() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'url' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUrl() {
      url = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'timestamp' field */
    public java.lang.Long getTimestamp() {
      return timestamp;
    }
    
    /** Sets the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setTimestamp(long value) {
      validate(fields()[2], value);
      this.timestamp = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'timestamp' field has been set */
    public boolean hasTimestamp() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearTimestamp() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'ip' field */
    public java.lang.CharSequence getIp() {
      return ip;
    }
    
    /** Sets the value of the 'ip' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setIp(java.lang.CharSequence value) {
      validate(fields()[3], value);
      this.ip = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'ip' field has been set */
    public boolean hasIp() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'ip' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearIp() {
      ip = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'httpMethod' field */
    public java.lang.CharSequence getHttpMethod() {
      return httpMethod;
    }
    
    /** Sets the value of the 'httpMethod' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpMethod(java.lang.CharSequence value) {
      validate(fields()[4], value);
      this.httpMethod = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'httpMethod' field has been set */
    public boolean hasHttpMethod() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'httpMethod' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpMethod() {
      httpMethod = null;
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'httpStatusCode' field */
    public java.lang.Integer getHttpStatusCode() {
      return httpStatusCode;
    }
    
    /** Sets the value of the 'httpStatusCode' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpStatusCode(int value) {
      validate(fields()[5], value);
      this.httpStatusCode = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'httpStatusCode' field has been set */
    public boolean hasHttpStatusCode() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'httpStatusCode' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpStatusCode() {
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'responseSize' field */
    public java.lang.Integer getResponseSize() {
      return responseSize;
    }
    
    /** Sets the value of the 'responseSize' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setResponseSize(int value) {
      validate(fields()[6], value);
      this.responseSize = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'responseSize' field has been set */
    public boolean hasResponseSize() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'responseSize' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearResponseSize() {
      fieldSetFlags()[6] = false;
      return this;
    }
    
    /** Gets the value of the 'referrer' field */
    public java.lang.CharSequence getReferrer() {
      return referrer;
    }
    
    /** Sets the value of the 'referrer' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setReferrer(java.lang.CharSequence value) {
      validate(fields()[7], value);
      this.referrer = value;
      fieldSetFlags()[7] = true;
      return this; 
    }
    
    /** Checks whether the 'referrer' field has been set */
    public boolean hasReferrer() {
      return fieldSetFlags()[7];
    }
    
    /** Clears the value of the 'referrer' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearReferrer() {
      referrer = null;
      fieldSetFlags()[7] = false;
      return this;
    }
    
    /** Gets the value of the 'userAgent' field */
    public java.lang.CharSequence getUserAgent() {
      return userAgent;
    }
    
    /** Sets the value of the 'userAgent' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setUserAgent(java.lang.CharSequence value) {
      validate(fields()[8], value);
      this.userAgent = value;
      fieldSetFlags()[8] = true;
      return this; 
    }
    
    /** Checks whether the 'userAgent' field has been set */
    public boolean hasUserAgent() {
      return fieldSetFlags()[8];
    }
    
    /** Clears the value of the 'userAgent' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUserAgent() {
      userAgent = null;
      fieldSetFlags()[8] = false;
      return this;
    }
    
    @Override
    public Pageview build() {
      try {
        Pageview record = new Pageview();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[2]);
        record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
        record.ip = fieldSetFlags()[3] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[3]);
        record.httpMethod = fieldSetFlags()[4] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[4]);
        record.httpStatusCode = fieldSetFlags()[5] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[5]);
        record.responseSize = fieldSetFlags()[6] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[6]);
        record.referrer = fieldSetFlags()[7] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[7]);
        record.userAgent = fieldSetFlags()[8] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[8]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }
  
  public Pageview.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Pageview newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Pageview implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'url' field.
		   */
	  public java.lang.CharSequence getUrl() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'url' field.
		   * @param value the value to set.
	   */
	  public void setUrl(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUrlDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'timestamp' field.
		   */
	  public java.lang.Long getTimestamp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'timestamp' field.
		   * @param value the value to set.
	   */
	  public void setTimestamp(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isTimestampDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'ip' field.
		   */
	  public java.lang.CharSequence getIp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'ip' field.
		   * @param value the value to set.
	   */
	  public void setIp(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isIpDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpMethod' field.
		   */
	  public java.lang.CharSequence getHttpMethod() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpMethod' field.
		   * @param value the value to set.
	   */
	  public void setHttpMethod(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpMethodDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpStatusCode' field.
		   */
	  public java.lang.Integer getHttpStatusCode() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpStatusCode' field.
		   * @param value the value to set.
	   */
	  public void setHttpStatusCode(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'responseSize' field.
		   */
	  public java.lang.Integer getResponseSize() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'responseSize' field.
		   * @param value the value to set.
	   */
	  public void setResponseSize(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isResponseSizeDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'referrer' field.
		   */
	  public java.lang.CharSequence getReferrer() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'referrer' field.
		   * @param value the value to set.
	   */
	  public void setReferrer(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isReferrerDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'userAgent' field.
		   */
	  public java.lang.CharSequence getUserAgent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'userAgent' field.
		   * @param value the value to set.
	   */
	  public void setUserAgent(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUserAgentDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
  }
  
}
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
    key = (K) ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
import java.util.concurrent.TimeUnit;
import org.apache.accumulo.core.client.BatchWriterConfig;
import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;
import org.apache.accumulo.core.client.security.tokens.PasswordToken;
import org.apache.accumulo.core.security.thrift.TCredentials;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericData;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.EncoderFactory;
import org.apache.gora.accumulo.encoders.BinaryEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;

  private TCredentials credentials;


  public Object fromBytes(Schema schema, byte data[]) throws GoraException {
    Schema fromSchema = null;
    if (schema.getType() == Type.UNION) {
      try {
        Decoder decoder = DecoderFactory.get().binaryDecoder(data, null);
        int unionIndex = decoder.readIndex();
        List<Schema> possibleTypes = schema.getTypes();
        fromSchema = possibleTypes.get(unionIndex);
        Schema effectiveSchema = possibleTypes.get(unionIndex);
        if (effectiveSchema.getType() == Type.NULL) {
          decoder.readNull();
          return null;
        } else {
          data = decoder.readBytes(null).array();
        }
      } catch (IOException e) {
        e.printStackTrace();
        throw new GoraException("Error decoding union type: ", e);
      }
    } else {
      fromSchema = schema;
    }
    return fromBytes(encoder, fromSchema, data);
    case BOOLEAN:
      return encoder.decodeBoolean(data);
    case DOUBLE:
      return encoder.decodeDouble(data);
    case FLOAT:
      return encoder.decodeFloat(data);
    case INT:
      return encoder.decodeInt(data);
    case LONG:
      return encoder.decodeLong(data);
    case STRING:
      return new Utf8(data);
    case BYTES:
      return ByteBuffer.wrap(data);
    case ENUM:
      return AvroUtils.getEnumValue(schema, encoder.decodeInt(data));
    case ARRAY:
      break;
    case FIXED:
      break;
    case MAP:
      break;
    case NULL:
      break;
    case RECORD:
      break;
    case UNION:
      break;
    default:
      break;


  public byte[] toBytes(Schema toSchema, Object o) {
    if (toSchema != null && toSchema.getType() == Type.UNION) {
      ByteArrayOutputStream baos = new ByteArrayOutputStream();
      org.apache.avro.io.BinaryEncoder avroEncoder = EncoderFactory.get().binaryEncoder(baos, null);
      int unionIndex = 0;
      try {
        if (o == null) {
          unionIndex = firstNullSchemaTypeIndex(toSchema);
          avroEncoder.writeIndex(unionIndex);
          avroEncoder.writeNull();
        } else {
          unionIndex = firstNotNullSchemaTypeIndex(toSchema);
          avroEncoder.writeIndex(unionIndex);
          avroEncoder.writeBytes(toBytes(o));
        }
        avroEncoder.flush();
        return baos.toByteArray();
      } catch (IOException e) {
        e.printStackTrace();
        return toBytes(o);
      }
    } else {     
      return toBytes(o);
    }
  }

  private int firstNullSchemaTypeIndex(Schema toSchema) {
    List<Schema> possibleTypes = toSchema.getTypes();
    int unionIndex = 0;
    for (int i = 0; i < possibleTypes.size(); i ) {
      Type pType = possibleTypes.get(i).getType();
      if (pType == Type.NULL) { // FIXME HUGE kludge to pass tests
        unionIndex = i; break;
      }
    }
    return unionIndex;
  }

  private int firstNotNullSchemaTypeIndex(Schema toSchema) {
    List<Schema> possibleTypes = toSchema.getTypes();
    int unionIndex = 0;
    for (int i = 0; i < possibleTypes.size(); i ) {
      Type pType = possibleTypes.get(i).getType();
      if (pType != Type.NULL) { // FIXME HUGE kludge to pass tests
        unionIndex = i; break;
      }
    }
    return unionIndex;
  }



        return copyIfNeeded(((Utf8) o).getBytes(), 0, ((Utf8) o).getByteLength());
        return encoder.encodeInt(((Enum<?>) o).ordinal());

        BatchWriterConfig batchWriterConfig = new BatchWriterConfig();
        batchWriterConfig.setMaxMemory(10000000);
        batchWriterConfig.setMaxLatency(60000l, TimeUnit.MILLISECONDS);
        batchWriterConfig.setMaxWriteThreads(4);
        batchWriter = conn.createBatchWriter(mapping.tableName, batchWriterConfig);



        encoder = new BinaryEncoder();

        AuthenticationToken token =  new PasswordToken(password);
          credentials = new TCredentials(user, 
              "org.apache.accumulo.core.client.security.tokens.PasswordToken", 
              ByteBuffer.wrap(password.getBytes()), instance);
          conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, token);
          conn = new MockInstance().getConnector(user, new PasswordToken(password));
          credentials = new TCredentials(user, 
              "org.apache.accumulo.core.client.security.tokens.PasswordToken", 
              ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());







        throw new GoraException("Please define the accumulo 'table' name mapping in "  filename  " for "  persistentClass.getCanonicalName());




    Map<Utf8, Object> currentMap = null;
    List currentArray = null;
    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(new byte[0], null);


      if (row == null) {
        row = entry.getKey().getRowData();
      }
      byte[] val = entry.getValue().get();

      Field field = fieldMap.get(getFieldName(entry));

          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
              fromBytes(currentSchema, entry.getValue().get()));
          persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
      case MAP:  // first entry only. Next are handled above on the next loop
        currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
        currentPos = field.pos();
        currentFam = entry.getKey().getColumnFamily();
        currentSchema = field.schema().getValueType();
        currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
            fromBytes(currentSchema, entry.getValue().get()));
        break;
      case ARRAY:
        currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
        currentPos = field.pos();
        currentFam = entry.getKey().getColumnFamily();
        currentSchema = field.schema().getElementType();
        currentField = field;

        currentArray.add(fromBytes(currentSchema, entry.getValue().get()));

        break;
      case UNION:// default value of null acts like union with null
        Schema effectiveSchema = field.schema().getTypes()
        .get(firstNotNullSchemaTypeIndex(field.schema()));
        // map and array were coded without union index so need to be read the same way
        if (effectiveSchema.getType() == Type.ARRAY) {
          currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
          currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
        }
        else if (effectiveSchema.getType() == Type.MAP) {
          currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
          currentPos = field.pos();
          currentFam = entry.getKey().getColumnFamily();
          currentSchema = effectiveSchema.getValueType();

          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
              fromBytes(currentSchema, entry.getValue().get()));
        }
        // continue like a regular top-level union
      case RECORD:
        SpecificDatumReader<?> reader = new SpecificDatumReader<Schema>(field.schema());
        persistent.put(field.pos(), reader.read(null, DecoderFactory.get().binaryDecoder(val, decoder)));
        break;
      default:
        persistent.put(field.pos(), fromBytes(field.schema(), entry.getValue().get()));

      persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
  /**
   * Retrieve field name from entry.
   * @param entry The Key-Value entry
   * @return String The field name
   */
  private String getFieldName(Entry<Key, Value> entry) {
    String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), 
        entry.getKey().getColumnQualifier()));
    if (fieldName == null) {
      fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), null));
    }
    return fieldName;
  }

      if (col != null) {
        if (col.getSecond() == null) {
          scanner.fetchColumnFamily(col.getFirst());
        } else {
          scanner.fetchColumn(col.getFirst(), col.getSecond());
        }
        LOG.error("Mapping not found for field: "  field);




      List<Field> fields = schema.getFields();

      for (int i = 1; i < fields.size(); i) {
        if (!val.isDirty(i)) {
        Field field = fields.get(i);

        Object o = val.get(field.pos());       

        case MAP:
          count = putMap(m, count, field.schema().getValueType(), o, col);
          break;
        case ARRAY:
          count = putArray(m, count, o, col);
          break;
        case UNION: // default value of null acts like union with null
          Schema effectiveSchema = field.schema().getTypes()
          .get(firstNotNullSchemaTypeIndex(field.schema()));
          // map and array need to compute qualifier
          if (effectiveSchema.getType() == Type.ARRAY) {
            count = putArray(m, count, o, col);
          }
          else if (effectiveSchema.getType() == Type.MAP) {
            count = putMap(m, count, effectiveSchema.getValueType(), o, col);
          }
          // continue like a regular top-level union
        case RECORD:
          SpecificDatumWriter<Object> writer = new SpecificDatumWriter<Object>(field.schema());
          ByteArrayOutputStream os = new ByteArrayOutputStream();
          org.apache.avro.io.BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
          writer.write(o, encoder);
          encoder.flush();
          m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
          count;
          break;
        default:
          m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
          count;



  private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col) throws GoraException {

    // First of all we delete map field on accumulo store
    Text rowKey = new Text(m.getRow());
    Query<K, T> query = newQuery();
    query.setFields(col.getFirst().toString());
    query.setStartKey((K)rowKey.toString());
    query.setEndKey((K)rowKey.toString());
    deleteByQuery(query);
    flush();
    if (o == null){
      return 0;
    }
    
    Set<?> es = ((Map<?, ?>)o).entrySet();
    for (Object entry : es) {
      Object mapKey = ((Entry<?, ?>) entry).getKey();
      Object mapVal = ((Entry<?, ?>) entry).getValue();                  
      if ((o instanceof DirtyMapWrapper && ((DirtyMapWrapper<?, ?>)o).isDirty())
          || !(o instanceof DirtyMapWrapper)) { //mapVal instanceof Dirtyable && ((Dirtyable)mapVal).isDirty()) {
        m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(valueType, mapVal)));
        count;
      }
      // TODO map value deletion
    }
    return count;
  }

  private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col) {

    // First of all we delete array field on accumulo store
    Text rowKey = new Text(m.getRow());
    Query<K, T> query = newQuery();
    query.setFields(col.getFirst().toString());
    query.setStartKey((K)rowKey.toString());
    query.setEndKey((K)rowKey.toString());
    deleteByQuery(query);
    flush();
    if (o == null){
      return 0;
    }
    
    List<?> array = (List<?>) o;  // both GenericArray and DirtyListWrapper
    int j = 0;
    for (Object item : array) {
      m.put(col.getFirst(), new Text(toBytes(j)), new Value(toBytes(item)));
      count;
    }
    return count;
  }
















        tl = TabletLocator.getInstance(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));


      while (tl.binRanges(Collections.singletonList(createRange(query)), binnedRanges, credentials).size() > 0) {







          PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, new String[] {location});






  @SuppressWarnings("unchecked")

import java.io.IOException;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
  
  
      if (schema.getType().equals(Type.RECORD) || schema.getType().equals(Type.MAP) ){
        try {
          value = AvroSerializerUtil.deserializer(value, schema);
        } catch (IOException e) {
          LOG.warn(field.name()  " named field could not be deserialized.");
        }
      }
      String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName().duplicate());
      String family = cassandraColumn.getFamily();  
      String fieldName = this.reverseMap.get(family  ":"  StringSerializer.get().fromByteBuffer(cassandraColumn.getName().duplicate()));
      
      if (fieldName != null) {
        if (fieldName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {

          int pos = this.persistent.getSchema().getField(fieldName).pos();
          Field field = fields.get(pos);
          Type fieldType = field.schema().getType();
          // LOG.info(StringSerializer.get().fromByteBuffer(cassandraColumn.getName())
          //  fieldName  " "  fieldType.name());
          if (fieldType.equals(Type.UNION)) {
            //getting UNION stored type
            CassandraColumn cc = getUnionTypeColumn(fieldName
                 CassandraStore.UNION_COL_SUFIX, cassandraRow.toArray());
            //creating temporary UNION Field
            Field unionField = new Field(fieldName
                 CassandraStore.UNION_COL_SUFIX, Schema.create(Type.INT),
                null, null);
            // get value of UNION stored type
            cc.setField(unionField);
            Object val = cc.getValue();
            cassandraColumn.setUnionType(Integer.parseInt(val.toString()));
          }

          // get value
          cassandraColumn.setField(field);
          Object value = cassandraColumn.getValue();

          this.persistent.put(pos, value);
          // this field does not need to be written back to the store
          this.persistent.clearDirty(pos);
      } else
  //TODO Should we remove this method?
  @SuppressWarnings("unused")
import java.util.List;
import java.util.Map;
import org.apache.gora.cassandra.serializers.ListSerializer;
import org.apache.gora.cassandra.serializers.MapSerializer;
  private Object getFieldValue(Type type, Schema fieldSchema, ByteBuffer byteBuffer){
    Object value = null;
    if (type.equals(Type.ARRAY)) {
      ListSerializer<?> serializer = ListSerializer.get(fieldSchema.getElementType());
      List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
      value = genericArray;
    } else if (type.equals(Type.MAP)) {
//      MapSerializer<?> serializer = MapSerializer.get(fieldSchema.getValueType());
//      Map<?, ?> map = serializer.fromByteBuffer(byteBuffer);
//      value = map;
      value = fromByteBuffer(fieldSchema, byteBuffer);
    } else if (type.equals(Type.RECORD)){
      value = fromByteBuffer(fieldSchema, byteBuffer);
    } else if (type.equals(Type.UNION)){
      // the selected union schema is obtained
      Schema unionFieldSchema = getUnionSchema(super.getUnionType(), fieldSchema);
      Type unionFieldType = unionFieldSchema.getType();
      // we use the selected union schema to deserialize our actual value
      //value = fromByteBuffer(unionFieldSchema, byteBuffer);
      value = getFieldValue(unionFieldType, unionFieldSchema, byteBuffer);
    } else {
      value = fromByteBuffer(fieldSchema, byteBuffer);
    }
    return value;
  }

    Object value = getFieldValue(type, fieldSchema, byteBuffer);
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import org.apache.gora.cassandra.serializers.CharSequenceSerializer;
import org.apache.gora.cassandra.store.CassandraStore;
 private Object getSuperValue(Field field, Schema fieldSchema, Type type){
        List<Object> array = new ArrayList<Object>();
        Map<CharSequence, Object> map = new HashMap<CharSequence, Object>();

          CharSequence mapKey = CharSequenceSerializer.get().fromByteBuffer(hColumn.getName());
          if (mapKey.toString().indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
            Object memberValue = null;
            // We need detect real type for UNION Fields
            if (fieldSchema.getValueType().getType().equals(Type.UNION)){
              
              HColumn<ByteBuffer, ByteBuffer> cc = getUnionTypeColumn(mapKey
                   CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns());
              Integer unionIndex = getUnionIndex(mapKey.toString(), cc);
              Schema realSchema = fieldSchema.getValueType().getTypes().get(unionIndex);
              memberValue = fromByteBuffer(realSchema, hColumn.getValue());
              
            }else{
              memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());            
            }            
            map.put(mapKey, memberValue);      
          }
            if (memberName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
              
            Schema memberSchema = memberField.schema();
            Type memberType = memberSchema.getType();
            
            
            if (memberType.equals(Type.UNION)){
              HColumn<ByteBuffer, ByteBuffer> hc = getUnionTypeColumn(memberField.name()
                   CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns().toArray());
              Integer unionIndex = getUnionIndex(memberField.name(),hc);
              cassandraColumn.setUnionType(unionIndex);
            }
            
            record.put(record.getSchema().getField(memberName).pos(), cassandraColumn.getValue());
          }
      case UNION:
        int schemaPos = this.getUnionType();
        Schema unioSchema = fieldSchema.getTypes().get(schemaPos);
        Type unionType = unioSchema.getType();
        value = getSuperValue(field, unioSchema, unionType);
        break;
        Object memberValue = null;
        // Using for UnionIndex of Union type field get value. UnionIndex always Integer.  
        for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          memberValue = fromByteBuffer(fieldSchema, hColumn.getValue());      
        }
        value = memberValue;
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name());
    return value;
  }

 private Integer getUnionIndex(String fieldName, HColumn<ByteBuffer, ByteBuffer> uc){
   Integer val = IntegerSerializer.get().fromByteBuffer(uc.getValue());
   return Integer.parseInt(val.toString());
 }
 
  private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName,
    List<HColumn<ByteBuffer, ByteBuffer>> columns) {
    return getUnionTypeColumn(fieldName, columns.toArray());
}

  private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName, Object[] hColumns) {
    for (int iCnt = 0; iCnt < hColumns.length; iCnt){
      @SuppressWarnings("unchecked")
      HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>)hColumns[iCnt];
      String columnName = StringSerializer.get().fromByteBuffer(hColumn.getNameBytes().duplicate());
      if (fieldName.equals(columnName))
        return hColumn;
    }
    return null;
}

  public Object getValue() {
    Field field = getField();
    Schema fieldSchema = field.schema();
    Type type = fieldSchema.getType();
    
    Object value = getSuperValue(field, fieldSchema, type);
import java.util.Map;
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.BytesArraySerializer;
import me.prettyprint.cassandra.serializers.ObjectSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.Persistent;
      serializer = CharSequenceSerializer.get();
      serializer = ListSerializer.get(schema);
    } else if (value instanceof Map) {
      Map map = (Map)value;
        serializer = MapSerializer.get(schema);
    } else if (value instanceof Persistent){
      serializer = ObjectSerializer.get();
    }
    else {
      serializer = CharSequenceSerializer.get();
    if (type.equals(Type.STRING)) {
      serializer = CharSequenceSerializer.get();
    } else if (type.equals(Type.BOOLEAN)) {
    } else if (type.equals(Type.BYTES)) {
    } else if (type.equals(Type.DOUBLE)) {
    } else if (type.equals(Type.FLOAT)) {
    } else if (type.equals(Type.INT)) {
    } else if (type.equals(Type.LONG)) {
    } else if (type.equals(Type.FIXED)) {
    } else if (type.equals(Type.ARRAY)) {
      serializer = ListSerializer.get(schema.getElementType());
    } else if (type.equals(Type.MAP)) {
    	serializer = MapSerializer.get(schema.getValueType());
    } else if (type.equals(Type.UNION)){
    } else if (type.equals(Type.RECORD)){
      serializer = BytesArraySerializer.get();
      serializer = CharSequenceSerializer.get();
      serializer = ListSerializer.get(elementType);
      serializer = MapSerializer.get(elementType);
import java.util.List;
import java.util.Map;
  public static Class<? extends Object> getClass(Object value) {
      return Schema.createArray( getElementSchema((GenericArray<?>)value) );
    } else if (clazz.isAssignableFrom(List.class)) {
    } else if (clazz.isAssignableFrom(Map.class)) {
  public static Class<?> getClass(Type type) {
      return List.class;
      return Map.class;
  public static Schema getSchema(Class<?> clazz) {
  public static Class<?> getClass(Schema schema) {
  public static int getFixedSize(Class<?> clazz) {
  public static Schema getElementSchema(GenericArray<?> array) {
  
  
  
    this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
    
    
    // Just create a Keyspace object on the client side, corresponding to an already existing keyspace with already created column families.
    
  
   * In this method, we also utilise Hector's {@ConfigurableConsistencyLevel}
   * logic. It is set by passing a ConfigurableConsistencyLevel object right 
   * when the Keyspace is created. Currently consistency level is .ONE which 
   * permits consistency to wait until one replica has responded. 
          "org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);      
      // LOG.info("Keyspace '"  this.cassandraMapping.getKeyspaceName()  "' in cluster '"  this.cassandraMapping.getClusterName()  "' was created on host '"  this.cassandraMapping.getHostName()  "'");
      
      // Create a customized Consistency Level
      ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
      // Define CL.ONE for ColumnFamily "ColumnFamily"
      clmap.put("ColumnFamily", HConsistencyLevel.ONE);

      // In this we use CL.ONE for read and writes. But you can use different CLs if needed.
      configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
      configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
      HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);

               ", not BytesType. It may cause a fatal error on column validation later.");
            LOG.debug("The comparator type of "  cfDef.getName()  " column family is " 
               comparatorType.getTypeName()  ".");
    if (ttlAttr == null)
   * Delete a row within the keyspace.
   * @param key
   * @param fieldName
   * @param columnName
   */
  public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
    synchronized(mutator) {
      HectorUtils.deleteColumn(mutator, key, familyName, columnName);
    }
  }

  /**
   * Deletes an entry based on its key.
   * @param key
   */
  public void deleteByKey(K key) {
    Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
    deleteColumn(key, familyMap.values().iterator().next().toString(), null);
  }

  /**
    
    if (ttlAttr == null)
    
   * Deletes a subColumn 
   * @param key
   * @param fieldName
   * @param columnName
   * Deletes all subcolumns from a super column.
   * @param key the row key.
   * @param fieldName the field name.
  public void deleteSubColumn(K key, String fieldName) {
    String columnFamily = this.cassandraMapping.getFamily(fieldName);
    String superColumnName = this.cassandraMapping.getColumn(fieldName);
    synchronized(mutator) {
      HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, null);
    }
  public void deleteGenericArray(K key, String fieldName) {
    //TODO Verify this. Everything that goes inside a genericArray will go inside a column so let's just delete that.
    deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
  }
          if (((List<?>)itemValue).size() == 0) {
        } else if (itemValue instanceof Map<?,?>) {
          if (((Map<?, ?>)itemValue).size() == 0) {

  public void deleteStatefulHashMap(K key, String fieldName) {
      deleteSubColumn(key, fieldName);
    } else {
      deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
    }
  }
  public void addStatefulHashMap(K key, String fieldName, Map<CharSequence,Object> map) {
    if (isSuper( cassandraMapping.getFamily(fieldName) )) {
      // as we don't know what has changed inside the map or If it's an empty map, then delete its content.
      deleteSubColumn(key, fieldName);
      // update if there is anything to update.
      if (!map.isEmpty()) {
        // If it's not empty, then update its content.
        for (CharSequence mapKey: map.keySet()) {
          // TODO: hack, do not store empty arrays
          Object mapValue = map.get(mapKey);
          if (mapValue instanceof GenericArray<?>) {
            if (((List<?>)mapValue).size() == 0) {
              continue;
            }
          } else if (mapValue instanceof Map<?,?>) {
            if (((Map<?, ?>)mapValue).size() == 0) {
              continue;
            }
          addSubColumn(key, fieldName, mapKey.toString(), mapValue);
    
    
    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
    
    
  
    // checking if it was a UNION field the one we are retrieving
    if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
      family = this.cassandraMapping.getFamily(pField.substring(0,pField.indexOf(CassandraStore.UNION_COL_SUFIX)));
    else
      family = this.cassandraMapping.getFamily(pField);
     return family;
   }
 
    if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
      column = pField;
    else
      column = this.cassandraMapping.getColumn(pField);
      return column;
    }
   * @return a map which keys are the family names and values the 
   * corresponding column names required to get all the query fields.
      
    
   * Retrieves the cassandraMapping which holds whatever was mapped 
   * from the gora-cassandra-mapping.xml
   * @return 
  
   * Select the field names according to the column names, which format 
   * if fully qualified: "family:column"
   * @return a map which keys are the fully qualified column 
   * names and values the query fields
      
    
  /**
   * Determines if a column is a superColumn or not.
   * @param family
   * @return boolean
   */
    
    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
    
    


	return this.cassandraMapping.getKeyspaceName();
  
        LOG.debug("Located gc_grace_seconds: '"  gcgrace_scs  "'" );




      // TODO we should find a way of storing more values into this map
   * Add new column to the CassandraMapping using the the below parameters
   * @param pFamilyName the column family name
   * @param pFieldName the Avro field from the Schema
   * @param pColumnName the column name within the column family.
 /**
      LOG.debug("persistentClassName="  className  " -> keyspaceName="  keyspaceName);
import java.util.HashMap;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.generic.GenericData.Array;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.commons.lang.ArrayUtils;
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
 * heavily on {@link org.apache.gora.cassandra.store.CassandraClient} for many operations
  private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
   * Fixed string with value "UnionIndex" used to generate an extra column based on 
   * the original field's name
   */
  public static String UNION_COL_SUFIX = "_UnionIndex";

  /**
   * Default schema index with value "0" used when AVRO Union data types are stored

   * We don't want to lock the entire collection before iterating over the keys, 
   * since in the meantime other threads are adding entries to the map.
  public static final ThreadLocal<BinaryEncoder> encoders =
      new ThreadLocal<BinaryEncoder>();
  
  /**
   * Create a {@link java.util.concurrent.ConcurrentHashMap} for the 
   * datum readers and writers. 
   * This is necessary because they are not thread safe, at least not before 
   * Avro 1.4.0 (See AVRO-650).
   * When they are thread safe, it is possible to maintain a single reader and
   * writer pair for every schema, instead of one for every thread.
   * @see <a href="https://issues.apache.org/jira/browse/AVRO-650">AVRO-650</a>
   */
  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = 
      new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
  
   * When we add sub/supercolumns, Gora keys are mapped to Cassandra partition keys only. 
      CassandraResultSet<K> cassandraResultSet) {
        cassandraRow = new CassandraRow<K>();
   * Flush the buffer which is a synchronized {@link java.util.LinkedHashMap}
   * storing fields pending to be stored by 
   * {@link org.apache.gora.cassandra.store.CassandraStore#put(Object, PersistentBase)}
   * operations. Invoking this method therefore writes the buffered rows
   * into Cassandra.
    @SuppressWarnings("unchecked")
    // iterating over the key set directly would throw 
    //ConcurrentModificationException with java.util.HashMap and subclasses
        LOG.info("Value to update is null for key: "  key);

          addOrUpdateField(key, field, field.schema(), value.get(field.pos()));
    // remove flushed rows from the buffer as all 
    // added or updated fields should now have been written.
    
    if (fields == null){
      fields = this.getFields();
    }
    // Generating UnionFields
    ArrayList<String> unionFields = new ArrayList<String>();
    for (String field: fields){
      Field schemaField =this.fieldMap.get(field);
      Type type = schemaField.schema().getType();
      if (type.getName().equals("UNION".toLowerCase())){
        unionFields.add(fieldUNION_COL_SUFIX);
      }
    }
    
    String[] arr = unionFields.toArray(new String[unionFields.size()]);
    String[] both = (String[]) ArrayUtils.addAll(fields, arr);
    
    query.setFields(both);

    // TODO GORA-298 Implement CassandraStore#getPartitions
   * 
   * When doing the 
   * {@link org.apache.gora.cassandra.store.CassandraStore#put(Object, PersistentBase)}
   * operation, the logic is as follows:
   * <ol>
   * <li>Obtain the Avro {@link org.apache.avro.Schema} for the object.</li>
   * <li>Create a new duplicate instance of the object (explained in more detail below) **.</li>
   * <li>Obtain a {@link java.util.List} of the {@link org.apache.avro.Schema} 
   * {@link org.apache.avro.Schema.Field}'s.</li>
   * <li>Iterate through the field {@link java.util.List}. This allows us to 
   * consequently process each item.</li>
   * <li>Check to see if the {@link org.apache.avro.Schema.Field} is NOT dirty. 
   * If this condition is true then we DO NOT process this field.</li>
   * <li>Obtain the element at the specified position in this list so we can 
   * directly operate on it.</li>
   * <li>Obtain the {@link org.apache.avro.Schema.Type} of the element obtained 
   * above and process it accordingly. N.B. For nested type ARRAY, MAP
   * RECORD or UNION, we shadow the checks in bullet point 5 above to infer that the 
   * {@link org.apache.avro.Schema.Field} is either at 
   * position 0 OR it is NOT dirty. If one of these conditions is true then we DO NOT
   * process this field. This is carried out in 
   * {@link org.apache.gora.cassandra.store.CassandraStore#getFieldValue(Schema, Type, Object)}</li>
   * <li>We then insert the Key and Object into the {@link java.util.LinkedHashMap} buffer 
   * before being flushed. This performs a structural modification of the map.</li>
   * </ol>
   * ** We create a duplicate instance of the object to be persisted and insert processed
   * objects into a synchronized {@link java.util.LinkedHashMap}. This allows 
   * us to keep all the objects in memory till flushing.
   * @see org.apache.gora.store.DataStore#put(java.lang.Object, 
   * org.apache.gora.persistency.Persistent).
   * @param key for the Avro Record (object).
   * @param value Record object to be persisted in Cassandra
    @SuppressWarnings("unchecked")
    T p = (T) SpecificData.get().newRecord(value, schema);
    List<Field> fields = schema.getFields();
    for (int i = 1; i < fields.size(); i) {
      if (!value.isDirty(i)) {
        continue;
      Field field = fields.get(i);
      Type type = field.schema().getType();
      Object fieldValue = value.get(field.pos());
      Schema fieldSchema = field.schema();
      // check if field has a nested structure (array, map, record or union)
      fieldValue = getFieldValue(fieldSchema, type, fieldValue);
      p.put(field.pos(), fieldValue);
   * For every field within an object, we pass in a field schema, Type and value.
   * This enables us to process fields (based on their characteristics) 
   * preparing them for persistence.
   * @param fieldSchema the associated field schema
   * @param type the field type
   * @param fieldValue the field value.
   * @return
   */
  private Object getFieldValue(Schema fieldSchema, Type type, Object fieldValue ){
    switch(type) {
    case RECORD:
      Persistent persistent = (Persistent) fieldValue;
      Persistent newRecord = (Persistent) SpecificData.get().newRecord(persistent, persistent.getSchema());
      for (Field member: fieldSchema.getFields()) {
        if (member.pos() == 0 || !persistent.isDirty()) {
          continue;
        }
        Schema memberSchema = member.schema();
        Type memberType = memberSchema.getType();
        Object memberValue = persistent.get(member.pos());
        newRecord.put(member.pos(), getFieldValue(memberSchema, memberType, memberValue));
      }
      fieldValue = newRecord;
      break;
    case MAP:
      Map<?, ?> map = (Map<?, ?>) fieldValue;
      fieldValue = map;
      break;
    case ARRAY:
      fieldValue = (List<?>) fieldValue;
      break;
    case UNION:
      // storing the union selected schema, the actual value will 
      // be stored as soon as we get break out.
      if (fieldValue != null){
        int schemaPos = getUnionSchema(fieldValue,fieldSchema);
        Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
        Type unionType = unionSchema.getType();
        fieldValue = getFieldValue(unionSchema, unionType, fieldValue);
      }
      //p.put( schemaPos, p.getSchema().getField(field.name()  CassandraStore.UNION_COL_SUFIX));
      //p.put(fieldPos, fieldValue);
      break;
    default:
      break;
    }    
    return fieldValue;
  }
  
  /**
   * @param schema  the schema belonging to the particular Avro field
  @SuppressWarnings({ "unchecked", "rawtypes" })
  private void addOrUpdateField(K key, Field field, Schema schema, Object value) {
    // checking if the value to be updated is used for saving union schema
    if (field.name().indexOf(CassandraStore.UNION_COL_SUFIX) < 0){
      switch (type) {
      case STRING:
      case BOOLEAN:
      case INT:
      case LONG:
      case BYTES:
      case FLOAT:
      case DOUBLE:
      case FIXED:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case RECORD:
        if (value != null) {
          if (value instanceof PersistentBase) {
            PersistentBase persistentBase = (PersistentBase) value;            
            try {
              byte[] byteValue = AvroSerializerUtil.serializer(persistentBase, schema);
              this.cassandraClient.addColumn(key, field.name(), byteValue);
            } catch (IOException e) {
              LOG.warn(field.name()  " named record could not be serialized.");
          } else {
            LOG.warn("Record with value: "  value.toString()  " not supported for field: "  field.name());
          LOG.warn("Setting content of: "  field.name()  " to null.");
          String familyName =  this.cassandraClient.getCassandraMapping().getFamily(field.name());
          this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
        break;
      case MAP:
        if (value != null) {
          if (value instanceof Map<?, ?>) {            
            Map<CharSequence,Object> map = (Map<CharSequence,Object>)value;
            Schema valueSchema = schema.getValueType();
            Type valueType = valueSchema.getType();
            if (Type.UNION.equals(valueType)){
              Map<CharSequence,Object> valueMap = new HashMap<CharSequence, Object>();
              for (CharSequence mapKey: map.keySet()) {
                Object mapValue = map.get(mapKey);
                int valueUnionIndex = getUnionSchema(mapValue, valueSchema);
                valueMap.put((mapKeyUNION_COL_SUFIX), valueUnionIndex);
                valueMap.put(mapKey, mapValue);
              }
              map = valueMap;
            }
            
            String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
            
            // If map is not super column. We using Avro serializer. 
            if (!this.cassandraClient.isSuper( familyName )){
              try {
                byte[] byteValue = AvroSerializerUtil.serializer(map, schema);
                this.cassandraClient.addColumn(key, field.name(), byteValue);
              } catch (IOException e) {
                LOG.warn(field.name()  " named map could not be serialized.");
              }
            }else{
              this.cassandraClient.addStatefulHashMap(key, field.name(), map);              
            }
          } else {
            LOG.warn("Map with value: "  value.toString()  " not supported for field: "  field.name());
          }
          // delete map
          LOG.warn("Setting content of: "  field.name()  " to null.");
          this.cassandraClient.deleteStatefulHashMap(key, field.name());
        break;
      case ARRAY:
        if (value != null) {
          if (value instanceof DirtyListWrapper<?>) {
            DirtyListWrapper fieldValue = (DirtyListWrapper<?>)value;
            GenericArray valueArray = new Array(fieldValue.size(), schema);
            for (int i = 0; i < fieldValue.size(); i) {
              valueArray.add(i, fieldValue.get(i));
            }
            this.cassandraClient.addGenericArray(key, field.name(), (GenericArray<?>)valueArray);
          } else {
            LOG.warn("Array with value: "  value.toString()  " not supported for field: "  field.name());
          }
          LOG.warn("Setting content of: "  field.name()  " to null.");
          this.cassandraClient.deleteGenericArray(key, field.name());
        break;
      case UNION:
     // adding union schema index
        String columnName = field.name()  UNION_COL_SUFIX;
        String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
        if(value != null) {
          int schemaPos = getUnionSchema(value, schema);
          LOG.debug("Union with value: "  value.toString()  " at index: "  schemaPos  " supported for field: "  field.name());
          this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
          if (this.cassandraClient.isSuper( familyName )){
            this.cassandraClient.addSubColumn(key, columnName, columnName, schemaPos);
          }else{
            this.cassandraClient.addColumn(key, columnName, schemaPos);
            
          }
          //this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
          // adding union value
          Schema unionSchema = schema.getTypes().get(schemaPos);
          addOrUpdateField(key, field, unionSchema, value);
          //this.cassandraClient.addColumn(key, field.name(), value);
        } else {
          LOG.warn("Setting content of: "  field.name()  " to null.");
          if (this.cassandraClient.isSuper( familyName )){
            this.cassandraClient.deleteSubColumn(key, field.name());
          } else {
            this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
          }
        }
        break;
      default:
        LOG.warn("Type: "  type.name()  " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
   * Given an object and the object schema this function obtains,
   * from within the UNION schema, the position of the type used.
   * If no data type can be inferred then we return a default value
   * of position 0.
   * @return the unionSchemaPosition.
//    String valueType = pValue.getClass().getSimpleName();
      Type schemaType = it.next().getType();
      if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
      else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
      else if (pValue instanceof Integer && schemaType.equals(Type.INT))
      else if (pValue instanceof Long && schemaType.equals(Type.LONG))
      else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
      else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
      else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
        return unionSchemaPos;
      else if (pValue instanceof Map && schemaType.equals(Type.MAP))
        return unionSchemaPos;
      else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
        return unionSchemaPos;
      else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
    return DEFAULT_UNION_SCHEMA;
   * Simple method to check if a Cassandra Keyspace exists.
   * @return true if a Keyspace exists.



    mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());

import java.util.ArrayList;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
    //"http://example.com",
    //"fck fck dck",
      throws IOException {
    try{
      WebPage page;
      log.info("creating web page data");
      
      for(int i=0; i<URLS.length; i) {
        page = WebPage.newBuilder().build();
        page.setUrl(new Utf8(URLS[i]));
        page.setParsedContent(new ArrayList<CharSequence>());
        if (CONTENTS[i]!=null){
          page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
          for(String token : CONTENTS[i].split(" ")) {
            page.getParsedContent().add(new Utf8(token));  
        }
        for(int j=0; j<LINKS[i].length; j) {
          page.getOutlinks().put(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
        }
        
        Metadata metadata = Metadata.newBuilder().build();
        metadata.setVersion(1);
        metadata.getData().put(new Utf8("metakey"), new Utf8("metavalue"));
        page.setMetadata(metadata);

        dataStore.put(URLS[i], page);
      }
      dataStore.flush();
      log.info("finished creating web page data");
    }
    catch(Exception e){
      log.info("error creating web page data");
    } 
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class Employee extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    NAME(1, "name"),
    DATE_OF_BIRTH(2, "dateOfBirth"),
    SSN(3, "ssn"),
    SALARY(4, "salary"),
    BOSS(5, "boss"),
    WEBPAGE(6, "webpage"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "name",
  "dateOfBirth",
  "ssn",
  "salary",
  "boss",
  "webpage",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence name;
  private java.lang.CharSequence ssn;
  private java.lang.Object boss;
  private org.apache.gora.examples.generated.WebPage webpage;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return name;
    case 2: return dateOfBirth;
    case 3: return ssn;
    case 4: return salary;
    case 5: return boss;
    case 6: return webpage;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: name = (java.lang.CharSequence)(value); break;
    case 2: dateOfBirth = (java.lang.Long)(value); break;
    case 3: ssn = (java.lang.CharSequence)(value); break;
    case 4: salary = (java.lang.Integer)(value); break;
    case 5: boss = (java.lang.Object)(value); break;
    case 6: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

   * Gets the value of the 'name' field.
  public java.lang.CharSequence getName() {
    return name;
  }

  /**
   * Sets the value of the 'name' field.
   * @param value the value to set.
   */
  public void setName(java.lang.CharSequence value) {
    this.name = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isNameDirty(java.lang.CharSequence value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'dateOfBirth' field.
   */
  public java.lang.Long getDateOfBirth() {
    return dateOfBirth;
  }

  /**
   * Sets the value of the 'dateOfBirth' field.
   * @param value the value to set.
   */
  public void setDateOfBirth(java.lang.Long value) {
    this.dateOfBirth = value;
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isDateOfBirthDirty(java.lang.Long value) {
    return isDirty(2);
  }

  /**
   * Gets the value of the 'ssn' field.
   */
  public java.lang.CharSequence getSsn() {
    return ssn;
  }

  /**
   * Sets the value of the 'ssn' field.
   * @param value the value to set.
   */
  public void setSsn(java.lang.CharSequence value) {
    this.ssn = value;
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isSsnDirty(java.lang.CharSequence value) {
    return isDirty(3);
  }

  /**
   * Gets the value of the 'salary' field.
   */
  public java.lang.Integer getSalary() {
    return salary;
  }

  /**
   * Sets the value of the 'salary' field.
   * @param value the value to set.
   */
  public void setSalary(java.lang.Integer value) {
    this.salary = value;
    setDirty(4);
  }
  
  /**
   * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isSalaryDirty(java.lang.Integer value) {
    return isDirty(4);
  }

  /**
   * Gets the value of the 'boss' field.
   */
  public java.lang.Object getBoss() {
    return boss;
  }

  /**
   * Sets the value of the 'boss' field.
   * @param value the value to set.
   */
  public void setBoss(java.lang.Object value) {
    this.boss = value;
    setDirty(5);
  }
  
  /**
   * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isBossDirty(java.lang.Object value) {
    return isDirty(5);
  }

  /**
   * Gets the value of the 'webpage' field.
   */
  public org.apache.gora.examples.generated.WebPage getWebpage() {
    return webpage;
  }

  /**
   * Sets the value of the 'webpage' field.
   * @param value the value to set.
   */
  public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
    this.webpage = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
    return isDirty(6);
  }

  /** Creates a new Employee RecordBuilder */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder() {
    return new org.apache.gora.examples.generated.Employee.Builder();
  }
  
  /** Creates a new Employee RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee.Builder other) {
    return new org.apache.gora.examples.generated.Employee.Builder(other);
  }
  
  /** Creates a new Employee RecordBuilder by copying an existing Employee instance */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee other) {
    return new org.apache.gora.examples.generated.Employee.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Employee instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Employee>
    implements org.apache.avro.data.RecordBuilder<Employee> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence name;
    private long dateOfBirth;
    private java.lang.CharSequence ssn;
    private int salary;
    private java.lang.Object boss;
    private org.apache.gora.examples.generated.WebPage webpage;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.Employee.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.Employee.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Employee instance */
    private Builder(org.apache.gora.examples.generated.Employee other) {
            super(org.apache.gora.examples.generated.Employee.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.name)) {
        this.name = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.name);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.dateOfBirth)) {
        this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.dateOfBirth);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.ssn)) {
        this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ssn);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.salary)) {
        this.salary = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.salary);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.boss)) {
        this.boss = (java.lang.Object) data().deepCopy(fields()[5].schema(), other.boss);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.webpage)) {
        this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[6].schema(), other.webpage);
        fieldSetFlags()[6] = true;
      }
    }

    /** Gets the value of the 'name' field */
    public java.lang.CharSequence getName() {
      return name;
    }
    
    /** Sets the value of the 'name' field */
    public org.apache.gora.examples.generated.Employee.Builder setName(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.name = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'name' field has been set */
    public boolean hasName() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'name' field */
    public org.apache.gora.examples.generated.Employee.Builder clearName() {
      name = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'dateOfBirth' field */
    public java.lang.Long getDateOfBirth() {
      return dateOfBirth;
    }
    
    /** Sets the value of the 'dateOfBirth' field */
    public org.apache.gora.examples.generated.Employee.Builder setDateOfBirth(long value) {
      validate(fields()[2], value);
      this.dateOfBirth = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'dateOfBirth' field has been set */
    public boolean hasDateOfBirth() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'dateOfBirth' field */
    public org.apache.gora.examples.generated.Employee.Builder clearDateOfBirth() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'ssn' field */
    public java.lang.CharSequence getSsn() {
      return ssn;
    }
    
    /** Sets the value of the 'ssn' field */
    public org.apache.gora.examples.generated.Employee.Builder setSsn(java.lang.CharSequence value) {
      validate(fields()[3], value);
      this.ssn = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'ssn' field has been set */
    public boolean hasSsn() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'ssn' field */
    public org.apache.gora.examples.generated.Employee.Builder clearSsn() {
      ssn = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'salary' field */
    public java.lang.Integer getSalary() {
      return salary;
    }
    
    /** Sets the value of the 'salary' field */
    public org.apache.gora.examples.generated.Employee.Builder setSalary(int value) {
      validate(fields()[4], value);
      this.salary = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'salary' field has been set */
    public boolean hasSalary() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'salary' field */
    public org.apache.gora.examples.generated.Employee.Builder clearSalary() {
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'boss' field */
    public java.lang.Object getBoss() {
      return boss;
    }
    
    /** Sets the value of the 'boss' field */
    public org.apache.gora.examples.generated.Employee.Builder setBoss(java.lang.Object value) {
      validate(fields()[5], value);
      this.boss = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'boss' field has been set */
    public boolean hasBoss() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'boss' field */
    public org.apache.gora.examples.generated.Employee.Builder clearBoss() {
      boss = null;
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'webpage' field */
    public org.apache.gora.examples.generated.WebPage getWebpage() {
      return webpage;
    }
    
    /** Sets the value of the 'webpage' field */
    public org.apache.gora.examples.generated.Employee.Builder setWebpage(org.apache.gora.examples.generated.WebPage value) {
      validate(fields()[6], value);
      this.webpage = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'webpage' field has been set */
    public boolean hasWebpage() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'webpage' field */
    public org.apache.gora.examples.generated.Employee.Builder clearWebpage() {
      webpage = null;
      fieldSetFlags()[6] = false;
      return this;
    }
    
    @Override
    public Employee build() {
      try {
        Employee record = new Employee();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.name = fieldSetFlags()[1] ? this.name : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.dateOfBirth = fieldSetFlags()[2] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[2]);
        record.ssn = fieldSetFlags()[3] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[3]);
        record.salary = fieldSetFlags()[4] ? this.salary : (java.lang.Integer) defaultValue(fields()[4]);
        record.boss = fieldSetFlags()[5] ? this.boss : (java.lang.Object) defaultValue(fields()[5]);
        record.webpage = fieldSetFlags()[6] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[6]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public Employee.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Employee newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'name' field.
		   */
	  public java.lang.CharSequence getName() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'name' field.
		   * @param value the value to set.
	   */
	  public void setName(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isNameDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'dateOfBirth' field.
		   */
	  public java.lang.Long getDateOfBirth() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'dateOfBirth' field.
		   * @param value the value to set.
	   */
	  public void setDateOfBirth(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isDateOfBirthDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'ssn' field.
		   */
	  public java.lang.CharSequence getSsn() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'ssn' field.
		   * @param value the value to set.
	   */
	  public void setSsn(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isSsnDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'salary' field.
		   */
	  public java.lang.Integer getSalary() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'salary' field.
		   * @param value the value to set.
	   */
	  public void setSalary(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isSalaryDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'boss' field.
		   */
	  public java.lang.Object getBoss() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'boss' field.
		   * @param value the value to set.
	   */
	  public void setBoss(java.lang.Object value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isBossDirty(java.lang.Object value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'webpage' field.
		   */
	  public org.apache.gora.examples.generated.WebPage getWebpage() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'webpage' field.
		   * @param value the value to set.
	   */
	  public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class Metadata extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    VERSION(1, "version"),
    DATA(2, "data"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "version",
  "data",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return version;
    case 2: return data;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: version = (java.lang.Integer)(value); break;
    case 2: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'version' field.
   */
  public java.lang.Integer getVersion() {
    return version;
  }

  /**
   * Sets the value of the 'version' field.
   * @param value the value to set.
   */
  public void setVersion(java.lang.Integer value) {
    this.version = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isVersionDirty(java.lang.Integer value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'data' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
    return data;
  }

  /**
   * Sets the value of the 'data' field.
   * @param value the value to set.
   */
  public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.data = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(2);
  }

  /** Creates a new Metadata RecordBuilder */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder() {
    return new org.apache.gora.examples.generated.Metadata.Builder();
  }
  
  /** Creates a new Metadata RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata.Builder other) {
    return new org.apache.gora.examples.generated.Metadata.Builder(other);
  }
  
  /** Creates a new Metadata RecordBuilder by copying an existing Metadata instance */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata other) {
    return new org.apache.gora.examples.generated.Metadata.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Metadata instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Metadata>
    implements org.apache.avro.data.RecordBuilder<Metadata> {

    private java.nio.ByteBuffer __g__dirty;
    private int version;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.Metadata.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Metadata instance */
    private Builder(org.apache.gora.examples.generated.Metadata other) {
            super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.version)) {
        this.version = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.version);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.data)) {
        this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.data);
        fieldSetFlags()[2] = true;
      }
    }

    /** Gets the value of the 'version' field */
    public java.lang.Integer getVersion() {
      return version;
    }
    
    /** Sets the value of the 'version' field */
    public org.apache.gora.examples.generated.Metadata.Builder setVersion(int value) {
      validate(fields()[1], value);
      this.version = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'version' field has been set */
    public boolean hasVersion() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'version' field */
    public org.apache.gora.examples.generated.Metadata.Builder clearVersion() {
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'data' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
      return data;
    }
    
    /** Sets the value of the 'data' field */
    public org.apache.gora.examples.generated.Metadata.Builder setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[2], value);
      this.data = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'data' field has been set */
    public boolean hasData() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'data' field */
    public org.apache.gora.examples.generated.Metadata.Builder clearData() {
      data = null;
      fieldSetFlags()[2] = false;
      return this;
    }
    
    @Override
    public Metadata build() {
      try {
        Metadata record = new Metadata();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.version = fieldSetFlags()[1] ? this.version : (java.lang.Integer) defaultValue(fields()[1]);
        record.data = fieldSetFlags()[2] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[2]));
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public Metadata.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Metadata newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'version' field.
		   */
	  public java.lang.Integer getVersion() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'version' field.
		   * @param value the value to set.
	   */
	  public void setVersion(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isVersionDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'data' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'data' field.
		   * @param value the value to set.
	   */
	  public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class TokenDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    COUNT(1, "count"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "count",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return count;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: count = (java.lang.Integer)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'count' field.
   */
  public java.lang.Integer getCount() {
    return count;
  }

  /**
   * Sets the value of the 'count' field.
   * @param value the value to set.
   */
  public void setCount(java.lang.Integer value) {
    this.count = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isCountDirty(java.lang.Integer value) {
    return isDirty(1);
  }

  /** Creates a new TokenDatum RecordBuilder */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder() {
    return new org.apache.gora.examples.generated.TokenDatum.Builder();
  }
  
  /** Creates a new TokenDatum RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
    return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
  }
  
  /** Creates a new TokenDatum RecordBuilder by copying an existing TokenDatum instance */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum other) {
    return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for TokenDatum instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<TokenDatum>
    implements org.apache.avro.data.RecordBuilder<TokenDatum> {

    private java.nio.ByteBuffer __g__dirty;
    private int count;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing TokenDatum instance */
    private Builder(org.apache.gora.examples.generated.TokenDatum other) {
            super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.count)) {
        this.count = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.count);
        fieldSetFlags()[1] = true;
      }
    }

    /** Gets the value of the 'count' field */
    public java.lang.Integer getCount() {
      return count;
    }
    
    /** Sets the value of the 'count' field */
    public org.apache.gora.examples.generated.TokenDatum.Builder setCount(int value) {
      validate(fields()[1], value);
      this.count = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'count' field has been set */
    public boolean hasCount() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'count' field */
    public org.apache.gora.examples.generated.TokenDatum.Builder clearCount() {
      fieldSetFlags()[1] = false;
      return this;
    }
    
    @Override
    public TokenDatum build() {
      try {
        TokenDatum record = new TokenDatum();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.count = fieldSetFlags()[1] ? this.count : (java.lang.Integer) defaultValue(fields()[1]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public TokenDatum.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public TokenDatum newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'count' field.
		   */
	  public java.lang.Integer getCount() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'count' field.
		   * @param value the value to set.
	   */
	  public void setCount(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isCountDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class WebPage extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    URL(1, "url"),
    CONTENT(2, "content"),
    PARSED_CONTENT(3, "parsedContent"),
    OUTLINKS(4, "outlinks"),
    HEADERS(5, "headers"),
    METADATA(6, "metadata"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "url",
  "content",
  "parsedContent",
  "outlinks",
  "headers",
  "metadata",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence url;
  private java.nio.ByteBuffer content;
  private java.util.List<java.lang.CharSequence> parsedContent;
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
  private org.apache.gora.examples.generated.Metadata metadata;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return url;
    case 2: return content;
    case 3: return parsedContent;
    case 4: return outlinks;
    case 5: return headers;
    case 6: return metadata;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: url = (java.lang.CharSequence)(value); break;
    case 2: content = (java.nio.ByteBuffer)(value); break;
    case 3: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
    case 4: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    case 5: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
    case 6: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'url' field.
   */
  public java.lang.CharSequence getUrl() {
    return url;
  }

  /**
   * Sets the value of the 'url' field.
   * @param value the value to set.
   */
  public void setUrl(java.lang.CharSequence value) {
    this.url = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUrlDirty(java.lang.CharSequence value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'content' field.
   */
  public java.nio.ByteBuffer getContent() {
    return content;
  }

  /**
   * Sets the value of the 'content' field.
   * @param value the value to set.
   */
  public void setContent(java.nio.ByteBuffer value) {
    this.content = value;
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isContentDirty(java.nio.ByteBuffer value) {
    return isDirty(2);
  }

  /**
   * Gets the value of the 'parsedContent' field.
   */
  public java.util.List<java.lang.CharSequence> getParsedContent() {
    return parsedContent;
  }

  /**
   * Sets the value of the 'parsedContent' field.
   * @param value the value to set.
   */
  public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
    this.parsedContent = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper(value);
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
    return isDirty(3);
  }

  /**
   * Gets the value of the 'outlinks' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
    return outlinks;
  }

  /**
   * Sets the value of the 'outlinks' field.
   * @param value the value to set.
   */
  public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.outlinks = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(4);
  }
  
  /**
   * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(4);
  }

  /**
   * Gets the value of the 'headers' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
    return headers;
  }

  /**
   * Sets the value of the 'headers' field.
   * @param value the value to set.
   */
  public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.headers = value;
    setDirty(5);
  }
  
  /**
   * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(5);
  }

  /**
   * Gets the value of the 'metadata' field.
   */
  public org.apache.gora.examples.generated.Metadata getMetadata() {
    return metadata;
  }

  /**
   * Sets the value of the 'metadata' field.
   * @param value the value to set.
   */
  public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
    this.metadata = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
    return isDirty(6);
  }

  /** Creates a new WebPage RecordBuilder */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder() {
    return new org.apache.gora.examples.generated.WebPage.Builder();
  }
  
  /** Creates a new WebPage RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage.Builder other) {
    return new org.apache.gora.examples.generated.WebPage.Builder(other);
  }
  
  /** Creates a new WebPage RecordBuilder by copying an existing WebPage instance */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage other) {
    return new org.apache.gora.examples.generated.WebPage.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for WebPage instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<WebPage>
    implements org.apache.avro.data.RecordBuilder<WebPage> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence url;
    private java.nio.ByteBuffer content;
    private java.util.List<java.lang.CharSequence> parsedContent;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
    private org.apache.gora.examples.generated.Metadata metadata;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.WebPage.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing WebPage instance */
    private Builder(org.apache.gora.examples.generated.WebPage other) {
            super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.content)) {
        this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[2].schema(), other.content);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.parsedContent)) {
        this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.parsedContent);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.outlinks)) {
        this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.outlinks);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.headers)) {
        this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[5].schema(), other.headers);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.metadata)) {
        this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[6].schema(), other.metadata);
        fieldSetFlags()[6] = true;
      }
    }

    /** Gets the value of the 'url' field */
    public java.lang.CharSequence getUrl() {
      return url;
    }
    
    /** Sets the value of the 'url' field */
    public org.apache.gora.examples.generated.WebPage.Builder setUrl(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.url = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'url' field has been set */
    public boolean hasUrl() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'url' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearUrl() {
      url = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'content' field */
    public java.nio.ByteBuffer getContent() {
      return content;
    }
    
    /** Sets the value of the 'content' field */
    public org.apache.gora.examples.generated.WebPage.Builder setContent(java.nio.ByteBuffer value) {
      validate(fields()[2], value);
      this.content = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'content' field has been set */
    public boolean hasContent() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'content' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearContent() {
      content = null;
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'parsedContent' field */
    public java.util.List<java.lang.CharSequence> getParsedContent() {
      return parsedContent;
    }
    
    /** Sets the value of the 'parsedContent' field */
    public org.apache.gora.examples.generated.WebPage.Builder setParsedContent(java.util.List<java.lang.CharSequence> value) {
      validate(fields()[3], value);
      this.parsedContent = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'parsedContent' field has been set */
    public boolean hasParsedContent() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'parsedContent' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearParsedContent() {
      parsedContent = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'outlinks' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
      return outlinks;
    }
    
    /** Sets the value of the 'outlinks' field */
    public org.apache.gora.examples.generated.WebPage.Builder setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[4], value);
      this.outlinks = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'outlinks' field has been set */
    public boolean hasOutlinks() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'outlinks' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearOutlinks() {
      outlinks = null;
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'headers' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
      return headers;
    }
    
    /** Sets the value of the 'headers' field */
    public org.apache.gora.examples.generated.WebPage.Builder setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[5], value);
      this.headers = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'headers' field has been set */
    public boolean hasHeaders() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'headers' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearHeaders() {
      headers = null;
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'metadata' field */
    public org.apache.gora.examples.generated.Metadata getMetadata() {
      return metadata;
    }
    
    /** Sets the value of the 'metadata' field */
    public org.apache.gora.examples.generated.WebPage.Builder setMetadata(org.apache.gora.examples.generated.Metadata value) {
      validate(fields()[6], value);
      this.metadata = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'metadata' field has been set */
    public boolean hasMetadata() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'metadata' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearMetadata() {
      metadata = null;
      fieldSetFlags()[6] = false;
      return this;
    }
    
    @Override
    public WebPage build() {
      try {
        WebPage record = new WebPage();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.content = fieldSetFlags()[2] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[2]);
        record.parsedContent = fieldSetFlags()[3] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[3]));
        record.outlinks = fieldSetFlags()[4] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[4]));
        record.headers = fieldSetFlags()[5] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[5]);
        record.metadata = fieldSetFlags()[6] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public WebPage.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public WebPage newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'url' field.
		   */
	  public java.lang.CharSequence getUrl() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'url' field.
		   * @param value the value to set.
	   */
	  public void setUrl(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUrlDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'content' field.
		   */
	  public java.nio.ByteBuffer getContent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'content' field.
		   * @param value the value to set.
	   */
	  public void setContent(java.nio.ByteBuffer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isContentDirty(java.nio.ByteBuffer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'parsedContent' field.
		   */
	  public java.util.List<java.lang.CharSequence> getParsedContent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'parsedContent' field.
		   * @param value the value to set.
	   */
	  public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'outlinks' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'outlinks' field.
		   * @param value the value to set.
	   */
	  public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'headers' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'headers' field.
		   * @param value the value to set.
	   */
	  public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'metadata' field.
		   */
	  public org.apache.gora.examples.generated.Metadata getMetadata() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metadata' field.
		   * @param value the value to set.
	   */
	  public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
        return EncoderFactory.get().binaryEncoder(getOrCreateOutputStream(), null);
        return EncoderFactory.get().jsonEncoder(schema, getOrCreateOutputStream());
        return DecoderFactory.get().binaryDecoder(getOrCreateInputStream(), null);
        return DecoderFactory.get().jsonDecoder(schema, getOrCreateInputStream());
    int fieldIndex = persistent.getSchema().getField(fieldName).pos();
    @SuppressWarnings("unchecked")
    int fieldIndex = persistent.getSchema().getField(fieldName).pos(); //.getIndexNamed(fieldName); throws org.apache.avro.AvroRuntimeException: Not a union:
  /**
   * Add our own serializer (obtained via the {@link PersistentSerialization} 
   * wrapper) to any other <code>io.serializations</code> which may be specified 
   * within existing Hadoop configuration.
   * 
   * @param conf the Hadoop configuration object
   * @param reuseObjects boolean parameter to reuse objects
   */
import org.apache.avro.specific.SpecificDatumReader;
* Hadoop deserializer using {@link SpecificDatumReader}
   implements Deserializer<Persistent> {
  private Class<? extends Persistent> persistentClass;
  private SpecificDatumReader<Persistent> datumReader;
  public PersistentDeserializer(Class<? extends Persistent> c, boolean reuseObjects) {
      datumReader = new SpecificDatumReader<Persistent>(schema);
  @Override
* supplies an input stream that is only valid until the end of one
* record serialization. Each time deserialize() is called, the IS
* is advanced to point to the right location, so we should not
* buffer the whole input stream at once.
*/
    decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);
  @Override
  public Persistent deserialize(Persistent persistent) throws IOException {
public class PersistentSerialization
implements Serialization<Persistent> {
  public Deserializer<Persistent> getDeserializer(Class<Persistent> c) {
  public Serializer<Persistent> getSerializer(Class<Persistent> c) {
 * or more contributor license agreements. See the NOTICE file
 * regarding copyright ownership. The ASF licenses this file
 * with the License. You may obtain a copy of the License at
 * http://www.apache.org/licenses/LICENSE-2.0
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
 * Hadoop serializer using Avro's {@link SpecificDatumWriter}
 * with {@link BinaryEncoder}.
public class PersistentSerializer implements Serializer<Persistent> {
  private SpecificDatumWriter<Persistent> datumWriter;
  private BinaryEncoder encoder;
    this.datumWriter = new SpecificDatumWriter<Persistent>();
  @Override
  /**
   * Open a connection for the {@link OutputStream}; should be
   * called before serialization occurs. N.B. the {@link PersistentSerializer#close()}
   * should be called 'finally' after serialization is complete.
   */
  @Override
    encoder = EncoderFactory.get().directBinaryEncoder(out, null);
  /**
   * Do the serialization of the {@link Persistent} object
   */
  public void serialize(Persistent persistent) throws IOException {

import org.apache.avro.Schema.Field;

import org.apache.gora.util.AvroUtils;
  try{
    long deletedRows = 0;
      Result<K,T> result = query.execute();

      while(result.next()) {
        if(delete(result.getKey()))
          deletedRows;
      }
      return 0;
    }
    catch(Exception e){
      return 0;
    }
    List<Field> otherFields = obj.getSchema().getFields();
    String[] otherFieldStrings = new String[otherFields.size()];
    for(int i = 0; i<otherFields.size(); i ){
      otherFieldStrings[i] = otherFields.get(i).name();
    }
    if(Arrays.equals(fields, otherFieldStrings)) { 
    T newObj = (T) AvroUtils.deepClonePersistent(obj); 
      for(int i = 0; i<otherFields.size(); i) {
      int index = otherFields.get(i).pos(); 
      newObj.put(index, obj.get(index));
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import java.util.List;

import org.apache.avro.Schema.Field;
import org.apache.avro.specific.SpecificRecord;

import org.apache.gora.persistency.Dirtyable;

* Objects that are persisted by Gora implements this interface.
*/
public interface Persistent extends SpecificRecord, Dirtyable {

  public static String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
* Clears the inner state of the object without any modification to the actual
* data on the data store. This method should be called before re-using the
* object to hold the data for another result.
*/

* Returns whether the field has been modified.
*
* @param fieldIndex
* the offset of the field in the object
* @return whether the field has been modified.
*/
* Returns whether the field has been modified.
*
* @param field
* the name of the field
* @return whether the field has been modified.
*/

* Sets all the fields of the object as dirty.
*/

* Sets the field as dirty.
*
* @param fieldIndex
* the offset of the field in the object
*/

* Sets the field as dirty.
*
* @param field
* the name of the field
*/

* Clears the field as dirty.
*
* @param fieldIndex
* the offset of the field in the object
*/

* Clears the field as dirty.
*
* @param field
* the name of the field
*/
* Get an object which can be used to mark this field as deleted (rather than
* state unknown, which is indicated by null).
*
* @return a tombstone.
*/
  public abstract Tombstone getTombstone();
* Get a list of fields from this persistent object's schema that are not
* managed by Gora.
*
* @return the unmanaged fields
*/
  public List<Field> getUnmanagedFields();
   * Constructs a new instance of the object by using appropriate builder. This
   * method is intended to be used by Gora framework.
   * 
   * @return a new instance of the object
  Persistent newInstance();
  /** Class of the key to be used */
  
  /** Class of the persistent objects to be stored */
  /** Constructor of the key */
  /** Object's key */
  
  /** Persistent object of class T */
  /** Flag to be used to determine if a key is persistent or not */
  /**
   * Default constructor for this class.
   * @param keyClass.
   * @param persistentClass
   */
    return keyClass.newInstance();
    return (T) persistent.newInstance();
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import java.util.Collection;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.gora.persistency.Dirtyable;
* Base classs implementing common functionality for Persistent classes.
*/
public abstract class PersistentBase extends SpecificRecordBase implements
    Persistent {
  public static class PersistentData extends SpecificData {
    private static final PersistentData INSTANCE = new PersistentData();
    public static PersistentData get() {
      return INSTANCE;
    public boolean equals(SpecificRecord obj1, SpecificRecord that) {
      if (that == obj1)
        return true; // identical object
      if (!(that instanceof SpecificRecord))
        return false; // not a record
      if (obj1.getClass() != that.getClass())
        return false; // not same schema
      return PersistentData.get().compare(obj1, that, obj1.getSchema(), true) == 0;
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirtyBytes.put(i, (byte) 0);
    for (Field field : getSchema().getFields()) {
      clearDirynessIfFieldIsDirtyable(field.pos());
  }

  private void clearDirynessIfFieldIsDirtyable(int fieldIndex) {
    if (fieldIndex == 0)
      return;
    Object value = get(fieldIndex);
    if (value instanceof Dirtyable) {
      ((Dirtyable) value).clearDirty();
  public void clearDirty(int fieldIndex) {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    currentByte = (byte) ((~(1 << bitOffset)) & currentByte);
    dirtyBytes.put(byteOffset, currentByte);
    clearDirynessIfFieldIsDirtyable(fieldIndex);

  @Override
  public void clearDirty(String field) {
    clearDirty(getSchema().getField(field).pos());
  }

  @Override
  public boolean isDirty() {
    boolean isSubRecordDirty = false;
    for (Field field : fields) {
      isSubRecordDirty = isSubRecordDirty || checkIfMutableFieldAndDirty(field);
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    boolean dirty = false;
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirty = dirty || dirtyBytes.get(i) != 0;
    }
    return isSubRecordDirty || dirty;
  }

  private boolean checkIfMutableFieldAndDirty(Field field) {
    if (field.pos() == 0)
      return false;
    switch (field.schema().getType()) {
    case RECORD:
    case MAP:
    case ARRAY:
      Object value = get(field.pos());
      return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
    case UNION:
      value = get(field.pos());
      return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
    default:
      break;
    }
    return false;
  }

  @Override
  public boolean isDirty(int fieldIndex) {
    Field field = getSchema().getFields().get(fieldIndex);
    boolean isSubRecordDirty = checkIfMutableFieldAndDirty(field);
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    return isSubRecordDirty || 0 != ((1 << bitOffset) & currentByte);
  }

  @Override
  public boolean isDirty(String fieldName) {
    Field field = getSchema().getField(fieldName);
    if(field == null){
      throw new IndexOutOfBoundsException
      ("Field " fieldName  " does not exist in this schema.");
    }
    return isDirty(field.pos());
  }

  @Override
  public void setDirty() {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirtyBytes.put(i, (byte) -128);
    }
  }

  @Override
  public void setDirty(int fieldIndex) {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    currentByte = (byte) ((1 << bitOffset) | currentByte);
    dirtyBytes.put(byteOffset, currentByte);
  }

  @Override
  public void setDirty(String field) {
    setDirty(getSchema().getField(field).pos());
  }

  private ByteBuffer getDirtyBytes() {
    return (ByteBuffer) get(0);
  }

  @Override
  public void clear() {
    Collection<Field> unmanagedFields = getUnmanagedFields();
    for (Field field : getSchema().getFields()) {
      if (!unmanagedFields.contains(field))
        continue;
      put(field.pos(), PersistentData.get().deepCopy(field.schema(), PersistentData.get().getDefaultValue(field)));
    }
    clearDirty();
  }

  @Override
  public boolean equals(Object that) {
    if (that == this) {
      return true;
    } else if (that instanceof Persistent) {
      return PersistentData.get().equals(this, (SpecificRecord) that);
    } else {
      return false;
    }
  public List<Field> getUnmanagedFields(){
    List<Field> fields = getSchema().getFields();
    return fields.subList(1, fields.size());
  
      return keyClass.newInstance();
    try {
      return (T) persistentClass.newInstance();
    } catch (InstantiationException e) {
      throw new RuntimeException(e);
    } catch (IllegalAccessException e) {
      e.printStackTrace();
      throw new RuntimeException(e);
    }
  private void clearReadable() {
	  // TODO Auto-generated method stub
	  
	@Override
    return true;
    return true;
  public static final String SCHEMA_NAME = "schema.name";

      Properties properties = new Properties();
          .getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
          throws GoraException {
          ReflectionUtils.newInstance(dataStoreClass);
          throws GoraException {
          throws GoraException {
          = (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
          throws GoraException {
  /**
   * Looks for the <code>gora-&lt;classname&gt;-mapping.xml</code> as a resource 
   * on the classpath. This can however also be specified within the 
   * <code>gora.properties</code> file with the key 
   * <code>gora.&lt;classname&gt;.mapping.file=</code>.
   * @param properties which hold keys from which we can obtain values for datastore mappings.
   * @param store {@link org.apache.gora.store.DataStore} object to get the mapping for.
   * @param defaultValue default value for the <code>gora-&lt;classname&gt;-mapping.xml</code>
   * @return mappingFilename if one is located.
   * @throws IOException if there is a problem reading or obtaining the mapping file.
   */
import java.util.ArrayList;
import java.util.List;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter; 
import org.apache.gora.persistency.Persistent;
  protected SpecificDatumReader<T> datumReader;
  protected SpecificDatumWriter<T> datumWriter;
    datumReader = new SpecificDatumReader<T>(schema);
    datumWriter = new SpecificDatumWriter<T>(schema);
    return getFields();
  }
  
  protected String[] getFields() {
    List<Field> schemaFields = beanFactory.getCachedPersistent().getSchema().getFields();
    
    List<Field> list = new ArrayList<Field>();
    for (Field field : schemaFields) {
      if (!Persistent.DIRTY_BYTES_FIELD_NAME.equalsIgnoreCase(field.name())) {
        list.add(field);
      }
    }
    schemaFields = list;
    
    String[] fieldNames = new String[schemaFields.size()];
    for(int i = 0; i<fieldNames.length; i ){
      fieldNames[i] = schemaFields.get(i).name();
    }
    
    return fieldNames;
   * First the schema name in the {@link Configuration} is used. If null,
   * the schema name in the defined properties is returned. If null then
    String confSchemaName = getOrCreateConf().get("preferred.schema.name");
    if (confSchemaName != null) {
      return confSchemaName;
    }
      return schemaName;
      return mappingSchemaName;
    return StringUtils.getClassname(persistentClass);
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
 * An utility class for Avro related tasks
    for (Field field : fields) {



  public static Schema getSchema(Class<? extends Persistent> clazz)
      throws SecurityException, NoSuchFieldException, IllegalArgumentException,
      IllegalAccessException {

    java.lang.reflect.Field field = clazz.getDeclaredField("SCHEMA$");

  /**
   * Return the field names from a persistent object
   * 
   * @param persistent
   *          the persistent object to get the fields names from
   * @return the field names
   */
  public static String[] getPersistentFieldNames(Persistent persistent) {
    return getSchemaFieldNames(persistent.getSchema());
  }

  /**
   * Return the field names from a schema object
   * 
   * @param persistent
   *          the persistent object to get the fields names from
   * @return the field names
   */
  public static String[] getSchemaFieldNames(Schema schema) {
    List<Field> fields = schema.getFields();
    String[] fieldNames = new String[fields.size() - 1];
    for (int i = 0; i < fieldNames.length; i) {
      fieldNames[i] = fields.get(i  1).name();
    }
    return fieldNames;
  }

  public static <T extends Persistent> T deepClonePersistent(T persistent) {
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    BinaryEncoder enc = EncoderFactory.get().binaryEncoder(bos, null);
    SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<Persistent>(
        persistent.getSchema());
    try {
      writer.write(persistent, enc);
    } catch (IOException e) {
      throw new RuntimeException(
          "Unable to serialize avro object to byte buffer - "
               "please report this issue to the Gora bugtracker "
               "or your administrator.");
    }
    byte[] value = bos.toByteArray();
    Decoder dec = DecoderFactory.get().binaryDecoder(value, null);
    @SuppressWarnings("unchecked")
    SpecificDatumReader<T> reader = new SpecificDatumReader<T>(
        (Class<T>) persistent.getClass());
    try {
      return reader.read(null, dec);
    } catch (IOException e) {
      throw new RuntimeException(
          "Unable to deserialize avro object from byte buffer - "
               "please report this issue to the Gora bugtracker "
               "or your administrator.");
    }

  }

import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
  public static <T> T fromBytes( byte[] val, Schema schema
      , SpecificDatumReader<T> datumReader, T object)
      return (T)Enum.valueOf(ReflectData.get().getClass(schema), symbol);
    case STRING:  return (T)new Utf8(toString(val));
    case BYTES:   return (T)ByteBuffer.wrap(val);
    case INT:     return (T)Integer.valueOf(bytesToVint(val));
    case LONG:    return (T)Long.valueOf(bytesToVlong(val));
    case FLOAT:   return (T)Float.valueOf(toFloat(val));
    case DOUBLE:  return (T)Double.valueOf(toDouble(val));
    case BOOLEAN: return (T)Boolean.valueOf(val[0] != 0);
    case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, schema, (SpecificRecord)object);
  @SuppressWarnings("unchecked")
  public static <T> byte[] toBytes(T o, Schema schema
      , SpecificDatumWriter<T> datumWriter)
    case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, schema, (SpecificRecord)o);
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter; 
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.util.ByteBufferInputStream;
import org.apache.avro.util.ByteBufferOutputStream;
  public static<T extends SpecificRecord> void serialize(OutputStream os,
      SpecificDatumWriter<T> datumWriter, Schema schema, T object)
    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
    datumWriter.write(object, encoder);
  public static<T> void serialize(OutputStream os,
      SpecificDatumWriter<T> datumWriter, Schema schema, T object)
      throws IOException {

    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
    datumWriter.write(object, encoder);
    encoder.flush();
  }
  
  /**
   * Serializes the field object using the datumWriter.
   */
  public static<T extends SpecificRecord> byte[] serialize(SpecificDatumWriter<T> datumWriter
      , Schema schema, T object) throws IOException {
  /**
   * Serializes the field object using the datumWriter.
   */
  public static<T> byte[] serialize(SpecificDatumWriter<T> datumWriter
      , Schema schema, T object) throws IOException {
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    serialize(os, datumWriter, schema, object);
    return os.toByteArray();
  }
  
  public static<K, T extends SpecificRecord> T deserialize(InputStream is,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
    decoder = DecoderFactory.get().binaryDecoder(is, decoder);
    return (T)datumReader.read(object, decoder);
  public static<K, T extends SpecificRecord> T deserialize(byte[] bytes,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
    decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
    return (T)datumReader.read(object, decoder);
   * Deserializes the field object using the datumReader.
  public static<K, T> T deserialize(byte[] bytes,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
      throws IOException {
    decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
    return (T)datumReader.read(object, decoder);
  
import java.lang.reflect.Method;

import org.apache.avro.specific.SpecificRecordBuilderBase;
import org.apache.gora.persistency.Persistent;
  
  public static <T extends Persistent> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) throws SecurityException
    , NoSuchMethodException, IllegalArgumentException, IllegalAccessException, InvocationTargetException {
    return (SpecificRecordBuilderBase<T>) clazz.getMethod("newBuilder").invoke(null);
  }
  
import org.apache.gora.persistency.Tombstone;
  public Tombstone getTombstone() {
    return new Tombstone(){};
  public Persistent newInstance() {
    return new MockPersistent();
      preferredSchema = properties.getProperty(PREF_SCH_NAME);
      dynamoDBClient = getClient(properties.getProperty(CLI_TYP_PROP),(AWSCredentials)getConf());
      dynamoDBClient.setEndpoint(properties.getProperty(ENDPOINT_PROP));
      consistency = properties.getProperty(CONSISTENCY_READS);
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


   * {@inheritDoc} Serializes the Persistent data and saves in HBase. Topmost
   * fields of the record are persisted in "raw" format (not avro serialized).
   * This behavior happens in maps and arrays too.
   * ["null","type"] type (a.k.a. optional field) is persisted like as if it is
   * ["type"], but the column get deleted if value==null (so value read after
   * will be null).
   * @param persistent
   *          Record to be persisted in HBase
    try {
      List<Field> fields = schema.getFields();
      for (int i = 1; i < fields.size(); i) {
        if (!persistent.isDirty(i)) {
        Field field = fields.get(i);
          throw new RuntimeException("HBase mapping for field ["
               persistent.getClass().getName()  "#"  field.name()
               "] not found. Wrong gora-hbase-mapping.xml?");
        addPutsAndDeletes(put, delete, o, field.schema().getType(),
            field.schema(), hcol, hcol.getQualifier());
      if (put.size() > 0) {
      if (delete.size() > 0) {
        table.delete(delete);
        table.delete(delete); // HBase sometimes does not delete arbitrarily
    } catch (IOException ex2) {
  private void addPutsAndDeletes(Put put, Delete delete, Object o, Type type,
      Schema schema, HBaseColumn hcol, byte[] qualifier) throws IOException {
    switch (type) {
    case UNION:
      if (isNullable(schema) && o == null) {
        if (qualifier == null) {
          delete.deleteFamily(hcol.getFamily());
        } else {
          delete.deleteColumn(hcol.getFamily(), qualifier);
        }
      } else {
//        int index = GenericData.get().resolveUnion(schema, o);
        int index = getResolvedUnionIndex(schema);
        if (index > 1) {  //if more than 2 type in union, serialize directly for now
          byte[] serializedBytes = toBytes(o, schema);
          put.add(hcol.getFamily(), qualifier, serializedBytes);
        } else {
          Schema resolvedSchema = schema.getTypes().get(index);
          addPutsAndDeletes(put, delete, o, resolvedSchema.getType(),
              resolvedSchema, hcol, qualifier);
        }
      }
      break;
    case MAP:
      // if it's a map that has been modified, then the content should be replaced by the new one
      // This is because we don't know if the content has changed or not.
      if (qualifier == null) {
        delete.deleteFamily(hcol.getFamily());
      } else {
        delete.deleteColumn(hcol.getFamily(), qualifier);
      }
      @SuppressWarnings({ "rawtypes", "unchecked" })
      Set<Entry> set = ((Map) o).entrySet();
      for (@SuppressWarnings("rawtypes") Entry entry : set) {
        byte[] qual = toBytes(entry.getKey());
        addPutsAndDeletes(put, delete, entry.getValue(), schema.getValueType()
            .getType(), schema.getValueType(), hcol, qual);
      }
      break;
    case ARRAY:
      List<?> array = (List<?>) o;
      int j = 0;
      for (Object item : array) {
        addPutsAndDeletes(put, delete, item, schema.getElementType().getType(),
            schema.getElementType(), hcol, Bytes.toBytes(j));
      }
      break;
    default:
      byte[] serializedBytes = toBytes(o, schema);
      put.add(hcol.getFamily(), qualifier, serializedBytes);
      break;
    }
  }

  private boolean isNullable(Schema unionSchema) {
    for (Schema innerSchema : unionSchema.getTypes()) {
      if (innerSchema.getType().equals(Schema.Type.NULL)) {
        return true;
      }
    }
    return false;
  }

      boolean isAllFields = Arrays.equals(fields, getFields());
      addFamilyOrColumn(get, col, fieldSchema);
  private void addFamilyOrColumn(Get get, HBaseColumn col, Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(get, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      get.addFamily(col.family);
      break;
    default:
      get.addColumn(col.family, col.qualifier);
      break;
    }
  }

  private void addFields(Scan scan, Query<K, T> query) throws IOException {
      addFamilyOrColumn(scan, col, fieldSchema);
  private void addFamilyOrColumn(Scan scan, HBaseColumn col, Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(scan, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      scan.addFamily(col.family);
      break;
    default:
      scan.addColumn(col.family, col.qualifier);
      break;
    }
  }

  // TODO: HBase Get, Scan, Delete should extend some common interface with
  // addFamily, etc
  private void addFields(Delete delete, Query<K, T> query)    throws IOException {
      addFamilyOrColumn(delete, col, fieldSchema);
    }
  }

  private void addFamilyOrColumn(Delete delete, HBaseColumn col,
      Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(delete, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      delete.deleteFamily(col.family);
      break;
    default:
      delete.deleteColumn(col.family, col.qualifier);
      break;
      setField(result,persistent, col, field, fieldSchema);
    }
    persistent.clearDirty();
    return persistent;
  }

  private void setField(Result result, T persistent, HBaseColumn col,
      Field field, Schema fieldSchema) throws IOException {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      if (index > 1) { //if more than 2 type in union, deserialize directly for now
        byte[] val = result.getValue(col.getFamily(), col.getQualifier());
        if (val == null) {
          return;
        }
        setField(persistent, field, val);
      } else {
        Schema resolvedSchema = fieldSchema.getTypes().get(index);
        setField(result, persistent, col, field, resolvedSchema);
      }
      break;
    case MAP:
      NavigableMap<byte[], byte[]> qualMap = result.getNoVersionMap().get(
          col.getFamily());
      if (qualMap == null) {
        return;
      }
      Schema valueSchema = fieldSchema.getValueType();
      Map<Utf8, Object> map = new HashMap<Utf8, Object>();
      for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
        map.put(new Utf8(Bytes.toString(e.getKey())),
            fromBytes(valueSchema, e.getValue()));
      }
      setField(persistent, field, map);
      break;
    case ARRAY:
      qualMap = result.getFamilyMap(col.getFamily());
      if (qualMap == null) {
        return;
      }
      valueSchema = fieldSchema.getElementType();
      ArrayList<Object> arrayList = new ArrayList<Object>();
      DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<Object>(arrayList);
      for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
        dirtyListWrapper.add(fromBytes(valueSchema, e.getValue()));
      }
      setField(persistent, field, arrayList);
      break;
    default:
      byte[] val = result.getValue(col.getFamily(), col.getQualifier());
      if (val == null) {
        return;
      }
      setField(persistent, field, val);
      break;
    }
  }

  //TODO temporary solution, has to be changed after implementation of saving the index of union type
  private int getResolvedUnionIndex(Schema unionScema) {
    if (unionScema.getTypes().size() == 2) {

      // schema [type0, type1]
      Type type0 = unionScema.getTypes().get(0).getType();
      Type type1 = unionScema.getTypes().get(1).getType();

      // Check if types are different and there's a "null", like ["null","type"]
      // or ["type","null"]
      if (!type0.equals(type1)
          && (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {

        if (type0.equals(Schema.Type.NULL))
          return 1;
        else
          return 0;
    return 2;
    persistent.put(field.pos(), new DirtyMapWrapper(map));
  @SuppressWarnings({ "rawtypes", "unchecked" })
  private void setField(T persistent, Field field, List list) {
    persistent.put(field.pos(), new DirtyListWrapper(list));
}
import java.util.Map;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.RowMutations;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;

  @Override
  public void batch(List<? extends Row> actions, Object[] results)
      throws IOException, InterruptedException {
    // TODO Auto-generated method stub
    getTable().batch(actions, results);
    
  }

  @Override
  public Object[] batch(List<? extends Row> actions) throws IOException,
      InterruptedException {
    // TODO Auto-generated method stub
    return getTable().batch(actions);
  }

  @Override
  public void mutateRow(RowMutations rm) throws IOException {
    // TODO Auto-generated method stub
    
  }

  @Override
  public Result append(Append append) throws IOException {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol> T coprocessorProxy(Class<T> protocol,
      byte[] row) {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol, R> Map<byte[], R> coprocessorExec(
      Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable)
      throws IOException, Throwable {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol, R> void coprocessorExec(
      Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable,
      Callback<R> callback) throws IOException, Throwable {
    // TODO Auto-generated method stub
    
  }

  @Override
  public void setAutoFlush(boolean autoFlush) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public long getWriteBufferSize() {
    // TODO Auto-generated method stub
    return 0;
  }

  @Override
  public void setWriteBufferSize(long writeBufferSize) throws IOException {
    // TODO Auto-generated method stub
    
  }
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.io.EncoderFactory;


  private static ThreadLocal<ByteArrayOutputStream> outputStream =
      new ThreadLocal<ByteArrayOutputStream>();
  
  public static final ThreadLocal<BinaryEncoder> encoders =
      new ThreadLocal<BinaryEncoder>();
  public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = 
      new ConcurrentHashMap<String, SpecificDatumReader<?>>();
     
  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = 
      new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
   * Deserializes an array of bytes matching the given schema to the proper basic 
   * (enum, Utf8,...) or complex type (Persistent/Record).
  @SuppressWarnings({ "rawtypes" })
      // For UNION schemas, must use a specific SpecificDatumReader
      String schemaId = schema.getType().equals(Schema.Type.UNION) ? String.valueOf(schema.hashCode()) : schema.getFullName();      
      
      SpecificDatumReader<?> reader = (SpecificDatumReader<?>)readerMap.get(schemaId);
      if (reader == null) {
        reader = new SpecificDatumReader(schema);// ignore dirty bits
        SpecificDatumReader localReader=null;
        if((localReader=readerMap.putIfAbsent(schemaId, reader))!=null) {
          reader = localReader;
      BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(val, null);
      return reader.read(null, decoder);
    case STRING:  return Bytes.toBytes(((CharSequence)o).toString()); // TODO: maybe ((Utf8)o).getBytes(); ?
      SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap.get(schema.getFullName());
      if (writer == null) {
        writer = new SpecificDatumWriter(schema);// ignore dirty bits
        writerMap.put(schema.getFullName(),writer);

      BinaryEncoder encoderFromCache = encoders.get();
      ByteArrayOutputStream bos = new ByteArrayOutputStream();
      outputStream.set(bos);
      BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(bos, null);
      if (encoderFromCache == null) {

      ByteArrayOutputStream os = outputStream.get();

      writer.write(o, encoder);
    //htu.getConfiguration().set("hbase.zookeeper.quorum", "localhost");
    //htu.getConfiguration().setInt("hbase.zookeeper.property.clientPort", 2181);
    
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;

  private static final Logger LOG = LoggerFactory.getLogger(SolrStore.class);


  // protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";

  /**
   * Default schema index with value "0" used when AVRO Union data types are
   * stored
   */
  public static int DEFAULT_UNION_SCHEMA = 0;

  /*
   * Create a threadlocal map for the datum readers and writers, because they
   * are not thread safe, at least not before Avro 1.4.0 (See AVRO-650). When
   * they are thread safe, it is possible to maintain a single reader and writer
   * pair for every schema, instead of one for every thread.
   */

  public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<String, SpecificDatumReader<?>>();

  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<String, SpecificDatumWriter<?>>();

  public void initialize(Class<K> keyClass, Class<T> persistentClass,
      Properties properties) {
    super.initialize(keyClass, persistentClass, properties);
      String mappingFile = DataStoreFactory.getMappingFile(properties, this,
          DEFAULT_MAPPING_FILE);
      mapping = readMapping(mappingFile);
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    solrServerUrl = DataStoreFactory.findProperty(properties, this,
        SOLR_URL_PROPERTY, null);
    solrConfig = DataStoreFactory.findProperty(properties, this,
        SOLR_CONFIG_PROPERTY, null);
    solrSchema = DataStoreFactory.findProperty(properties, this,
        SOLR_SCHEMA_PROPERTY, null);
    LOG.info("Using Solr server at "  solrServerUrl);
    adminServer = new HttpSolrServer(solrServerUrl);
    server = new HttpSolrServer(solrServerUrl  "/"  mapping.getCoreName());
    if (autoCreateSchema) {
    String batchSizeString = DataStoreFactory.findProperty(properties, this,
        SOLR_BATCH_SIZE_PROPERTY, null);
    if (batchSizeString != null) {
        batchSize = Integer.parseInt(batchSizeString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid batch size '"  batchSizeString  "', using default "
             DEFAULT_BATCH_SIZE);
    batch = new ArrayList<SolrInputDocument>(batchSize);
    String commitWithinString = DataStoreFactory.findProperty(properties, this,
        SOLR_COMMIT_WITHIN_PROPERTY, null);
    if (commitWithinString != null) {
        commitWithin = Integer.parseInt(commitWithinString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid commit within '"  commitWithinString
             "', using default "  DEFAULT_COMMIT_WITHIN);
    String resultsSizeString = DataStoreFactory.findProperty(properties, this,
        SOLR_RESULTS_SIZE_PROPERTY, null);
    if (resultsSizeString != null) {
        resultsSize = Integer.parseInt(resultsSizeString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid results size '"  resultsSizeString
             "', using default "  DEFAULT_RESULTS_SIZE);
  private SolrMapping readMapping(String filename) throws IOException {
      Document doc = builder.build(getClass().getClassLoader()
          .getResourceAsStream(filename));
      List<Element> classes = doc.getRootElement().getChildren("class");
      for (Element classElement : classes) {
        if (classElement.getAttributeValue("keyClass").equals(
            keyClass.getCanonicalName())
            && classElement.getAttributeValue("name").equals(
                persistentClass.getCanonicalName())) {
          String tableName = getSchemaName(
              classElement.getAttributeValue("table"), persistentClass);
          map.setCoreName(tableName);
          Element primaryKeyEl = classElement.getChild("primarykey");
          map.setPrimaryKey(primaryKeyEl.getAttributeValue("column"));
          List<Element> fields = classElement.getChildren("field");
          for (Element field : fields) {
            String fieldName = field.getAttributeValue("name");
            String columnName = field.getAttributeValue("column");
            map.addField(fieldName, columnName);
    } catch (Exception ex) {
      throw new IOException(ex);
      if (!schemaExists())
        CoreAdminRequest.createCore(mapping.getCoreName(),
            mapping.getCoreName(), adminServer, solrConfig, solrSchema);
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
      server.deleteByQuery("*:*");
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
      server.deleteByQuery("*:*");
    } catch (Exception e) {
      // ignore?
      // LOG.error(e.getMessage());
      // LOG.error(e.getStackTrace().toString());
      CoreAdminRequest.unloadCore(mapping.getCoreName(), adminServer);
    } catch (Exception e) {
      if (e.getMessage().contains("No such core")) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
      CoreAdminResponse rsp = CoreAdminRequest.getStatus(mapping.getCoreName(),
          adminServer);
      exists = rsp.getUptime(mapping.getCoreName()) != null;
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  private static final String toDelimitedString(String[] arr, String sep) {
    if (arr == null || arr.length == 0) {
    for (int i = 0; i < arr.length; i) {
      if (i > 0)
        sb.append(sep);
      sb.append(arr[i]);
  public static String escapeQueryKey(String key) {
    if (key == null) {
    for (int i = 0; i < key.length(); i) {
      char c = key.charAt(i);
      switch (c) {
      case ':':
      case '*':
        sb.append("\\"  c);
        break;
      default:
        sb.append(c);
  public T get(K key, String[] fields) {
    params.set(CommonParams.QT, "/get");
    params.set(CommonParams.FL, toDelimitedString(fields, ","));
    params.set("id", key.toString());
      QueryResponse rsp = server.query(params);
      Object o = rsp.getResponse().get("doc");
      if (o == null) {
      return newInstance((SolrDocument) o, fields);
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public T newInstance(SolrDocument doc, String[] fields) throws IOException {
    if (fields == null) {
      fields = fieldMap.keySet().toArray(new String[fieldMap.size()]);
    for (String f : fields) {
      Field field = fieldMap.get(f);
      if (pk.equals(f)) {
        sf = mapping.getSolrField(f);
      Object sv = doc.get(sf);
      if (sv == null) {

      Object v = deserializeFieldValue(field, fieldSchema, sv, persistent);
      persistent.put(field.pos(), v);
      persistent.setDirty(field.pos());

  @SuppressWarnings("rawtypes")
  private SpecificDatumReader getDatumReader(String schemaId, Schema fieldSchema) {
    SpecificDatumReader<?> reader = (SpecificDatumReader<?>) readerMap
        .get(schemaId);
    if (reader == null) {
      reader = new SpecificDatumReader(fieldSchema);// ignore dirty bits
      SpecificDatumReader localReader = null;
      if ((localReader = readerMap.putIfAbsent(schemaId, reader)) != null) {
        reader = localReader;
      }
    }
    return reader;
  }

  @SuppressWarnings("rawtypes")
  private SpecificDatumWriter getDatumWriter(String schemaId, Schema fieldSchema) {
    SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap
        .get(schemaId);
    if (writer == null) {
      writer = new SpecificDatumWriter(fieldSchema);// ignore dirty bits
      writerMap.put(schemaId, writer);
    }

    return writer;
  }

  @SuppressWarnings("unchecked")
  private Object deserializeFieldValue(Field field, Schema fieldSchema,
      Object solrValue, T persistent) throws IOException {
    Object fieldValue = null;
    switch (fieldSchema.getType()) {
    case MAP:
    case ARRAY:
    case RECORD:
      @SuppressWarnings("rawtypes")
      SpecificDatumReader reader = getDatumReader(fieldSchema.getFullName(),
          fieldSchema);
      fieldValue = IOUtils.deserialize((byte[]) solrValue, reader, fieldSchema,
          persistent.get(field.pos()));
      break;
    case ENUM:
      fieldValue = AvroUtils.getEnumValue(fieldSchema, (String) solrValue);
      break;
    case FIXED:
      throw new IOException("???");
      // break;
    case BYTES:
      fieldValue = ByteBuffer.wrap((byte[]) solrValue);
      break;
    case STRING:
      fieldValue = new Utf8(solrValue.toString());
      break;
    case UNION:
      if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
        // schema [type0, type1]
        Type type0 = fieldSchema.getTypes().get(0).getType();
        Type type1 = fieldSchema.getTypes().get(1).getType();

        // Check if types are different and there's a "null", like
        // ["null","type"] or ["type","null"]
        if (!type0.equals(type1)) {
          if (type0.equals(Schema.Type.NULL))
            fieldSchema = fieldSchema.getTypes().get(1);
          else
            fieldSchema = fieldSchema.getTypes().get(0);
        } else {
          fieldSchema = fieldSchema.getTypes().get(0);
        }
        fieldValue = deserializeFieldValue(field, fieldSchema, solrValue,
            persistent);
      } else {
        @SuppressWarnings("rawtypes")
        SpecificDatumReader unionReader = getDatumReader(
            String.valueOf(fieldSchema.hashCode()), fieldSchema);
        fieldValue = IOUtils.deserialize((byte[]) solrValue, unionReader,
            fieldSchema, persistent.get(field.pos()));
        break;
      }
      break;
    default:
      fieldValue = solrValue;
    }
    return fieldValue;
  }

  public void put(K key, T persistent) {
    if (!persistent.isDirty()) {
    doc.addField(mapping.getPrimaryKey(), key);
    for (Field field : fields) {
      String sf = mapping.getSolrField(field.name());
      if (sf == null) {
      Object v = persistent.get(field.pos());
      if (v == null) {
      v = serializeFieldValue(fieldSchema, v);
      doc.addField(sf, v);

    LOG.info("DOCUMENT: "  doc);
    batch.add(doc);
    if (batch.size() >= batchSize) {
        add(batch, commitWithin);
      } catch (Exception e) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
  @SuppressWarnings("unchecked")
  private Object serializeFieldValue(Schema fieldSchema, Object fieldValue) {
    switch (fieldSchema.getType()) {
    case MAP:
    case ARRAY:
    case RECORD:
      byte[] data = null;
      try {
        @SuppressWarnings("rawtypes")
        SpecificDatumWriter writer = getDatumWriter(fieldSchema.getFullName(),
            fieldSchema);
        data = IOUtils.serialize(writer, fieldSchema, fieldValue);
      } catch (IOException e) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
      }
      fieldValue = data;
      break;
    case BYTES:
      fieldValue = ((ByteBuffer) fieldValue).array();
      break;
    case ENUM:
    case STRING:
      fieldValue = fieldValue.toString();
      break;
    case UNION:
      // If field's schema is null and one type, we do undertake serialization.
      // All other types are serialized.
      if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
        int schemaPos = getUnionSchema(fieldValue, fieldSchema);
        Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
        fieldValue = serializeFieldValue(unionSchema, fieldValue);
      } else {
        byte[] serilazeData = null;
        try {
          @SuppressWarnings("rawtypes")
          SpecificDatumWriter writer = getDatumWriter(
              String.valueOf(fieldSchema.hashCode()), fieldSchema);
          serilazeData = IOUtils.serialize(writer, fieldSchema, fieldValue);
        } catch (IOException e) {
          LOG.error(e.getMessage(), e.getStackTrace().toString());
        }
        fieldValue = serilazeData;
      }
      break;
    default:
      // LOG.error("Unknown field type: "  fieldSchema.getType());
      break;
    }
    return fieldValue;
  }

  private boolean isNullable(Schema unionSchema) {
    for (Schema innerSchema : unionSchema.getTypes()) {
      if (innerSchema.getType().equals(Schema.Type.NULL)) {
        return true;
      }
    }
    return false;
  }

  /**
   * Given an object and the object schema this function obtains, from within
   * the UNION schema, the position of the type used. If no data type can be
   * inferred then we return a default value of position 0.
   * 
   * @param pValue
   * @param pUnionSchema
   * @return the unionSchemaPosition.
   */
  private int getUnionSchema(Object pValue, Schema pUnionSchema) {
    int unionSchemaPos = 0;
    Iterator<Schema> it = pUnionSchema.getTypes().iterator();
    while (it.hasNext()) {
      Type schemaType = it.next().getType();
      if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
        return unionSchemaPos;
      else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
        return unionSchemaPos;
      else if (pValue instanceof Integer && schemaType.equals(Type.INT))
        return unionSchemaPos;
      else if (pValue instanceof Long && schemaType.equals(Type.LONG))
        return unionSchemaPos;
      else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
        return unionSchemaPos;
      else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
        return unionSchemaPos;
      else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
        return unionSchemaPos;
      else if (pValue instanceof Map && schemaType.equals(Type.MAP))
        return unionSchemaPos;
      else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
        return unionSchemaPos;
      else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
        return unionSchemaPos;
      unionSchemaPos;
    }
    // if we weren't able to determine which data type it is, then we return the
    // default
    return DEFAULT_UNION_SCHEMA;
  }

  public boolean delete(K key) {
      UpdateResponse rsp = server.deleteByQuery(keyField  ":"
           escapeQueryKey(key.toString()));
      LOG.info(rsp.toString());
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public long deleteByQuery(Query<K, T> query) {
    String q = ((SolrQuery<K, T>) query).toSolrQuery();
      UpdateResponse rsp = server.deleteByQuery(q);
      LOG.info(rsp.toString());
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public Result<K, T> execute(Query<K, T> query) {
      return new SolrResult<K, T>(this, query, server, resultsSize);
    } catch (IOException e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
    return new SolrQuery<K, T>(this);
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
      if (batch.size() > 0) {
        add(batch, commitWithin);
    } catch (Exception e) {
    // flush();

  private void add(ArrayList<SolrInputDocument> batch, int commitWithin)
      throws SolrServerException, IOException {
      server.add(batch);
      server.commit(false, true, true);
      server.add(batch, commitWithin);
  }
      CharSequence url = pageview.getUrl();
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.tutorial.log.generated;  
public class MetricDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}]}");
  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence metricDimension;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return metricDimension;
    case 2: return timestamp;
    case 3: return metric;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: metricDimension = (java.lang.CharSequence)(value); break;
    case 2: timestamp = (java.lang.Long)(value); break;
    case 3: metric = (java.lang.Long)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  /**
   * Gets the value of the 'metricDimension' field.
   */
  public java.lang.CharSequence getMetricDimension() {
    return metricDimension;

  /**
   * Sets the value of the 'metricDimension' field.
   * @param value the value to set.
   */
  public void setMetricDimension(java.lang.CharSequence value) {
    this.metricDimension = value;
    setDirty(1);
  
  /**
   * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
    return isDirty(1);

  /**
   * Gets the value of the 'timestamp' field.
   */
  public java.lang.Long getTimestamp() {
    return timestamp;

  /**
   * Sets the value of the 'timestamp' field.
   * @param value the value to set.
   */
  public void setTimestamp(java.lang.Long value) {
    this.timestamp = value;
    setDirty(2);
  
  /**
   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isTimestampDirty(java.lang.Long value) {
    return isDirty(2);

  /**
   * Gets the value of the 'metric' field.
   */
  public java.lang.Long getMetric() {
    return metric;
  }

  /**
   * Sets the value of the 'metric' field.
   * @param value the value to set.
   */
  public void setMetric(java.lang.Long value) {
    this.metric = value;
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetricDirty(java.lang.Long value) {
    return isDirty(3);
  }

  /** Creates a new MetricDatum RecordBuilder */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder() {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder();
  }
  
  /** Creates a new MetricDatum RecordBuilder by copying an existing Builder */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
  }
  
  /** Creates a new MetricDatum RecordBuilder by copying an existing MetricDatum instance */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for MetricDatum instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MetricDatum>
    implements org.apache.avro.data.RecordBuilder<MetricDatum> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence metricDimension;
    private long timestamp;
    private long metric;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing MetricDatum instance */
    private Builder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
            super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.metricDimension)) {
        this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.metricDimension);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.metric)) {
        this.metric = (java.lang.Long) data().deepCopy(fields()[3].schema(), other.metric);
        fieldSetFlags()[3] = true;
      }
    }

    /** Gets the value of the 'metricDimension' field */
    public java.lang.CharSequence getMetricDimension() {
      return metricDimension;
    }
    
    /** Sets the value of the 'metricDimension' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetricDimension(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.metricDimension = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'metricDimension' field has been set */
    public boolean hasMetricDimension() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'metricDimension' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetricDimension() {
      metricDimension = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'timestamp' field */
    public java.lang.Long getTimestamp() {
      return timestamp;
    }
    
    /** Sets the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setTimestamp(long value) {
      validate(fields()[2], value);
      this.timestamp = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'timestamp' field has been set */
    public boolean hasTimestamp() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearTimestamp() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'metric' field */
    public java.lang.Long getMetric() {
      return metric;
    }
    
    /** Sets the value of the 'metric' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetric(long value) {
      validate(fields()[3], value);
      this.metric = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'metric' field has been set */
    public boolean hasMetric() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'metric' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetric() {
      fieldSetFlags()[3] = false;
      return this;
    }
    
    @Override
    public MetricDatum build() {
      try {
        MetricDatum record = new MetricDatum();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.metricDimension = fieldSetFlags()[1] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
        record.metric = fieldSetFlags()[3] ? this.metric : (java.lang.Long) defaultValue(fields()[3]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }
  
  public MetricDatum.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public MetricDatum newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'metricDimension' field.
		   */
	  public java.lang.CharSequence getMetricDimension() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metricDimension' field.
		   * @param value the value to set.
	   */
	  public void setMetricDimension(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'timestamp' field.
		   */
	  public java.lang.Long getTimestamp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'timestamp' field.
		   * @param value the value to set.
	   */
	  public void setTimestamp(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isTimestampDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'metric' field.
		   */
	  public java.lang.Long getMetric() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metric' field.
		   * @param value the value to set.
	   */
	  public void setMetric(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetricDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
  }
  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.tutorial.log.generated;  
public class Pageview extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AAA=\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}]}");
  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[2]);
  private java.lang.CharSequence url;
  private java.lang.CharSequence ip;
  private java.lang.CharSequence httpMethod;
  private java.lang.CharSequence referrer;
  private java.lang.CharSequence userAgent;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return url;
    case 2: return timestamp;
    case 3: return ip;
    case 4: return httpMethod;
    case 5: return httpStatusCode;
    case 6: return responseSize;
    case 7: return referrer;
    case 8: return userAgent;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: url = (java.lang.CharSequence)(value); break;
    case 2: timestamp = (java.lang.Long)(value); break;
    case 3: ip = (java.lang.CharSequence)(value); break;
    case 4: httpMethod = (java.lang.CharSequence)(value); break;
    case 5: httpStatusCode = (java.lang.Integer)(value); break;
    case 6: responseSize = (java.lang.Integer)(value); break;
    case 7: referrer = (java.lang.CharSequence)(value); break;
    case 8: userAgent = (java.lang.CharSequence)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  /**
   * Gets the value of the 'url' field.
   */
  public java.lang.CharSequence getUrl() {
    return url;

  /**
   * Sets the value of the 'url' field.
   * @param value the value to set.
   */
  public void setUrl(java.lang.CharSequence value) {
    this.url = value;
    setDirty(1);
  
  /**
   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUrlDirty(java.lang.CharSequence value) {
    return isDirty(1);

  /**
   * Gets the value of the 'timestamp' field.
   */
  public java.lang.Long getTimestamp() {
    return timestamp;

  /**
   * Sets the value of the 'timestamp' field.
   * @param value the value to set.
   */
  public void setTimestamp(java.lang.Long value) {
    this.timestamp = value;
    setDirty(2);
  
  /**
   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isTimestampDirty(java.lang.Long value) {
    return isDirty(2);

  /**
   * Gets the value of the 'ip' field.
   */
  public java.lang.CharSequence getIp() {
    return ip;

  /**
   * Sets the value of the 'ip' field.
   * @param value the value to set.
   */
  public void setIp(java.lang.CharSequence value) {
    this.ip = value;
    setDirty(3);
  
  /**
   * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isIpDirty(java.lang.CharSequence value) {
    return isDirty(3);

  /**
   * Gets the value of the 'httpMethod' field.
   */
  public java.lang.CharSequence getHttpMethod() {
    return httpMethod;

  /**
   * Sets the value of the 'httpMethod' field.
   * @param value the value to set.
   */
  public void setHttpMethod(java.lang.CharSequence value) {
    this.httpMethod = value;
    setDirty(4);
  
  /**
   * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHttpMethodDirty(java.lang.CharSequence value) {
    return isDirty(4);

  /**
   * Gets the value of the 'httpStatusCode' field.
   */
  public java.lang.Integer getHttpStatusCode() {
    return httpStatusCode;

  /**
   * Sets the value of the 'httpStatusCode' field.
   * @param value the value to set.
   */
  public void setHttpStatusCode(java.lang.Integer value) {
    this.httpStatusCode = value;
    setDirty(5);
  
  /**
   * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
    return isDirty(5);

  /**
   * Gets the value of the 'responseSize' field.
   */
  public java.lang.Integer getResponseSize() {
    return responseSize;

  /**
   * Sets the value of the 'responseSize' field.
   * @param value the value to set.
   */
  public void setResponseSize(java.lang.Integer value) {
    this.responseSize = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isResponseSizeDirty(java.lang.Integer value) {
    return isDirty(6);
  }

  /**
   * Gets the value of the 'referrer' field.
   */
  public java.lang.CharSequence getReferrer() {
    return referrer;
  }

  /**
   * Sets the value of the 'referrer' field.
   * @param value the value to set.
   */
  public void setReferrer(java.lang.CharSequence value) {
    this.referrer = value;
    setDirty(7);
  }
  
  /**
   * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isReferrerDirty(java.lang.CharSequence value) {
    return isDirty(7);
  }

  /**
   * Gets the value of the 'userAgent' field.
   */
  public java.lang.CharSequence getUserAgent() {
    return userAgent;
  }

  /**
   * Sets the value of the 'userAgent' field.
   * @param value the value to set.
   */
  public void setUserAgent(java.lang.CharSequence value) {
    this.userAgent = value;
    setDirty(8);
  }
  
  /**
   * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUserAgentDirty(java.lang.CharSequence value) {
    return isDirty(8);
  }

  /** Creates a new Pageview RecordBuilder */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder() {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder();
  }
  
  /** Creates a new Pageview RecordBuilder by copying an existing Builder */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
  }
  
  /** Creates a new Pageview RecordBuilder by copying an existing Pageview instance */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview other) {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Pageview instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Pageview>
    implements org.apache.avro.data.RecordBuilder<Pageview> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence url;
    private long timestamp;
    private java.lang.CharSequence ip;
    private java.lang.CharSequence httpMethod;
    private int httpStatusCode;
    private int responseSize;
    private java.lang.CharSequence referrer;
    private java.lang.CharSequence userAgent;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Pageview instance */
    private Builder(org.apache.gora.tutorial.log.generated.Pageview other) {
            super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.ip)) {
        this.ip = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ip);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.httpMethod)) {
        this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[4].schema(), other.httpMethod);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.httpStatusCode)) {
        this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.httpStatusCode);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.responseSize)) {
        this.responseSize = (java.lang.Integer) data().deepCopy(fields()[6].schema(), other.responseSize);
        fieldSetFlags()[6] = true;
      }
      if (isValidValue(fields()[7], other.referrer)) {
        this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.referrer);
        fieldSetFlags()[7] = true;
      }
      if (isValidValue(fields()[8], other.userAgent)) {
        this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[8].schema(), other.userAgent);
        fieldSetFlags()[8] = true;
      }
    }

    /** Gets the value of the 'url' field */
    public java.lang.CharSequence getUrl() {
      return url;
    }
    
    /** Sets the value of the 'url' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setUrl(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.url = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'url' field has been set */
    public boolean hasUrl() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'url' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUrl() {
      url = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'timestamp' field */
    public java.lang.Long getTimestamp() {
      return timestamp;
    }
    
    /** Sets the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setTimestamp(long value) {
      validate(fields()[2], value);
      this.timestamp = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'timestamp' field has been set */
    public boolean hasTimestamp() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearTimestamp() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'ip' field */
    public java.lang.CharSequence getIp() {
      return ip;
    }
    
    /** Sets the value of the 'ip' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setIp(java.lang.CharSequence value) {
      validate(fields()[3], value);
      this.ip = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'ip' field has been set */
    public boolean hasIp() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'ip' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearIp() {
      ip = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'httpMethod' field */
    public java.lang.CharSequence getHttpMethod() {
      return httpMethod;
    }
    
    /** Sets the value of the 'httpMethod' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpMethod(java.lang.CharSequence value) {
      validate(fields()[4], value);
      this.httpMethod = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'httpMethod' field has been set */
    public boolean hasHttpMethod() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'httpMethod' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpMethod() {
      httpMethod = null;
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'httpStatusCode' field */
    public java.lang.Integer getHttpStatusCode() {
      return httpStatusCode;
    }
    
    /** Sets the value of the 'httpStatusCode' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpStatusCode(int value) {
      validate(fields()[5], value);
      this.httpStatusCode = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'httpStatusCode' field has been set */
    public boolean hasHttpStatusCode() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'httpStatusCode' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpStatusCode() {
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'responseSize' field */
    public java.lang.Integer getResponseSize() {
      return responseSize;
    }
    
    /** Sets the value of the 'responseSize' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setResponseSize(int value) {
      validate(fields()[6], value);
      this.responseSize = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'responseSize' field has been set */
    public boolean hasResponseSize() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'responseSize' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearResponseSize() {
      fieldSetFlags()[6] = false;
      return this;
    }
    
    /** Gets the value of the 'referrer' field */
    public java.lang.CharSequence getReferrer() {
      return referrer;
    }
    
    /** Sets the value of the 'referrer' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setReferrer(java.lang.CharSequence value) {
      validate(fields()[7], value);
      this.referrer = value;
      fieldSetFlags()[7] = true;
      return this; 
    }
    
    /** Checks whether the 'referrer' field has been set */
    public boolean hasReferrer() {
      return fieldSetFlags()[7];
    }
    
    /** Clears the value of the 'referrer' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearReferrer() {
      referrer = null;
      fieldSetFlags()[7] = false;
      return this;
    }
    
    /** Gets the value of the 'userAgent' field */
    public java.lang.CharSequence getUserAgent() {
      return userAgent;
    }
    
    /** Sets the value of the 'userAgent' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setUserAgent(java.lang.CharSequence value) {
      validate(fields()[8], value);
      this.userAgent = value;
      fieldSetFlags()[8] = true;
      return this; 
    }
    
    /** Checks whether the 'userAgent' field has been set */
    public boolean hasUserAgent() {
      return fieldSetFlags()[8];
    }
    
    /** Clears the value of the 'userAgent' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUserAgent() {
      userAgent = null;
      fieldSetFlags()[8] = false;
      return this;
    }
    
    @Override
    public Pageview build() {
      try {
        Pageview record = new Pageview();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[2]);
        record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
        record.ip = fieldSetFlags()[3] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[3]);
        record.httpMethod = fieldSetFlags()[4] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[4]);
        record.httpStatusCode = fieldSetFlags()[5] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[5]);
        record.responseSize = fieldSetFlags()[6] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[6]);
        record.referrer = fieldSetFlags()[7] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[7]);
        record.userAgent = fieldSetFlags()[8] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[8]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }
  
  public Pageview.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Pageview newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Pageview implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'url' field.
		   */
	  public java.lang.CharSequence getUrl() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'url' field.
		   * @param value the value to set.
	   */
	  public void setUrl(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUrlDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'timestamp' field.
		   */
	  public java.lang.Long getTimestamp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'timestamp' field.
		   * @param value the value to set.
	   */
	  public void setTimestamp(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isTimestampDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'ip' field.
		   */
	  public java.lang.CharSequence getIp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'ip' field.
		   * @param value the value to set.
	   */
	  public void setIp(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isIpDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpMethod' field.
		   */
	  public java.lang.CharSequence getHttpMethod() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpMethod' field.
		   * @param value the value to set.
	   */
	  public void setHttpMethod(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpMethodDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpStatusCode' field.
		   */
	  public java.lang.Integer getHttpStatusCode() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpStatusCode' field.
		   * @param value the value to set.
	   */
	  public void setHttpStatusCode(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'responseSize' field.
		   */
	  public java.lang.Integer getResponseSize() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'responseSize' field.
		   * @param value the value to set.
	   */
	  public void setResponseSize(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isResponseSizeDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'referrer' field.
		   */
	  public java.lang.CharSequence getReferrer() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'referrer' field.
		   * @param value the value to set.
	   */
	  public void setReferrer(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isReferrerDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'userAgent' field.
		   */
	  public java.lang.CharSequence getUserAgent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'userAgent' field.
		   * @param value the value to set.
	   */
	  public void setUserAgent(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUserAgentDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
  }
  
}
 b/gora-compiler/src/main/java/org/apache/gora/compiler/utils/LicenseHeaders.java
package org.apache.gora.compiler.utils;
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
    Schema.Parser parser = new Schema.Parser();
    return parser.parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
 b/gora-compiler/src/main/java/org/apache/gora/compiler/utils/LicenseHeaders.java
package org.apache.gora.compiler.utils;
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
    Schema.Parser parser = new Schema.Parser();
    return parser.parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
import org.apache.gora.store.DataStoreFactory;
  private static final String SCANNER_CACHING_PROPERTIES_KEY = "scanner.caching" ;
  private static final int SCANNER_CACHING_PROPERTIES_DEFAULT = 0 ;
  
  private int scannerCaching = SCANNER_CACHING_PROPERTIES_DEFAULT ;
  
    // Set scanner caching option
    try {
      this.setScannerCaching(
          Integer.valueOf(DataStoreFactory.findProperty(this.properties, this,
              SCANNER_CACHING_PROPERTIES_KEY,
              String.valueOf(SCANNER_CACHING_PROPERTIES_DEFAULT)))) ;
    }catch(Exception e){
      LOG.error("Can not load "  SCANNER_CACHING_PROPERTIES_KEY  " from gora.properties. Setting to default value: "  SCANNER_CACHING_PROPERTIES_DEFAULT, e) ;
      this.setScannerCaching(SCANNER_CACHING_PROPERTIES_DEFAULT) ; // Default value if something is wrong
    }
    
    
    scan.setCaching(this.getScannerCaching()) ; 
    
  /**
   * Gets the Scanner Caching optimization value
   * @return The value used internally in {@link Scan#setCaching(int)}
   */
  public int getScannerCaching() {
    return this.scannerCaching ;
  }
  
  /**
   * Sets the value for Scanner Caching optimization
   * 
   * @see Scan#setCaching(int)
   * 
   * @param numRows the number of rows for caching >= 0
   * @return &lt;&lt;Fluent interface&gt;&gt;
   */
  public HBaseStore<K, T> setScannerCaching(int numRows) {
    if (numRows < 0) {
      LOG.warn("Invalid Scanner Caching optimization value. Cannot set to: "  numRows  ".") ;
      return this ;
    }
    this.scannerCaching = numRows ;
    return this ;
  }
  
    try{
      store.close();
    }catch(Exception e){
      LOG.warn("Exception at GoraRecordWriter.class while closing datastore."  e.getMessage());
    }
    try{
      store.put(key, (Persistent) value);
      counter.increment();
      if (counter.isModulo()) {
        LOG.info("Flushing the datastore after "  counter.getRecordsNumber()  " records");
        store.flush();
      }
    }catch(Exception e){
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore."  e.getMessage());
    }





      LOG.warn("Schema: "  schema.getName()  " is not supported. No serializer "
           "could be found. Please report this to dev@gora.apache.org");
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name());
   * Serialize value to ByteBuffer using 
   * {@link org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer#getSerializer(Object)}.
   * @param value the member value {@link java.lang.Object}.
    Serializer<Object> serializer = GoraSerializerTypeInferer.getSerializer(value);
      LOG.warn("Serializer not found for: "  value.toString());
      LOG.debug(serializer.getClass()  " selected as appropriate Serializer.");
      LOG.warn("Serialization value for: "  value.getClass().getName()  " = null");
























        case RECORD:
          PersistentBase persistent = (PersistentBase) fieldValue;
          PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
          for (Field member: fieldSchema.getFields()) {
            newRecord.put(member.pos(), persistent.get(member.pos()));
          }
          fieldValue = newRecord;
          break;
        case MAP:
          StatefulHashMap map = (StatefulHashMap) fieldValue;
          StatefulHashMap newMap = new StatefulHashMap();
          for (Object mapKey : map.keySet()) {
            newMap.put(mapKey, map.get(mapKey));
            newMap.putState(mapKey, map.getState(mapKey));
          }
          fieldValue = newMap;
          break;
        case ARRAY:
          GenericArray array = (GenericArray) fieldValue;
          ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
          Iterator iter = array.iterator();
          while (iter.hasNext()) {
            newArray.add(iter.next());
          }
          fieldValue = newArray;
          break;
        case UNION:
          // storing the union selected schema, the actual value will be stored as soon as getting out of here
          // TODO determine which schema we are using: int schemaPos = getUnionSchema(fieldValue,fieldSchema);
          // and save it p.put( p.getFieldIndex(field.name()  CassandraStore.UNION_COL_SUFIX), schemaPos);
          break;


  }
    switch (type) {
    case STRING:
    case BOOLEAN:
    case INT:
    case LONG:
    case BYTES:
    case FLOAT:
    case DOUBLE:
    case FIXED:
      this.cassandraClient.addColumn(key, field.name(), value);
      break;
    case RECORD:
      if (value != null) {
        if (value instanceof PersistentBase) {
          PersistentBase persistentBase = (PersistentBase) value;
          for (Field member: schema.getFields()) {

            // TODO: hack, do not store empty arrays
            Object memberValue = persistentBase.get(member.pos());
            if (memberValue instanceof GenericArray<?>) {
              if (((GenericArray)memberValue).size() == 0) {
                continue;
            }
            this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
        } else {
          LOG.warn("Record with value: "  value.toString()  " not supported for field: "  field.name()); 
      }
      break;
    case MAP:
      if (value != null) {
        if (value instanceof StatefulHashMap<?, ?>) {
          this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
        } else {
          LOG.warn("Map with value: "  value.toString()  " not supported for field: "  field.name());
      }
      break;
    case ARRAY:
      if (value != null) {
        if (value instanceof GenericArray<?>) {
          this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
        } else {
          LOG.warn("Array with value: "  value.toString()  " not supported for field: "  field.name());
      }
      break;
    case UNION:
      if(value != null) {
        LOG.debug("Union with value: "  value.toString()  " at index: "  getUnionSchema(value, schema)  " supported for field: "  field.name());
        this.cassandraClient.addColumn(key, field.name(), value);
      } else {
        LOG.warn("Union with value: "  value.toString()  " at index: "  getUnionSchema(value, schema)  " not supported for field: "  field.name());
      }
    default:
      LOG.warn("Type: "  type.name()  " with value: "  value.toString()  
          " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
  @SuppressWarnings("unchecked")
  @SuppressWarnings("unchecked")
  @SuppressWarnings("unchecked")
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore. "  e.getMessage());
        LOG.warn("Union with 'null' value not supported for field: "  field.name());
      LOG.warn("Type: "  type.name()  " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
      , String defaultValue) throws IOException {

    String mappingFilename = findProperty(properties, store, MAPPING_FILE, defaultValue);

    InputStream mappingFile = store.getClass().getClassLoader().getResourceAsStream(mappingFilename);

    if (mappingFile == null)
      throw new IOException("Unable to open mapping file: "mappingFilename);

    return mappingFilename;

      String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
      keyStates.remove(key);
import org.apache.gora.filter.Filter;
  
  /**
   * @param Set a filter on this query.
   */
  public void setFilter(Filter<K, T> filter);
  
  /**
   * @return The filter on this query, or <code>null</code> if none.
   */
  public Filter<K, T> getFilter();
  
  /**
   * Set whether the local filter is enabled. This is usually called by
   * data store implementations that install the filter remotely
   * (for efficiency reasons) and therefore disable the local filter.
   * @param enable
   */
  void setLocalFilterEnabled(boolean enable);
  
  /**
   * @return Whether the local filter is enabled.
   * See {@link #setLocalFilterEnabled(boolean)}.
   */
  boolean isLocalFilterEnabled();
import org.apache.gora.filter.Filter;
  
  @Override
  public Filter<K, T> getFilter() {
    return baseQuery.getFilter();
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    baseQuery.setFilter(filter);
  }
import org.apache.gora.filter.Filter;
  protected Filter<K, T> filter;
  protected boolean localFilterEnabled=true;
  
  @Override
  public Filter<K, T> getFilter() {
    return filter;
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    this.filter=filter;
  }
  
  @Override
  public boolean isLocalFilterEnabled() {
    return localFilterEnabled;
  }
  
  @Override
  public void setLocalFilterEnabled(boolean enable) {
    this.localFilterEnabled=enable;
  }
  
    if(!nullFields[4]) {
      String filterClass = Text.readString(in);
      try {
        filter = (Filter<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(filterClass), conf);
        filter.readFields(in);
      } catch (ClassNotFoundException e) {
        throw new IOException(e);
      }
    }
    localFilterEnabled = in.readBoolean(); 
    if(filter != null) {
      Text.writeString(out, filter.getClass().getCanonicalName());
      filter.write(out);
    }
    out.writeBoolean(localFilterEnabled);
      builder.append(localFilterEnabled, that.localFilterEnabled);
    builder.append(localFilterEnabled);
    builder.append("localFilterEnabled", localFilterEnabled);
import org.apache.gora.filter.Filter;
import java.io.IOException;

    if(isLimitReached()) {
      return false;
    }
      
    boolean ret;
    do {
      clear();
      persistent = getOrCreatePersistent(persistent);
      ret = nextInner();
      if (ret == false) {
        //this is the end
        break;
      }
      //we keep looping until we get a row that is not filtered out
    } while (filter(key, persistent));
    if(ret) offset;
    return ret;
  }
  
  protected boolean filter(K key, T persistent) {
    if (!query.isLocalFilterEnabled()) {
      return false;
    }
    
    Filter<K, T> filter = query.getFilter();
    if (filter == null) {
      return false;
    }
    
    return filter.filter(key, persistent);
import org.apache.gora.filter.Filter;
import java.util.Arrays;

 * Webservices implementation for {@link PartitionQuery}.
  
  @Override
  public Filter<K, T> getFilter() {
    return filter;
  }
  
  @Override
  public void setFilter(Filter<K, T> filter) {
    this.filter=filter;
  }
  
  @Override
  public boolean isLocalFilterEnabled() {
    return localFilterEnabled;
  }
  
  @Override
  public void setLocalFilterEnabled(boolean enable) {
    this.localFilterEnabled=enable;
  }
  
import org.apache.gora.filter.Filter;
  protected Filter<K, T> filter;
  protected boolean localFilterEnabled=true;
import org.apache.gora.filter.Filter;

  @Override
  public void setFilter(Filter<K, T> filter) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public Filter<K, T> getFilter() {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public void setLocalFilterEnabled(boolean enable) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public boolean isLocalFilterEnabled() {
    // TODO Auto-generated method stub
    return false;
  }
public class HBaseColumn {
import org.apache.gora.hbase.util.HBaseFilterUtil;
  
  private HBaseFilterUtil<K, T> filterUtil;
      filterUtil = new HBaseFilterUtil<K, T>(this.conf);
  
  public HBaseMapping getMapping() {
    return mapping;
  }
                  default :
                    break;
      String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getServerAddress().getHostname();
            = new HBaseScannerResult<K,T>(this, query, scanner);
    if (query.getFilter() != null) {
      boolean succeeded = filterUtil.setFilter(scan, query.getFilter(), this);
      if (succeeded) {
        // don't need local filter
        query.setLocalFilterEnabled(false);
      }
    }
    } else if (clazz.isArray() && clazz.getComponentType().equals(Byte.TYPE)) {
      return (byte[])o;
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name()  ". Please report this to dev@gora.apache.org");
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;

import java.util.Properties;
/**
 * CassandraClient is where all of the primary datastore functionality is 
 * executed. Typically CassandraClient is invoked by calling 
 * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}.
 * CassandraClient deals with Cassandra data model definition, mutation, 
 * and general/specific mappings.
 * @see {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)} 
 *
 * @param <K>
 * @param <T>
 */

  /** The logging implementation */


  /** Object containing the XML mapping for Cassandra. */
  /** Hector client default column family consistency level. */
  public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";

  /** Cassandra serializer to be used for serializing Gora's keys. */

  /**
   * Given our key, persistentClass from 
   * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}
   * we make best efforts to dictate our data model. 
   * We make a quick check within {@link org.apache.gora.cassandra.store.CassandraClient#checkKeyspace(String)
   * to see if our keyspace has already been invented, this simple check prevents us from 
   * recreating the keyspace if it already exists. 
   * We then simple specify (based on the input keyclass) an appropriate serializer
   * via {@link org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer} before
   * defining a mutator from and by which we can mutate this object.
   * @param keyClass the Key by which we wish o assign a record object
   * @param persistentClass the generated {@link org.apache.org.gora.persistency.Peristent} bean representing the data.
   * @throws Exception
   */
    this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), 
        new CassandraHostConfigurator(this.cassandraMapping.getHostName()));


    // Just create a Keyspace object on the client side, corresponding to an 
    // already existing keyspace with already created column families.


   * In this method, we also utilize Hector's 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel}
   * logic. 
   * It is set by passing a 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel} object right 
   * when the {@link me.prettyprint.hector.api.Keyspace} is created. 
   * If we cannot find a consistency level within <code>gora.properites</code>, 
   * then column family consistency level is set to QUORUM (by default) which permits 
   * consistency to wait for a quorum of replicas to respond regardless of data center.
   * QUORUM is Hector Client's default setting and we respect that here as well.
   * 
   * @see http://hector-client.github.io/hector/build/html/content/consistency_level.html
      List<ColumnFamilyDefinition> columnFamilyDefinitions = this.cassandraMapping.getColumnFamilyDefinitions();
      keyspaceDefinition = HFactory.createKeyspaceDefinition(this.cassandraMapping.getKeyspaceName(), 
          "org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
      // GORA-167 Create a customized Consistency Level
      ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
      // Column family consistency levels
      ccl.setReadCfConsistencyLevels(clmap);
      ccl.setWriteCfConsistencyLevels(clmap);
      // Operations consistency levels
      String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector read consistency configured to '"  opConsisLvl  "'.");
      opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector write consistency configured to '"  opConsisLvl  "'.");
      HFactory.createKeyspace("Keyspace", this.cluster, ccl);
                 ", not BytesType. It may cause a fatal error on column validation later.");

  /**
   * Method in charge of setting the consistency level for defined column families.
   * @param pColFams  Column families
   * @return Map<String, HConsistencyLevel> with the mapping between colFams and consistency level.
   */
  private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
    Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
    // Get columnFamily consistency level.
    String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
    LOG.debug("ColumnFamily consistency level configured to '"  colFamConsisLvl  "'.");
    // Define consistency for ColumnFamily "ColumnFamily"
    for (ColumnFamilyDefinition colFamDef : pColFams)
      clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
    return clMap;
  }





  public void addGenericArray(K key, String fieldName, GenericArray<?> array) {


    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = 
        HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());




    family = this.cassandraMapping.getFamily(pField);

    column = this.cassandraMapping.getColumn(pField);
   * @return a map which keys are the family names and values the corresponding column 
   * names required to get all the query fields.









    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = 
        HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), 
            ByteBufferSerializer.get(), ByteBufferSerializer.get());


    return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
  /** Consistency property level for Cassandra column families */
  private static final String COL_FAM_CL = "cf.consistency.level";

  /** Consistency property level for Cassandra read operations. */
  private static final String READ_OP_CL = "read.consistency.level";

  /** Consistency property level for Cassandra write operations. */
  private static final String WRITE_OP_CL = "write.consistency.level";

  /** Variables to hold different consistency levels defined by the properties. */
  public static String colFamConsLvl;
  public static String readOpConsLvl;
  public static String writeOpConsLvl;

  private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
  
      if (autoCreateSchema) {
        // If this is not set, then each Cassandra client should set its default
        // column family
        colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
        // operations
        readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
        writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
      }
      CassandraResultSet<K> cassandraResultSet) {
  /**
   * Checks to see if a Cassandra Keyspace actually exists.
   * Returns true if it does.
   */
          "sure to include this property in gora.properties file");
import org.apache.gora.store.DataStoreFactory;
      preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
      dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
      dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
      consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
    if (ttlAttr == null) {
      ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
    }
      HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer, ttlAttr);
    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
    if (ttlAttr  == null) {
      ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
    }
      HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer, ttlAttr);
  /**
   * Deletes a subColumn which is a field inside a column.
   * @param key Identifying the row.
   * @param fieldName The field's name.
   * @param columnName The column's name.
   */
  /**
   * Delete a row within the keyspace.
    * @param key
    * @param fieldName
    * @param columnName
    */
  public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
    synchronized(mutator) {
      HectorUtils.deleteColumn(mutator, key, familyName, columnName);
      }
    }

  /**
   * Delete all content related to a key.
   * @param key
   */
  public void deleteByKey(K key) {
    Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
    deleteColumn(key, familyMap.values().iterator().next().toString(), null);
  }
  private static final String GCGRACE_SECONDS_ATTRIBUTE = "gc_grace_seconds";
  private static final String COLUMNS_TTL_ATTRIBUTE = "ttl";
  public static final String DEFAULT_COLUMNS_TTL = "60";
  public static final int DEFAULT_GCGRACE_SECONDS = 30;
   * Helps storing attributes defined for each field.
   */
  private Map<String, String> columnAttrMap = new HashMap<String, String>();
  
  /**
      LOG.error("Error locating Cassandra Keyspace name attribute!");
      String gcgrace_scs = element.getAttributeValue(GCGRACE_SECONDS_ATTRIBUTE);
      if (gcgrace_scs == null) {
        LOG.warn("Error locating gc_grace_seconds attribute for '"  familyName  "' column family");
        LOG.warn("Using default set to: "  DEFAULT_GCGRACE_SECONDS);
      } else {
        if (LOG.isDebugEnabled()) {
          LOG.debug("Located gc_grace_seconds: '"  gcgrace_scs  "'" );
        }
      }
      cfDef.setGcGraceSeconds(gcgrace_scs!=null?Integer.parseInt(gcgrace_scs):DEFAULT_GCGRACE_SECONDS);
      String ttlValue = element.getAttributeValue(COLUMNS_TTL_ATTRIBUTE);

      if (ttlValue == null) {
        LOG.warn("TTL value is not defined for \""  fieldName  "\" field. \n Using default value: "  DEFAULT_COLUMNS_TTL);
      }
      // TODO we should find a way of storing more values into this map i.e. more column attributes
      this.columnAttrMap.put(columnName, ttlValue!=null?ttlValue:DEFAULT_COLUMNS_TTL);
    }
  /**
   * Gets the columnFamily related to the column name.
   * @param name
   * @return
   */
  /**
   * Gets the column related to a field.
   * @param name
   * @return
   */
   * Gets all the columnFamilies defined.
   * @return
   */
  public Map<String,String> getFamilyMap(){
    return this.familyMap;
  }

  /**
   * Gets all attributes related to a column.
   * @return
   */
  public Map<String, String> getColumnsAttribs(){
    return this.columnAttrMap;
  }

  /**
    this.cassandraClient.deleteByKey(key);
    return true;
import me.prettyprint.hector.api.mutation.MutationResult;
  /** Methods to insert columns. */
  public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
  public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, String columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
  /** Methods to create columns. */
  public static<K> HColumn<ByteBuffer,ByteBuffer> createColumn(ByteBuffer name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  public static<K> HColumn<String,ByteBuffer> createColumn(String name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  public static<K> HColumn<Integer,ByteBuffer> createColumn(Integer name, ByteBuffer value, String ttlAttr) {
    return HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
  /** Methods to create subColumns. */
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
    mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
  /** Methods to delete subColumns. */
  /** Methods do delete columns. */
  public static<K> void deleteColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName){
    MutationResult mr = mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
    System.out.println(mr.toString());
  }
  /** Methods to create superColumns. */
  public static<K> HSuperColumn<String,ByteBuffer,ByteBuffer> createSuperColumn(String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
  public static<K> HSuperColumn<String,String,ByteBuffer> createSuperColumn(String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), StringSerializer.get(), ByteBufferSerializer.get());
  public static<K> HSuperColumn<String,Integer,ByteBuffer> createSuperColumn(String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
    return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), IntegerSerializer.get(), ByteBufferSerializer.get());
          LOG.debug("Keyclass and nameclass match.");
          //TODO this might not be the desired behavior as the user might have actually made a mistake.
            LOG.warn("Mismatching schema's names. Mappingfile schema: '"  tableNameFromMapping 
                 "'. PersistentClass schema's name: '"  tableName  "'"
                 "Assuming they are the same.");
}
        } else {
          LOG.error("KeyClass in gora-hbase-mapping is not the same as the one in the databean.");
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
      @SuppressWarnings("resource")
    key = (K) ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
import java.util.concurrent.TimeUnit;
import org.apache.accumulo.core.client.BatchWriterConfig;
import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;
import org.apache.accumulo.core.client.security.tokens.PasswordToken;
import org.apache.accumulo.core.security.thrift.TCredentials;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericData;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.EncoderFactory;
import org.apache.gora.accumulo.encoders.BinaryEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;

  private TCredentials credentials;


  public Object fromBytes(Schema schema, byte data[]) throws GoraException {
    Schema fromSchema = null;
    if (schema.getType() == Type.UNION) {
      try {
        Decoder decoder = DecoderFactory.get().binaryDecoder(data, null);
        int unionIndex = decoder.readIndex();
        List<Schema> possibleTypes = schema.getTypes();
        fromSchema = possibleTypes.get(unionIndex);
        Schema effectiveSchema = possibleTypes.get(unionIndex);
        if (effectiveSchema.getType() == Type.NULL) {
          decoder.readNull();
          return null;
        } else {
          data = decoder.readBytes(null).array();
        }
      } catch (IOException e) {
        e.printStackTrace();
        throw new GoraException("Error decoding union type: ", e);
      }
    } else {
      fromSchema = schema;
    }
    return fromBytes(encoder, fromSchema, data);
    case BOOLEAN:
      return encoder.decodeBoolean(data);
    case DOUBLE:
      return encoder.decodeDouble(data);
    case FLOAT:
      return encoder.decodeFloat(data);
    case INT:
      return encoder.decodeInt(data);
    case LONG:
      return encoder.decodeLong(data);
    case STRING:
      return new Utf8(data);
    case BYTES:
      return ByteBuffer.wrap(data);
    case ENUM:
      return AvroUtils.getEnumValue(schema, encoder.decodeInt(data));
    case ARRAY:
      break;
    case FIXED:
      break;
    case MAP:
      break;
    case NULL:
      break;
    case RECORD:
      break;
    case UNION:
      break;
    default:
      break;


  public byte[] toBytes(Schema toSchema, Object o) {
    if (toSchema != null && toSchema.getType() == Type.UNION) {
      ByteArrayOutputStream baos = new ByteArrayOutputStream();
      org.apache.avro.io.BinaryEncoder avroEncoder = EncoderFactory.get().binaryEncoder(baos, null);
      int unionIndex = 0;
      try {
        if (o == null) {
          unionIndex = firstNullSchemaTypeIndex(toSchema);
          avroEncoder.writeIndex(unionIndex);
          avroEncoder.writeNull();
        } else {
          unionIndex = firstNotNullSchemaTypeIndex(toSchema);
          avroEncoder.writeIndex(unionIndex);
          avroEncoder.writeBytes(toBytes(o));
        }
        avroEncoder.flush();
        return baos.toByteArray();
      } catch (IOException e) {
        e.printStackTrace();
        return toBytes(o);
      }
    } else {     
      return toBytes(o);
    }
  }

  private int firstNullSchemaTypeIndex(Schema toSchema) {
    List<Schema> possibleTypes = toSchema.getTypes();
    int unionIndex = 0;
    for (int i = 0; i < possibleTypes.size(); i ) {
      Type pType = possibleTypes.get(i).getType();
      if (pType == Type.NULL) { // FIXME HUGE kludge to pass tests
        unionIndex = i; break;
      }
    }
    return unionIndex;
  }

  private int firstNotNullSchemaTypeIndex(Schema toSchema) {
    List<Schema> possibleTypes = toSchema.getTypes();
    int unionIndex = 0;
    for (int i = 0; i < possibleTypes.size(); i ) {
      Type pType = possibleTypes.get(i).getType();
      if (pType != Type.NULL) { // FIXME HUGE kludge to pass tests
        unionIndex = i; break;
      }
    }
    return unionIndex;
  }



        return copyIfNeeded(((Utf8) o).getBytes(), 0, ((Utf8) o).getByteLength());
        return encoder.encodeInt(((Enum<?>) o).ordinal());

        BatchWriterConfig batchWriterConfig = new BatchWriterConfig();
        batchWriterConfig.setMaxMemory(10000000);
        batchWriterConfig.setMaxLatency(60000l, TimeUnit.MILLISECONDS);
        batchWriterConfig.setMaxWriteThreads(4);
        batchWriter = conn.createBatchWriter(mapping.tableName, batchWriterConfig);



        encoder = new BinaryEncoder();

        AuthenticationToken token =  new PasswordToken(password);
          credentials = new TCredentials(user, 
              "org.apache.accumulo.core.client.security.tokens.PasswordToken", 
              ByteBuffer.wrap(password.getBytes()), instance);
          conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, token);
          conn = new MockInstance().getConnector(user, new PasswordToken(password));
          credentials = new TCredentials(user, 
              "org.apache.accumulo.core.client.security.tokens.PasswordToken", 
              ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());







        throw new GoraException("Please define the accumulo 'table' name mapping in "  filename  " for "  persistentClass.getCanonicalName());




    Map<Utf8, Object> currentMap = null;
    List currentArray = null;
    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(new byte[0], null);


      if (row == null) {
        row = entry.getKey().getRowData();
      }
      byte[] val = entry.getValue().get();

      Field field = fieldMap.get(getFieldName(entry));

          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
              fromBytes(currentSchema, entry.getValue().get()));
          persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
      case MAP:  // first entry only. Next are handled above on the next loop
        currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
        currentPos = field.pos();
        currentFam = entry.getKey().getColumnFamily();
        currentSchema = field.schema().getValueType();
        currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
            fromBytes(currentSchema, entry.getValue().get()));
        break;
      case ARRAY:
        currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
        currentPos = field.pos();
        currentFam = entry.getKey().getColumnFamily();
        currentSchema = field.schema().getElementType();
        currentField = field;

        currentArray.add(fromBytes(currentSchema, entry.getValue().get()));

        break;
      case UNION:// default value of null acts like union with null
        Schema effectiveSchema = field.schema().getTypes()
        .get(firstNotNullSchemaTypeIndex(field.schema()));
        // map and array were coded without union index so need to be read the same way
        if (effectiveSchema.getType() == Type.ARRAY) {
          currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
          currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
        }
        else if (effectiveSchema.getType() == Type.MAP) {
          currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
          currentPos = field.pos();
          currentFam = entry.getKey().getColumnFamily();
          currentSchema = effectiveSchema.getValueType();

          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()), 
              fromBytes(currentSchema, entry.getValue().get()));
        }
        // continue like a regular top-level union
      case RECORD:
        SpecificDatumReader<?> reader = new SpecificDatumReader<Schema>(field.schema());
        persistent.put(field.pos(), reader.read(null, DecoderFactory.get().binaryDecoder(val, decoder)));
        break;
      default:
        persistent.put(field.pos(), fromBytes(field.schema(), entry.getValue().get()));

      persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
  /**
   * Retrieve field name from entry.
   * @param entry The Key-Value entry
   * @return String The field name
   */
  private String getFieldName(Entry<Key, Value> entry) {
    String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), 
        entry.getKey().getColumnQualifier()));
    if (fieldName == null) {
      fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), null));
    }
    return fieldName;
  }

      if (col != null) {
        if (col.getSecond() == null) {
          scanner.fetchColumnFamily(col.getFirst());
        } else {
          scanner.fetchColumn(col.getFirst(), col.getSecond());
        }
        LOG.error("Mapping not found for field: "  field);




      List<Field> fields = schema.getFields();

      for (int i = 1; i < fields.size(); i) {
        if (!val.isDirty(i)) {
        Field field = fields.get(i);

        Object o = val.get(field.pos());       

        case MAP:
          count = putMap(m, count, field.schema().getValueType(), o, col);
          break;
        case ARRAY:
          count = putArray(m, count, o, col);
          break;
        case UNION: // default value of null acts like union with null
          Schema effectiveSchema = field.schema().getTypes()
          .get(firstNotNullSchemaTypeIndex(field.schema()));
          // map and array need to compute qualifier
          if (effectiveSchema.getType() == Type.ARRAY) {
            count = putArray(m, count, o, col);
          }
          else if (effectiveSchema.getType() == Type.MAP) {
            count = putMap(m, count, effectiveSchema.getValueType(), o, col);
          }
          // continue like a regular top-level union
        case RECORD:
          SpecificDatumWriter<Object> writer = new SpecificDatumWriter<Object>(field.schema());
          ByteArrayOutputStream os = new ByteArrayOutputStream();
          org.apache.avro.io.BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
          writer.write(o, encoder);
          encoder.flush();
          m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
          count;
          break;
        default:
          m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
          count;



  private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col) throws GoraException {

    // First of all we delete map field on accumulo store
    Text rowKey = new Text(m.getRow());
    Query<K, T> query = newQuery();
    query.setFields(col.getFirst().toString());
    query.setStartKey((K)rowKey.toString());
    query.setEndKey((K)rowKey.toString());
    deleteByQuery(query);
    flush();
    if (o == null){
      return 0;
    }
    
    Set<?> es = ((Map<?, ?>)o).entrySet();
    for (Object entry : es) {
      Object mapKey = ((Entry<?, ?>) entry).getKey();
      Object mapVal = ((Entry<?, ?>) entry).getValue();                  
      if ((o instanceof DirtyMapWrapper && ((DirtyMapWrapper<?, ?>)o).isDirty())
          || !(o instanceof DirtyMapWrapper)) { //mapVal instanceof Dirtyable && ((Dirtyable)mapVal).isDirty()) {
        m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(valueType, mapVal)));
        count;
      }
      // TODO map value deletion
    }
    return count;
  }

  private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col) {

    // First of all we delete array field on accumulo store
    Text rowKey = new Text(m.getRow());
    Query<K, T> query = newQuery();
    query.setFields(col.getFirst().toString());
    query.setStartKey((K)rowKey.toString());
    query.setEndKey((K)rowKey.toString());
    deleteByQuery(query);
    flush();
    if (o == null){
      return 0;
    }
    
    List<?> array = (List<?>) o;  // both GenericArray and DirtyListWrapper
    int j = 0;
    for (Object item : array) {
      m.put(col.getFirst(), new Text(toBytes(j)), new Value(toBytes(item)));
      count;
    }
    return count;
  }
















        tl = TabletLocator.getInstance(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));


      while (tl.binRanges(Collections.singletonList(createRange(query)), binnedRanges, credentials).size() > 0) {







          PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, new String[] {location});






  @SuppressWarnings("unchecked")

import java.io.IOException;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
  
  
      if (schema.getType().equals(Type.RECORD) || schema.getType().equals(Type.MAP) ){
        try {
          value = AvroSerializerUtil.deserializer(value, schema);
        } catch (IOException e) {
          LOG.warn(field.name()  " named field could not be deserialized.");
        }
      }
      String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName().duplicate());
      String family = cassandraColumn.getFamily();  
      String fieldName = this.reverseMap.get(family  ":"  StringSerializer.get().fromByteBuffer(cassandraColumn.getName().duplicate()));
      
      if (fieldName != null) {
        if (fieldName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {

          int pos = this.persistent.getSchema().getField(fieldName).pos();
          Field field = fields.get(pos);
          Type fieldType = field.schema().getType();
          // LOG.info(StringSerializer.get().fromByteBuffer(cassandraColumn.getName())
          //  fieldName  " "  fieldType.name());
          if (fieldType.equals(Type.UNION)) {
            //getting UNION stored type
            CassandraColumn cc = getUnionTypeColumn(fieldName
                 CassandraStore.UNION_COL_SUFIX, cassandraRow.toArray());
            //creating temporary UNION Field
            Field unionField = new Field(fieldName
                 CassandraStore.UNION_COL_SUFIX, Schema.create(Type.INT),
                null, null);
            // get value of UNION stored type
            cc.setField(unionField);
            Object val = cc.getValue();
            cassandraColumn.setUnionType(Integer.parseInt(val.toString()));
          }

          // get value
          cassandraColumn.setField(field);
          Object value = cassandraColumn.getValue();

          this.persistent.put(pos, value);
          // this field does not need to be written back to the store
          this.persistent.clearDirty(pos);
      } else
  //TODO Should we remove this method?
  @SuppressWarnings("unused")
import java.util.List;
import java.util.Map;
import org.apache.gora.cassandra.serializers.ListSerializer;
import org.apache.gora.cassandra.serializers.MapSerializer;
  private Object getFieldValue(Type type, Schema fieldSchema, ByteBuffer byteBuffer){
    Object value = null;
    if (type.equals(Type.ARRAY)) {
      ListSerializer<?> serializer = ListSerializer.get(fieldSchema.getElementType());
      List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
      value = genericArray;
    } else if (type.equals(Type.MAP)) {
//      MapSerializer<?> serializer = MapSerializer.get(fieldSchema.getValueType());
//      Map<?, ?> map = serializer.fromByteBuffer(byteBuffer);
//      value = map;
      value = fromByteBuffer(fieldSchema, byteBuffer);
    } else if (type.equals(Type.RECORD)){
      value = fromByteBuffer(fieldSchema, byteBuffer);
    } else if (type.equals(Type.UNION)){
      // the selected union schema is obtained
      Schema unionFieldSchema = getUnionSchema(super.getUnionType(), fieldSchema);
      Type unionFieldType = unionFieldSchema.getType();
      // we use the selected union schema to deserialize our actual value
      //value = fromByteBuffer(unionFieldSchema, byteBuffer);
      value = getFieldValue(unionFieldType, unionFieldSchema, byteBuffer);
    } else {
      value = fromByteBuffer(fieldSchema, byteBuffer);
    }
    return value;
  }

    Object value = getFieldValue(type, fieldSchema, byteBuffer);
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import org.apache.gora.cassandra.serializers.CharSequenceSerializer;
import org.apache.gora.cassandra.store.CassandraStore;
 private Object getSuperValue(Field field, Schema fieldSchema, Type type){
        List<Object> array = new ArrayList<Object>();
        Map<CharSequence, Object> map = new HashMap<CharSequence, Object>();

          CharSequence mapKey = CharSequenceSerializer.get().fromByteBuffer(hColumn.getName());
          if (mapKey.toString().indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
            Object memberValue = null;
            // We need detect real type for UNION Fields
            if (fieldSchema.getValueType().getType().equals(Type.UNION)){
              
              HColumn<ByteBuffer, ByteBuffer> cc = getUnionTypeColumn(mapKey
                   CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns());
              Integer unionIndex = getUnionIndex(mapKey.toString(), cc);
              Schema realSchema = fieldSchema.getValueType().getTypes().get(unionIndex);
              memberValue = fromByteBuffer(realSchema, hColumn.getValue());
              
            }else{
              memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());            
            }            
            map.put(mapKey, memberValue);      
          }
            if (memberName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
              
            Schema memberSchema = memberField.schema();
            Type memberType = memberSchema.getType();
            
            
            if (memberType.equals(Type.UNION)){
              HColumn<ByteBuffer, ByteBuffer> hc = getUnionTypeColumn(memberField.name()
                   CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns().toArray());
              Integer unionIndex = getUnionIndex(memberField.name(),hc);
              cassandraColumn.setUnionType(unionIndex);
            }
            
            record.put(record.getSchema().getField(memberName).pos(), cassandraColumn.getValue());
          }
      case UNION:
        int schemaPos = this.getUnionType();
        Schema unioSchema = fieldSchema.getTypes().get(schemaPos);
        Type unionType = unioSchema.getType();
        value = getSuperValue(field, unioSchema, unionType);
        break;
        Object memberValue = null;
        // Using for UnionIndex of Union type field get value. UnionIndex always Integer.  
        for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
          memberValue = fromByteBuffer(fieldSchema, hColumn.getValue());      
        }
        value = memberValue;
        LOG.warn("Type: "  type.name()  " not supported for field: "  field.name());
    return value;
  }

 private Integer getUnionIndex(String fieldName, HColumn<ByteBuffer, ByteBuffer> uc){
   Integer val = IntegerSerializer.get().fromByteBuffer(uc.getValue());
   return Integer.parseInt(val.toString());
 }
 
  private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName,
    List<HColumn<ByteBuffer, ByteBuffer>> columns) {
    return getUnionTypeColumn(fieldName, columns.toArray());
}

  private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName, Object[] hColumns) {
    for (int iCnt = 0; iCnt < hColumns.length; iCnt){
      @SuppressWarnings("unchecked")
      HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>)hColumns[iCnt];
      String columnName = StringSerializer.get().fromByteBuffer(hColumn.getNameBytes().duplicate());
      if (fieldName.equals(columnName))
        return hColumn;
    }
    return null;
}

  public Object getValue() {
    Field field = getField();
    Schema fieldSchema = field.schema();
    Type type = fieldSchema.getType();
    
    Object value = getSuperValue(field, fieldSchema, type);
import java.util.Map;
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.BytesArraySerializer;
import me.prettyprint.cassandra.serializers.ObjectSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.Persistent;
      serializer = CharSequenceSerializer.get();
      serializer = ListSerializer.get(schema);
    } else if (value instanceof Map) {
      Map map = (Map)value;
        serializer = MapSerializer.get(schema);
    } else if (value instanceof Persistent){
      serializer = ObjectSerializer.get();
    }
    else {
      serializer = CharSequenceSerializer.get();
    if (type.equals(Type.STRING)) {
      serializer = CharSequenceSerializer.get();
    } else if (type.equals(Type.BOOLEAN)) {
    } else if (type.equals(Type.BYTES)) {
    } else if (type.equals(Type.DOUBLE)) {
    } else if (type.equals(Type.FLOAT)) {
    } else if (type.equals(Type.INT)) {
    } else if (type.equals(Type.LONG)) {
    } else if (type.equals(Type.FIXED)) {
    } else if (type.equals(Type.ARRAY)) {
      serializer = ListSerializer.get(schema.getElementType());
    } else if (type.equals(Type.MAP)) {
    	serializer = MapSerializer.get(schema.getValueType());
    } else if (type.equals(Type.UNION)){
    } else if (type.equals(Type.RECORD)){
      serializer = BytesArraySerializer.get();
      serializer = CharSequenceSerializer.get();
      serializer = ListSerializer.get(elementType);
      serializer = MapSerializer.get(elementType);
import java.util.List;
import java.util.Map;
  public static Class<? extends Object> getClass(Object value) {
      return Schema.createArray( getElementSchema((GenericArray<?>)value) );
    } else if (clazz.isAssignableFrom(List.class)) {
    } else if (clazz.isAssignableFrom(Map.class)) {
  public static Class<?> getClass(Type type) {
      return List.class;
      return Map.class;
  public static Schema getSchema(Class<?> clazz) {
  public static Class<?> getClass(Schema schema) {
  public static int getFixedSize(Class<?> clazz) {
  public static Schema getElementSchema(GenericArray<?> array) {
  
  
  
    this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
    
    
    // Just create a Keyspace object on the client side, corresponding to an already existing keyspace with already created column families.
    
  
   * In this method, we also utilise Hector's {@ConfigurableConsistencyLevel}
   * logic. It is set by passing a ConfigurableConsistencyLevel object right 
   * when the Keyspace is created. Currently consistency level is .ONE which 
   * permits consistency to wait until one replica has responded. 
          "org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);      
      // LOG.info("Keyspace '"  this.cassandraMapping.getKeyspaceName()  "' in cluster '"  this.cassandraMapping.getClusterName()  "' was created on host '"  this.cassandraMapping.getHostName()  "'");
      
      // Create a customized Consistency Level
      ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
      // Define CL.ONE for ColumnFamily "ColumnFamily"
      clmap.put("ColumnFamily", HConsistencyLevel.ONE);

      // In this we use CL.ONE for read and writes. But you can use different CLs if needed.
      configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
      configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
      HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);

               ", not BytesType. It may cause a fatal error on column validation later.");
            LOG.debug("The comparator type of "  cfDef.getName()  " column family is " 
               comparatorType.getTypeName()  ".");
    if (ttlAttr == null)
   * Delete a row within the keyspace.
   * @param key
   * @param fieldName
   * @param columnName
   */
  public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
    synchronized(mutator) {
      HectorUtils.deleteColumn(mutator, key, familyName, columnName);
    }
  }

  /**
   * Deletes an entry based on its key.
   * @param key
   */
  public void deleteByKey(K key) {
    Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
    deleteColumn(key, familyMap.values().iterator().next().toString(), null);
  }

  /**
    
    if (ttlAttr == null)
    
   * Deletes a subColumn 
   * @param key
   * @param fieldName
   * @param columnName
   * Deletes all subcolumns from a super column.
   * @param key the row key.
   * @param fieldName the field name.
  public void deleteSubColumn(K key, String fieldName) {
    String columnFamily = this.cassandraMapping.getFamily(fieldName);
    String superColumnName = this.cassandraMapping.getColumn(fieldName);
    synchronized(mutator) {
      HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, null);
    }
  public void deleteGenericArray(K key, String fieldName) {
    //TODO Verify this. Everything that goes inside a genericArray will go inside a column so let's just delete that.
    deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
  }
          if (((List<?>)itemValue).size() == 0) {
        } else if (itemValue instanceof Map<?,?>) {
          if (((Map<?, ?>)itemValue).size() == 0) {

  public void deleteStatefulHashMap(K key, String fieldName) {
      deleteSubColumn(key, fieldName);
    } else {
      deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
    }
  }
  public void addStatefulHashMap(K key, String fieldName, Map<CharSequence,Object> map) {
    if (isSuper( cassandraMapping.getFamily(fieldName) )) {
      // as we don't know what has changed inside the map or If it's an empty map, then delete its content.
      deleteSubColumn(key, fieldName);
      // update if there is anything to update.
      if (!map.isEmpty()) {
        // If it's not empty, then update its content.
        for (CharSequence mapKey: map.keySet()) {
          // TODO: hack, do not store empty arrays
          Object mapValue = map.get(mapKey);
          if (mapValue instanceof GenericArray<?>) {
            if (((List<?>)mapValue).size() == 0) {
              continue;
            }
          } else if (mapValue instanceof Map<?,?>) {
            if (((Map<?, ?>)mapValue).size() == 0) {
              continue;
            }
          addSubColumn(key, fieldName, mapKey.toString(), mapValue);
    
    
    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
    
    
  
    // checking if it was a UNION field the one we are retrieving
    if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
      family = this.cassandraMapping.getFamily(pField.substring(0,pField.indexOf(CassandraStore.UNION_COL_SUFIX)));
    else
      family = this.cassandraMapping.getFamily(pField);
     return family;
   }
 
    if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
      column = pField;
    else
      column = this.cassandraMapping.getColumn(pField);
      return column;
    }
   * @return a map which keys are the family names and values the 
   * corresponding column names required to get all the query fields.
      
    
   * Retrieves the cassandraMapping which holds whatever was mapped 
   * from the gora-cassandra-mapping.xml
   * @return 
  
   * Select the field names according to the column names, which format 
   * if fully qualified: "family:column"
   * @return a map which keys are the fully qualified column 
   * names and values the query fields
      
    
  /**
   * Determines if a column is a superColumn or not.
   * @param family
   * @return boolean
   */
    
    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
    
    


	return this.cassandraMapping.getKeyspaceName();
  
        LOG.debug("Located gc_grace_seconds: '"  gcgrace_scs  "'" );




      // TODO we should find a way of storing more values into this map
   * Add new column to the CassandraMapping using the the below parameters
   * @param pFamilyName the column family name
   * @param pFieldName the Avro field from the Schema
   * @param pColumnName the column name within the column family.
 /**
      LOG.debug("persistentClassName="  className  " -> keyspaceName="  keyspaceName);
import java.util.HashMap;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.generic.GenericData.Array;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.commons.lang.ArrayUtils;
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
 * heavily on {@link org.apache.gora.cassandra.store.CassandraClient} for many operations
  private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
   * Fixed string with value "UnionIndex" used to generate an extra column based on 
   * the original field's name
   */
  public static String UNION_COL_SUFIX = "_UnionIndex";

  /**
   * Default schema index with value "0" used when AVRO Union data types are stored

   * We don't want to lock the entire collection before iterating over the keys, 
   * since in the meantime other threads are adding entries to the map.
  public static final ThreadLocal<BinaryEncoder> encoders =
      new ThreadLocal<BinaryEncoder>();
  
  /**
   * Create a {@link java.util.concurrent.ConcurrentHashMap} for the 
   * datum readers and writers. 
   * This is necessary because they are not thread safe, at least not before 
   * Avro 1.4.0 (See AVRO-650).
   * When they are thread safe, it is possible to maintain a single reader and
   * writer pair for every schema, instead of one for every thread.
   * @see <a href="https://issues.apache.org/jira/browse/AVRO-650">AVRO-650</a>
   */
  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = 
      new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
  
   * When we add sub/supercolumns, Gora keys are mapped to Cassandra partition keys only. 
      CassandraResultSet<K> cassandraResultSet) {
        cassandraRow = new CassandraRow<K>();
   * Flush the buffer which is a synchronized {@link java.util.LinkedHashMap}
   * storing fields pending to be stored by 
   * {@link org.apache.gora.cassandra.store.CassandraStore#put(Object, PersistentBase)}
   * operations. Invoking this method therefore writes the buffered rows
   * into Cassandra.
    @SuppressWarnings("unchecked")
    // iterating over the key set directly would throw 
    //ConcurrentModificationException with java.util.HashMap and subclasses
        LOG.info("Value to update is null for key: "  key);

          addOrUpdateField(key, field, field.schema(), value.get(field.pos()));
    // remove flushed rows from the buffer as all 
    // added or updated fields should now have been written.
    
    if (fields == null){
      fields = this.getFields();
    }
    // Generating UnionFields
    ArrayList<String> unionFields = new ArrayList<String>();
    for (String field: fields){
      Field schemaField =this.fieldMap.get(field);
      Type type = schemaField.schema().getType();
      if (type.getName().equals("UNION".toLowerCase())){
        unionFields.add(fieldUNION_COL_SUFIX);
      }
    }
    
    String[] arr = unionFields.toArray(new String[unionFields.size()]);
    String[] both = (String[]) ArrayUtils.addAll(fields, arr);
    
    query.setFields(both);

    // TODO GORA-298 Implement CassandraStore#getPartitions
   * 
   * When doing the 
   * {@link org.apache.gora.cassandra.store.CassandraStore#put(Object, PersistentBase)}
   * operation, the logic is as follows:
   * <ol>
   * <li>Obtain the Avro {@link org.apache.avro.Schema} for the object.</li>
   * <li>Create a new duplicate instance of the object (explained in more detail below) **.</li>
   * <li>Obtain a {@link java.util.List} of the {@link org.apache.avro.Schema} 
   * {@link org.apache.avro.Schema.Field}'s.</li>
   * <li>Iterate through the field {@link java.util.List}. This allows us to 
   * consequently process each item.</li>
   * <li>Check to see if the {@link org.apache.avro.Schema.Field} is NOT dirty. 
   * If this condition is true then we DO NOT process this field.</li>
   * <li>Obtain the element at the specified position in this list so we can 
   * directly operate on it.</li>
   * <li>Obtain the {@link org.apache.avro.Schema.Type} of the element obtained 
   * above and process it accordingly. N.B. For nested type ARRAY, MAP
   * RECORD or UNION, we shadow the checks in bullet point 5 above to infer that the 
   * {@link org.apache.avro.Schema.Field} is either at 
   * position 0 OR it is NOT dirty. If one of these conditions is true then we DO NOT
   * process this field. This is carried out in 
   * {@link org.apache.gora.cassandra.store.CassandraStore#getFieldValue(Schema, Type, Object)}</li>
   * <li>We then insert the Key and Object into the {@link java.util.LinkedHashMap} buffer 
   * before being flushed. This performs a structural modification of the map.</li>
   * </ol>
   * ** We create a duplicate instance of the object to be persisted and insert processed
   * objects into a synchronized {@link java.util.LinkedHashMap}. This allows 
   * us to keep all the objects in memory till flushing.
   * @see org.apache.gora.store.DataStore#put(java.lang.Object, 
   * org.apache.gora.persistency.Persistent).
   * @param key for the Avro Record (object).
   * @param value Record object to be persisted in Cassandra
    @SuppressWarnings("unchecked")
    T p = (T) SpecificData.get().newRecord(value, schema);
    List<Field> fields = schema.getFields();
    for (int i = 1; i < fields.size(); i) {
      if (!value.isDirty(i)) {
        continue;
      Field field = fields.get(i);
      Type type = field.schema().getType();
      Object fieldValue = value.get(field.pos());
      Schema fieldSchema = field.schema();
      // check if field has a nested structure (array, map, record or union)
      fieldValue = getFieldValue(fieldSchema, type, fieldValue);
      p.put(field.pos(), fieldValue);
   * For every field within an object, we pass in a field schema, Type and value.
   * This enables us to process fields (based on their characteristics) 
   * preparing them for persistence.
   * @param fieldSchema the associated field schema
   * @param type the field type
   * @param fieldValue the field value.
   * @return
   */
  private Object getFieldValue(Schema fieldSchema, Type type, Object fieldValue ){
    switch(type) {
    case RECORD:
      Persistent persistent = (Persistent) fieldValue;
      Persistent newRecord = (Persistent) SpecificData.get().newRecord(persistent, persistent.getSchema());
      for (Field member: fieldSchema.getFields()) {
        if (member.pos() == 0 || !persistent.isDirty()) {
          continue;
        }
        Schema memberSchema = member.schema();
        Type memberType = memberSchema.getType();
        Object memberValue = persistent.get(member.pos());
        newRecord.put(member.pos(), getFieldValue(memberSchema, memberType, memberValue));
      }
      fieldValue = newRecord;
      break;
    case MAP:
      Map<?, ?> map = (Map<?, ?>) fieldValue;
      fieldValue = map;
      break;
    case ARRAY:
      fieldValue = (List<?>) fieldValue;
      break;
    case UNION:
      // storing the union selected schema, the actual value will 
      // be stored as soon as we get break out.
      if (fieldValue != null){
        int schemaPos = getUnionSchema(fieldValue,fieldSchema);
        Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
        Type unionType = unionSchema.getType();
        fieldValue = getFieldValue(unionSchema, unionType, fieldValue);
      }
      //p.put( schemaPos, p.getSchema().getField(field.name()  CassandraStore.UNION_COL_SUFIX));
      //p.put(fieldPos, fieldValue);
      break;
    default:
      break;
    }    
    return fieldValue;
  }
  
  /**
   * @param schema  the schema belonging to the particular Avro field
  @SuppressWarnings({ "unchecked", "rawtypes" })
  private void addOrUpdateField(K key, Field field, Schema schema, Object value) {
    // checking if the value to be updated is used for saving union schema
    if (field.name().indexOf(CassandraStore.UNION_COL_SUFIX) < 0){
      switch (type) {
      case STRING:
      case BOOLEAN:
      case INT:
      case LONG:
      case BYTES:
      case FLOAT:
      case DOUBLE:
      case FIXED:
        this.cassandraClient.addColumn(key, field.name(), value);
        break;
      case RECORD:
        if (value != null) {
          if (value instanceof PersistentBase) {
            PersistentBase persistentBase = (PersistentBase) value;            
            try {
              byte[] byteValue = AvroSerializerUtil.serializer(persistentBase, schema);
              this.cassandraClient.addColumn(key, field.name(), byteValue);
            } catch (IOException e) {
              LOG.warn(field.name()  " named record could not be serialized.");
          } else {
            LOG.warn("Record with value: "  value.toString()  " not supported for field: "  field.name());
          LOG.warn("Setting content of: "  field.name()  " to null.");
          String familyName =  this.cassandraClient.getCassandraMapping().getFamily(field.name());
          this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
        break;
      case MAP:
        if (value != null) {
          if (value instanceof Map<?, ?>) {            
            Map<CharSequence,Object> map = (Map<CharSequence,Object>)value;
            Schema valueSchema = schema.getValueType();
            Type valueType = valueSchema.getType();
            if (Type.UNION.equals(valueType)){
              Map<CharSequence,Object> valueMap = new HashMap<CharSequence, Object>();
              for (CharSequence mapKey: map.keySet()) {
                Object mapValue = map.get(mapKey);
                int valueUnionIndex = getUnionSchema(mapValue, valueSchema);
                valueMap.put((mapKeyUNION_COL_SUFIX), valueUnionIndex);
                valueMap.put(mapKey, mapValue);
              }
              map = valueMap;
            }
            
            String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
            
            // If map is not super column. We using Avro serializer. 
            if (!this.cassandraClient.isSuper( familyName )){
              try {
                byte[] byteValue = AvroSerializerUtil.serializer(map, schema);
                this.cassandraClient.addColumn(key, field.name(), byteValue);
              } catch (IOException e) {
                LOG.warn(field.name()  " named map could not be serialized.");
              }
            }else{
              this.cassandraClient.addStatefulHashMap(key, field.name(), map);              
            }
          } else {
            LOG.warn("Map with value: "  value.toString()  " not supported for field: "  field.name());
          }
          // delete map
          LOG.warn("Setting content of: "  field.name()  " to null.");
          this.cassandraClient.deleteStatefulHashMap(key, field.name());
        break;
      case ARRAY:
        if (value != null) {
          if (value instanceof DirtyListWrapper<?>) {
            DirtyListWrapper fieldValue = (DirtyListWrapper<?>)value;
            GenericArray valueArray = new Array(fieldValue.size(), schema);
            for (int i = 0; i < fieldValue.size(); i) {
              valueArray.add(i, fieldValue.get(i));
            }
            this.cassandraClient.addGenericArray(key, field.name(), (GenericArray<?>)valueArray);
          } else {
            LOG.warn("Array with value: "  value.toString()  " not supported for field: "  field.name());
          }
          LOG.warn("Setting content of: "  field.name()  " to null.");
          this.cassandraClient.deleteGenericArray(key, field.name());
        break;
      case UNION:
     // adding union schema index
        String columnName = field.name()  UNION_COL_SUFIX;
        String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
        if(value != null) {
          int schemaPos = getUnionSchema(value, schema);
          LOG.debug("Union with value: "  value.toString()  " at index: "  schemaPos  " supported for field: "  field.name());
          this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
          if (this.cassandraClient.isSuper( familyName )){
            this.cassandraClient.addSubColumn(key, columnName, columnName, schemaPos);
          }else{
            this.cassandraClient.addColumn(key, columnName, schemaPos);
            
          }
          //this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
          // adding union value
          Schema unionSchema = schema.getTypes().get(schemaPos);
          addOrUpdateField(key, field, unionSchema, value);
          //this.cassandraClient.addColumn(key, field.name(), value);
        } else {
          LOG.warn("Setting content of: "  field.name()  " to null.");
          if (this.cassandraClient.isSuper( familyName )){
            this.cassandraClient.deleteSubColumn(key, field.name());
          } else {
            this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
          }
        }
        break;
      default:
        LOG.warn("Type: "  type.name()  " not considered for field: "  field.name()  ". Please report this to dev@gora.apache.org");
   * Given an object and the object schema this function obtains,
   * from within the UNION schema, the position of the type used.
   * If no data type can be inferred then we return a default value
   * of position 0.
   * @return the unionSchemaPosition.
//    String valueType = pValue.getClass().getSimpleName();
      Type schemaType = it.next().getType();
      if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
      else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
      else if (pValue instanceof Integer && schemaType.equals(Type.INT))
      else if (pValue instanceof Long && schemaType.equals(Type.LONG))
      else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
      else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
      else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
        return unionSchemaPos;
      else if (pValue instanceof Map && schemaType.equals(Type.MAP))
        return unionSchemaPos;
      else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
        return unionSchemaPos;
      else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
    return DEFAULT_UNION_SCHEMA;
   * Simple method to check if a Cassandra Keyspace exists.
   * @return true if a Keyspace exists.



    mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());

import java.util.ArrayList;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
    //"http://example.com",
    //"fck fck dck",
      throws IOException {
    try{
      WebPage page;
      log.info("creating web page data");
      
      for(int i=0; i<URLS.length; i) {
        page = WebPage.newBuilder().build();
        page.setUrl(new Utf8(URLS[i]));
        page.setParsedContent(new ArrayList<CharSequence>());
        if (CONTENTS[i]!=null){
          page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
          for(String token : CONTENTS[i].split(" ")) {
            page.getParsedContent().add(new Utf8(token));  
        }
        for(int j=0; j<LINKS[i].length; j) {
          page.getOutlinks().put(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
        }
        
        Metadata metadata = Metadata.newBuilder().build();
        metadata.setVersion(1);
        metadata.getData().put(new Utf8("metakey"), new Utf8("metavalue"));
        page.setMetadata(metadata);

        dataStore.put(URLS[i], page);
      }
      dataStore.flush();
      log.info("finished creating web page data");
    }
    catch(Exception e){
      log.info("error creating web page data");
    } 
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class Employee extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    NAME(1, "name"),
    DATE_OF_BIRTH(2, "dateOfBirth"),
    SSN(3, "ssn"),
    SALARY(4, "salary"),
    BOSS(5, "boss"),
    WEBPAGE(6, "webpage"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "name",
  "dateOfBirth",
  "ssn",
  "salary",
  "boss",
  "webpage",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence name;
  private java.lang.CharSequence ssn;
  private java.lang.Object boss;
  private org.apache.gora.examples.generated.WebPage webpage;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return name;
    case 2: return dateOfBirth;
    case 3: return ssn;
    case 4: return salary;
    case 5: return boss;
    case 6: return webpage;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: name = (java.lang.CharSequence)(value); break;
    case 2: dateOfBirth = (java.lang.Long)(value); break;
    case 3: ssn = (java.lang.CharSequence)(value); break;
    case 4: salary = (java.lang.Integer)(value); break;
    case 5: boss = (java.lang.Object)(value); break;
    case 6: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

   * Gets the value of the 'name' field.
  public java.lang.CharSequence getName() {
    return name;
  }

  /**
   * Sets the value of the 'name' field.
   * @param value the value to set.
   */
  public void setName(java.lang.CharSequence value) {
    this.name = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isNameDirty(java.lang.CharSequence value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'dateOfBirth' field.
   */
  public java.lang.Long getDateOfBirth() {
    return dateOfBirth;
  }

  /**
   * Sets the value of the 'dateOfBirth' field.
   * @param value the value to set.
   */
  public void setDateOfBirth(java.lang.Long value) {
    this.dateOfBirth = value;
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isDateOfBirthDirty(java.lang.Long value) {
    return isDirty(2);
  }

  /**
   * Gets the value of the 'ssn' field.
   */
  public java.lang.CharSequence getSsn() {
    return ssn;
  }

  /**
   * Sets the value of the 'ssn' field.
   * @param value the value to set.
   */
  public void setSsn(java.lang.CharSequence value) {
    this.ssn = value;
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isSsnDirty(java.lang.CharSequence value) {
    return isDirty(3);
  }

  /**
   * Gets the value of the 'salary' field.
   */
  public java.lang.Integer getSalary() {
    return salary;
  }

  /**
   * Sets the value of the 'salary' field.
   * @param value the value to set.
   */
  public void setSalary(java.lang.Integer value) {
    this.salary = value;
    setDirty(4);
  }
  
  /**
   * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isSalaryDirty(java.lang.Integer value) {
    return isDirty(4);
  }

  /**
   * Gets the value of the 'boss' field.
   */
  public java.lang.Object getBoss() {
    return boss;
  }

  /**
   * Sets the value of the 'boss' field.
   * @param value the value to set.
   */
  public void setBoss(java.lang.Object value) {
    this.boss = value;
    setDirty(5);
  }
  
  /**
   * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isBossDirty(java.lang.Object value) {
    return isDirty(5);
  }

  /**
   * Gets the value of the 'webpage' field.
   */
  public org.apache.gora.examples.generated.WebPage getWebpage() {
    return webpage;
  }

  /**
   * Sets the value of the 'webpage' field.
   * @param value the value to set.
   */
  public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
    this.webpage = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
    return isDirty(6);
  }

  /** Creates a new Employee RecordBuilder */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder() {
    return new org.apache.gora.examples.generated.Employee.Builder();
  }
  
  /** Creates a new Employee RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee.Builder other) {
    return new org.apache.gora.examples.generated.Employee.Builder(other);
  }
  
  /** Creates a new Employee RecordBuilder by copying an existing Employee instance */
  public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee other) {
    return new org.apache.gora.examples.generated.Employee.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Employee instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Employee>
    implements org.apache.avro.data.RecordBuilder<Employee> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence name;
    private long dateOfBirth;
    private java.lang.CharSequence ssn;
    private int salary;
    private java.lang.Object boss;
    private org.apache.gora.examples.generated.WebPage webpage;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.Employee.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.Employee.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Employee instance */
    private Builder(org.apache.gora.examples.generated.Employee other) {
            super(org.apache.gora.examples.generated.Employee.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.name)) {
        this.name = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.name);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.dateOfBirth)) {
        this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.dateOfBirth);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.ssn)) {
        this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ssn);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.salary)) {
        this.salary = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.salary);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.boss)) {
        this.boss = (java.lang.Object) data().deepCopy(fields()[5].schema(), other.boss);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.webpage)) {
        this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[6].schema(), other.webpage);
        fieldSetFlags()[6] = true;
      }
    }

    /** Gets the value of the 'name' field */
    public java.lang.CharSequence getName() {
      return name;
    }
    
    /** Sets the value of the 'name' field */
    public org.apache.gora.examples.generated.Employee.Builder setName(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.name = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'name' field has been set */
    public boolean hasName() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'name' field */
    public org.apache.gora.examples.generated.Employee.Builder clearName() {
      name = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'dateOfBirth' field */
    public java.lang.Long getDateOfBirth() {
      return dateOfBirth;
    }
    
    /** Sets the value of the 'dateOfBirth' field */
    public org.apache.gora.examples.generated.Employee.Builder setDateOfBirth(long value) {
      validate(fields()[2], value);
      this.dateOfBirth = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'dateOfBirth' field has been set */
    public boolean hasDateOfBirth() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'dateOfBirth' field */
    public org.apache.gora.examples.generated.Employee.Builder clearDateOfBirth() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'ssn' field */
    public java.lang.CharSequence getSsn() {
      return ssn;
    }
    
    /** Sets the value of the 'ssn' field */
    public org.apache.gora.examples.generated.Employee.Builder setSsn(java.lang.CharSequence value) {
      validate(fields()[3], value);
      this.ssn = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'ssn' field has been set */
    public boolean hasSsn() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'ssn' field */
    public org.apache.gora.examples.generated.Employee.Builder clearSsn() {
      ssn = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'salary' field */
    public java.lang.Integer getSalary() {
      return salary;
    }
    
    /** Sets the value of the 'salary' field */
    public org.apache.gora.examples.generated.Employee.Builder setSalary(int value) {
      validate(fields()[4], value);
      this.salary = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'salary' field has been set */
    public boolean hasSalary() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'salary' field */
    public org.apache.gora.examples.generated.Employee.Builder clearSalary() {
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'boss' field */
    public java.lang.Object getBoss() {
      return boss;
    }
    
    /** Sets the value of the 'boss' field */
    public org.apache.gora.examples.generated.Employee.Builder setBoss(java.lang.Object value) {
      validate(fields()[5], value);
      this.boss = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'boss' field has been set */
    public boolean hasBoss() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'boss' field */
    public org.apache.gora.examples.generated.Employee.Builder clearBoss() {
      boss = null;
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'webpage' field */
    public org.apache.gora.examples.generated.WebPage getWebpage() {
      return webpage;
    }
    
    /** Sets the value of the 'webpage' field */
    public org.apache.gora.examples.generated.Employee.Builder setWebpage(org.apache.gora.examples.generated.WebPage value) {
      validate(fields()[6], value);
      this.webpage = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'webpage' field has been set */
    public boolean hasWebpage() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'webpage' field */
    public org.apache.gora.examples.generated.Employee.Builder clearWebpage() {
      webpage = null;
      fieldSetFlags()[6] = false;
      return this;
    }
    
    @Override
    public Employee build() {
      try {
        Employee record = new Employee();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.name = fieldSetFlags()[1] ? this.name : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.dateOfBirth = fieldSetFlags()[2] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[2]);
        record.ssn = fieldSetFlags()[3] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[3]);
        record.salary = fieldSetFlags()[4] ? this.salary : (java.lang.Integer) defaultValue(fields()[4]);
        record.boss = fieldSetFlags()[5] ? this.boss : (java.lang.Object) defaultValue(fields()[5]);
        record.webpage = fieldSetFlags()[6] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[6]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public Employee.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Employee newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'name' field.
		   */
	  public java.lang.CharSequence getName() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'name' field.
		   * @param value the value to set.
	   */
	  public void setName(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isNameDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'dateOfBirth' field.
		   */
	  public java.lang.Long getDateOfBirth() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'dateOfBirth' field.
		   * @param value the value to set.
	   */
	  public void setDateOfBirth(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isDateOfBirthDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'ssn' field.
		   */
	  public java.lang.CharSequence getSsn() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'ssn' field.
		   * @param value the value to set.
	   */
	  public void setSsn(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isSsnDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'salary' field.
		   */
	  public java.lang.Integer getSalary() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'salary' field.
		   * @param value the value to set.
	   */
	  public void setSalary(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isSalaryDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'boss' field.
		   */
	  public java.lang.Object getBoss() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'boss' field.
		   * @param value the value to set.
	   */
	  public void setBoss(java.lang.Object value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isBossDirty(java.lang.Object value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'webpage' field.
		   */
	  public org.apache.gora.examples.generated.WebPage getWebpage() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'webpage' field.
		   * @param value the value to set.
	   */
	  public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class Metadata extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    VERSION(1, "version"),
    DATA(2, "data"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "version",
  "data",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return version;
    case 2: return data;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: version = (java.lang.Integer)(value); break;
    case 2: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'version' field.
   */
  public java.lang.Integer getVersion() {
    return version;
  }

  /**
   * Sets the value of the 'version' field.
   * @param value the value to set.
   */
  public void setVersion(java.lang.Integer value) {
    this.version = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isVersionDirty(java.lang.Integer value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'data' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
    return data;
  }

  /**
   * Sets the value of the 'data' field.
   * @param value the value to set.
   */
  public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.data = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(2);
  }

  /** Creates a new Metadata RecordBuilder */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder() {
    return new org.apache.gora.examples.generated.Metadata.Builder();
  }
  
  /** Creates a new Metadata RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata.Builder other) {
    return new org.apache.gora.examples.generated.Metadata.Builder(other);
  }
  
  /** Creates a new Metadata RecordBuilder by copying an existing Metadata instance */
  public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata other) {
    return new org.apache.gora.examples.generated.Metadata.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Metadata instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Metadata>
    implements org.apache.avro.data.RecordBuilder<Metadata> {

    private java.nio.ByteBuffer __g__dirty;
    private int version;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.Metadata.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Metadata instance */
    private Builder(org.apache.gora.examples.generated.Metadata other) {
            super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.version)) {
        this.version = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.version);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.data)) {
        this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.data);
        fieldSetFlags()[2] = true;
      }
    }

    /** Gets the value of the 'version' field */
    public java.lang.Integer getVersion() {
      return version;
    }
    
    /** Sets the value of the 'version' field */
    public org.apache.gora.examples.generated.Metadata.Builder setVersion(int value) {
      validate(fields()[1], value);
      this.version = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'version' field has been set */
    public boolean hasVersion() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'version' field */
    public org.apache.gora.examples.generated.Metadata.Builder clearVersion() {
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'data' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
      return data;
    }
    
    /** Sets the value of the 'data' field */
    public org.apache.gora.examples.generated.Metadata.Builder setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[2], value);
      this.data = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'data' field has been set */
    public boolean hasData() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'data' field */
    public org.apache.gora.examples.generated.Metadata.Builder clearData() {
      data = null;
      fieldSetFlags()[2] = false;
      return this;
    }
    
    @Override
    public Metadata build() {
      try {
        Metadata record = new Metadata();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.version = fieldSetFlags()[1] ? this.version : (java.lang.Integer) defaultValue(fields()[1]);
        record.data = fieldSetFlags()[2] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[2]));
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public Metadata.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Metadata newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'version' field.
		   */
	  public java.lang.Integer getVersion() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'version' field.
		   * @param value the value to set.
	   */
	  public void setVersion(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isVersionDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'data' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'data' field.
		   * @param value the value to set.
	   */
	  public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class TokenDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    COUNT(1, "count"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "count",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return count;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: count = (java.lang.Integer)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'count' field.
   */
  public java.lang.Integer getCount() {
    return count;
  }

  /**
   * Sets the value of the 'count' field.
   * @param value the value to set.
   */
  public void setCount(java.lang.Integer value) {
    this.count = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isCountDirty(java.lang.Integer value) {
    return isDirty(1);
  }

  /** Creates a new TokenDatum RecordBuilder */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder() {
    return new org.apache.gora.examples.generated.TokenDatum.Builder();
  }
  
  /** Creates a new TokenDatum RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
    return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
  }
  
  /** Creates a new TokenDatum RecordBuilder by copying an existing TokenDatum instance */
  public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum other) {
    return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for TokenDatum instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<TokenDatum>
    implements org.apache.avro.data.RecordBuilder<TokenDatum> {

    private java.nio.ByteBuffer __g__dirty;
    private int count;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing TokenDatum instance */
    private Builder(org.apache.gora.examples.generated.TokenDatum other) {
            super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.count)) {
        this.count = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.count);
        fieldSetFlags()[1] = true;
      }
    }

    /** Gets the value of the 'count' field */
    public java.lang.Integer getCount() {
      return count;
    }
    
    /** Sets the value of the 'count' field */
    public org.apache.gora.examples.generated.TokenDatum.Builder setCount(int value) {
      validate(fields()[1], value);
      this.count = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'count' field has been set */
    public boolean hasCount() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'count' field */
    public org.apache.gora.examples.generated.TokenDatum.Builder clearCount() {
      fieldSetFlags()[1] = false;
      return this;
    }
    
    @Override
    public TokenDatum build() {
      try {
        TokenDatum record = new TokenDatum();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.count = fieldSetFlags()[1] ? this.count : (java.lang.Integer) defaultValue(fields()[1]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public TokenDatum.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public TokenDatum newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'count' field.
		   */
	  public java.lang.Integer getCount() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'count' field.
		   * @param value the value to set.
	   */
	  public void setCount(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isCountDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;  
public class WebPage extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");

  /** Enum containing all data bean's fields. */
    __G__DIRTY(0, "__g__dirty"),
    URL(1, "url"),
    CONTENT(2, "content"),
    PARSED_CONTENT(3, "parsedContent"),
    OUTLINKS(4, "outlinks"),
    HEADERS(5, "headers"),
    METADATA(6, "metadata"),






  public static final String[] _ALL_FIELDS = {
  "__g__dirty",
  "url",
  "content",
  "parsedContent",
  "outlinks",
  "headers",
  "metadata",
  };

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence url;
  private java.nio.ByteBuffer content;
  private java.util.List<java.lang.CharSequence> parsedContent;
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
  private org.apache.gora.examples.generated.Metadata metadata;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return url;
    case 2: return content;
    case 3: return parsedContent;
    case 4: return outlinks;
    case 5: return headers;
    case 6: return metadata;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: url = (java.lang.CharSequence)(value); break;
    case 2: content = (java.nio.ByteBuffer)(value); break;
    case 3: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
    case 4: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    case 5: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
    case 6: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'url' field.
   */
  public java.lang.CharSequence getUrl() {
    return url;
  }

  /**
   * Sets the value of the 'url' field.
   * @param value the value to set.
   */
  public void setUrl(java.lang.CharSequence value) {
    this.url = value;
    setDirty(1);
  }
  
  /**
   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUrlDirty(java.lang.CharSequence value) {
    return isDirty(1);
  }

  /**
   * Gets the value of the 'content' field.
   */
  public java.nio.ByteBuffer getContent() {
    return content;
  }

  /**
   * Sets the value of the 'content' field.
   * @param value the value to set.
   */
  public void setContent(java.nio.ByteBuffer value) {
    this.content = value;
    setDirty(2);
  }
  
  /**
   * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isContentDirty(java.nio.ByteBuffer value) {
    return isDirty(2);
  }

  /**
   * Gets the value of the 'parsedContent' field.
   */
  public java.util.List<java.lang.CharSequence> getParsedContent() {
    return parsedContent;
  }

  /**
   * Sets the value of the 'parsedContent' field.
   * @param value the value to set.
   */
  public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
    this.parsedContent = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper(value);
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
    return isDirty(3);
  }

  /**
   * Gets the value of the 'outlinks' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
    return outlinks;
  }

  /**
   * Sets the value of the 'outlinks' field.
   * @param value the value to set.
   */
  public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.outlinks = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(4);
  }
  
  /**
   * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(4);
  }

  /**
   * Gets the value of the 'headers' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
    return headers;
  }

  /**
   * Sets the value of the 'headers' field.
   * @param value the value to set.
   */
  public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.headers = value;
    setDirty(5);
  }
  
  /**
   * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    return isDirty(5);
  }

  /**
   * Gets the value of the 'metadata' field.
   */
  public org.apache.gora.examples.generated.Metadata getMetadata() {
    return metadata;
  }

  /**
   * Sets the value of the 'metadata' field.
   * @param value the value to set.
   */
  public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
    this.metadata = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
    return isDirty(6);
  }

  /** Creates a new WebPage RecordBuilder */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder() {
    return new org.apache.gora.examples.generated.WebPage.Builder();
  }
  
  /** Creates a new WebPage RecordBuilder by copying an existing Builder */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage.Builder other) {
    return new org.apache.gora.examples.generated.WebPage.Builder(other);
  }
  
  /** Creates a new WebPage RecordBuilder by copying an existing WebPage instance */
  public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage other) {
    return new org.apache.gora.examples.generated.WebPage.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for WebPage instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<WebPage>
    implements org.apache.avro.data.RecordBuilder<WebPage> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence url;
    private java.nio.ByteBuffer content;
    private java.util.List<java.lang.CharSequence> parsedContent;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
    private org.apache.gora.examples.generated.Metadata metadata;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.examples.generated.WebPage.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing WebPage instance */
    private Builder(org.apache.gora.examples.generated.WebPage other) {
            super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.content)) {
        this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[2].schema(), other.content);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.parsedContent)) {
        this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.parsedContent);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.outlinks)) {
        this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.outlinks);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.headers)) {
        this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[5].schema(), other.headers);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.metadata)) {
        this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[6].schema(), other.metadata);
        fieldSetFlags()[6] = true;
      }
    }

    /** Gets the value of the 'url' field */
    public java.lang.CharSequence getUrl() {
      return url;
    }
    
    /** Sets the value of the 'url' field */
    public org.apache.gora.examples.generated.WebPage.Builder setUrl(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.url = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'url' field has been set */
    public boolean hasUrl() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'url' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearUrl() {
      url = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'content' field */
    public java.nio.ByteBuffer getContent() {
      return content;
    }
    
    /** Sets the value of the 'content' field */
    public org.apache.gora.examples.generated.WebPage.Builder setContent(java.nio.ByteBuffer value) {
      validate(fields()[2], value);
      this.content = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'content' field has been set */
    public boolean hasContent() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'content' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearContent() {
      content = null;
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'parsedContent' field */
    public java.util.List<java.lang.CharSequence> getParsedContent() {
      return parsedContent;
    }
    
    /** Sets the value of the 'parsedContent' field */
    public org.apache.gora.examples.generated.WebPage.Builder setParsedContent(java.util.List<java.lang.CharSequence> value) {
      validate(fields()[3], value);
      this.parsedContent = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'parsedContent' field has been set */
    public boolean hasParsedContent() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'parsedContent' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearParsedContent() {
      parsedContent = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'outlinks' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
      return outlinks;
    }
    
    /** Sets the value of the 'outlinks' field */
    public org.apache.gora.examples.generated.WebPage.Builder setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[4], value);
      this.outlinks = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'outlinks' field has been set */
    public boolean hasOutlinks() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'outlinks' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearOutlinks() {
      outlinks = null;
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'headers' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
      return headers;
    }
    
    /** Sets the value of the 'headers' field */
    public org.apache.gora.examples.generated.WebPage.Builder setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[5], value);
      this.headers = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'headers' field has been set */
    public boolean hasHeaders() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'headers' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearHeaders() {
      headers = null;
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'metadata' field */
    public org.apache.gora.examples.generated.Metadata getMetadata() {
      return metadata;
    }
    
    /** Sets the value of the 'metadata' field */
    public org.apache.gora.examples.generated.WebPage.Builder setMetadata(org.apache.gora.examples.generated.Metadata value) {
      validate(fields()[6], value);
      this.metadata = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'metadata' field has been set */
    public boolean hasMetadata() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'metadata' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearMetadata() {
      metadata = null;
      fieldSetFlags()[6] = false;
      return this;
    }
    
    @Override
    public WebPage build() {
      try {
        WebPage record = new WebPage();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.content = fieldSetFlags()[2] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[2]);
        record.parsedContent = fieldSetFlags()[3] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[3]));
        record.outlinks = fieldSetFlags()[4] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[4]));
        record.headers = fieldSetFlags()[5] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[5]);
        record.metadata = fieldSetFlags()[6] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
  public WebPage.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public WebPage newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'url' field.
		   */
	  public java.lang.CharSequence getUrl() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'url' field.
		   * @param value the value to set.
	   */
	  public void setUrl(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUrlDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'content' field.
		   */
	  public java.nio.ByteBuffer getContent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'content' field.
		   * @param value the value to set.
	   */
	  public void setContent(java.nio.ByteBuffer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isContentDirty(java.nio.ByteBuffer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'parsedContent' field.
		   */
	  public java.util.List<java.lang.CharSequence> getParsedContent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'parsedContent' field.
		   * @param value the value to set.
	   */
	  public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'outlinks' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'outlinks' field.
		   * @param value the value to set.
	   */
	  public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'headers' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'headers' field.
		   * @param value the value to set.
	   */
	  public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'metadata' field.
		   */
	  public org.apache.gora.examples.generated.Metadata getMetadata() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metadata' field.
		   * @param value the value to set.
	   */
	  public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
}
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
        return EncoderFactory.get().binaryEncoder(getOrCreateOutputStream(), null);
        return EncoderFactory.get().jsonEncoder(schema, getOrCreateOutputStream());
        return DecoderFactory.get().binaryDecoder(getOrCreateInputStream(), null);
        return DecoderFactory.get().jsonDecoder(schema, getOrCreateInputStream());
    int fieldIndex = persistent.getSchema().getField(fieldName).pos();
    @SuppressWarnings("unchecked")
    int fieldIndex = persistent.getSchema().getField(fieldName).pos(); //.getIndexNamed(fieldName); throws org.apache.avro.AvroRuntimeException: Not a union:
  /**
   * Add our own serializer (obtained via the {@link PersistentSerialization} 
   * wrapper) to any other <code>io.serializations</code> which may be specified 
   * within existing Hadoop configuration.
   * 
   * @param conf the Hadoop configuration object
   * @param reuseObjects boolean parameter to reuse objects
   */
import org.apache.avro.specific.SpecificDatumReader;
* Hadoop deserializer using {@link SpecificDatumReader}
   implements Deserializer<Persistent> {
  private Class<? extends Persistent> persistentClass;
  private SpecificDatumReader<Persistent> datumReader;
  public PersistentDeserializer(Class<? extends Persistent> c, boolean reuseObjects) {
      datumReader = new SpecificDatumReader<Persistent>(schema);
  @Override
* supplies an input stream that is only valid until the end of one
* record serialization. Each time deserialize() is called, the IS
* is advanced to point to the right location, so we should not
* buffer the whole input stream at once.
*/
    decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);
  @Override
  public Persistent deserialize(Persistent persistent) throws IOException {
public class PersistentSerialization
implements Serialization<Persistent> {
  public Deserializer<Persistent> getDeserializer(Class<Persistent> c) {
  public Serializer<Persistent> getSerializer(Class<Persistent> c) {
 * or more contributor license agreements. See the NOTICE file
 * regarding copyright ownership. The ASF licenses this file
 * with the License. You may obtain a copy of the License at
 * http://www.apache.org/licenses/LICENSE-2.0
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
 * Hadoop serializer using Avro's {@link SpecificDatumWriter}
 * with {@link BinaryEncoder}.
public class PersistentSerializer implements Serializer<Persistent> {
  private SpecificDatumWriter<Persistent> datumWriter;
  private BinaryEncoder encoder;
    this.datumWriter = new SpecificDatumWriter<Persistent>();
  @Override
  /**
   * Open a connection for the {@link OutputStream}; should be
   * called before serialization occurs. N.B. the {@link PersistentSerializer#close()}
   * should be called 'finally' after serialization is complete.
   */
  @Override
    encoder = EncoderFactory.get().directBinaryEncoder(out, null);
  /**
   * Do the serialization of the {@link Persistent} object
   */
  public void serialize(Persistent persistent) throws IOException {

import org.apache.avro.Schema.Field;

import org.apache.gora.util.AvroUtils;
  try{
    long deletedRows = 0;
      Result<K,T> result = query.execute();

      while(result.next()) {
        if(delete(result.getKey()))
          deletedRows;
      }
      return 0;
    }
    catch(Exception e){
      return 0;
    }
    List<Field> otherFields = obj.getSchema().getFields();
    String[] otherFieldStrings = new String[otherFields.size()];
    for(int i = 0; i<otherFields.size(); i ){
      otherFieldStrings[i] = otherFields.get(i).name();
    }
    if(Arrays.equals(fields, otherFieldStrings)) { 
    T newObj = (T) AvroUtils.deepClonePersistent(obj); 
      for(int i = 0; i<otherFields.size(); i) {
      int index = otherFields.get(i).pos(); 
      newObj.put(index, obj.get(index));
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import java.util.List;

import org.apache.avro.Schema.Field;
import org.apache.avro.specific.SpecificRecord;

import org.apache.gora.persistency.Dirtyable;

* Objects that are persisted by Gora implements this interface.
*/
public interface Persistent extends SpecificRecord, Dirtyable {

  public static String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
* Clears the inner state of the object without any modification to the actual
* data on the data store. This method should be called before re-using the
* object to hold the data for another result.
*/

* Returns whether the field has been modified.
*
* @param fieldIndex
* the offset of the field in the object
* @return whether the field has been modified.
*/
* Returns whether the field has been modified.
*
* @param field
* the name of the field
* @return whether the field has been modified.
*/

* Sets all the fields of the object as dirty.
*/

* Sets the field as dirty.
*
* @param fieldIndex
* the offset of the field in the object
*/

* Sets the field as dirty.
*
* @param field
* the name of the field
*/

* Clears the field as dirty.
*
* @param fieldIndex
* the offset of the field in the object
*/

* Clears the field as dirty.
*
* @param field
* the name of the field
*/
* Get an object which can be used to mark this field as deleted (rather than
* state unknown, which is indicated by null).
*
* @return a tombstone.
*/
  public abstract Tombstone getTombstone();
* Get a list of fields from this persistent object's schema that are not
* managed by Gora.
*
* @return the unmanaged fields
*/
  public List<Field> getUnmanagedFields();
   * Constructs a new instance of the object by using appropriate builder. This
   * method is intended to be used by Gora framework.
   * 
   * @return a new instance of the object
  Persistent newInstance();
  /** Class of the key to be used */
  
  /** Class of the persistent objects to be stored */
  /** Constructor of the key */
  /** Object's key */
  
  /** Persistent object of class T */
  /** Flag to be used to determine if a key is persistent or not */
  /**
   * Default constructor for this class.
   * @param keyClass.
   * @param persistentClass
   */
    return keyClass.newInstance();
    return (T) persistent.newInstance();
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import java.util.Collection;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.gora.persistency.Dirtyable;
* Base classs implementing common functionality for Persistent classes.
*/
public abstract class PersistentBase extends SpecificRecordBase implements
    Persistent {
  public static class PersistentData extends SpecificData {
    private static final PersistentData INSTANCE = new PersistentData();
    public static PersistentData get() {
      return INSTANCE;
    public boolean equals(SpecificRecord obj1, SpecificRecord that) {
      if (that == obj1)
        return true; // identical object
      if (!(that instanceof SpecificRecord))
        return false; // not a record
      if (obj1.getClass() != that.getClass())
        return false; // not same schema
      return PersistentData.get().compare(obj1, that, obj1.getSchema(), true) == 0;
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirtyBytes.put(i, (byte) 0);
    for (Field field : getSchema().getFields()) {
      clearDirynessIfFieldIsDirtyable(field.pos());
  }

  private void clearDirynessIfFieldIsDirtyable(int fieldIndex) {
    if (fieldIndex == 0)
      return;
    Object value = get(fieldIndex);
    if (value instanceof Dirtyable) {
      ((Dirtyable) value).clearDirty();
  public void clearDirty(int fieldIndex) {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    currentByte = (byte) ((~(1 << bitOffset)) & currentByte);
    dirtyBytes.put(byteOffset, currentByte);
    clearDirynessIfFieldIsDirtyable(fieldIndex);

  @Override
  public void clearDirty(String field) {
    clearDirty(getSchema().getField(field).pos());
  }

  @Override
  public boolean isDirty() {
    boolean isSubRecordDirty = false;
    for (Field field : fields) {
      isSubRecordDirty = isSubRecordDirty || checkIfMutableFieldAndDirty(field);
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    boolean dirty = false;
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirty = dirty || dirtyBytes.get(i) != 0;
    }
    return isSubRecordDirty || dirty;
  }

  private boolean checkIfMutableFieldAndDirty(Field field) {
    if (field.pos() == 0)
      return false;
    switch (field.schema().getType()) {
    case RECORD:
    case MAP:
    case ARRAY:
      Object value = get(field.pos());
      return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
    case UNION:
      value = get(field.pos());
      return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
    default:
      break;
    }
    return false;
  }

  @Override
  public boolean isDirty(int fieldIndex) {
    Field field = getSchema().getFields().get(fieldIndex);
    boolean isSubRecordDirty = checkIfMutableFieldAndDirty(field);
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    return isSubRecordDirty || 0 != ((1 << bitOffset) & currentByte);
  }

  @Override
  public boolean isDirty(String fieldName) {
    Field field = getSchema().getField(fieldName);
    if(field == null){
      throw new IndexOutOfBoundsException
      ("Field " fieldName  " does not exist in this schema.");
    }
    return isDirty(field.pos());
  }

  @Override
  public void setDirty() {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    for (int i = 0; i < dirtyBytes.limit(); i) {
      dirtyBytes.put(i, (byte) -128);
    }
  }

  @Override
  public void setDirty(int fieldIndex) {
    ByteBuffer dirtyBytes = getDirtyBytes();
    assert (dirtyBytes.position() == 0);
    int byteOffset = fieldIndex / 8;
    int bitOffset = fieldIndex % 8;
    byte currentByte = dirtyBytes.get(byteOffset);
    currentByte = (byte) ((1 << bitOffset) | currentByte);
    dirtyBytes.put(byteOffset, currentByte);
  }

  @Override
  public void setDirty(String field) {
    setDirty(getSchema().getField(field).pos());
  }

  private ByteBuffer getDirtyBytes() {
    return (ByteBuffer) get(0);
  }

  @Override
  public void clear() {
    Collection<Field> unmanagedFields = getUnmanagedFields();
    for (Field field : getSchema().getFields()) {
      if (!unmanagedFields.contains(field))
        continue;
      put(field.pos(), PersistentData.get().deepCopy(field.schema(), PersistentData.get().getDefaultValue(field)));
    }
    clearDirty();
  }

  @Override
  public boolean equals(Object that) {
    if (that == this) {
      return true;
    } else if (that instanceof Persistent) {
      return PersistentData.get().equals(this, (SpecificRecord) that);
    } else {
      return false;
    }
  public List<Field> getUnmanagedFields(){
    List<Field> fields = getSchema().getFields();
    return fields.subList(1, fields.size());
  
      return keyClass.newInstance();
    try {
      return (T) persistentClass.newInstance();
    } catch (InstantiationException e) {
      throw new RuntimeException(e);
    } catch (IllegalAccessException e) {
      e.printStackTrace();
      throw new RuntimeException(e);
    }
  private void clearReadable() {
	  // TODO Auto-generated method stub
	  
	@Override
    return true;
    return true;
  public static final String SCHEMA_NAME = "schema.name";

      Properties properties = new Properties();
          .getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
          throws GoraException {
          ReflectionUtils.newInstance(dataStoreClass);
          throws GoraException {
          throws GoraException {
          = (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
          throws GoraException {
  /**
   * Looks for the <code>gora-&lt;classname&gt;-mapping.xml</code> as a resource 
   * on the classpath. This can however also be specified within the 
   * <code>gora.properties</code> file with the key 
   * <code>gora.&lt;classname&gt;.mapping.file=</code>.
   * @param properties which hold keys from which we can obtain values for datastore mappings.
   * @param store {@link org.apache.gora.store.DataStore} object to get the mapping for.
   * @param defaultValue default value for the <code>gora-&lt;classname&gt;-mapping.xml</code>
   * @return mappingFilename if one is located.
   * @throws IOException if there is a problem reading or obtaining the mapping file.
   */
import java.util.ArrayList;
import java.util.List;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter; 
import org.apache.gora.persistency.Persistent;
  protected SpecificDatumReader<T> datumReader;
  protected SpecificDatumWriter<T> datumWriter;
    datumReader = new SpecificDatumReader<T>(schema);
    datumWriter = new SpecificDatumWriter<T>(schema);
    return getFields();
  }
  
  protected String[] getFields() {
    List<Field> schemaFields = beanFactory.getCachedPersistent().getSchema().getFields();
    
    List<Field> list = new ArrayList<Field>();
    for (Field field : schemaFields) {
      if (!Persistent.DIRTY_BYTES_FIELD_NAME.equalsIgnoreCase(field.name())) {
        list.add(field);
      }
    }
    schemaFields = list;
    
    String[] fieldNames = new String[schemaFields.size()];
    for(int i = 0; i<fieldNames.length; i ){
      fieldNames[i] = schemaFields.get(i).name();
    }
    
    return fieldNames;
   * First the schema name in the {@link Configuration} is used. If null,
   * the schema name in the defined properties is returned. If null then
    String confSchemaName = getOrCreateConf().get("preferred.schema.name");
    if (confSchemaName != null) {
      return confSchemaName;
    }
      return schemaName;
      return mappingSchemaName;
    return StringUtils.getClassname(persistentClass);
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
 * An utility class for Avro related tasks
    for (Field field : fields) {



  public static Schema getSchema(Class<? extends Persistent> clazz)
      throws SecurityException, NoSuchFieldException, IllegalArgumentException,
      IllegalAccessException {

    java.lang.reflect.Field field = clazz.getDeclaredField("SCHEMA$");

  /**
   * Return the field names from a persistent object
   * 
   * @param persistent
   *          the persistent object to get the fields names from
   * @return the field names
   */
  public static String[] getPersistentFieldNames(Persistent persistent) {
    return getSchemaFieldNames(persistent.getSchema());
  }

  /**
   * Return the field names from a schema object
   * 
   * @param persistent
   *          the persistent object to get the fields names from
   * @return the field names
   */
  public static String[] getSchemaFieldNames(Schema schema) {
    List<Field> fields = schema.getFields();
    String[] fieldNames = new String[fields.size() - 1];
    for (int i = 0; i < fieldNames.length; i) {
      fieldNames[i] = fields.get(i  1).name();
    }
    return fieldNames;
  }

  public static <T extends Persistent> T deepClonePersistent(T persistent) {
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    BinaryEncoder enc = EncoderFactory.get().binaryEncoder(bos, null);
    SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<Persistent>(
        persistent.getSchema());
    try {
      writer.write(persistent, enc);
    } catch (IOException e) {
      throw new RuntimeException(
          "Unable to serialize avro object to byte buffer - "
               "please report this issue to the Gora bugtracker "
               "or your administrator.");
    }
    byte[] value = bos.toByteArray();
    Decoder dec = DecoderFactory.get().binaryDecoder(value, null);
    @SuppressWarnings("unchecked")
    SpecificDatumReader<T> reader = new SpecificDatumReader<T>(
        (Class<T>) persistent.getClass());
    try {
      return reader.read(null, dec);
    } catch (IOException e) {
      throw new RuntimeException(
          "Unable to deserialize avro object from byte buffer - "
               "please report this issue to the Gora bugtracker "
               "or your administrator.");
    }

  }

import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
  public static <T> T fromBytes( byte[] val, Schema schema
      , SpecificDatumReader<T> datumReader, T object)
      return (T)Enum.valueOf(ReflectData.get().getClass(schema), symbol);
    case STRING:  return (T)new Utf8(toString(val));
    case BYTES:   return (T)ByteBuffer.wrap(val);
    case INT:     return (T)Integer.valueOf(bytesToVint(val));
    case LONG:    return (T)Long.valueOf(bytesToVlong(val));
    case FLOAT:   return (T)Float.valueOf(toFloat(val));
    case DOUBLE:  return (T)Double.valueOf(toDouble(val));
    case BOOLEAN: return (T)Boolean.valueOf(val[0] != 0);
    case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, schema, (SpecificRecord)object);
  @SuppressWarnings("unchecked")
  public static <T> byte[] toBytes(T o, Schema schema
      , SpecificDatumWriter<T> datumWriter)
    case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, schema, (SpecificRecord)o);
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter; 
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.util.ByteBufferInputStream;
import org.apache.avro.util.ByteBufferOutputStream;
  public static<T extends SpecificRecord> void serialize(OutputStream os,
      SpecificDatumWriter<T> datumWriter, Schema schema, T object)
    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
    datumWriter.write(object, encoder);
  public static<T> void serialize(OutputStream os,
      SpecificDatumWriter<T> datumWriter, Schema schema, T object)
      throws IOException {

    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
    datumWriter.write(object, encoder);
    encoder.flush();
  }
  
  /**
   * Serializes the field object using the datumWriter.
   */
  public static<T extends SpecificRecord> byte[] serialize(SpecificDatumWriter<T> datumWriter
      , Schema schema, T object) throws IOException {
  /**
   * Serializes the field object using the datumWriter.
   */
  public static<T> byte[] serialize(SpecificDatumWriter<T> datumWriter
      , Schema schema, T object) throws IOException {
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    serialize(os, datumWriter, schema, object);
    return os.toByteArray();
  }
  
  public static<K, T extends SpecificRecord> T deserialize(InputStream is,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
    decoder = DecoderFactory.get().binaryDecoder(is, decoder);
    return (T)datumReader.read(object, decoder);
  public static<K, T extends SpecificRecord> T deserialize(byte[] bytes,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
    decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
    return (T)datumReader.read(object, decoder);
   * Deserializes the field object using the datumReader.
  public static<K, T> T deserialize(byte[] bytes,
      SpecificDatumReader<T> datumReader, Schema schema, T object)
      throws IOException {
    decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
    return (T)datumReader.read(object, decoder);
  
import java.lang.reflect.Method;

import org.apache.avro.specific.SpecificRecordBuilderBase;
import org.apache.gora.persistency.Persistent;
  
  public static <T extends Persistent> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) throws SecurityException
    , NoSuchMethodException, IllegalArgumentException, IllegalAccessException, InvocationTargetException {
    return (SpecificRecordBuilderBase<T>) clazz.getMethod("newBuilder").invoke(null);
  }
  
import org.apache.gora.persistency.Tombstone;
  public Tombstone getTombstone() {
    return new Tombstone(){};
  public Persistent newInstance() {
    return new MockPersistent();
      preferredSchema = properties.getProperty(PREF_SCH_NAME);
      dynamoDBClient = getClient(properties.getProperty(CLI_TYP_PROP),(AWSCredentials)getConf());
      dynamoDBClient.setEndpoint(properties.getProperty(ENDPOINT_PROP));
      consistency = properties.getProperty(CONSISTENCY_READS);
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


   * {@inheritDoc} Serializes the Persistent data and saves in HBase. Topmost
   * fields of the record are persisted in "raw" format (not avro serialized).
   * This behavior happens in maps and arrays too.
   * ["null","type"] type (a.k.a. optional field) is persisted like as if it is
   * ["type"], but the column get deleted if value==null (so value read after
   * will be null).
   * @param persistent
   *          Record to be persisted in HBase
    try {
      List<Field> fields = schema.getFields();
      for (int i = 1; i < fields.size(); i) {
        if (!persistent.isDirty(i)) {
        Field field = fields.get(i);
          throw new RuntimeException("HBase mapping for field ["
               persistent.getClass().getName()  "#"  field.name()
               "] not found. Wrong gora-hbase-mapping.xml?");
        addPutsAndDeletes(put, delete, o, field.schema().getType(),
            field.schema(), hcol, hcol.getQualifier());
      if (put.size() > 0) {
      if (delete.size() > 0) {
        table.delete(delete);
        table.delete(delete); // HBase sometimes does not delete arbitrarily
    } catch (IOException ex2) {
  private void addPutsAndDeletes(Put put, Delete delete, Object o, Type type,
      Schema schema, HBaseColumn hcol, byte[] qualifier) throws IOException {
    switch (type) {
    case UNION:
      if (isNullable(schema) && o == null) {
        if (qualifier == null) {
          delete.deleteFamily(hcol.getFamily());
        } else {
          delete.deleteColumn(hcol.getFamily(), qualifier);
        }
      } else {
//        int index = GenericData.get().resolveUnion(schema, o);
        int index = getResolvedUnionIndex(schema);
        if (index > 1) {  //if more than 2 type in union, serialize directly for now
          byte[] serializedBytes = toBytes(o, schema);
          put.add(hcol.getFamily(), qualifier, serializedBytes);
        } else {
          Schema resolvedSchema = schema.getTypes().get(index);
          addPutsAndDeletes(put, delete, o, resolvedSchema.getType(),
              resolvedSchema, hcol, qualifier);
        }
      }
      break;
    case MAP:
      // if it's a map that has been modified, then the content should be replaced by the new one
      // This is because we don't know if the content has changed or not.
      if (qualifier == null) {
        delete.deleteFamily(hcol.getFamily());
      } else {
        delete.deleteColumn(hcol.getFamily(), qualifier);
      }
      @SuppressWarnings({ "rawtypes", "unchecked" })
      Set<Entry> set = ((Map) o).entrySet();
      for (@SuppressWarnings("rawtypes") Entry entry : set) {
        byte[] qual = toBytes(entry.getKey());
        addPutsAndDeletes(put, delete, entry.getValue(), schema.getValueType()
            .getType(), schema.getValueType(), hcol, qual);
      }
      break;
    case ARRAY:
      List<?> array = (List<?>) o;
      int j = 0;
      for (Object item : array) {
        addPutsAndDeletes(put, delete, item, schema.getElementType().getType(),
            schema.getElementType(), hcol, Bytes.toBytes(j));
      }
      break;
    default:
      byte[] serializedBytes = toBytes(o, schema);
      put.add(hcol.getFamily(), qualifier, serializedBytes);
      break;
    }
  }

  private boolean isNullable(Schema unionSchema) {
    for (Schema innerSchema : unionSchema.getTypes()) {
      if (innerSchema.getType().equals(Schema.Type.NULL)) {
        return true;
      }
    }
    return false;
  }

      boolean isAllFields = Arrays.equals(fields, getFields());
      addFamilyOrColumn(get, col, fieldSchema);
  private void addFamilyOrColumn(Get get, HBaseColumn col, Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(get, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      get.addFamily(col.family);
      break;
    default:
      get.addColumn(col.family, col.qualifier);
      break;
    }
  }

  private void addFields(Scan scan, Query<K, T> query) throws IOException {
      addFamilyOrColumn(scan, col, fieldSchema);
  private void addFamilyOrColumn(Scan scan, HBaseColumn col, Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(scan, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      scan.addFamily(col.family);
      break;
    default:
      scan.addColumn(col.family, col.qualifier);
      break;
    }
  }

  // TODO: HBase Get, Scan, Delete should extend some common interface with
  // addFamily, etc
  private void addFields(Delete delete, Query<K, T> query)    throws IOException {
      addFamilyOrColumn(delete, col, fieldSchema);
    }
  }

  private void addFamilyOrColumn(Delete delete, HBaseColumn col,
      Schema fieldSchema) {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      Schema resolvedSchema = fieldSchema.getTypes().get(index);
      addFamilyOrColumn(delete, col, resolvedSchema);
      break;
    case MAP:
    case ARRAY:
      delete.deleteFamily(col.family);
      break;
    default:
      delete.deleteColumn(col.family, col.qualifier);
      break;
      setField(result,persistent, col, field, fieldSchema);
    }
    persistent.clearDirty();
    return persistent;
  }

  private void setField(Result result, T persistent, HBaseColumn col,
      Field field, Schema fieldSchema) throws IOException {
    switch (fieldSchema.getType()) {
    case UNION:
      int index = getResolvedUnionIndex(fieldSchema);
      if (index > 1) { //if more than 2 type in union, deserialize directly for now
        byte[] val = result.getValue(col.getFamily(), col.getQualifier());
        if (val == null) {
          return;
        }
        setField(persistent, field, val);
      } else {
        Schema resolvedSchema = fieldSchema.getTypes().get(index);
        setField(result, persistent, col, field, resolvedSchema);
      }
      break;
    case MAP:
      NavigableMap<byte[], byte[]> qualMap = result.getNoVersionMap().get(
          col.getFamily());
      if (qualMap == null) {
        return;
      }
      Schema valueSchema = fieldSchema.getValueType();
      Map<Utf8, Object> map = new HashMap<Utf8, Object>();
      for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
        map.put(new Utf8(Bytes.toString(e.getKey())),
            fromBytes(valueSchema, e.getValue()));
      }
      setField(persistent, field, map);
      break;
    case ARRAY:
      qualMap = result.getFamilyMap(col.getFamily());
      if (qualMap == null) {
        return;
      }
      valueSchema = fieldSchema.getElementType();
      ArrayList<Object> arrayList = new ArrayList<Object>();
      DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<Object>(arrayList);
      for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
        dirtyListWrapper.add(fromBytes(valueSchema, e.getValue()));
      }
      setField(persistent, field, arrayList);
      break;
    default:
      byte[] val = result.getValue(col.getFamily(), col.getQualifier());
      if (val == null) {
        return;
      }
      setField(persistent, field, val);
      break;
    }
  }

  //TODO temporary solution, has to be changed after implementation of saving the index of union type
  private int getResolvedUnionIndex(Schema unionScema) {
    if (unionScema.getTypes().size() == 2) {

      // schema [type0, type1]
      Type type0 = unionScema.getTypes().get(0).getType();
      Type type1 = unionScema.getTypes().get(1).getType();

      // Check if types are different and there's a "null", like ["null","type"]
      // or ["type","null"]
      if (!type0.equals(type1)
          && (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {

        if (type0.equals(Schema.Type.NULL))
          return 1;
        else
          return 0;
    return 2;
    persistent.put(field.pos(), new DirtyMapWrapper(map));
  @SuppressWarnings({ "rawtypes", "unchecked" })
  private void setField(T persistent, Field field, List list) {
    persistent.put(field.pos(), new DirtyListWrapper(list));
}
import java.util.Map;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.RowMutations;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;

  @Override
  public void batch(List<? extends Row> actions, Object[] results)
      throws IOException, InterruptedException {
    // TODO Auto-generated method stub
    getTable().batch(actions, results);
    
  }

  @Override
  public Object[] batch(List<? extends Row> actions) throws IOException,
      InterruptedException {
    // TODO Auto-generated method stub
    return getTable().batch(actions);
  }

  @Override
  public void mutateRow(RowMutations rm) throws IOException {
    // TODO Auto-generated method stub
    
  }

  @Override
  public Result append(Append append) throws IOException {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol> T coprocessorProxy(Class<T> protocol,
      byte[] row) {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol, R> Map<byte[], R> coprocessorExec(
      Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable)
      throws IOException, Throwable {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends CoprocessorProtocol, R> void coprocessorExec(
      Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable,
      Callback<R> callback) throws IOException, Throwable {
    // TODO Auto-generated method stub
    
  }

  @Override
  public void setAutoFlush(boolean autoFlush) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail) {
    // TODO Auto-generated method stub
    
  }

  @Override
  public long getWriteBufferSize() {
    // TODO Auto-generated method stub
    return 0;
  }

  @Override
  public void setWriteBufferSize(long writeBufferSize) throws IOException {
    // TODO Auto-generated method stub
    
  }
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.io.EncoderFactory;


  private static ThreadLocal<ByteArrayOutputStream> outputStream =
      new ThreadLocal<ByteArrayOutputStream>();
  
  public static final ThreadLocal<BinaryEncoder> encoders =
      new ThreadLocal<BinaryEncoder>();
  public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = 
      new ConcurrentHashMap<String, SpecificDatumReader<?>>();
     
  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = 
      new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
   * Deserializes an array of bytes matching the given schema to the proper basic 
   * (enum, Utf8,...) or complex type (Persistent/Record).
  @SuppressWarnings({ "rawtypes" })
      // For UNION schemas, must use a specific SpecificDatumReader
      String schemaId = schema.getType().equals(Schema.Type.UNION) ? String.valueOf(schema.hashCode()) : schema.getFullName();      
      
      SpecificDatumReader<?> reader = (SpecificDatumReader<?>)readerMap.get(schemaId);
      if (reader == null) {
        reader = new SpecificDatumReader(schema);// ignore dirty bits
        SpecificDatumReader localReader=null;
        if((localReader=readerMap.putIfAbsent(schemaId, reader))!=null) {
          reader = localReader;
      BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(val, null);
      return reader.read(null, decoder);
    case STRING:  return Bytes.toBytes(((CharSequence)o).toString()); // TODO: maybe ((Utf8)o).getBytes(); ?
      SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap.get(schema.getFullName());
      if (writer == null) {
        writer = new SpecificDatumWriter(schema);// ignore dirty bits
        writerMap.put(schema.getFullName(),writer);

      BinaryEncoder encoderFromCache = encoders.get();
      ByteArrayOutputStream bos = new ByteArrayOutputStream();
      outputStream.set(bos);
      BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(bos, null);
      if (encoderFromCache == null) {

      ByteArrayOutputStream os = outputStream.get();

      writer.write(o, encoder);
    //htu.getConfiguration().set("hbase.zookeeper.quorum", "localhost");
    //htu.getConfiguration().setInt("hbase.zookeeper.property.clientPort", 2181);
    
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;

  private static final Logger LOG = LoggerFactory.getLogger(SolrStore.class);


  // protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";

  /**
   * Default schema index with value "0" used when AVRO Union data types are
   * stored
   */
  public static int DEFAULT_UNION_SCHEMA = 0;

  /*
   * Create a threadlocal map for the datum readers and writers, because they
   * are not thread safe, at least not before Avro 1.4.0 (See AVRO-650). When
   * they are thread safe, it is possible to maintain a single reader and writer
   * pair for every schema, instead of one for every thread.
   */

  public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<String, SpecificDatumReader<?>>();

  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<String, SpecificDatumWriter<?>>();

  public void initialize(Class<K> keyClass, Class<T> persistentClass,
      Properties properties) {
    super.initialize(keyClass, persistentClass, properties);
      String mappingFile = DataStoreFactory.getMappingFile(properties, this,
          DEFAULT_MAPPING_FILE);
      mapping = readMapping(mappingFile);
    } catch (IOException e) {
      LOG.error(e.getMessage());
      LOG.error(e.getStackTrace().toString());
    solrServerUrl = DataStoreFactory.findProperty(properties, this,
        SOLR_URL_PROPERTY, null);
    solrConfig = DataStoreFactory.findProperty(properties, this,
        SOLR_CONFIG_PROPERTY, null);
    solrSchema = DataStoreFactory.findProperty(properties, this,
        SOLR_SCHEMA_PROPERTY, null);
    LOG.info("Using Solr server at "  solrServerUrl);
    adminServer = new HttpSolrServer(solrServerUrl);
    server = new HttpSolrServer(solrServerUrl  "/"  mapping.getCoreName());
    if (autoCreateSchema) {
    String batchSizeString = DataStoreFactory.findProperty(properties, this,
        SOLR_BATCH_SIZE_PROPERTY, null);
    if (batchSizeString != null) {
        batchSize = Integer.parseInt(batchSizeString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid batch size '"  batchSizeString  "', using default "
             DEFAULT_BATCH_SIZE);
    batch = new ArrayList<SolrInputDocument>(batchSize);
    String commitWithinString = DataStoreFactory.findProperty(properties, this,
        SOLR_COMMIT_WITHIN_PROPERTY, null);
    if (commitWithinString != null) {
        commitWithin = Integer.parseInt(commitWithinString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid commit within '"  commitWithinString
             "', using default "  DEFAULT_COMMIT_WITHIN);
    String resultsSizeString = DataStoreFactory.findProperty(properties, this,
        SOLR_RESULTS_SIZE_PROPERTY, null);
    if (resultsSizeString != null) {
        resultsSize = Integer.parseInt(resultsSizeString);
      } catch (NumberFormatException nfe) {
        LOG.warn("Invalid results size '"  resultsSizeString
             "', using default "  DEFAULT_RESULTS_SIZE);
  private SolrMapping readMapping(String filename) throws IOException {
      Document doc = builder.build(getClass().getClassLoader()
          .getResourceAsStream(filename));
      List<Element> classes = doc.getRootElement().getChildren("class");
      for (Element classElement : classes) {
        if (classElement.getAttributeValue("keyClass").equals(
            keyClass.getCanonicalName())
            && classElement.getAttributeValue("name").equals(
                persistentClass.getCanonicalName())) {
          String tableName = getSchemaName(
              classElement.getAttributeValue("table"), persistentClass);
          map.setCoreName(tableName);
          Element primaryKeyEl = classElement.getChild("primarykey");
          map.setPrimaryKey(primaryKeyEl.getAttributeValue("column"));
          List<Element> fields = classElement.getChildren("field");
          for (Element field : fields) {
            String fieldName = field.getAttributeValue("name");
            String columnName = field.getAttributeValue("column");
            map.addField(fieldName, columnName);
    } catch (Exception ex) {
      throw new IOException(ex);
      if (!schemaExists())
        CoreAdminRequest.createCore(mapping.getCoreName(),
            mapping.getCoreName(), adminServer, solrConfig, solrSchema);
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
      server.deleteByQuery("*:*");
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
      server.deleteByQuery("*:*");
    } catch (Exception e) {
      // ignore?
      // LOG.error(e.getMessage());
      // LOG.error(e.getStackTrace().toString());
      CoreAdminRequest.unloadCore(mapping.getCoreName(), adminServer);
    } catch (Exception e) {
      if (e.getMessage().contains("No such core")) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
      CoreAdminResponse rsp = CoreAdminRequest.getStatus(mapping.getCoreName(),
          adminServer);
      exists = rsp.getUptime(mapping.getCoreName()) != null;
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  private static final String toDelimitedString(String[] arr, String sep) {
    if (arr == null || arr.length == 0) {
    for (int i = 0; i < arr.length; i) {
      if (i > 0)
        sb.append(sep);
      sb.append(arr[i]);
  public static String escapeQueryKey(String key) {
    if (key == null) {
    for (int i = 0; i < key.length(); i) {
      char c = key.charAt(i);
      switch (c) {
      case ':':
      case '*':
        sb.append("\\"  c);
        break;
      default:
        sb.append(c);
  public T get(K key, String[] fields) {
    params.set(CommonParams.QT, "/get");
    params.set(CommonParams.FL, toDelimitedString(fields, ","));
    params.set("id", key.toString());
      QueryResponse rsp = server.query(params);
      Object o = rsp.getResponse().get("doc");
      if (o == null) {
      return newInstance((SolrDocument) o, fields);
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public T newInstance(SolrDocument doc, String[] fields) throws IOException {
    if (fields == null) {
      fields = fieldMap.keySet().toArray(new String[fieldMap.size()]);
    for (String f : fields) {
      Field field = fieldMap.get(f);
      if (pk.equals(f)) {
        sf = mapping.getSolrField(f);
      Object sv = doc.get(sf);
      if (sv == null) {

      Object v = deserializeFieldValue(field, fieldSchema, sv, persistent);
      persistent.put(field.pos(), v);
      persistent.setDirty(field.pos());

  @SuppressWarnings("rawtypes")
  private SpecificDatumReader getDatumReader(String schemaId, Schema fieldSchema) {
    SpecificDatumReader<?> reader = (SpecificDatumReader<?>) readerMap
        .get(schemaId);
    if (reader == null) {
      reader = new SpecificDatumReader(fieldSchema);// ignore dirty bits
      SpecificDatumReader localReader = null;
      if ((localReader = readerMap.putIfAbsent(schemaId, reader)) != null) {
        reader = localReader;
      }
    }
    return reader;
  }

  @SuppressWarnings("rawtypes")
  private SpecificDatumWriter getDatumWriter(String schemaId, Schema fieldSchema) {
    SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap
        .get(schemaId);
    if (writer == null) {
      writer = new SpecificDatumWriter(fieldSchema);// ignore dirty bits
      writerMap.put(schemaId, writer);
    }

    return writer;
  }

  @SuppressWarnings("unchecked")
  private Object deserializeFieldValue(Field field, Schema fieldSchema,
      Object solrValue, T persistent) throws IOException {
    Object fieldValue = null;
    switch (fieldSchema.getType()) {
    case MAP:
    case ARRAY:
    case RECORD:
      @SuppressWarnings("rawtypes")
      SpecificDatumReader reader = getDatumReader(fieldSchema.getFullName(),
          fieldSchema);
      fieldValue = IOUtils.deserialize((byte[]) solrValue, reader, fieldSchema,
          persistent.get(field.pos()));
      break;
    case ENUM:
      fieldValue = AvroUtils.getEnumValue(fieldSchema, (String) solrValue);
      break;
    case FIXED:
      throw new IOException("???");
      // break;
    case BYTES:
      fieldValue = ByteBuffer.wrap((byte[]) solrValue);
      break;
    case STRING:
      fieldValue = new Utf8(solrValue.toString());
      break;
    case UNION:
      if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
        // schema [type0, type1]
        Type type0 = fieldSchema.getTypes().get(0).getType();
        Type type1 = fieldSchema.getTypes().get(1).getType();

        // Check if types are different and there's a "null", like
        // ["null","type"] or ["type","null"]
        if (!type0.equals(type1)) {
          if (type0.equals(Schema.Type.NULL))
            fieldSchema = fieldSchema.getTypes().get(1);
          else
            fieldSchema = fieldSchema.getTypes().get(0);
        } else {
          fieldSchema = fieldSchema.getTypes().get(0);
        }
        fieldValue = deserializeFieldValue(field, fieldSchema, solrValue,
            persistent);
      } else {
        @SuppressWarnings("rawtypes")
        SpecificDatumReader unionReader = getDatumReader(
            String.valueOf(fieldSchema.hashCode()), fieldSchema);
        fieldValue = IOUtils.deserialize((byte[]) solrValue, unionReader,
            fieldSchema, persistent.get(field.pos()));
        break;
      }
      break;
    default:
      fieldValue = solrValue;
    }
    return fieldValue;
  }

  public void put(K key, T persistent) {
    if (!persistent.isDirty()) {
    doc.addField(mapping.getPrimaryKey(), key);
    for (Field field : fields) {
      String sf = mapping.getSolrField(field.name());
      if (sf == null) {
      Object v = persistent.get(field.pos());
      if (v == null) {
      v = serializeFieldValue(fieldSchema, v);
      doc.addField(sf, v);

    LOG.info("DOCUMENT: "  doc);
    batch.add(doc);
    if (batch.size() >= batchSize) {
        add(batch, commitWithin);
      } catch (Exception e) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
  @SuppressWarnings("unchecked")
  private Object serializeFieldValue(Schema fieldSchema, Object fieldValue) {
    switch (fieldSchema.getType()) {
    case MAP:
    case ARRAY:
    case RECORD:
      byte[] data = null;
      try {
        @SuppressWarnings("rawtypes")
        SpecificDatumWriter writer = getDatumWriter(fieldSchema.getFullName(),
            fieldSchema);
        data = IOUtils.serialize(writer, fieldSchema, fieldValue);
      } catch (IOException e) {
        LOG.error(e.getMessage(), e.getStackTrace().toString());
      }
      fieldValue = data;
      break;
    case BYTES:
      fieldValue = ((ByteBuffer) fieldValue).array();
      break;
    case ENUM:
    case STRING:
      fieldValue = fieldValue.toString();
      break;
    case UNION:
      // If field's schema is null and one type, we do undertake serialization.
      // All other types are serialized.
      if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
        int schemaPos = getUnionSchema(fieldValue, fieldSchema);
        Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
        fieldValue = serializeFieldValue(unionSchema, fieldValue);
      } else {
        byte[] serilazeData = null;
        try {
          @SuppressWarnings("rawtypes")
          SpecificDatumWriter writer = getDatumWriter(
              String.valueOf(fieldSchema.hashCode()), fieldSchema);
          serilazeData = IOUtils.serialize(writer, fieldSchema, fieldValue);
        } catch (IOException e) {
          LOG.error(e.getMessage(), e.getStackTrace().toString());
        }
        fieldValue = serilazeData;
      }
      break;
    default:
      // LOG.error("Unknown field type: "  fieldSchema.getType());
      break;
    }
    return fieldValue;
  }

  private boolean isNullable(Schema unionSchema) {
    for (Schema innerSchema : unionSchema.getTypes()) {
      if (innerSchema.getType().equals(Schema.Type.NULL)) {
        return true;
      }
    }
    return false;
  }

  /**
   * Given an object and the object schema this function obtains, from within
   * the UNION schema, the position of the type used. If no data type can be
   * inferred then we return a default value of position 0.
   * 
   * @param pValue
   * @param pUnionSchema
   * @return the unionSchemaPosition.
   */
  private int getUnionSchema(Object pValue, Schema pUnionSchema) {
    int unionSchemaPos = 0;
    Iterator<Schema> it = pUnionSchema.getTypes().iterator();
    while (it.hasNext()) {
      Type schemaType = it.next().getType();
      if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
        return unionSchemaPos;
      else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
        return unionSchemaPos;
      else if (pValue instanceof Integer && schemaType.equals(Type.INT))
        return unionSchemaPos;
      else if (pValue instanceof Long && schemaType.equals(Type.LONG))
        return unionSchemaPos;
      else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
        return unionSchemaPos;
      else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
        return unionSchemaPos;
      else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
        return unionSchemaPos;
      else if (pValue instanceof Map && schemaType.equals(Type.MAP))
        return unionSchemaPos;
      else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
        return unionSchemaPos;
      else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
        return unionSchemaPos;
      unionSchemaPos;
    }
    // if we weren't able to determine which data type it is, then we return the
    // default
    return DEFAULT_UNION_SCHEMA;
  }

  public boolean delete(K key) {
      UpdateResponse rsp = server.deleteByQuery(keyField  ":"
           escapeQueryKey(key.toString()));
      LOG.info(rsp.toString());
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public long deleteByQuery(Query<K, T> query) {
    String q = ((SolrQuery<K, T>) query).toSolrQuery();
      UpdateResponse rsp = server.deleteByQuery(q);
      LOG.info(rsp.toString());
    } catch (Exception e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
  public Result<K, T> execute(Query<K, T> query) {
      return new SolrResult<K, T>(this, query, server, resultsSize);
    } catch (IOException e) {
      LOG.error(e.getMessage(), e.getStackTrace().toString());
    return new SolrQuery<K, T>(this);
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
      if (batch.size() > 0) {
        add(batch, commitWithin);
    } catch (Exception e) {
    // flush();

  private void add(ArrayList<SolrInputDocument> batch, int commitWithin)
      throws SolrServerException, IOException {
      server.add(batch);
      server.commit(false, true, true);
      server.add(batch, commitWithin);
  }
      CharSequence url = pageview.getUrl();
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.tutorial.log.generated;  
public class MetricDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}]}");
  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
  private java.lang.CharSequence metricDimension;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return metricDimension;
    case 2: return timestamp;
    case 3: return metric;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: metricDimension = (java.lang.CharSequence)(value); break;
    case 2: timestamp = (java.lang.Long)(value); break;
    case 3: metric = (java.lang.Long)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  /**
   * Gets the value of the 'metricDimension' field.
   */
  public java.lang.CharSequence getMetricDimension() {
    return metricDimension;

  /**
   * Sets the value of the 'metricDimension' field.
   * @param value the value to set.
   */
  public void setMetricDimension(java.lang.CharSequence value) {
    this.metricDimension = value;
    setDirty(1);
  
  /**
   * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
    return isDirty(1);

  /**
   * Gets the value of the 'timestamp' field.
   */
  public java.lang.Long getTimestamp() {
    return timestamp;

  /**
   * Sets the value of the 'timestamp' field.
   * @param value the value to set.
   */
  public void setTimestamp(java.lang.Long value) {
    this.timestamp = value;
    setDirty(2);
  
  /**
   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isTimestampDirty(java.lang.Long value) {
    return isDirty(2);

  /**
   * Gets the value of the 'metric' field.
   */
  public java.lang.Long getMetric() {
    return metric;
  }

  /**
   * Sets the value of the 'metric' field.
   * @param value the value to set.
   */
  public void setMetric(java.lang.Long value) {
    this.metric = value;
    setDirty(3);
  }
  
  /**
   * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isMetricDirty(java.lang.Long value) {
    return isDirty(3);
  }

  /** Creates a new MetricDatum RecordBuilder */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder() {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder();
  }
  
  /** Creates a new MetricDatum RecordBuilder by copying an existing Builder */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
  }
  
  /** Creates a new MetricDatum RecordBuilder by copying an existing MetricDatum instance */
  public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
    return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for MetricDatum instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MetricDatum>
    implements org.apache.avro.data.RecordBuilder<MetricDatum> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence metricDimension;
    private long timestamp;
    private long metric;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing MetricDatum instance */
    private Builder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
            super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.metricDimension)) {
        this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.metricDimension);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.metric)) {
        this.metric = (java.lang.Long) data().deepCopy(fields()[3].schema(), other.metric);
        fieldSetFlags()[3] = true;
      }
    }

    /** Gets the value of the 'metricDimension' field */
    public java.lang.CharSequence getMetricDimension() {
      return metricDimension;
    }
    
    /** Sets the value of the 'metricDimension' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetricDimension(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.metricDimension = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'metricDimension' field has been set */
    public boolean hasMetricDimension() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'metricDimension' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetricDimension() {
      metricDimension = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'timestamp' field */
    public java.lang.Long getTimestamp() {
      return timestamp;
    }
    
    /** Sets the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setTimestamp(long value) {
      validate(fields()[2], value);
      this.timestamp = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'timestamp' field has been set */
    public boolean hasTimestamp() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearTimestamp() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'metric' field */
    public java.lang.Long getMetric() {
      return metric;
    }
    
    /** Sets the value of the 'metric' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetric(long value) {
      validate(fields()[3], value);
      this.metric = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'metric' field has been set */
    public boolean hasMetric() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'metric' field */
    public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetric() {
      fieldSetFlags()[3] = false;
      return this;
    }
    
    @Override
    public MetricDatum build() {
      try {
        MetricDatum record = new MetricDatum();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
        record.metricDimension = fieldSetFlags()[1] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
        record.metric = fieldSetFlags()[3] ? this.metric : (java.lang.Long) defaultValue(fields()[3]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }
  
  public MetricDatum.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public MetricDatum newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'metricDimension' field.
		   */
	  public java.lang.CharSequence getMetricDimension() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metricDimension' field.
		   * @param value the value to set.
	   */
	  public void setMetricDimension(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'timestamp' field.
		   */
	  public java.lang.Long getTimestamp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'timestamp' field.
		   * @param value the value to set.
	   */
	  public void setTimestamp(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isTimestampDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'metric' field.
		   */
	  public java.lang.Long getMetric() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'metric' field.
		   * @param value the value to set.
	   */
	  public void setMetric(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isMetricDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
  }
  
}
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.tutorial.log.generated;  
public class Pageview extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AAA=\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}]}");
  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[2]);
  private java.lang.CharSequence url;
  private java.lang.CharSequence ip;
  private java.lang.CharSequence httpMethod;
  private java.lang.CharSequence referrer;
  private java.lang.CharSequence userAgent;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return __g__dirty;
    case 1: return url;
    case 2: return timestamp;
    case 3: return ip;
    case 4: return httpMethod;
    case 5: return httpStatusCode;
    case 6: return responseSize;
    case 7: return referrer;
    case 8: return userAgent;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
  
  // Used by DatumReader.  Applications should not call. 
  public void put(int field$, java.lang.Object value) {
    switch (field$) {
    case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
    case 1: url = (java.lang.CharSequence)(value); break;
    case 2: timestamp = (java.lang.Long)(value); break;
    case 3: ip = (java.lang.CharSequence)(value); break;
    case 4: httpMethod = (java.lang.CharSequence)(value); break;
    case 5: httpStatusCode = (java.lang.Integer)(value); break;
    case 6: responseSize = (java.lang.Integer)(value); break;
    case 7: referrer = (java.lang.CharSequence)(value); break;
    case 8: userAgent = (java.lang.CharSequence)(value); break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  /**
   * Gets the value of the 'url' field.
   */
  public java.lang.CharSequence getUrl() {
    return url;

  /**
   * Sets the value of the 'url' field.
   * @param value the value to set.
   */
  public void setUrl(java.lang.CharSequence value) {
    this.url = value;
    setDirty(1);
  
  /**
   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUrlDirty(java.lang.CharSequence value) {
    return isDirty(1);

  /**
   * Gets the value of the 'timestamp' field.
   */
  public java.lang.Long getTimestamp() {
    return timestamp;

  /**
   * Sets the value of the 'timestamp' field.
   * @param value the value to set.
   */
  public void setTimestamp(java.lang.Long value) {
    this.timestamp = value;
    setDirty(2);
  
  /**
   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isTimestampDirty(java.lang.Long value) {
    return isDirty(2);

  /**
   * Gets the value of the 'ip' field.
   */
  public java.lang.CharSequence getIp() {
    return ip;

  /**
   * Sets the value of the 'ip' field.
   * @param value the value to set.
   */
  public void setIp(java.lang.CharSequence value) {
    this.ip = value;
    setDirty(3);
  
  /**
   * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isIpDirty(java.lang.CharSequence value) {
    return isDirty(3);

  /**
   * Gets the value of the 'httpMethod' field.
   */
  public java.lang.CharSequence getHttpMethod() {
    return httpMethod;

  /**
   * Sets the value of the 'httpMethod' field.
   * @param value the value to set.
   */
  public void setHttpMethod(java.lang.CharSequence value) {
    this.httpMethod = value;
    setDirty(4);
  
  /**
   * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHttpMethodDirty(java.lang.CharSequence value) {
    return isDirty(4);

  /**
   * Gets the value of the 'httpStatusCode' field.
   */
  public java.lang.Integer getHttpStatusCode() {
    return httpStatusCode;

  /**
   * Sets the value of the 'httpStatusCode' field.
   * @param value the value to set.
   */
  public void setHttpStatusCode(java.lang.Integer value) {
    this.httpStatusCode = value;
    setDirty(5);
  
  /**
   * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
    return isDirty(5);

  /**
   * Gets the value of the 'responseSize' field.
   */
  public java.lang.Integer getResponseSize() {
    return responseSize;

  /**
   * Sets the value of the 'responseSize' field.
   * @param value the value to set.
   */
  public void setResponseSize(java.lang.Integer value) {
    this.responseSize = value;
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isResponseSizeDirty(java.lang.Integer value) {
    return isDirty(6);
  }

  /**
   * Gets the value of the 'referrer' field.
   */
  public java.lang.CharSequence getReferrer() {
    return referrer;
  }

  /**
   * Sets the value of the 'referrer' field.
   * @param value the value to set.
   */
  public void setReferrer(java.lang.CharSequence value) {
    this.referrer = value;
    setDirty(7);
  }
  
  /**
   * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isReferrerDirty(java.lang.CharSequence value) {
    return isDirty(7);
  }

  /**
   * Gets the value of the 'userAgent' field.
   */
  public java.lang.CharSequence getUserAgent() {
    return userAgent;
  }

  /**
   * Sets the value of the 'userAgent' field.
   * @param value the value to set.
   */
  public void setUserAgent(java.lang.CharSequence value) {
    this.userAgent = value;
    setDirty(8);
  }
  
  /**
   * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isUserAgentDirty(java.lang.CharSequence value) {
    return isDirty(8);
  }

  /** Creates a new Pageview RecordBuilder */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder() {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder();
  }
  
  /** Creates a new Pageview RecordBuilder by copying an existing Builder */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
  }
  
  /** Creates a new Pageview RecordBuilder by copying an existing Pageview instance */
  public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview other) {
    return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
  }
  
  private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
      java.nio.ByteBuffer input) {
    java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
    int position = input.position();
    input.reset();
    int mark = input.position();
    int limit = input.limit();
    input.rewind();
    input.limit(input.capacity());
    copy.put(input);
    input.rewind();
    copy.rewind();
    input.position(mark);
    input.mark();
    copy.position(mark);
    copy.mark();
    input.position(position);
    copy.position(position);
    input.limit(limit);
    copy.limit(limit);
    return copy.asReadOnlyBuffer();
  }
  
  /**
   * RecordBuilder for Pageview instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Pageview>
    implements org.apache.avro.data.RecordBuilder<Pageview> {

    private java.nio.ByteBuffer __g__dirty;
    private java.lang.CharSequence url;
    private long timestamp;
    private java.lang.CharSequence ip;
    private java.lang.CharSequence httpMethod;
    private int httpStatusCode;
    private int responseSize;
    private java.lang.CharSequence referrer;
    private java.lang.CharSequence userAgent;

    /** Creates a new Builder */
    private Builder() {
      super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
      super(other);
    }
    
    /** Creates a Builder by copying an existing Pageview instance */
    private Builder(org.apache.gora.tutorial.log.generated.Pageview other) {
            super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
      if (isValidValue(fields()[0], other.__g__dirty)) {
        this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.ip)) {
        this.ip = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ip);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.httpMethod)) {
        this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[4].schema(), other.httpMethod);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.httpStatusCode)) {
        this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.httpStatusCode);
        fieldSetFlags()[5] = true;
      }
      if (isValidValue(fields()[6], other.responseSize)) {
        this.responseSize = (java.lang.Integer) data().deepCopy(fields()[6].schema(), other.responseSize);
        fieldSetFlags()[6] = true;
      }
      if (isValidValue(fields()[7], other.referrer)) {
        this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.referrer);
        fieldSetFlags()[7] = true;
      }
      if (isValidValue(fields()[8], other.userAgent)) {
        this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[8].schema(), other.userAgent);
        fieldSetFlags()[8] = true;
      }
    }

    /** Gets the value of the 'url' field */
    public java.lang.CharSequence getUrl() {
      return url;
    }
    
    /** Sets the value of the 'url' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setUrl(java.lang.CharSequence value) {
      validate(fields()[1], value);
      this.url = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'url' field has been set */
    public boolean hasUrl() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'url' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUrl() {
      url = null;
      fieldSetFlags()[1] = false;
      return this;
    }
    
    /** Gets the value of the 'timestamp' field */
    public java.lang.Long getTimestamp() {
      return timestamp;
    }
    
    /** Sets the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setTimestamp(long value) {
      validate(fields()[2], value);
      this.timestamp = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'timestamp' field has been set */
    public boolean hasTimestamp() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'timestamp' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearTimestamp() {
      fieldSetFlags()[2] = false;
      return this;
    }
    
    /** Gets the value of the 'ip' field */
    public java.lang.CharSequence getIp() {
      return ip;
    }
    
    /** Sets the value of the 'ip' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setIp(java.lang.CharSequence value) {
      validate(fields()[3], value);
      this.ip = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the 'ip' field has been set */
    public boolean hasIp() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the 'ip' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearIp() {
      ip = null;
      fieldSetFlags()[3] = false;
      return this;
    }
    
    /** Gets the value of the 'httpMethod' field */
    public java.lang.CharSequence getHttpMethod() {
      return httpMethod;
    }
    
    /** Sets the value of the 'httpMethod' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpMethod(java.lang.CharSequence value) {
      validate(fields()[4], value);
      this.httpMethod = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the 'httpMethod' field has been set */
    public boolean hasHttpMethod() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the 'httpMethod' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpMethod() {
      httpMethod = null;
      fieldSetFlags()[4] = false;
      return this;
    }
    
    /** Gets the value of the 'httpStatusCode' field */
    public java.lang.Integer getHttpStatusCode() {
      return httpStatusCode;
    }
    
    /** Sets the value of the 'httpStatusCode' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpStatusCode(int value) {
      validate(fields()[5], value);
      this.httpStatusCode = value;
      fieldSetFlags()[5] = true;
      return this; 
    }
    
    /** Checks whether the 'httpStatusCode' field has been set */
    public boolean hasHttpStatusCode() {
      return fieldSetFlags()[5];
    }
    
    /** Clears the value of the 'httpStatusCode' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpStatusCode() {
      fieldSetFlags()[5] = false;
      return this;
    }
    
    /** Gets the value of the 'responseSize' field */
    public java.lang.Integer getResponseSize() {
      return responseSize;
    }
    
    /** Sets the value of the 'responseSize' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setResponseSize(int value) {
      validate(fields()[6], value);
      this.responseSize = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'responseSize' field has been set */
    public boolean hasResponseSize() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'responseSize' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearResponseSize() {
      fieldSetFlags()[6] = false;
      return this;
    }
    
    /** Gets the value of the 'referrer' field */
    public java.lang.CharSequence getReferrer() {
      return referrer;
    }
    
    /** Sets the value of the 'referrer' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setReferrer(java.lang.CharSequence value) {
      validate(fields()[7], value);
      this.referrer = value;
      fieldSetFlags()[7] = true;
      return this; 
    }
    
    /** Checks whether the 'referrer' field has been set */
    public boolean hasReferrer() {
      return fieldSetFlags()[7];
    }
    
    /** Clears the value of the 'referrer' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearReferrer() {
      referrer = null;
      fieldSetFlags()[7] = false;
      return this;
    }
    
    /** Gets the value of the 'userAgent' field */
    public java.lang.CharSequence getUserAgent() {
      return userAgent;
    }
    
    /** Sets the value of the 'userAgent' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder setUserAgent(java.lang.CharSequence value) {
      validate(fields()[8], value);
      this.userAgent = value;
      fieldSetFlags()[8] = true;
      return this; 
    }
    
    /** Checks whether the 'userAgent' field has been set */
    public boolean hasUserAgent() {
      return fieldSetFlags()[8];
    }
    
    /** Clears the value of the 'userAgent' field */
    public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUserAgent() {
      userAgent = null;
      fieldSetFlags()[8] = false;
      return this;
    }
    
    @Override
    public Pageview build() {
      try {
        Pageview record = new Pageview();
        record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[2]);
        record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
        record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
        record.ip = fieldSetFlags()[3] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[3]);
        record.httpMethod = fieldSetFlags()[4] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[4]);
        record.httpStatusCode = fieldSetFlags()[5] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[5]);
        record.responseSize = fieldSetFlags()[6] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[6]);
        record.referrer = fieldSetFlags()[7] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[7]);
        record.userAgent = fieldSetFlags()[8] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[8]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }
  
  public Pageview.Tombstone getTombstone(){
  	return TOMBSTONE;
  }

  public Pageview newInstance(){
    return newBuilder().build();
  }

  private static final Tombstone TOMBSTONE = new Tombstone();
  
  public static final class Tombstone extends Pageview implements org.apache.gora.persistency.Tombstone {
  
      private Tombstone() { }
  
	  				  /**
	   * Gets the value of the 'url' field.
		   */
	  public java.lang.CharSequence getUrl() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'url' field.
		   * @param value the value to set.
	   */
	  public void setUrl(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUrlDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'timestamp' field.
		   */
	  public java.lang.Long getTimestamp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'timestamp' field.
		   * @param value the value to set.
	   */
	  public void setTimestamp(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isTimestampDirty(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'ip' field.
		   */
	  public java.lang.CharSequence getIp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'ip' field.
		   * @param value the value to set.
	   */
	  public void setIp(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isIpDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpMethod' field.
		   */
	  public java.lang.CharSequence getHttpMethod() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpMethod' field.
		   * @param value the value to set.
	   */
	  public void setHttpMethod(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpMethodDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpStatusCode' field.
		   */
	  public java.lang.Integer getHttpStatusCode() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpStatusCode' field.
		   * @param value the value to set.
	   */
	  public void setHttpStatusCode(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'responseSize' field.
		   */
	  public java.lang.Integer getResponseSize() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'responseSize' field.
		   * @param value the value to set.
	   */
	  public void setResponseSize(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isResponseSizeDirty(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'referrer' field.
		   */
	  public java.lang.CharSequence getReferrer() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'referrer' field.
		   * @param value the value to set.
	   */
	  public void setReferrer(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isReferrerDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'userAgent' field.
		   */
	  public java.lang.CharSequence getUserAgent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'userAgent' field.
		   * @param value the value to set.
	   */
	  public void setUserAgent(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUserAgentDirty(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
  }
  
}
 b/gora-compiler/src/main/java/org/apache/gora/compiler/utils/LicenseHeaders.java
package org.apache.gora.compiler.utils;
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
  @SuppressWarnings("unused")
    Schema.Parser parser = new Schema.Parser();
    return parser.parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
          String dbFieldName = mapping.getDocumentField(k);
          if (dbFieldName != null && dbFieldName.length() > 0) {
              proj.put(dbFieldName, true);
          }
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
    if (obj.isDirty()) {
      persistent.clearDirty();
                Map<Utf8, Object> rmap = new HashMap<Utf8, Object>();
                result = new DirtyMapWrapper(rmap);
                        List<Utf8> arrS = new ArrayList<Utf8>();
                        result = new DirtyListWrapper<Utf8>(arrS);
                        List<ByteBuffer> arrB = new ArrayList<ByteBuffer>();
                        result = new DirtyListWrapper<ByteBuffer>(arrB);
                        List<Object> arrT = new ArrayList<Object>();
                        result = new DirtyListWrapper<Object>(arrT);
      if (persistent.isDirty(f.pos()) && (persistent.get(f.pos()) != null)) {
      if (persistent.isDirty(f.pos()) && (persistent.get(f.pos()) == null)) {
                result = toMongoList((List<?>) value, elementSchema.getType());
          toMongoList((List<?>) value, field.schema().getElementType()
              toMongoList((List<?>) recValue, member.schema()
  private BasicDBList toMongoList(Collection<?> array, Type type) {
import com.mongodb.BasicDBObject;
import com.mongodb.DBObject;

    for (String k : fields) {
      String dbFieldName = mapping.getDocumentField(k);
      if (dbFieldName != null && dbFieldName.length() > 0) {
        proj.put(dbFieldName, true);
    }
public class MongoDBResult<K, T extends PersistentBase> extends
    ResultBase<K, T> {
        getQuery().getFields());
import org.apache.gora.persistency.impl.PersistentBase;
import org.jdom.Document;
import org.jdom.Element;
import org.jdom.input.SAXBuilder;

               " mappingfile schema is '"  docNameFromMapping
               "' vs actual schema '"  collName
               "' , assuming they are the same.");
              field.getAttributeValue(ATT_FIELD),
              field.getAttributeValue(ATT_TYPE));
import org.apache.gora.mongodb.utils.BSONDecorator;
import org.apache.gora.mongodb.utils.GoraDBEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
      return;
        mapping.getCollectionName(), new BasicDBObject()); // send a DBObject to
                                                           // force creation
    // otherwise creation is deferred
   * 
    MongoDBQuery<K, T> query = new MongoDBQuery<K, T>(this);
    query.setFields(getFieldsToQuery(null));
    return query;
    PartitionQueryImpl<K, T> partitionQuery = new PartitionQueryImpl<K, T>(
        query);
      LOG.debug(
          "Load from DBObject (MAIN), field:{}, schemaType:{}, docField:{}, storeType:{}",
          new Object[] { field.name(), fieldSchema.getType(), docf, storeType });
      Object result = fromDBObject(fieldSchema, storeType, field, docf,
          easybson);
    persistent.clearDirty();
  private Object fromDBObject(final Schema fieldSchema,
      final DocumentFieldType storeType, final Field field, final String docf,
      final BSONDecorator easybson) {
    Object result = null;
    switch (fieldSchema.getType()) {
    case MAP:
      BasicDBObject map = easybson.getDBObject(docf);
      if (map != null) {
        Map<Utf8, Object> rmap = new HashMap<Utf8, Object>();
        for (Entry<String, Object> e : map.entrySet()) {
          // ensure Key decoding -> middle dots replaced with dots
          // FIXME: better approach ?
          String oKey = e.getKey().replace("\u00B7", ".");
          switch (fieldSchema.getValueType().getType()) {
          case STRING:
            rmap.put(new Utf8(oKey), new Utf8((String) e.getValue()));
            break;
          case BYTES:
            rmap.put(new Utf8(oKey), ByteBuffer.wrap((byte[]) e.getValue()));
            break;
          default:
            rmap.put(new Utf8(oKey), e.getValue());
            break;
          }
        result = new DirtyMapWrapper(rmap);
      }
      break;
    case ARRAY:
      List<Object> list = easybson.getDBList(docf);
      switch (fieldSchema.getElementType().getType()) {
      case STRING:
        List<Utf8> arrS = new ArrayList<Utf8>();
        for (Object o : list)
          arrS.add(new Utf8((String) o));
        result = new DirtyListWrapper<Utf8>(arrS);
        break;
      case BYTES:
        List<ByteBuffer> arrB = new ArrayList<ByteBuffer>();
        for (Object o : list)
          arrB.add(ByteBuffer.wrap((byte[]) o));
        result = new DirtyListWrapper<ByteBuffer>(arrB);
        break;
      default:
        List<Object> arrT = new ArrayList<Object>();
        for (Object o : list)
          arrT.add(o);
        result = new DirtyListWrapper<Object>(arrT);
        break;
      }
      break;
    case RECORD:
      DBObject rec = easybson.getDBObject(docf);
      if (rec == null) {
      }
      BSONDecorator innerBson = new BSONDecorator(rec);
      Class<?> clazz = null;
      try {
        clazz = ClassLoadingUtils.loadClass(fieldSchema.getFullName());
      } catch (ClassNotFoundException e) {
      }
      Persistent record = new BeanFactoryImpl(keyClass, clazz).newPersistent();
      for (Field recField : fieldSchema.getFields()) {
        Schema innerSchema = recField.schema();
        DocumentFieldType innerStoreType = mapping
            .getDocumentFieldType(innerSchema.getName());
        String innerDocField = mapping.getDocumentField(recField.name()) != null ? mapping
            .getDocumentField(recField.name()) : recField.name();
        String fieldPath = docf  "."  innerDocField;
        LOG.debug(
            "Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}",
            new Object[] { recField.name(), innerSchema.getType(), fieldPath,
                innerStoreType });
        ((PersistentBase) record).put(
            recField.pos(),
            fromDBObject(innerSchema, innerStoreType, recField, innerDocField,
                innerBson));
      }
      result = record;
      break;
    case BOOLEAN:
      result = easybson.getBoolean(docf);
      break;
    case DOUBLE:
      result = easybson.getDouble(docf);
      break;
    case FLOAT:
      result = easybson.getDouble(docf).floatValue();
      break;
    case INT:
      result = easybson.getInt(docf);
      break;
    case LONG:
      result = easybson.getLong(docf);
      break;
    case STRING:
      if (storeType == DocumentFieldType.OBJECTID) {
        // Try auto-conversion of BSON data to ObjectId
        // It will work if data is stored as String or as ObjectId
        final Object bin = easybson.get(docf);
        final ObjectId id = ObjectId.massageToObjectId(bin);
        result = new Utf8(id.toString());
      } else if (storeType == DocumentFieldType.DATE) {
        final Object bin = easybson.get(docf);
        if (bin instanceof Date) {
          Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
          calendar.setTime((Date) bin);
          result = new Utf8(DatatypeConverter.printDateTime(calendar));
        } else {
          result = new Utf8(bin.toString());
        }
      } else {
        result = easybson.getUtf8String(docf);
      }
      break;
    case ENUM:
      result = AvroUtils.getEnumValue(fieldSchema, easybson.getUtf8String(docf)
          .toString());
      break;
    case BYTES:
    case FIXED:
      result = easybson.getBytes(docf);
      break;
    case NULL:
      result = null;
      break;
    case UNION:
      // schema [type0, type1]
      Type type0 = fieldSchema.getTypes().get(0).getType();
      Type type1 = fieldSchema.getTypes().get(1).getType();
      // Check if types are different and there's a "null", like ["null","type"]
      // or ["type","null"]
      if (!type0.equals(type1)
          && (type0.equals(Type.NULL) || type1.equals(Type.NULL))) {
        Schema innerSchema = fieldSchema.getTypes().get(1);
        DocumentFieldType innerStoreType = mapping
            .getDocumentFieldType(innerSchema.getName());
        LOG.debug(
            "Load from DBObject (UNION), schemaType:{}, docField:{}, storeType:{}",
            new Object[] { innerSchema.getType(), docf, innerStoreType });
        result = fromDBObject(innerSchema, innerStoreType, field, docf,
            easybson); // Deserialize as if schema was ["type"]
      } else {
        throw new IllegalStateException(
            "MongoStore doesn't support 3 types union field yet. Please update your mapping");
      }
      break;
    default:
      LOG.warn("Unable to read {}", docf);
      break;
    }
    return result;
  }

  // ////////////////////////////////////////////////////////// SERIALIZATION
   * {@link MongoStore#newInstance(org.apache.gora.persistency.impl.PersistentBase)}
   * one from two points:
        LOG.debug(
            "Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}",
            new Object[] { docf, f.schema().getType(), storeType });
        result.put(docf,
            toDBObject(f.schema(), f.schema().getType(), storeType, value));
   * {@link MongoStore#newInstance(org.apache.gora.persistency.impl.PersistentBase)}
   * one from two points:
        LOG.debug(
            "Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}",
            new Object[] { docf, f.schema().getType(), storeType });
        Object o = toDBObject(f.schema(), f.schema().getType(), storeType,
            value);
  private Object toDBObject(Schema fieldSchema, Type fieldType,
      DocumentFieldType storeType, Object value) {
    Object result = null;
    switch (fieldType) {
    case MAP:
      if (storeType != null && storeType != DocumentFieldType.DOCUMENT) {
        throw new IllegalStateException(
            "Field "
                 fieldSchema.getType()
                 ": to store a Gora 'map', target Mongo mapping have to be of 'document' type");
      }
      Schema valueSchema = fieldSchema.getValueType();
      result = toMongoMap((Map<Utf8, ?>) value, valueSchema.getType());
      break;
    case ARRAY:
      if (storeType != null && storeType != DocumentFieldType.LIST) {
        throw new IllegalStateException(
            "Field "
                 fieldSchema.getType()
                 ": To store a Gora 'array', target Mongo mapping have to be of 'list' type");
      }
      Schema elementSchema = fieldSchema.getElementType();
      result = toMongoList((List<?>) value, elementSchema.getType());
      break;
    case BYTES:
      // Beware of ByteBuffer not being safely serialized
      if (value != null) {
        result = ((ByteBuffer) value).array();
      }
      break;
    case INT:
    case LONG:
    case FLOAT:
    case DOUBLE:
    case BOOLEAN:
      result = value;
      break;
    case STRING:
      if (storeType == DocumentFieldType.OBJECTID) {
        if (value != null) {
          ObjectId id;
          try {
            id = new ObjectId(value.toString());
          } catch (IllegalArgumentException e1) {
            // Unable to parse anything from Utf8 value, throw error
            throw new IllegalStateException("Field "  fieldSchema.getType()
                 ": Invalid string: unable to convert to ObjectId");
          }
          result = id;
      } else if (storeType == DocumentFieldType.DATE) {
        if (value != null) {
          // Try to parse date from Utf8 value
          Calendar calendar = null;
          try {
            // Parse as date  time
            calendar = DatatypeConverter.parseDateTime(value.toString());
          } catch (IllegalArgumentException e1) {
            try {
              // Parse as date only
              calendar = DatatypeConverter.parseDate(value.toString());
            } catch (IllegalArgumentException e2) {
              // No-op
            }
          }
          if (calendar == null) {
            // Unable to parse anything from Utf8 value, throw error
            throw new IllegalStateException("Field "  fieldSchema.getType()
                 ": Invalid date format '"  value  "'");
          }
          result = calendar.getTime();
        }
      } else {
        // Beware of Utf8 not being safely serialized
        if (value != null) {
          result = value.toString();
        }
      }
      break;
    case ENUM:
      // Beware of Utf8 not being safely serialized
      if (value != null)
        result = value.toString();
      break;
    case RECORD:
      if (value == null)
        break;
      BasicDBObject record = new BasicDBObject();
      for (Field member : fieldSchema.getFields()) {
        Object innerValue = ((PersistentBase) value).get(member.pos());
        String innerDoc = mapping.getDocumentField(member.name());
        Type innerType = member.schema().getType();
        DocumentFieldType innerStoreType = mapping
            .getDocumentFieldType(innerDoc);
        LOG.debug(
            "Transform value to DBObject (RECORD), docField:{}, schemaType:{}, storeType:{}",
            new Object[] { member.name(), member.schema().getType(),
                innerStoreType });
        record.put(member.name(),
            toDBObject(member.schema(), innerType, innerStoreType, innerValue));
      }
      result = record;
      break;
    case UNION:
      // schema [type0, type1]
      Type type0 = fieldSchema.getTypes().get(0).getType();
      Type type1 = fieldSchema.getTypes().get(1).getType();
      // Check if types are different and there's a "null", like ["null","type"]
      // or ["type","null"]
      if (!type0.equals(type1)
          && (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {
        Schema innerSchema = fieldSchema.getTypes().get(1);
        DocumentFieldType innerStoreType = mapping
            .getDocumentFieldType(innerSchema.getName());
        LOG.debug(
            "Transform value to DBObject (UNION), schemaType:{}, type1:{}, storeType:{}",
            new Object[] { innerSchema.getType(), type1, innerStoreType });
        result = toDBObject(innerSchema, type1, innerStoreType, value); // Deserialize
                                                                        // as if
                                                                        // schema
                                                                        // was
                                                                        // ["type"]
      } else {
        throw new IllegalStateException(
            "MongoStore doesn't support 3 types union field yet. Please update your mapping");
      }
      break;
    case FIXED:
      result = value;
      break;

    default:
      LOG.error("Unknown field type: "  fieldSchema.getType());
      break;
    return result;
  }

  /**
      easybson.put(key, value);
      break;
              toMongoList((List<?>) recValue, member.schema().getElementType()
                  .getType()));
          easybson.put(key, value);
          break;
    // force creation
      result = toMongoMap((Map<CharSequence, ?>) value, valueSchema.getType());
          toMongoMap((Map<CharSequence, ?>) value, field.schema()
              .getValueType().getType()));
              toMongoMap((Map<CharSequence, ?>) recValue, member.schema()
  private BasicDBObject toMongoMap(Map<CharSequence, ?> jmap, Type type) {
    for (Entry<CharSequence, ?> e : jmap.entrySet()) {
    for (String field : fields) {
      final String docf = mapping.getDocumentField(field);
      if (docf != null) {
        proj.put(docf, true);
      }
    }
      if (docf == null || !easybson.containsField(docf))
      obj.clearDirty();
            new Object[] { innerSchema.getType(), docf, storeType });
        // Deserialize as if schema was ["type"]
        result = fromDBObject(innerSchema, storeType, field, docf, easybson);
            new Object[] { innerSchema.getType(), type1, storeType });
        // Deserialize as if schema was ["type"]
        result = toDBObject(innerSchema, type1, storeType, value);
      performPut(key, obj);
  private void performPut(K key, T obj) {
          String host = paramsIterator.next();
            String port = paramsIterator.next();
    DB db = mapsOfClients.get(servers).getDB(dbname);
      String docf = mapping.getDocumentField(field);
    T persistent = newInstance(res, fields);
    WriteResult writeResult = mongoClientColl.remove(removeKey);
    WriteResult writeResult = mongoClientColl.remove(q);
        Object bin = easybson.get(docf);
        ObjectId id = ObjectId.massageToObjectId(bin);
        Object bin = easybson.get(docf);
          String mKey = decodeFieldKey(e.getKey());
          Object mValue = e.getValue();
            rmap.put(new Utf8(mKey), new Utf8((String) mValue));
            rmap.put(new Utf8(mKey), ByteBuffer.wrap((byte[]) mValue));
            rmap.put(new Utf8(mKey), mValue);
        record.put(
      String mKey = encodeFieldKey(e.getKey().toString());
      Object mValue = e.getValue();
        map.put(mKey, mValue.toString());
        map.put(mKey, ((ByteBuffer) mValue).array());
        break;
  // //////////////////////////////////////////////////////// CLEANUP
  /**
   * Ensure Key encoding -> dots replaced with middle dots
   * 
   * @param key
   *          char with only dots.
   * @return encoded string with "\u00B7" chars..
   */
  protected String encodeFieldKey(final String key) {
    if (key == null) {
      return null;
    }
    return key.replace(".", "\u00B7");
  }

  /**
   * Ensure Key decoding -> middle dots replaced with dots
   * 
   * @param key
   *          encoded string with "\u00B7" chars.
   * @return Cleanup up char with only dots.
   */
  protected String decodeFieldKey(final String key) {
    if (key == null) {
      return null;
    }
    return key.replace("\u00B7", ".");
  }
import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.DOCUMENT;
import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.LIST;
import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.valueOf;

import com.google.common.collect.ImmutableList;

        if (!ImmutableList.of(DOCUMENT, LIST).contains(
            documentFields.get(intermediateFieldName)))
      newDocumentField(docFieldName, valueOf(fieldType.toUpperCase()));
        result = fromMongoMap(fieldSchema, map);
      if (list != null) {
        result = fromMongoList(fieldSchema, list);
      result = fromMongoString(storeType, docf, easybson);
  private Object fromMongoList(Schema fieldSchema, List<Object> list) {
    Object result;
    switch (fieldSchema.getElementType().getType()) {
    case STRING:
      List<Utf8> arrS = new ArrayList<Utf8>();
      for (Object o : list)
        arrS.add(new Utf8((String) o));
      result = new DirtyListWrapper<Utf8>(arrS);
      break;
    case BYTES:
      List<ByteBuffer> arrB = new ArrayList<ByteBuffer>();
      for (Object o : list)
        arrB.add(ByteBuffer.wrap((byte[]) o));
      result = new DirtyListWrapper<ByteBuffer>(arrB);
      break;
    default:
      List<Object> arrT = new ArrayList<Object>();
      for (Object o : list)
        arrT.add(o);
      result = new DirtyListWrapper<Object>(arrT);
      break;
    }
    return result;
  }

  private Object fromMongoMap(Schema fieldSchema, BasicDBObject map) {
    Object result;
    Map<Utf8, Object> rmap = new HashMap<Utf8, Object>();
    for (Entry<String, Object> e : map.entrySet()) {
      String mKey = decodeFieldKey(e.getKey());
      Object mValue = e.getValue();
      switch (fieldSchema.getValueType().getType()) {
      case STRING:
        rmap.put(new Utf8(mKey), new Utf8((String) mValue));
        break;
      case BYTES:
        rmap.put(new Utf8(mKey), ByteBuffer.wrap((byte[]) mValue));
        break;
      default:
        rmap.put(new Utf8(mKey), mValue);
        break;
      }
    }
    result = new DirtyMapWrapper(rmap);
    return result;
  }

  private Object fromMongoString(final DocumentFieldType storeType,
      final String docf, final BSONDecorator easybson) {
    Object result;
    if (storeType == DocumentFieldType.OBJECTID) {
      // Try auto-conversion of BSON data to ObjectId
      // It will work if data is stored as String or as ObjectId
      Object bin = easybson.get(docf);
      ObjectId id = ObjectId.massageToObjectId(bin);
      result = new Utf8(id.toString());
    } else if (storeType == DocumentFieldType.DATE) {
      Object bin = easybson.get(docf);
      if (bin instanceof Date) {
        Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
        calendar.setTime((Date) bin);
        result = new Utf8(DatatypeConverter.printDateTime(calendar));
      } else {
        result = new Utf8(bin.toString());
      }
    } else {
      result = easybson.getUtf8String(docf);
    }
    return result;
  }

      result = mapToMongo((Map<CharSequence, ?>) value, valueSchema.getType());
      result = listToMongo((List<?>) value, elementSchema.getType());
      result = stringToMongo(fieldSchema, storeType, value);
  private Object stringToMongo(final Schema fieldSchema,
      final DocumentFieldType storeType, final Object value) {
    Object result = null;
    if (storeType == DocumentFieldType.OBJECTID) {
      if (value != null) {
        ObjectId id;
        try {
          id = new ObjectId(value.toString());
        } catch (IllegalArgumentException e1) {
          // Unable to parse anything from Utf8 value, throw error
          throw new IllegalStateException("Field "  fieldSchema.getType()
               ": Invalid string: unable to convert to ObjectId");
        }
        result = id;
      }
    } else if (storeType == DocumentFieldType.DATE) {
      if (value != null) {
        // Try to parse date from Utf8 value
        Calendar calendar = null;
        try {
          // Parse as date  time
          calendar = DatatypeConverter.parseDateTime(value.toString());
        } catch (IllegalArgumentException e1) {
          try {
            // Parse as date only
            calendar = DatatypeConverter.parseDate(value.toString());
          } catch (IllegalArgumentException e2) {
            // No-op
          }
        }
        if (calendar == null) {
          // Unable to parse anything from Utf8 value, throw error
          throw new IllegalStateException("Field "  fieldSchema.getType()
               ": Invalid date format '"  value  "'");
        }
        result = calendar.getTime();
      }
    } else {
      // Beware of Utf8 not being safely serialized
      if (value != null) {
        result = value.toString();
      }
    }
    return result;
  }

  private BasicDBObject mapToMongo(Map<CharSequence, ?> jmap, Type type) {
        map.put(mKey, e.getValue());
  private BasicDBList listToMongo(Collection<?> array, Type type) {
  public void initialize(final Class<K> keyClass,
      final Class<T> pPersistentClass, final Properties properties) {
  private MongoClient getClient(final String servers)
      throws UnknownHostException {
  private DB getDB(final String servers, final String dbname,
      final String login, final String secret) throws UnknownHostException {
  public String getSchemaName(final String mappingSchemaName,
      final Class<?> persistentClass) {
  public T get(final K key, final String[] fields) {
    String[] dbFields = getFieldsToQuery(fields);
    for (String field : dbFields) {
    T persistent = newInstance(res, dbFields);
  public void put(final K key, final T obj) {
  private void performPut(final K key, final T obj) {
  public boolean delete(final K key) {
  public long deleteByQuery(final Query<K, T> query) {
  public Result<K, T> execute(final Query<K, T> query) {
  public List<PartitionQuery<K, T>> getPartitions(final Query<K, T> query)
  public T newInstance(final DBObject obj, final String[] fields) {
    String[] dbFields = getFieldsToQuery(fields);
    for (String f : dbFields) {
        result = null;
        break;
      result = fromMongoRecord(fieldSchema, docf, rec);
      result = fromMongoUnion(fieldSchema, storeType, field, docf, easybson);
  private Object fromMongoUnion(final Schema fieldSchema,
      final DocumentFieldType storeType, final Field field, final String docf,
      final BSONDecorator easybson) {
    Object result;// schema [type0, type1]
    Type type0 = fieldSchema.getTypes().get(0).getType();
    Type type1 = fieldSchema.getTypes().get(1).getType();

    // Check if types are different and there's a "null", like ["null","type"]
    // or ["type","null"]
    if (!type0.equals(type1)
        && (type0.equals(Type.NULL) || type1.equals(Type.NULL))) {
      Schema innerSchema = fieldSchema.getTypes().get(1);
      LOG.debug(
          "Load from DBObject (UNION), schemaType:{}, docField:{}, storeType:{}",
          new Object[] { innerSchema.getType(), docf, storeType });
      // Deserialize as if schema was ["type"]
      result = fromDBObject(innerSchema, storeType, field, docf, easybson);
    } else {
      throw new IllegalStateException(
          "MongoStore doesn't support 3 types union field yet. Please update your mapping");
    }
    return result;
  }

  private Object fromMongoRecord(final Schema fieldSchema, final String docf,
      final DBObject rec) {
    Object result;
    BSONDecorator innerBson = new BSONDecorator(rec);
    Class<?> clazz = null;
    try {
      clazz = ClassLoadingUtils.loadClass(fieldSchema.getFullName());
    } catch (ClassNotFoundException e) {
    }
    Persistent record = new BeanFactoryImpl(keyClass, clazz).newPersistent();
    for (Field recField : fieldSchema.getFields()) {
      Schema innerSchema = recField.schema();
      DocumentFieldType innerStoreType = mapping
          .getDocumentFieldType(innerSchema.getName());
      String innerDocField = mapping.getDocumentField(recField.name()) != null ? mapping
          .getDocumentField(recField.name()) : recField.name();
      String fieldPath = docf  "."  innerDocField;
      LOG.debug(
          "Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}",
          new Object[] { recField.name(), innerSchema.getType(), fieldPath,
              innerStoreType });
      record.put(
          recField.pos(),
          fromDBObject(innerSchema, innerStoreType, recField, innerDocField,
              innerBson));
    }
    result = record;
    return result;
  }

  private Object fromMongoList(final Schema fieldSchema, final List<Object> list) {
  private Object fromMongoMap(final Schema fieldSchema, final BasicDBObject map) {
  private BasicDBObject newUpdateSetInstance(final T persistent) {
  private BasicDBObject newUpdateUnsetInstance(final T persistent) {
  private Object toDBObject(final Schema fieldSchema, final Type fieldType,
      final DocumentFieldType storeType, final Object value) {
      result = recordToMongo(fieldSchema, value);
      result = unionToMongo(fieldSchema, storeType, value);
  private Object unionToMongo(final Schema fieldSchema,
      final DocumentFieldType storeType, final Object value) {
    Object result;// schema [type0, type1]
    Type type0 = fieldSchema.getTypes().get(0).getType();
    Type type1 = fieldSchema.getTypes().get(1).getType();

    // Check if types are different and there's a "null", like ["null","type"]
    // or ["type","null"]
    if (!type0.equals(type1)
        && (type0.equals(Type.NULL) || type1.equals(Type.NULL))) {
      Schema innerSchema = fieldSchema.getTypes().get(1);
      LOG.debug(
          "Transform value to DBObject (UNION), schemaType:{}, type1:{}, storeType:{}",
          new Object[] { innerSchema.getType(), type1, storeType });
      // Deserialize as if schema was ["type"]
      result = toDBObject(innerSchema, type1, storeType, value);
    } else {
      throw new IllegalStateException(
          "MongoStore doesn't support 3 types union field yet. Please update your mapping");
    }
    return result;
  }

  private BasicDBObject recordToMongo(final Schema fieldSchema,
      final Object value) {
    BasicDBObject record = new BasicDBObject();
    for (Field member : fieldSchema.getFields()) {
      Object innerValue = ((PersistentBase) value).get(member.pos());
      String innerDoc = mapping.getDocumentField(member.name());
      Type innerType = member.schema().getType();
      DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerDoc);
      LOG.debug(
          "Transform value to DBObject (RECORD), docField:{}, schemaType:{}, storeType:{}",
          new Object[] { member.name(), member.schema().getType(),
              innerStoreType });
      record.put(member.name(),
          toDBObject(member.schema(), innerType, innerStoreType, innerValue));
    }
    return record;
  }

  private BasicDBObject mapToMongo(final Map<CharSequence, ?> jmap,
      final Type type) {
  private BasicDBList listToMongo(final Collection<?> array, final Type type) {
      result = fromMongoMap(docf, fieldSchema, easybson, field);
      result = fromMongoList(docf, fieldSchema, easybson, field);
  private Object fromMongoList(final String docf, final Schema fieldSchema,
      final BSONDecorator easybson, final Field f) {
    List<Object> list = easybson.getDBList(docf);
    if (list == null) {
      return null;

    List<Object> rlist = new ArrayList<Object>();

    for (Object item : list) {
      DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
      Object o = fromDBObject(fieldSchema.getElementType(), storeType, f,
          "item", new BSONDecorator(new BasicDBObject("item", item)));
      rlist.add(o);
    }
    return new DirtyListWrapper(rlist);
  private Object fromMongoMap(final String docf, final Schema fieldSchema,
      final BSONDecorator easybson, final Field f) {
    BasicDBObject map = easybson.getDBObject(docf);
    if (map == null) {
      return null;
    }
      String mapKey = e.getKey();
      String decodedMapKey = decodeFieldKey(mapKey);

      DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
      Object o = fromDBObject(fieldSchema.getValueType(), storeType, f, mapKey,
          new BSONDecorator(map));
      rmap.put(new Utf8(decodedMapKey), o);
    return new DirtyMapWrapper(rmap);
        Object o = toDBObject(docf, f.schema(), f.schema().getType(),
            storeType, value);
        result.put(docf, o);
        Object o = toDBObject(docf, f.schema(), f.schema().getType(),
            storeType, value);
  private Object toDBObject(final String docf, final Schema fieldSchema,
      final Type fieldType, final DocumentFieldType storeType,
      final Object value) {
      result = mapToMongo(docf, (Map<CharSequence, ?>) value, valueSchema,
          valueSchema.getType());
      result = listToMongo(docf, (List<?>) value, elementSchema,
          elementSchema.getType());
      result = recordToMongo(docf, fieldSchema, value);
      result = unionToMongo(docf, fieldSchema, storeType, value);
      LOG.error("Unknown field type: {}", fieldSchema.getType());
  private Object unionToMongo(final String docf, final Schema fieldSchema,
      result = toDBObject(docf, innerSchema, type1, storeType, value);
  private BasicDBObject recordToMongo(final String docf,
      final Schema fieldSchema, final Object value) {
      record.put(
          member.name(),
          toDBObject(docf, member.schema(), innerType, innerStoreType,
              innerValue));
   * @param value
   * @param fieldType
  private BasicDBObject mapToMongo(final String docf,
      final Map<CharSequence, ?> value, final Schema fieldSchema,
      final Type fieldType) {
    if (value == null)

    for (Entry<CharSequence, ?> e : value.entrySet()) {
      String mapKey = e.getKey().toString();
      String encodedMapKey = encodeFieldKey(mapKey);
      Object mapValue = e.getValue();

      DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
      Object result = toDBObject(docf, fieldSchema, fieldType, storeType,
          mapValue);
      map.put(encodedMapKey, result);

   * @param fieldType
  private BasicDBList listToMongo(final String docf, final Collection<?> array,
      final Schema fieldSchema, final Type fieldType) {

      DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
      Object result = toDBObject(docf, fieldSchema, fieldType, storeType, item);
      list.add(result);

import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.*;
import org.apache.gora.mongodb.filters.MongoFilterUtil;
  private MongoFilterUtil<K, T> filterUtil;

  public MongoStore() {
    // Create a default mapping that will be overriden in initialize method
    this.mapping = new MongoMapping();
  }

      filterUtil = new MongoFilterUtil<K, T>(getConf());

  public MongoMapping getMapping() {
    return mapping;
  }

    if (query.getFilter() != null) {
      boolean succeeded = filterUtil.setFilter(q, query.getFilter(), this);
      if (succeeded) {
        // don't need local filter
        query.setLocalFilterEnabled(false);
      }
    }

  public String encodeFieldKey(final String key) {
  public String decodeFieldKey(final String key) {
import java.net.MalformedURLException;
import org.apache.hadoop.util.StringUtils;
import org.apache.solr.client.solrj.impl.CloudSolrServer;
import org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer;
import org.apache.solr.client.solrj.impl.LBHttpSolrServer;
  protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";
  private String solrServerUrl, solrConfig, solrSchema, solrJServerImpl;
    solrJServerImpl = DataStoreFactory.findProperty(properties, this, 
        SOLR_SOLRJSERVER_IMPL, "http");
    String solrJServerType = ((solrJServerImpl == null || solrJServerImpl.equals(""))?"http":solrJServerImpl);
    // HttpSolrServer - denoted by "http" in properties
    if (solrJServerType.toString().toLowerCase().equals("http")) {
      LOG.info("Using HttpSolrServer Solrj implementation.");
      this.adminServer = new HttpSolrServer(solrServerUrl);
      this.server = new HttpSolrServer( solrServerUrl  "/"  mapping.getCoreName() );
      // CloudSolrServer - denoted by "cloud" in properties
    } else if (solrJServerType.toString().toLowerCase().equals("cloud")) {
      LOG.info("Using CloudSolrServer Solrj implementation.");
      try {
        this.adminServer = new CloudSolrServer(solrServerUrl);
      } catch (MalformedURLException e) {
        e.printStackTrace();
      }
      try {
        this.server = new CloudSolrServer( solrServerUrl  "/"  mapping.getCoreName() );
      } catch (MalformedURLException e) {
        e.printStackTrace();
      }
      // ConcurrentUpdateSolrServer - denoted by "concurrent" in properties
    } else if (solrJServerType.toString().toLowerCase().equals("concurrent")) {
      LOG.info("Using ConcurrentUpdateSolrServer Solrj implementation.");
      this.adminServer = new ConcurrentUpdateSolrServer(solrServerUrl, 1000, 10);
      this.server = new ConcurrentUpdateSolrServer( solrServerUrl  "/"  mapping.getCoreName(), 1000, 10);
      // LBHttpSolrServer - denoted by "loadbalance" in properties
    } else if (solrJServerType.toString().toLowerCase().equals("loadbalance")) {
      LOG.info("Using LBHttpSolrServer Solrj implementation.");
      String[] solrUrlElements = StringUtils.split(solrServerUrl);
      try {
        this.adminServer = new LBHttpSolrServer(solrUrlElements);
      } catch (MalformedURLException e) {
        e.printStackTrace();
      }
      try {
        this.server = new LBHttpSolrServer( solrUrlElements  "/"  mapping.getCoreName() );
      } catch (MalformedURLException e) {
        e.printStackTrace();
      }
    }
    LOG.info("Putting DOCUMENT: "  doc);
        factory.setFilterUtil(this);
  /** The default file name value to be used for obtaining the Solr object field mapping's */
  /** The URL of the Solr server - defined in <code>gora.properties</code> */
  /** The <code>solrconfig.xml</code> file to be used - defined in <code>gora.properties</code>*/
  /** The <code>schema.xml</code> file to be used - defined in <code>gora.properties</code>*/
  /** A batch size unit (ArrayList) of SolrDocument's to be used for writing to Solr.
   * Should be defined in <code>gora.properties</code>. 
   * A default value of 100 is used if this value is absent. This value must be of type <b>Integer</b>.
   */
  /** The solrj implementation to use. This has a default value of <i>http</i> for HttpSolrServer.
   * Available options include <b>http</b>, <b>cloud</b>, <b>concurrent</b> and <b>loadbalance</b>. 
   * Defined in <code>gora.properties</code>
   * This value must be of type <b>String</b>.
   */
  /** A batch commit unit for SolrDocument's used when making (commit) calls to Solr.
   * Should be defined in <code>gora.properties</code>. 
   * A default value of 1000 is used if this value is absent. This value must be of type <b>Integer</b>.
   */
  /** The maximum number of result to return when we make a call to 
   * {@link org.apache.gora.solr.store.SolrStore#execute(Query)}. This should be 
   * defined in <code>gora.properties</code>. This value must be of type <b>Integer</b>.
   */
  /** The default batch size (ArrayList) of SolrDocuments to be used in the event of an absent 
   * value for <code>solr.batchSize</code>. 
   * Set to 100 by default.
   */
  /** The default commit size of SolrDocuments to be used in the event of an absent 
   * value for <code>solr.commitSize</code>. 
   * Set to 1000 by default.
   */
  /** The default results size of SolrDocuments to be used in the event of an absent 
   * value for <code>solr.resultsSize</code>. 
   * Set to 100 by default.
   */
      this.adminServer = new CloudSolrServer(solrServerUrl);
      this.server = new CloudSolrServer( solrServerUrl  "/"  mapping.getCoreName() );
import org.apache.avro.Schema;
import org.apache.avro.Schema.Type;
import org.apache.gora.store.DataStore;
    Schema persistentSchema = query.getDataStore().newPersistent().getSchema();
      if (persistentSchema.getField(field).schema().getType() == Type.UNION)
        list.add(column  CassandraStore.UNION_COL_SUFIX);
    Schema persistentSchema = query.getDataStore().newPersistent().getSchema();
      if (persistentSchema.getField(field).schema().getType() == Type.UNION)
        map.put(family  ":"  column  CassandraStore.UNION_COL_SUFIX, field  CassandraStore.UNION_COL_SUFIX);
	    LOG.error("Error reading Gora records");
	    e.printStackTrace();
	    return false;
      keyspaceDefinition = HFactory.createKeyspaceDefinition(
    	this.cassandraMapping.getKeyspaceName(), 
        this.cassandraMapping.getKeyspaceReplicationStrategy(),
        this.cassandraMapping.getKeyspaceReplicationFactor(),
        columnFamilyDefinitions
      );
      
    	LOG.debug( "field:"fieldName", its value is null.");
    
    	LOG.warn("Column name is null for field="  fieldName );
        return;
      
    if( LOG.isDebugEnabled() ) LOG.debug( "fieldName:"fieldName " columnName:"  columnName );
    
    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(columnName);
    
    if ( null == ttlAttr ){
    	ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
    	if( LOG.isDebugEnabled() ) LOG.debug( "ttl was not set for field:"  fieldName  " .Using "  ttlAttr );
    } else {
    	if( LOG.isDebugEnabled() ) LOG.debug( "ttl for field:"  fieldName  " is "  ttlAttr );
    }

    String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(superColumnName);
    if ( null == ttlAttr ) {
      if( LOG.isDebugEnabled() ) LOG.debug( "ttl was not set for field:"  fieldName  " .Using "  ttlAttr );
    } else {
      if( LOG.isDebugEnabled() ) LOG.debug( "ttl for field:"  fieldName  " is "  ttlAttr );
    }

  private static final String REPLICATION_FACTOR_ATTRIBUTE = "replication_factor"; 	
  private static final String REPLICATION_STRATEGY_ATTRIBUTE = "placement_strategy";
  
  public static final String DEFAULT_REPLICATION_FACTOR = "1";		 
  public static final String DEFAULT_REPLICATION_STRATEGY = "org.apache.cassandra.locator.SimpleStrategy";
  public static final String DEFAULT_COLUMNS_TTL = "0";
  public static final String DEFAULT_GCGRACE_SECONDS = "0";
  private String keyspaceStrategy;
  private int 	 keyspaceRF;
  
  /**
   * gets the replication strategy
   * @return string class name to be used for strategy
   */
  public String getKeyspaceReplicationStrategy() {
	return this.keyspaceStrategy;
  }
  
  /**
   * gets the replication factor
   * @return int replication factor
   */
  public int getKeyspaceReplicationFactor() {
	return this.keyspaceRF;
  }
    // setting replication strategy
    this.keyspaceStrategy = keyspace.getAttributeValue( REPLICATION_STRATEGY_ATTRIBUTE );
    if( null == this.keyspaceStrategy ) {
    	this.keyspaceStrategy = DEFAULT_REPLICATION_STRATEGY;
    }
	if( LOG.isDebugEnabled() ) {
		LOG.debug( "setting Keyspace replication strategy to "  this.keyspaceStrategy );
	}

	// setting replication factor
	String tmp = keyspace.getAttributeValue( REPLICATION_FACTOR_ATTRIBUTE );	
	if( null == tmp ) {
		tmp = DEFAULT_REPLICATION_FACTOR;
	}
	this.keyspaceRF = Integer.parseInt( tmp );
	if( LOG.isDebugEnabled() ) {
		LOG.debug( "setting Keyspace replication factor to "  this.keyspaceRF );
	}

    
        LOG.warn("Using gc_grace_seconds default value which is: "  DEFAULT_GCGRACE_SECONDS 
        		 " and is viable ONLY FOR A SINGLE NODE CLUSTER");
        LOG.warn("please update the gora-cassandra-mapping.xml file to avoid seeing this warning");
      cfDef.setGcGraceSeconds(Integer.parseInt( gcgrace_scs!=null?gcgrace_scs:DEFAULT_GCGRACE_SECONDS));
	int ttl = Integer.parseInt(ttlAttr);
	HColumn<ByteBuffer,ByteBuffer> col = HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get());
	
	if( 0 < ttl ) {
		col.setTtl( ttl ); 
	}
	
	return col;
	int ttl = Integer.parseInt(ttlAttr);
	HColumn<String,ByteBuffer> col = HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get());
	
	if( 0 < ttl ) {
		col.setTtl( ttl );
	}
	
	return col;
    int ttl = Integer.parseInt(ttlAttr);
    HColumn<Integer,ByteBuffer> col = HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get());
    
    if( 0 < ttl ) {
    	col.setTtl( ttl );
    }
    
	return col;
 * WebServiceBackedDataStore supplies necessary interfaces to set input
 * and output paths for data stored which are web based.
      "file backed data stores");
 * A Factory for Web-based {@link DataStore}s. DataStoreFactory instances are thread-safe.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param auth an authentication {@link Object} to be used for communication.
   * @param properties The properties to be used be the store.
   * @param store the datastore class we wish to search for the property within.
   * @param baseKey the key associated with the property we are trying to find. 
   * @param defaultValue a default (fallback) value for the property we wish to assert.
   * @param properties The properties to be used be the store.
   * @param store the datastore class we wish to search for the property within.
   * @param baseKey the key associated with the property we are trying to find. 
  /**
   * Asserts whether or not we can find a particular property based on the property 
   * with the given baseKey. First the property
   * key constructed as "gora.&lt;classname&gt;.&lt;baseKey&gt;" is searched.
   * If not found, the property keys for all superclasses is recursively
   * tested. Lastly, the property key constructed as
   * "gora.datastore.&lt;baseKey&gt;" is searched. We parse the boolean output
   * and assert whether such a property exists or not.
   * @param properties The properties to be used be the store.
   * @param store the datastore class we wish to search for the property within.
   * @param baseKey the key associated with the property we are trying to find. 
   * @param defaultValue a default (fallback) value for the property we wish to assert.
   * @return the first found value, or throws IOException
   */
  /**
   * Asserts whether schema/datamodel autocreation is supported within datastore
   * initialization. Assentially we scan the datastore class for a static AUTO_CREATE_SCHEMA
   * constant. Additionally, we expect this to be true for each datastore.
   * @param properties The properties to be used be the store.
   * @param store the datastore class we wish to search for the property within.
   * @return the first found value, or throws IOException
   */
  /**
   * Returns a particular instance of an XML mapping file for some particular 
   * properties and datastore. We can specify numerous mapping files per 
   * mapping configuration with this option and obtain them at runtime.
   * @param properties The properties to be used be the store.
   * @param store The datastore class we wish to search for the property within.
   * @param defaultValue The name of the XML mapping file we wish to use.
   * @return
   */
   * @param obj Object to read from.
   * @param obj Object to write.
   * Get the service provider name.
   * @return the service provider name.
   * @param wsProvider Name to set the service provider to.
  @SuppressWarnings({ "unchecked", "rawtypes" })
    return new DirtyListWrapper<Object>(rlist);
    return new DirtyMapWrapper<Utf8, Object>(rmap);
  @SuppressWarnings("unchecked")
        LOG.warn("Check that 'keyClass' and 'name' parameters in gora-solr-mapping.xml "
             "match with intended values. A mapping mismatch has been found therefore "
             "no mapping has been initialized for class mapping at position " 
             classes.indexOf(classElement)  " in mapping file.");
        list.add(field  CassandraStore.UNION_COL_SUFIX);
        map.put(family  ":"  field  CassandraStore.UNION_COL_SUFIX, field  CassandraStore.UNION_COL_SUFIX);
    query.setFields(fields);
      enc.flush();
    return ByteBuffer.wrap(get(0).toString().getBytes());
    if (obj == null) {
      return null;
    }
    return (ByteBuffer) get(0);
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;

import java.util.Properties;
/**
 * CassandraClient is where all of the primary datastore functionality is 
 * executed. Typically CassandraClient is invoked by calling 
 * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}.
 * CassandraClient deals with Cassandra data model definition, mutation, 
 * and general/specific mappings.
 * @see {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)} 
 *
 * @param <K>
 * @param <T>
 */
  
  /** The logging implementation */
  /** Object which holds the XML mapping for Cassandra. */
  /** Hector client default column family consistency level. */
  public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";
  
  /** Cassandra serializer to be used for serializing Gora's keys. */
  /**
   * Given our key, persistentClass from 
   * {@link org.apache.gora.cassandra.store.CassandraStore#initialize(Class, Class, Properties)}
   * we make best efforts to dictate our data model. 
   * We make a quick check within {@link org.apache.gora.cassandra.store.CassandraClient#checkKeyspace(String)
   * to see if our keyspace has already been invented, this simple check prevents us from 
   * recreating the keyspace if it already exists. 
   * We then simple specify (based on the input keyclass) an appropriate serializer
   * via {@link org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer} before
   * defining a mutator from and by which we can mutate this object.
   * @param keyClass the Key by which we wish o assign a record object
   * @param persistentClass the generated {@link org.apache.org.gora.persistency.Peristent} bean representing the data.
   * @throws Exception
   */
    this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), 
        new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
    // Just create a Keyspace object on the client side, corresponding to an already 
    // existing keyspace with already created column families.
   * In this method, we also utilize Hector's 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel} logic. 
   * It is set by passing a 
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel} object right 
   * when the {@link me.prettyprint.hector.api.Keyspace} is created. 
   * If we cannot find a consistency level within <code>gora.properites</code>, 
   * then column family consistency level is set to QUORUM (by default) which permits 
   * consistency to wait for a quorum of replicas to respond regardless of data center.
   * QUORUM is Hector Client's default setting and we respect that here as well.
   * 
   * @see http://hector-client.github.io/hector/build/html/content/consistency_level.html
        this.cassandraMapping.getKeyspaceName(), 
      // GORA-167 Create a customized Consistency Level
      ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
      Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
      // Column family consistency levels
      ccl.setReadCfConsistencyLevels(clmap);
      ccl.setWriteCfConsistencyLevels(clmap);
      // Operations consistency levels
      String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector read consistency configured to '"  opConsisLvl  "'.");
      opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
      ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
      LOG.debug("Hector write consistency configured to '"  opConsisLvl  "'.");
      HFactory.createKeyspace("Keyspace", this.cluster, ccl);
   * Method in charge of setting the consistency level for defined column families.
   * @param pColFams  Column families
   * @return Map<String, HConsistencyLevel> with the mapping between colFams and consistency level.
   */
  private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
    Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
    // Get columnFamily consistency level.
    String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
    LOG.debug("ColumnFamily consistency level configured to '"  colFamConsisLvl  "'.");
    // Define consistency for ColumnFamily "ColumnFamily"
    for (ColumnFamilyDefinition colFamDef : pColFams)
      clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
    return clMap;
  }
  
  /**
   * @param key the row key
   * @param fieldName the field name
   * @param columnName the column name (the member name, or the index of array)
   * @param value the member value
   * @param key the row key
   * @param fieldName the field name
   * @param columnName the column name (the member name, or the index of array)
   * @param value the member value
  
    RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery
        (this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
    RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery
        (this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
    return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
  /** Consistency property level for Cassandra column families */
  private static final String COL_FAM_CL = "cf.consistency.level";
   
  /** Consistency property level for Cassandra read operations. */
  private static final String READ_OP_CL = "read.consistency.level";
  
  /** Consistency property level for Cassandra write operations. */
  private static final String WRITE_OP_CL = "write.consistency.level";
  
  /** Variables to hold different consistency levels defined by the properties. */
  public static String colFamConsLvl;
  public static String readOpConsLvl;
  public static String writeOpConsLvl;
  
  private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
      if (autoCreateSchema) {
        // If this is not set, then each Cassandra client should set its default
        // column family
        colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
        // operations
        readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
        writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
      }
      preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
      dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
      dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
      consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
import org.apache.accumulo.core.security.CredentialHelper;
 * Directs CRUD operations into Accumulo.
    } else {
        AuthenticationToken token = new PasswordToken(password);
          conn = new MockInstance().getConnector(user, token);
        credentials = CredentialHelper.create(user, token, conn.getInstance().getInstanceID());
    } catch(IOException e){
    }
          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
        currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
          currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
    String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(),
        Object o = val.get(field.pos());

      Object mapVal = ((Entry<?, ?>) entry).getValue();

    }
    }
    }
import java.util.regex.Matcher;
import java.util.regex.Pattern;
      String fullKey = GORA  "."  org.apache.gora.util.StringUtils.getClassname(clazz).toLowerCase()  "."  baseKey;
    String regex = "[a-z_\\.]*";
    if (!key.matches(regex)) {
      log.warn("Keys should be LOWERCASE. Please change that!");
      log.warn("Using lowecase for key "  key);
      key = key.toLowerCase();
    }
  protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batch_size";
  protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commit_within";
  protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.results_size";
      for (int i = 0; i < fields.size(); i) {
      //Schema newSchema = getSchemaWithDirtySupport(originalSchema, queue);
      Schema newSchema = originalSchema;
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}],\"default\":null}");
    NAME(0, "name"),
    DATE_OF_BIRTH(1, "dateOfBirth"),
    SSN(2, "ssn"),
    SALARY(3, "salary"),
    BOSS(4, "boss"),
    WEBPAGE(5, "webpage"),
  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return Employee._ALL_FIELDS.length;
  }

    case 0: return name;
    case 1: return dateOfBirth;
    case 2: return ssn;
    case 3: return salary;
    case 4: return boss;
    case 5: return webpage;
  @SuppressWarnings(value="unchecked")
    case 0: name = (java.lang.CharSequence)(value); break;
    case 1: dateOfBirth = (java.lang.Long)(value); break;
    case 2: ssn = (java.lang.CharSequence)(value); break;
    case 3: salary = (java.lang.Integer)(value); break;
    case 4: boss = (java.lang.Object)(value); break;
    case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
    setDirty(0);
    return isDirty(0);
    setDirty(1);
    return isDirty(1);
    setDirty(2);
    return isDirty(2);
    setDirty(3);
    return isDirty(3);
    setDirty(4);
    return isDirty(4);
    setDirty(5);
    return isDirty(5);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.name)) {
        this.name = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.name);
      if (isValidValue(fields()[1], other.dateOfBirth)) {
        this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[1].schema(), other.dateOfBirth);
      if (isValidValue(fields()[2], other.ssn)) {
        this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[2].schema(), other.ssn);
      if (isValidValue(fields()[3], other.salary)) {
        this.salary = (java.lang.Integer) data().deepCopy(fields()[3].schema(), other.salary);
      if (isValidValue(fields()[4], other.boss)) {
        this.boss = (java.lang.Object) data().deepCopy(fields()[4].schema(), other.boss);
      if (isValidValue(fields()[5], other.webpage)) {
        this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[5].schema(), other.webpage);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
      validate(fields()[1], value);
      fieldSetFlags()[1] = true;
      return fieldSetFlags()[1];
      fieldSetFlags()[1] = false;
      validate(fields()[2], value);
      fieldSetFlags()[2] = true;
      return fieldSetFlags()[2];
      fieldSetFlags()[2] = false;
      validate(fields()[3], value);
      fieldSetFlags()[3] = true;
      return fieldSetFlags()[3];
      fieldSetFlags()[3] = false;
      validate(fields()[4], value);
      fieldSetFlags()[4] = true;
      return fieldSetFlags()[4];
      fieldSetFlags()[4] = false;
      validate(fields()[5], value);
      fieldSetFlags()[5] = true;
      return fieldSetFlags()[5];
      fieldSetFlags()[5] = false;
        record.name = fieldSetFlags()[0] ? this.name : (java.lang.CharSequence) defaultValue(fields()[0]);
        record.dateOfBirth = fieldSetFlags()[1] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[1]);
        record.ssn = fieldSetFlags()[2] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[2]);
        record.salary = fieldSetFlags()[3] ? this.salary : (java.lang.Integer) defaultValue(fields()[3]);
        record.boss = fieldSetFlags()[4] ? this.boss : (java.lang.Object) defaultValue(fields()[4]);
        record.webpage = fieldSetFlags()[5] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[5]);
	  		  /**
}

  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"ImmutableFields\",\"namespace\":\"org.apache.gora.examples.generated\",\"doc\":\"Record with only immutable or dirtyable fields, used for testing\",\"fields\":[{\"name\":\"v1\",\"type\":\"int\",\"default\":0},{\"name\":\"v2\",\"type\":[{\"type\":\"record\",\"name\":\"V2\",\"fields\":[{\"name\":\"v3\",\"type\":\"int\",\"default\":0}]},\"null\"],\"default\":null}]}");
    V1(0, "v1"),
    V2(1, "v2"),
  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return ImmutableFields._ALL_FIELDS.length;
  }

    case 0: return v1;
    case 1: return v2;
    case 0: v1 = (java.lang.Integer)(value); break;
    case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
    setDirty(0);
    return isDirty(0);
    setDirty(1);
    return isDirty(1);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.v1)) {
        this.v1 = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.v1);
      if (isValidValue(fields()[1], other.v2)) {
        this.v2 = (org.apache.gora.examples.generated.V2) data().deepCopy(fields()[1].schema(), other.v2);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
      validate(fields()[1], value);
      fieldSetFlags()[1] = true;
      return fieldSetFlags()[1];
      fieldSetFlags()[1] = false;
        record.v1 = fieldSetFlags()[0] ? this.v1 : (java.lang.Integer) defaultValue(fields()[0]);
        record.v2 = fieldSetFlags()[1] ? this.v2 : (org.apache.gora.examples.generated.V2) defaultValue(fields()[1]);
	  		  /**
}

  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");
    VERSION(0, "version"),
    DATA(1, "data"),
  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return Metadata._ALL_FIELDS.length;
  }

    case 0: return version;
    case 1: return data;
    case 0: version = (java.lang.Integer)(value); break;
    case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    setDirty(0);
    return isDirty(0);
    setDirty(1);
    return isDirty(1);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.version)) {
        this.version = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.version);
      if (isValidValue(fields()[1], other.data)) {
        this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[1].schema(), other.data);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
      validate(fields()[1], value);
      fieldSetFlags()[1] = true;
      return fieldSetFlags()[1];
      fieldSetFlags()[1] = false;
        record.version = fieldSetFlags()[0] ? this.version : (java.lang.Integer) defaultValue(fields()[0]);
        record.data = fieldSetFlags()[1] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[1]));
	  		  /**
}

  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");
    COUNT(0, "count"),
  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return TokenDatum._ALL_FIELDS.length;
  }

    case 0: return count;
    case 0: count = (java.lang.Integer)(value); break;
    setDirty(0);
    return isDirty(0);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.count)) {
        this.count = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.count);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
        record.count = fieldSetFlags()[0] ? this.count : (java.lang.Integer) defaultValue(fields()[0]);
	  		  /**
}

  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"V2\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"v3\",\"type\":\"int\",\"default\":0}]}");
    V3(0, "v3"),
  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return V2._ALL_FIELDS.length;
  }

    case 0: return v3;
    case 0: v3 = (java.lang.Integer)(value); break;
    setDirty(0);
    return isDirty(0);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.v3)) {
        this.v3 = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.v3);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
        record.v3 = fieldSetFlags()[0] ? this.v3 : (java.lang.Integer) defaultValue(fields()[0]);
	  		  /**
}

  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":null},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]},\"default\":null}],\"default\":null}");
    URL(0, "url"),
    CONTENT(1, "content"),
    PARSED_CONTENT(2, "parsedContent"),
    OUTLINKS(3, "outlinks"),
    HEADERS(4, "headers"),
    METADATA(5, "metadata"),
  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return WebPage._ALL_FIELDS.length;
  }

    case 0: return url;
    case 1: return content;
    case 2: return parsedContent;
    case 3: return outlinks;
    case 4: return headers;
    case 5: return metadata;
    case 0: url = (java.lang.CharSequence)(value); break;
    case 1: content = (java.nio.ByteBuffer)(value); break;
    case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
    case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
    case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
    setDirty(0);
    return isDirty(0);
    setDirty(1);
    return isDirty(1);
    setDirty(2);
    return isDirty(2);
    setDirty(3);
    return isDirty(3);
    setDirty(4);
    return isDirty(4);
    setDirty(5);
    return isDirty(5);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.url);
      if (isValidValue(fields()[1], other.content)) {
        this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[1].schema(), other.content);
      if (isValidValue(fields()[2], other.parsedContent)) {
        this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.parsedContent);
      if (isValidValue(fields()[3], other.outlinks)) {
        this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.outlinks);
      if (isValidValue(fields()[4], other.headers)) {
        this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.headers);
      if (isValidValue(fields()[5], other.metadata)) {
        this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[5].schema(), other.metadata);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
      validate(fields()[1], value);
      fieldSetFlags()[1] = true;
      return fieldSetFlags()[1];
      fieldSetFlags()[1] = false;
      validate(fields()[2], value);
      fieldSetFlags()[2] = true;
      return fieldSetFlags()[2];
      fieldSetFlags()[2] = false;
      validate(fields()[3], value);
      fieldSetFlags()[3] = true;
      return fieldSetFlags()[3];
      fieldSetFlags()[3] = false;
      validate(fields()[4], value);
      fieldSetFlags()[4] = true;
      return fieldSetFlags()[4];
      fieldSetFlags()[4] = false;
      validate(fields()[5], value);
      fieldSetFlags()[5] = true;
      return fieldSetFlags()[5];
      fieldSetFlags()[5] = false;
        record.url = fieldSetFlags()[0] ? this.url : (java.lang.CharSequence) defaultValue(fields()[0]);
        record.content = fieldSetFlags()[1] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[1]);
        record.parsedContent = fieldSetFlags()[2] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[2]));
        record.outlinks = fieldSetFlags()[3] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[3]));
        record.headers = fieldSetFlags()[4] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[4]);
        record.metadata = fieldSetFlags()[5] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
	  		  /**
}

  /** Bytes used to represent weather or not a field is dirty. */
  private java.nio.ByteBuffer __g__dirty;

  public PersistentBase() {
    __g__dirty = java.nio.ByteBuffer.wrap(new byte[getFieldsCount()]);
  }

  public abstract int getFieldsCount();

    return __g__dirty;
    //return fields.subList(1, fields.size());
    return fields;
    String[] fieldNames = new String[fields.size()];
      fieldNames[i] = fields.get(i).name();

  /**
   * Gets the total field count.
   */
  public int getFieldsCount() {
    return MockPersistent._ALL_FIELDS.length;
  }

      for (int i = 0; i < fields.size(); i) {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}],\"default\":null}");

  /** Enum containing all data bean's fields. */
  public static enum Field {
    METRIC_DIMENSION(0, "metricDimension"),
    TIMESTAMP(1, "timestamp"),
    METRIC(2, "metric"),
    ;
    /**
     * Field's index.
     */
    private int index;

    /**
     * Field's name.
     */
    private String name;

    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    Field(int index, String name) {this.index=index;this.name=name;}

    /**
     * Gets field's index.
     * @return int field's index.
     */
    public int getIndex() {return index;}

    /**
     * Gets field's name.
     * @return String field's name.
     */
    public String getName() {return name;}

    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    public String toString() {return name;}
  };

  public static final String[] _ALL_FIELDS = {
  "metricDimension",
  "timestamp",
  "metric",
  };

  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return MetricDatum._ALL_FIELDS.length;
  }

    case 0: return metricDimension;
    case 1: return timestamp;
    case 2: return metric;
    case 0: metricDimension = (java.lang.CharSequence)(value); break;
    case 1: timestamp = (java.lang.Long)(value); break;
    case 2: metric = (java.lang.Long)(value); break;
    setDirty(0);
    return isDirty(0);
    setDirty(1);
    return isDirty(1);
    setDirty(2);
    return isDirty(2);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.metricDimension)) {
        this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.metricDimension);
      if (isValidValue(fields()[1], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[1].schema(), other.timestamp);
      if (isValidValue(fields()[2], other.metric)) {
        this.metric = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.metric);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
      validate(fields()[1], value);
      fieldSetFlags()[1] = true;
      return fieldSetFlags()[1];
      fieldSetFlags()[1] = false;
      validate(fields()[2], value);
      fieldSetFlags()[2] = true;
      return fieldSetFlags()[2];
      fieldSetFlags()[2] = false;
        record.metricDimension = fieldSetFlags()[0] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[0]);
        record.timestamp = fieldSetFlags()[1] ? this.timestamp : (java.lang.Long) defaultValue(fields()[1]);
        record.metric = fieldSetFlags()[2] ? this.metric : (java.lang.Long) defaultValue(fields()[2]);
	  		  /**
}

  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}],\"default\":null}");

  /** Enum containing all data bean's fields. */
  public static enum Field {
    URL(0, "url"),
    TIMESTAMP(1, "timestamp"),
    IP(2, "ip"),
    HTTP_METHOD(3, "httpMethod"),
    HTTP_STATUS_CODE(4, "httpStatusCode"),
    RESPONSE_SIZE(5, "responseSize"),
    REFERRER(6, "referrer"),
    USER_AGENT(7, "userAgent"),
    ;
    /**
     * Field's index.
     */
    private int index;

    /**
     * Field's name.
     */
    private String name;

    /**
     * Field's constructor
     * @param index field's index.
     * @param name field's name.
     */
    Field(int index, String name) {this.index=index;this.name=name;}

    /**
     * Gets field's index.
     * @return int field's index.
     */
    public int getIndex() {return index;}

    /**
     * Gets field's name.
     * @return String field's name.
     */
    public String getName() {return name;}

    /**
     * Gets field's attributes to string.
     * @return String field's attributes to string.
     */
    public String toString() {return name;}
  };

  public static final String[] _ALL_FIELDS = {
  "url",
  "timestamp",
  "ip",
  "httpMethod",
  "httpStatusCode",
  "responseSize",
  "referrer",
  "userAgent",
  };

  /**
   * Gets the total field count.
   * @return int field count
   */
  public int getFieldsCount() {
    return Pageview._ALL_FIELDS.length;
  }

    case 0: return url;
    case 1: return timestamp;
    case 2: return ip;
    case 3: return httpMethod;
    case 4: return httpStatusCode;
    case 5: return responseSize;
    case 6: return referrer;
    case 7: return userAgent;
    case 0: url = (java.lang.CharSequence)(value); break;
    case 1: timestamp = (java.lang.Long)(value); break;
    case 2: ip = (java.lang.CharSequence)(value); break;
    case 3: httpMethod = (java.lang.CharSequence)(value); break;
    case 4: httpStatusCode = (java.lang.Integer)(value); break;
    case 5: responseSize = (java.lang.Integer)(value); break;
    case 6: referrer = (java.lang.CharSequence)(value); break;
    case 7: userAgent = (java.lang.CharSequence)(value); break;
    setDirty(0);
    return isDirty(0);
    setDirty(1);
    return isDirty(1);
    setDirty(2);
    return isDirty(2);
    setDirty(3);
    return isDirty(3);
    setDirty(4);
    return isDirty(4);
    setDirty(5);
    return isDirty(5);
    setDirty(6);
    return isDirty(6);
    setDirty(7);
    return isDirty(7);
  private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
      if (isValidValue(fields()[0], other.url)) {
        this.url = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.url);
      if (isValidValue(fields()[1], other.timestamp)) {
        this.timestamp = (java.lang.Long) data().deepCopy(fields()[1].schema(), other.timestamp);
      if (isValidValue(fields()[2], other.ip)) {
        this.ip = (java.lang.CharSequence) data().deepCopy(fields()[2].schema(), other.ip);
      if (isValidValue(fields()[3], other.httpMethod)) {
        this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.httpMethod);
      if (isValidValue(fields()[4], other.httpStatusCode)) {
        this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.httpStatusCode);
      if (isValidValue(fields()[5], other.responseSize)) {
        this.responseSize = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.responseSize);
      if (isValidValue(fields()[6], other.referrer)) {
        this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[6].schema(), other.referrer);
      if (isValidValue(fields()[7], other.userAgent)) {
        this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.userAgent);
      validate(fields()[0], value);
      fieldSetFlags()[0] = true;
      return fieldSetFlags()[0];
      fieldSetFlags()[0] = false;
      validate(fields()[1], value);
      fieldSetFlags()[1] = true;
      return fieldSetFlags()[1];
      fieldSetFlags()[1] = false;
      validate(fields()[2], value);
      fieldSetFlags()[2] = true;
      return fieldSetFlags()[2];
      fieldSetFlags()[2] = false;
      validate(fields()[3], value);
      fieldSetFlags()[3] = true;
      return fieldSetFlags()[3];
      fieldSetFlags()[3] = false;
      validate(fields()[4], value);
      fieldSetFlags()[4] = true;
      return fieldSetFlags()[4];
      fieldSetFlags()[4] = false;
      validate(fields()[5], value);
      fieldSetFlags()[5] = true;
      return fieldSetFlags()[5];
      fieldSetFlags()[5] = false;
      validate(fields()[6], value);
      fieldSetFlags()[6] = true;
      return fieldSetFlags()[6];
      fieldSetFlags()[6] = false;
      validate(fields()[7], value);
      fieldSetFlags()[7] = true;
      return fieldSetFlags()[7];
      fieldSetFlags()[7] = false;
        record.url = fieldSetFlags()[0] ? this.url : (java.lang.CharSequence) defaultValue(fields()[0]);
        record.timestamp = fieldSetFlags()[1] ? this.timestamp : (java.lang.Long) defaultValue(fields()[1]);
        record.ip = fieldSetFlags()[2] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[2]);
        record.httpMethod = fieldSetFlags()[3] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[3]);
        record.httpStatusCode = fieldSetFlags()[4] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[4]);
        record.responseSize = fieldSetFlags()[5] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[5]);
        record.referrer = fieldSetFlags()[6] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[6]);
        record.userAgent = fieldSetFlags()[7] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[7]);
    return TOMBSTONE;
          /**
     * Gets the value of the 'url' field.
       */
    public java.lang.CharSequence getUrl() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'url' field.
       * @param value the value to set.
     */
    public void setUrl(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isUrlDirty(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
          /**
     * Gets the value of the 'timestamp' field.
       */
    public java.lang.Long getTimestamp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'timestamp' field.
       * @param value the value to set.
     */
    public void setTimestamp(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isTimestampDirty(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
          /**
     * Gets the value of the 'ip' field.
       */
    public java.lang.CharSequence getIp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'ip' field.
       * @param value the value to set.
     */
    public void setIp(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isIpDirty(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
          /**
     * Gets the value of the 'httpMethod' field.
       */
    public java.lang.CharSequence getHttpMethod() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'httpMethod' field.
       * @param value the value to set.
     */
    public void setHttpMethod(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isHttpMethodDirty(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
          /**
     * Gets the value of the 'httpStatusCode' field.
       */
    public java.lang.Integer getHttpStatusCode() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'httpStatusCode' field.
       * @param value the value to set.
     */
    public void setHttpStatusCode(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
          /**
     * Gets the value of the 'responseSize' field.
       */
    public java.lang.Integer getResponseSize() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'responseSize' field.
       * @param value the value to set.
     */
    public void setResponseSize(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isResponseSizeDirty(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
          /**
     * Gets the value of the 'referrer' field.
       */
    public java.lang.CharSequence getReferrer() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'referrer' field.
       * @param value the value to set.
     */
    public void setReferrer(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isReferrerDirty(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
          /**
     * Gets the value of the 'userAgent' field.
       */
    public java.lang.CharSequence getUserAgent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
  
    /**
     * Sets the value of the 'userAgent' field.
       * @param value the value to set.
     */
    public void setUserAgent(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    
    /**
     * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
       * @param value the value to set.
     */
    public boolean isUserAgentDirty(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }
  
      
}

import java.net.MalformedURLException;
import java.net.URLClassLoader;
    } catch (MalformedURLException ex) {
      LOG.error("Error while trying to read the mapping file {}. "
               "Expected to be in the classpath "
               "(ClassLoader#getResource(java.lang.String)).",
              filename) ;
      LOG.error("Actual classpath = {}", Arrays.asList(
          ((URLClassLoader) getClass().getClassLoader()).getURLs()));
      throw ex ;
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
import java.net.MalformedURLException;
import java.net.URLClassLoader;
    } catch (MalformedURLException ex) {
      LOG.error("Error while trying to read the mapping file {}. "
               "Expected to be in the classpath "
               "(ClassLoader#getResource(java.lang.String)).",
              filename) ;
      LOG.error("Actual classpath = {}", Arrays.asList(
          ((URLClassLoader) getClass().getClassLoader()).getURLs()));
      throw ex ;
import org.apache.gora.shims.hadoop.HadoopShim;
import org.apache.gora.shims.hadoop.HadoopShimFactory;
  private static HadoopShim hadoopShim = HadoopShimFactory.INSTANCE().getHadoopShim();

      Job job = hadoopShim.createJob(conf);
      return hadoopShim.createJobContext(job.getConfiguration());
    return hadoopShim.createJobContext(conf);
  @SuppressWarnings("unchecked")
  private static final HadoopShim hadoopShim = HadoopShimFactory.INSTANCE().getHadoopShim();
   * Method to maintain backward compatibility with earlier versions. 
  */
  public void initialize(Class<K> keyClass, Class<T> persistentClass)
    throws Exception {
	initialize(keyClass, persistentClass, null);
  }
  
  /**
   * @param properties key value pairs from gora.properties
  public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) throws Exception {
	Map<String, String> accessMap = null;
	if (properties != null) {
		String username = properties
				.getProperty("gora.cassandrastore.username");
		if (username != null) {
			accessMap = new HashMap<String, String>();
			accessMap.put("username", username);
			String password = properties
					.getProperty("gora.cassandrastore.password");
			if (password != null) {
				accessMap.put("password", password);
			}
		}
	}
        new CassandraHostConfigurator(this.cassandraMapping.getHostName()), accessMap);
      this.cassandraClient.initialize(keyClass, persistent, properties);
    	LOG.warn("Column name is null for field: "  fieldName );
    if( LOG.isDebugEnabled() ) LOG.debug( "fieldName: "fieldName " columnName: "  columnName );
    	if( LOG.isDebugEnabled() ) LOG.debug( "ttl was not set for field: "  fieldName  ". Using "  ttlAttr );
    	if( LOG.isDebugEnabled() ) LOG.debug( "ttl for field: "  fieldName  " is "  ttlAttr );
      LOG.warn("Trace: "  e.getStackTrace());
      e.printStackTrace();
      LOG.warn("Trace: "  e.getStackTrace());
      e.printStackTrace();
   * Method to maintain backward compatibility with earlier versions. 
  */
  public void initialize(Class<K> keyClass, Class<T> persistentClass)
    throws Exception {
	initialize(keyClass, persistentClass, null);
  }
  
  /**
   * @param properties key value pairs from gora.properties
  public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) throws Exception {
	Map<String, String> accessMap = null;
	if (properties != null) {
		String username = properties
				.getProperty("gora.cassandrastore.username");
		if (username != null) {
			accessMap = new HashMap<String, String>();
			accessMap.put("username", username);
			String password = properties
					.getProperty("gora.cassandrastore.password");
			if (password != null) {
				accessMap.put("password", password);
			}
		}
	}
        new CassandraHostConfigurator(this.cassandraMapping.getHostName()), accessMap);
    	LOG.warn("Column name is null for field: "  fieldName );
    if( LOG.isDebugEnabled() ) LOG.debug( "fieldName: "fieldName " columnName: "  columnName );
    	if( LOG.isDebugEnabled() ) LOG.debug( "ttl was not set for field: "  fieldName  ". Using "  ttlAttr );
    	if( LOG.isDebugEnabled() ) LOG.debug( "ttl for field: "  fieldName  " is "  ttlAttr );
      this.cassandraClient.initialize(keyClass, persistent, properties);
      LOG.warn("Trace: "  e.getStackTrace());
      e.printStackTrace();
      LOG.warn("Trace: "  e.getStackTrace());
      e.printStackTrace();
    Map<String, String> accessMap = null;
    if (properties != null) {
      String username = properties
          .getProperty("gora.cassandrastore.username");
      if (username != null) {
        accessMap = new HashMap<String, String>();
        accessMap.put("username", username);
        String password = properties
            .getProperty("gora.cassandrastore.password");
        if (password != null) {
          accessMap.put("password", password);
        }
      }
    }
 * 
  public boolean isNameDirty() {
  public boolean isDateOfBirthDirty() {
  public boolean isSsnDirty() {
  public boolean isSalaryDirty() {
  public boolean isBossDirty() {
  public boolean isWebpageDirty() {
	  public boolean isNameDirty() {
	  public boolean isDateOfBirthDirty() {
	  public boolean isSsnDirty() {
	  public boolean isSalaryDirty() {
	  public boolean isBossDirty() {
	  public boolean isWebpageDirty() {
 * 
  public boolean isV1Dirty() {
  public boolean isV2Dirty() {
	  public boolean isV1Dirty() {
	  public boolean isV2Dirty() {
 * 
  public boolean isVersionDirty() {
  public boolean isDataDirty() {
	  public boolean isVersionDirty() {
	  public boolean isDataDirty() {
 * 
  public boolean isCountDirty() {
	  public boolean isCountDirty() {
 * 
  public boolean isV3Dirty() {
	  public boolean isV3Dirty() {
 * 
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");
  public boolean isUrlDirty() {
  public boolean isContentDirty() {
  public boolean isParsedContentDirty() {
  public boolean isOutlinksDirty() {
  public boolean isHeadersDirty() {
  public boolean isMetadataDirty() {
	  public boolean isUrlDirty() {
	  public boolean isContentDirty() {
	  public boolean isParsedContentDirty() {
	  public boolean isOutlinksDirty() {
	  public boolean isHeadersDirty() {
	  public boolean isMetadataDirty() {
 * 
  public boolean isPrevDirty() {
  public boolean isClientDirty() {
  public boolean isCountDirty() {
	  public boolean isPrevDirty() {
	  public boolean isClientDirty() {
	  public boolean isCountDirty() {
 * 
  public boolean isCountDirty() {
	  public boolean isCountDirty() {
  public boolean isMetricDimensionDirty() {
  public boolean isTimestampDirty() {
  public boolean isMetricDirty() {
	  public boolean isMetricDimensionDirty() {
	  public boolean isTimestampDirty() {
	  public boolean isMetricDirty() {
  public boolean isUrlDirty() {
  public boolean isTimestampDirty() {
  public boolean isIpDirty() {
  public boolean isHttpMethodDirty() {
  public boolean isHttpStatusCodeDirty() {
  public boolean isResponseSizeDirty() {
  public boolean isReferrerDirty() {
  public boolean isUserAgentDirty() {
  	return TOMBSTONE;
	  		  /**
	   * Gets the value of the 'url' field.
		   */
	  public java.lang.CharSequence getUrl() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'url' field.
		   * @param value the value to set.
	   */
	  public void setUrl(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUrlDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'timestamp' field.
		   */
	  public java.lang.Long getTimestamp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'timestamp' field.
		   * @param value the value to set.
	   */
	  public void setTimestamp(java.lang.Long value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isTimestampDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'ip' field.
		   */
	  public java.lang.CharSequence getIp() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'ip' field.
		   * @param value the value to set.
	   */
	  public void setIp(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isIpDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpMethod' field.
		   */
	  public java.lang.CharSequence getHttpMethod() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpMethod' field.
		   * @param value the value to set.
	   */
	  public void setHttpMethod(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpMethodDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'httpStatusCode' field.
		   */
	  public java.lang.Integer getHttpStatusCode() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'httpStatusCode' field.
		   * @param value the value to set.
	   */
	  public void setHttpStatusCode(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isHttpStatusCodeDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'responseSize' field.
		   */
	  public java.lang.Integer getResponseSize() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'responseSize' field.
		   * @param value the value to set.
	   */
	  public void setResponseSize(java.lang.Integer value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isResponseSizeDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'referrer' field.
		   */
	  public java.lang.CharSequence getReferrer() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'referrer' field.
		   * @param value the value to set.
	   */
	  public void setReferrer(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isReferrerDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'userAgent' field.
		   */
	  public java.lang.CharSequence getUserAgent() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'userAgent' field.
		   * @param value the value to set.
	   */
	  public void setUserAgent(java.lang.CharSequence value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isUserAgentDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
		  
    // Build the corresponding persistent
    return newInstance(res, dbFields);
  /* pp */ Object fromMongoList(final String docf, final Schema fieldSchema,
                       final BSONDecorator easybson, final Field f) {
    List<Object> rlist = new ArrayList<Object>();
      return new DirtyListWrapper(rlist);
  /* pp */ Object fromMongoMap(final String docf, final Schema fieldSchema,
                      final BSONDecorator easybson, final Field f) {
    if (map == null) {
        return new DirtyMapWrapper(rmap);
    }
    BasicDBObject map = new BasicDBObject();
      return map;
    BasicDBList list = new BasicDBList();
      return list;
    } else if (value instanceof CharSequence) {
      serializer = CharSequenceSerializer.get();
    if (valueClass.equals(Utf8.class) || valueClass.equals(CharSequence.class)) {
    if (this.keySerializer == null)
      LOG.error("Serializer for "  keyClass  " not found.");
      if (pValue instanceof CharSequence && schemaType.equals(Type.STRING))
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.io.compress.Compression.Algorithm;
import org.apache.hadoop.hbase.regionserver.BloomType;
    private TableName tableName;
      return tableName.getNameAsString();
      this.tableName = TableName.valueOf(tableName);
      Map<String, HColumnDescriptor> families = tableToFamilies.get(tableName.getNameAsString());
      String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getHostname();
import java.io.InterruptedIOException;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Durability;
import org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException;
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
import com.google.protobuf.Descriptors.MethodDescriptor;
import com.google.protobuf.Message;
import com.google.protobuf.Service;
import com.google.protobuf.ServiceException;

  private final boolean autoFlush;
  private final TableName tableName;
    this.tableName = TableName.valueOf(tableName);
    this.autoFlush = autoflush;
        public synchronized void flushCommits() throws RetriesExhaustedWithDetailsException, InterruptedIOException {
      table.setAutoFlushTo(autoFlush);
    return tableName.getName();
    return autoFlush;
  @Deprecated
    getTable().mutateRow(rm);    
    return getTable().append(append);
  public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail){
  }

  @Override
  public TableName getName() {
    return tableName;
  }

  @Override
  public Boolean[] exists(List<Get> gets) throws IOException {
    return getTable().exists(gets);
  }

  @Override
  public <R> void
      batchCallback(List<? extends Row> actions, Object[] results, Callback<R> callback)
          throws IOException, InterruptedException {
    getTable().batchCallback(actions, results, callback);

  @Deprecated
  @Override
  public <R> Object[] batchCallback(List<? extends Row> actions, Callback<R> callback)
      throws IOException, InterruptedException {
    return getTable().batchCallback(actions, callback);
  }

  @Override
  public long incrementColumnValue(byte[] row, byte[] family, byte[] qualifier, long amount,
      Durability durability) throws IOException {
    return getTable().incrementColumnValue(row, family, qualifier, amount,durability);
  }

  @Override
  public CoprocessorRpcChannel coprocessorService(byte[] row) {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public <T extends Service, R> Map<byte[], R> coprocessorService(Class<T> service,
      byte[] startKey, byte[] endKey, Call<T, R> callable) throws ServiceException, Throwable {
    return getTable().coprocessorService(service, startKey, endKey, callable);
  }

  @Override
  public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
      byte[] endKey, Call<T, R> callable, Callback<R> callback) throws ServiceException, Throwable {
    getTable().coprocessorService(service, startKey, endKey, callable, callback);;
  }

  @Override
  public void setAutoFlushTo(boolean autoFlush) {
    // TODO Auto-generated method stub    
  }

  @Override
  public <R extends Message> Map<byte[], R> batchCoprocessorService(
      MethodDescriptor methodDescriptor, Message request, byte[] startKey, byte[] endKey,
      R responsePrototype) throws ServiceException, Throwable {
    return getTable().batchCoprocessorService(methodDescriptor, request, startKey, endKey, responsePrototype);
  }

  @Override
  public <R extends Message> void batchCoprocessorService(MethodDescriptor methodDescriptor,
      Message request, byte[] startKey, byte[] endKey, R responsePrototype, Callback<R> callback)
      throws ServiceException, Throwable {
    getTable().batchCoprocessorService(methodDescriptor, request, startKey, endKey, responsePrototype, callback);
    
  }

  @Deprecated
  @Override
  public Object[] batch(List<? extends Row> actions) throws IOException, InterruptedException {
    return getTable().batch(actions);
  }

  @Override
  public boolean checkAndMutate(byte[] arg0, byte[] arg1, byte[] arg2, CompareOp arg3, byte[] arg4,
      RowMutations arg5) throws IOException {
    // TODO Auto-generated method stub
    return false;
  }
      final FilterOp filterOp, final List<Object> rawOperands) {
    List<String> operands = convertOperandsToString(rawOperands);
  /**
   * Transform all Utf8 into String before preparing MongoDB query.
   * <p>Otherwise, you'll get <tt>RuntimeException: json can't serialize type : Utf8</tt></p>
   *
   * @see <a href="https://issues.apache.org/jira/browse/GORA-388">GORA-388</a>
   */
  private List<String> convertOperandsToString(List<Object> rawOperands) {
    List<String> operands = new ArrayList<String>(rawOperands.size());
    for (Object rawOperand : rawOperands) {
      if (rawOperand != null) {
        operands.add(rawOperand.toString());
      }
    }
    return operands;
  }

      LOG.error(e.getMessage(), e);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(e.getMessage(), e);
      LOG.error(ge.getMessage(), ge);
          LOG.error(ex1.getMessage(), ex1);
      LOG.error(ex2.getMessage(), ex2);
      LOG.error(ex2.getMessage(), ex2);
      LOG.error(ex2.getMessage(), ex2);
      LOG.error(ex2.getMessage(), ex2);
      LOG.error(ex2.getMessage(), ex2);
      LOG.error(ex2.getMessage(), ex2);
      LOG.error(ex2.getMessage(), ex2);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(ex.getMessage(), ex);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      // LOG.error(e.getMessage(), e);
        LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
        LOG.error(e.getMessage(), e);
        LOG.error(e.getMessage(), e);
          LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
      LOG.error(e.getMessage(), e);
   * {@link HBaseMapping} using simple immutability.
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.solr.client.solrj.impl.*;
  /** Whether to use secured Solr client or not.
   * Available options include <b>true</b>, and <b>false</b>.
   * Defined in <code>gora.properties</code>
   * This value must be of type <b>boolean</b>.
   */
  protected static final String SOLR_SERVER_USER_AUTH = "solr.solrjserver.user_auth";

  /** Solr client username.
   * Solr client user authentication should be enabled for this property.
   * Defined in <code>gora.properties</code>
   * This value must be of type <b>String</b>.
   */
  protected static final String SOLR_SERVER_USERNAME = "solr.solrjserver.username";

  /** Solr client password.
   * Solr client user authentication should be enabled for this property.
   * Defined in <code>gora.properties</code>
   * This value must be of type <b>String</b>.
   */
  protected static final String SOLR_SERVER_PASSWORD = "solr.solrjserver.password";

  private boolean serverUserAuth;

  private String serverUsername;

  private String serverPassword;

    serverUserAuth = DataStoreFactory.findBooleanProperty(properties, this,
        SOLR_SERVER_USER_AUTH, "false");
    if (serverUserAuth) {
      serverUsername = DataStoreFactory.findProperty(properties, this,
          SOLR_SERVER_USERNAME, null);
      serverPassword = DataStoreFactory.findProperty(properties, this,
          SOLR_SERVER_PASSWORD, null);
    }
      if (serverUserAuth) {
        HttpClientUtil.setBasicAuth(
            (DefaultHttpClient) ((HttpSolrServer) adminServer).getHttpClient(),
            serverUsername, serverPassword);
        HttpClientUtil.setBasicAuth(
            (DefaultHttpClient) ((HttpSolrServer) server).getHttpClient(),
            serverUsername, serverPassword);
      }
      if (serverUserAuth) {
        HttpClientUtil.setBasicAuth(
            (DefaultHttpClient) ((CloudSolrServer) adminServer).getLbServer().getHttpClient(),
            serverUsername, serverPassword);
        HttpClientUtil.setBasicAuth(
            (DefaultHttpClient) ((CloudSolrServer) server).getLbServer().getHttpClient(),
            serverUsername, serverPassword);
      }
      if (serverUserAuth) {
        HttpClientUtil.setBasicAuth(
            (DefaultHttpClient) ((LBHttpSolrServer) adminServer).getHttpClient(),
            serverUsername, serverPassword);
        HttpClientUtil.setBasicAuth(
            (DefaultHttpClient) ((LBHttpSolrServer) server).getHttpClient(),
            serverUsername, serverPassword);
      }
 *
   *
   *
   *
   *
   *
   *
   *
   *
   *
   *
    return (value != null) ? new Utf8(value) : null;
   *
   *
   *
   *
   *
 *
        ResultBase<K, T> {
  private int size;
    if (cursor == null) {
    } else if (size == 0) {
    } else {
      return offset / (float) size;
    }
            getQuery().getFields());
   *
    this.size = cursor.size();
import org.apache.hadoop.conf.Configurable;
    this.clazzV = clazzV;
  public JavaPairRDD<K, V> initializeInput(JavaSparkContext sparkContext,
    GoraMapReduceUtils.setIOSerializations(conf, true);

    try {
      IOUtils
          .storeToConf(dataStore.newQuery(), conf, GoraInputFormat.QUERY_KEY);
    } catch (IOException ioex) {
      throw new RuntimeException(ioex.getMessage());
    }

    return sparkContext.newAPIHadoopRDD(conf, GoraInputFormat.class, clazzK,
        clazzV);
  }

  public JavaPairRDD<K, V> initializeInput(JavaSparkContext sparkContext,
      DataStore<K, V> dataStore) {
    Configuration hadoopConf;

      if ((dataStore instanceof Configurable) && ((Configurable) dataStore).getConf() != null) {
          hadoopConf = ((Configurable) dataStore).getConf();
      } else {
          hadoopConf = new Configuration();
      }

      GoraMapReduceUtils.setIOSerializations(hadoopConf, true);

    try {
      IOUtils.storeToConf(dataStore.newQuery(), hadoopConf,
              GoraInputFormat.QUERY_KEY);
    } catch (IOException ioex) {
      throw new RuntimeException(ioex.getMessage());
    }

    return sparkContext.newAPIHadoopRDD(hadoopConf, GoraInputFormat.class,
        clazzK, clazzV);
  }
import org.apache.hadoop.conf.Configuration;
public class LogAnalyticsSpark {
    if (args.length < 2) {
      System.err.println(USAGE);
      System.exit(1);

    String inStoreClass = args[0];
    String outStoreClass = args[1];

    LogAnalyticsSpark logAnalyticsSpark = new LogAnalyticsSpark();
    int ret = logAnalyticsSpark.run(inStoreClass, outStoreClass);

    System.exit(ret);
  public int run(String inStoreClass, String outStoreClass) throws Exception {
    GoraSpark<Long, Pageview> goraSpark = new GoraSpark<>(Long.class,
        Pageview.class);
    SparkConf sparkConf = new SparkConf().setAppName(
    JavaSparkContext sc = new JavaSparkContext(sparkConf);
    Configuration hadoopConf = new Configuration();

        inStoreClass, Long.class, Pageview.class, hadoopConf);
    JavaPairRDD<Long, Pageview> goraRDD = goraSpark.initializeInput(sc, dataStore);
      
    //todo _fk change architectural desigm
    Class[] c = new Class[1];
    c[0] = Pageview.class;
    sparkConf.registerKryoClasses(c);
    //

    String firstOneURL = goraRDD.first()._2().getUrl().toString();
    System.out.println(firstOneURL);

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;

import scala.Tuple2;

import java.util.concurrent.TimeUnit;
  /** The number of milliseconds in a day */
  private static final long DAY_MILIS = 1000 * 60 * 60 * 24;

  //todo _fk consider using Kyro serialization
  private static Function<Pageview, Tuple2<String, Long>> s = new Function<Pageview, Tuple2<String, Long>>() {
    @Override
    public Tuple2<String, Long> call(Pageview pageview) throws Exception {
      String key = pageview.getUrl().toString();
      Long value = getDay(pageview.getTimestamp());
      return new Tuple2<>(key, value);
    }
  };

  /**
   * Rolls up the given timestamp to the day cardinality, so that data can be
   * aggregated daily
   */
  private static long getDay(long timeStamp) {
    return (timeStamp / DAY_MILIS) * DAY_MILIS;
  }


    // todo _fk consider alternative architectural design
    JavaPairRDD<Long, Pageview> goraRDD = goraSpark.initializeInput(sc,
        dataStore);
    JavaRDD<Tuple2<String, Long>> mappedGoraRdd = goraRDD.values().map(s);

  private static Function<Pageview, Tuple2<Tuple2<String, Long>, Long>> s = new Function<Pageview, Tuple2<Tuple2<String, Long>, Long>> () {
    public Tuple2<Tuple2<String, Long>, Long> call(Pageview pageview) throws Exception {
      String url = pageview.getUrl().toString();
      Long day = getDay(pageview.getTimestamp());
      Tuple2<String, Long> keyTuple =new Tuple2<>(url, day);

      return new Tuple2<>(keyTuple, 1L);
    JavaRDD<Tuple2<Tuple2<String, Long>, Long>> mappedGoraRdd = goraRDD.values().map(s);
  public JavaPairRDD<K, V> initialize(JavaSparkContext sparkContext,
  public JavaPairRDD<K, V> initialize(JavaSparkContext sparkContext,
import org.apache.gora.tutorial.log.generated.MetricDatum;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
  // todo _fk consider using Kyro serialization
  private static Function<Pageview, Tuple2<Tuple2<String, Long>, Long>> mapFunc = new Function<Pageview, Tuple2<Tuple2<String, Long>, Long>>() {
    public Tuple2<Tuple2<String, Long>, Long> call(Pageview pageview)
        throws Exception {
      Tuple2<String, Long> keyTuple = new Tuple2<>(url, day);
  private static Function2<Long, Long, Long> redFunc = new Function2<Long, Long, Long>() {
    @Override
    public Long call(Long aLong, Long aLong2) throws Exception {
      return aLong  aLong2;
    }
  };

  private static PairFunction<Tuple2<Tuple2<String, Long>, Long>, String, MetricDatum> metricFunc = new PairFunction<Tuple2<Tuple2<String, Long>, Long>, String, MetricDatum>() {
    @Override
    public Tuple2<String, MetricDatum> call(
        Tuple2<Tuple2<String, Long>, Long> tuple2LongTuple2) throws Exception {
      String dimension = tuple2LongTuple2._1()._1();
      long timestamp = tuple2LongTuple2._1()._2();

      MetricDatum metricDatum = new MetricDatum();
      metricDatum.setMetricDimension(dimension);
      metricDatum.setTimestamp(timestamp);

      String key = metricDatum.getMetricDimension().toString();
      key = "_"  Long.toString(timestamp);
      metricDatum.setMetric(tuple2LongTuple2._2());
      return new Tuple2<>(key, metricDatum);
    }
  };

    JavaPairRDD<Long, Pageview> goraRDD = goraSpark.initialize(sc,
    JavaRDD<Tuple2<Tuple2<String, Long>, Long>> mappedGoraRdd = goraRDD
        .values().map(mapFunc);

    JavaPairRDD<String, MetricDatum> reducedGoraRdd = JavaPairRDD
        .fromJavaRDD(mappedGoraRdd).reduceByKey(redFunc).mapToPair(metricFunc);

    System.out.println("MetricDatum count:"  reducedGoraRdd.count());
 b/gora-core/src/main/java/org/apache/gora/spark/GoraSparkEngine.java
 * Base class for Gora - Spark integration.
public class GoraSparkEngine<K, V extends Persistent> {
  public GoraSparkEngine(Class<K> clazzK, Class<V> clazzV) {
  /**
   * Initializes a {@link JavaPairRDD} from given Spark context, Hadoop
   * configuration and data store.
   * 
   * @param sparkContext
   *          Spark context
   * @param conf
   *          Hadoop configuration
   * @param dataStore
   *          Data store
   * @return initialized rdd
   */
  /**
   * Initializes a {@link JavaPairRDD} from given Spark context and data store.
   * If given data store is {@link Configurable} and has not a configuration
   * than a Hadoop configuration is created otherwise existed configuration is
   * used.
   * 
   * @param sparkContext
   *          Spark context
   * @param dataStore
   *          Data store
   * @return initialized rdd
   */
    if ((dataStore instanceof Configurable)
        && ((Configurable) dataStore).getConf() != null) {
      hadoopConf = ((Configurable) dataStore).getConf();
    } else {
      hadoopConf = new Configuration();
    return initialize(sparkContext, hadoopConf, dataStore);
import java.util.Map;

import org.apache.gora.spark.GoraSparkEngine;
/**
 * LogAnalyticsSpark is the tutorial class to illustrate Gora Spark API. The
 * Spark job reads the web access data stored earlier by the {@link LogManager},
 * and calculates the aggregate daily pageviews. The output of the job is stored
 * in a Gora compatible data store.
 *
 * This class illustrates the same functionality with {@link LogAnalytics} via
 * Spark.
 *
 * <p>
 * See the tutorial.html file in docs or go to the <a
 * href="http://incubator.apache.org/gora/docs/current/tutorial.html"> web
 * site</a>for more information.
 * </p>
 */
  /**
   * map function used in calculation
   */
    /**
     * reduce function used in calculation
     */
    /**
     * metric function used after map phase
     */
    GoraSparkEngine<Long, Pageview> goraSparkEngine = new GoraSparkEngine<>(Long.class,
    JavaPairRDD<Long, Pageview> goraRDD = goraSparkEngine.initialize(sc, dataStore);
    System.out.println("Total Log Count: "  count);
    Map<String, MetricDatum> metricDatumMap = reducedGoraRdd.collectAsMap();
    for (String key : metricDatumMap.keySet()) {
      System.out.println(key);
    }

import org.apache.gora.mapreduce.GoraMapReduceUtils;
import org.apache.gora.mapreduce.GoraOutputFormat;
import org.apache.gora.persistency.Persistent;
import org.apache.hadoop.mapreduce.Job;
    // todo design inStore and outStore initialization parts as like LogAnalytics.java
    // todo consider creating job and manipulating it at input part as like LogAnalytics.java
    DataStore<Long, Pageview> inStore = DataStoreFactory.getDataStore(
    JavaPairRDD<Long, Pageview> goraRDD = goraSparkEngine.initialize(sc, inStore);
    //print screen output
    //

    //write output to datastore
    DataStore<String, MetricDatum> outStore = DataStoreFactory.getDataStore(
            outStoreClass, String.class, MetricDatum.class, hadoopConf);

    GoraMapReduceUtils.setIOSerializations(hadoopConf, true);

    Job job = Job.getInstance(hadoopConf);
    job.setOutputFormatClass(GoraOutputFormat.class);
    job.setOutputKeyClass(outStore.getKeyClass());
    job.setOutputValueClass(outStore.getPersistentClass());

    job.getConfiguration().setClass(GoraOutputFormat.DATA_STORE_CLASS, outStore.getClass(),
              DataStore.class);
    job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_KEY_CLASS, outStore.getKeyClass(), Object.class);
    job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_VALUE_CLASS,
            outStore.getPersistentClass(), Persistent.class);

    reducedGoraRdd.saveAsNewAPIHadoopDataset(job.getConfiguration());
    //

    inStore.close();
    outStore.close();
    System.out.println("First entry's first URL:"  firstOneURL);
    /*
    */
    cursor.addOption(Bytes.QUERYOPTION_NOTIMEOUT);
      MongoStoreParameters parameters = MongoStoreParameters.load(properties, getConf());
          new Object[] { parameters.getMappingFile() });
      builder.fromFile(parameters.getMappingFile());
      mongoClientDB = getDB(parameters);
              parameters.getDbname(), parameters.getServers() });
   *
  private MongoClient getClient(MongoStoreParameters params)
    MongoClientOptions.Builder optBuilder = new MongoClientOptions.Builder()
            .dbEncoderFactory(GoraDBEncoder.FACTORY); // Utf8 serialization!
    if (params.getReadPreference() != null) {
      optBuilder.readPreference(ReadPreference.valueOf(params.getReadPreference()));
    }
    if (params.getWriteConcern() != null) {
      optBuilder.writeConcern(WriteConcern.valueOf(params.getWriteConcern()));
    }
    // If configuration contains a login  secret, try to authenticated with DB
    List<MongoCredential> credentials = null;
    if (params.getLogin() != null && params.getSecret() != null) {
      credentials = new ArrayList<MongoCredential>();
      credentials.add(MongoCredential.createCredential(params.getLogin(), params.getDbname(), params.getSecret().toCharArray()));
    }
    Iterable<String> serversArray = Splitter.on(",").split(params.getServers());
        Iterator<String> paramsIterator = Splitter.on(":").trimResults().split(server).iterator();
    return new MongoClient(addrs, credentials, optBuilder.build());
  private DB getDB(MongoStoreParameters parameters) throws UnknownHostException {
    if (!mapsOfClients.containsKey(parameters.getServers()))
      mapsOfClients.put(parameters.getServers(), getClient(parameters));
    DB db = mapsOfClients.get(parameters.getServers()).getDB(parameters.getDbname());
    return db;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  private static final Logger log = LoggerFactory.getLogger(LogAnalyticsSpark.class);

    try {
      int ret = logAnalyticsSpark.run(args);
      System.exit(ret);
    } catch (Exception ex){
      log.error("Error occurred!");
    }

  public int run(String[] args) throws Exception {

    DataStore<Long, Pageview> inStore;
    DataStore<String, MetricDatum> outStore;
    Configuration hadoopConf = new Configuration();

    if (args.length > 0) {
      String dataStoreClass = args[0];
      inStore = DataStoreFactory.getDataStore(
      dataStoreClass, Long.class, Pageview.class, hadoopConf);
      if (args.length > 1) {
        dataStoreClass = args[1];
      }
        outStore = DataStoreFactory.getDataStore(
        dataStoreClass, String.class, MetricDatum.class, hadoopConf);
      } else {
        inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, hadoopConf);
        outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, hadoopConf);
    }

    //Spark engine initialization
    log.info("Log completed with success");

import org.apache.gora.mapreduce.GoraOutputFormat;
import org.apache.hadoop.mapreduce.Job;

    /**
     * Sets the output parameters for the job
     * @param job the job to set the properties for
     * @param dataStore the datastore as the output
     * @param reuseObjects whether to reuse objects in serialization
     */
    public <K, V extends Persistent> Configuration setOutput(Job job,
        DataStore<K, V> dataStore, boolean reuseObjects) {
      return setOutput(job, dataStore.getClass(), dataStore.getKeyClass(),
           dataStore.getPersistentClass(), reuseObjects);
    }

    /**
     * Sets the output parameters for the job
     *
     * @param job             the job to set the properties for
     * @param dataStoreClass  the datastore class
     * @param keyClass        output key class
     * @param persistentClass output value class
     * @param reuseObjects    whether to reuse objects in serialization
     */
    @SuppressWarnings("rawtypes")
    public <K, V extends Persistent> Configuration setOutput(Job job,
        Class<? extends DataStore> dataStoreClass,
        Class<K> keyClass, Class<V> persistentClass,
        boolean reuseObjects) {

      job.setOutputFormatClass(GoraOutputFormat.class);
      job.setOutputKeyClass(keyClass);
      job.setOutputValueClass(persistentClass);

      job.getConfiguration().setClass(GoraOutputFormat.DATA_STORE_CLASS, dataStoreClass,
              DataStore.class);
      job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_KEY_CLASS, keyClass, Object.class);
      job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_VALUE_CLASS,
              persistentClass, Persistent.class);
      return job.getConfiguration();
    }
    //Print output for debug purpose
    Configuration sparkHadoopConf = goraSparkEngine.setOutput(job, outStore, true);
    reducedGoraRdd.saveAsNewAPIHadoopDataset(sparkHadoopConf);
     * Creates a job and sets the output parameters for the conf that Spark will use
    public <K, V extends Persistent> Configuration generateOutputConf(DataStore<K, V> dataStore,
        boolean reuseObjects) throws IOException {

      Configuration hadoopConf = new Configuration();
      GoraMapReduceUtils.setIOSerializations(hadoopConf, true);
      Job job = Job.getInstance(hadoopConf);

      return generateOutputConf(job, dataStore.getClass(), dataStore.getKeyClass(),
     * Sets the output parameters for the conf that Spark will use
     * @param job the job to set the properties for
     * @param dataStore the datastore as the output
     * @param reuseObjects whether to reuse objects in serialization
     */
    public <K, V extends Persistent> Configuration generateOutputConf(Job job,
        DataStore<K, V> dataStore, boolean reuseObjects) {
      return generateOutputConf(job, dataStore.getClass(), dataStore.getKeyClass(),
              dataStore.getPersistentClass(), reuseObjects);
    }

    /**
     * Sets the output parameters for the conf that Spark will use
    public <K, V extends Persistent> Configuration generateOutputConf(Job job,
    Configuration sparkHadoopConf = goraSparkEngine.generateOutputConf(outStore, true);
    public <K, V extends Persistent> Configuration generateOutputConf(DataStore<K, V> dataStore)
       throws IOException {
           dataStore.getPersistentClass());
              dataStore.getPersistentClass());
        Class<K> keyClass, Class<V> persistentClass) {
    Configuration sparkHadoopConf = goraSparkEngine.generateOutputConf(outStore);
        "Gora Spark Integration Application").setMaster("local");
    c[0] = inStore.getPersistentClass();
    System.out.println("Total Web page count: "  count);
        if (autoCreateSchema && !schemaExists())
          count = putMap(m, count, field.schema().getValueType(), o, col, field.name());
          count = putArray(m, count, o, col, field.name());
            count = putArray(m, count, o, col, field.name());
            count = putMap(m, count, effectiveSchema.getValueType(), o, col, field.name());
  private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col, String fieldName) throws GoraException {
    query.setFields(fieldName);
  private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col, String fieldName) {
    query.setFields(fieldName);
import java.util.concurrent.ConcurrentNavigableMap;
import java.util.concurrent.ConcurrentSkipListMap;
  // This map behaves like DB, has to be static and concurrent collection
  @SuppressWarnings("rawtypes")
  public static ConcurrentSkipListMap map = new ConcurrentSkipListMap();
      return deletedRows;
    } catch (Exception e) {
  
  @SuppressWarnings("unchecked")
      startKey = (K) map.firstKey();
      endKey = (K) map.lastKey();
    ConcurrentNavigableMap<K,T> submap = map.subMap(startKey, true, endKey, true);
  
  @SuppressWarnings("unchecked")
    T obj = (T) map.get(key);
  
  @SuppressWarnings("unchecked")
  public void flush() {
    map.clear();
  }
import java.util.TreeMap;
  private TreeMap<K, T> map = new TreeMap<K, T>();
      return 0;
    }
    catch(Exception e){

      startKey = map.firstKey();
      endKey = map.lastKey();
    NavigableMap<K, T> submap = map.subMap(startKey, true, endKey, true);

    T obj = map.get(key);

    map.clear();
  public void flush() { }
import java.util.concurrent.ConcurrentNavigableMap;
import java.util.concurrent.ConcurrentSkipListMap;
  // This map behaves like DB, has to be static and concurrent collection
  @SuppressWarnings("rawtypes")
  public static ConcurrentSkipListMap map = new ConcurrentSkipListMap();
      return deletedRows;
    } catch (Exception e) {
  
  @SuppressWarnings("unchecked")
      startKey = (K) map.firstKey();
      endKey = (K) map.lastKey();
    ConcurrentNavigableMap<K,T> submap = map.subMap(startKey, true, endKey, true);
  
  @SuppressWarnings("unchecked")
    T obj = (T) map.get(key);
  
  @SuppressWarnings("unchecked")
      } catch (IOException e) {;
      throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(GoraCompilerCLI.class);

      LOG.error("Must supply at least one source file and an output directory.");
      LOG.error("Must supply a directory for output");
        LOG.error("Input must be a file.");
      LOG.info("Compiler executed SUCCESSFULL.");
      LOG.error("Error while compiling schema files. Check that the schemas are properly formatted.");
      throw new RuntimeException(e);
    LOG.info("Usage: gora-compiler ( -h | --help ) | (<input> [<input>...] <output>)");
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(GoraCompiler.class);

      LOG.info("Compiling: {}", src.getAbsolutePath());
      LOG.info("Compiled into: {}", dest.getAbsolutePath());
      throw new RuntimeException(e);
      throw new RuntimeException(e);
      throw new RuntimeException(e);
      throw new RuntimeException(e);
import java.nio.charset.Charset;
          page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes(Charset.defaultCharset())));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(QueryCounter.class);

      LOG.info("Usage QueryCounter <keyClass> <persistentClass> [dataStoreClass]");
    LOG.info("Number of result to the query:"  results);
import java.nio.charset.Charset;
        String content = new String(page.getContent().array(), Charset.defaultCharset());
	    throw new RuntimeException(e);
      throw new RuntimeException(e);
      throw new RuntimeException(e);
import java.util.Locale;
      String fullKey = GORA  "."  org.apache.gora.util.StringUtils.getClassname(clazz).toLowerCase(Locale.getDefault())  "."  baseKey;
      key = key.toLowerCase(Locale.getDefault());
      throw new RuntimeException(e);
import java.util.Locale;
      value = getProperty(properties, fullKey.toLowerCase(Locale.getDefault()));
      throw new RuntimeException(e);
      throw new RuntimeException(e);
import java.util.Locale;
        NumberFormat nf = NumberFormat.getInstance(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(VersionInfo.class);
    LOG.info("Gora "  getVersion());
    LOG.info("Subversion "  getUrl()  " -r "  getRevision());
    LOG.info("Compiled by "  getUser()  " on "  getDate());
    LOG.info("From source with checksum "  getSrcChecksum());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Delete.class);
      LOG.info("Usage : {} <node to delete>", Delete.class.getSimpleName());
    LOG.info("Delete returned {}", ret);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Generator.class);
      LOG.info("num {}", num);
      LOG.error("Failed to parse command line {}", e.getMessage());
    LOG.info("Running Generator with numMappers={}, numNodes={}", numMappers, numNodes);
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Loop.class); 
    LOG.info("Verify finished with succees. Total nodes={}", expectedNumNodes);
      LOG.error("Failed to parse command line {}", e.getMessage());
      throw new RuntimeException("Number of node per mapper is not a multiple of "  String.format(Locale.getDefault(), "%,d", Generator.WRAP));
      LOG.info("Starting iteration = {}", i);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Print.class);
      LOG.error("Failed to parse command line "  e.getMessage());
      LOG.info("%016x:%016x:%012d:%s\n {} {} {} {}", new Object[] {rs.getKey(), 
        node.getPrev(), node.getCount(), node.getClient()});
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Verify.class);
          sb.append(String.format(Locale.getDefault(), "%016x", ref));
        context.write(new Text(String.format(Locale.getDefault(), "%016x", key.get())), new Text(sb.toString()));
      LOG.error("Failed to parse command line "  e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Walker.class);
      LOG.error("Failed to parse command line {}", e.getMessage());
        LOG.info("CQ %d %016x \n {}", new Object [] {t2 - t1, prev});
        LOG.info("HQ %d %016x \n {}", new Object [] {t2 - t1, prev});
        LOG.info("FSR %d %016x\n {}", new Object[] {t2 - t1, rs.getKey()});
      throw new RuntimeException(e);
    LOG.info("FSR {}", new Object [] {(t2 - t1)});
  public static void main(String[] args) throws NoSuchElementException, InstantiationException, IllegalAccessException, IOException {
          throw new IOException(e);
          throw new IOException(e);
import java.nio.charset.Charset;
      BufferedReader br = new BufferedReader(new InputStreamReader(process.getInputStream(), Charset.defaultCharset()));
        LOG.info("Setting dfs.datanode.data.dir.perm to {}",  perms);
      LOG.warn("Couldn't get umask {}", e);
import java.util.Locale;
      newDocumentField(docFieldName, valueOf(fieldType.toUpperCase(Locale.getDefault())));
        Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.getDefault());

import java.util.Locale;
    if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("http")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("cloud")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("concurrent")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("loadbalance")) {
        throw new RuntimeException(e);
        throw new RuntimeException(e);
        LOG.warn("Invalid batch size '{}', using default {}", batchSizeString, DEFAULT_BATCH_SIZE);
        LOG.warn("Invalid commit within '{}' , using default {}", commitWithinString, DEFAULT_COMMIT_WITHIN);
        LOG.warn("Invalid results size '{}' , using default {}", resultsSizeString, DEFAULT_RESULTS_SIZE);
             " {} in mapping file.", classes.indexOf(classElement));
    log.info("Creating Hadoop Job: {}", job.getJobName());
    log.info("Log completed with {}", (success ? "success" : "failure"));
      log.info(USAGE);
import java.nio.charset.Charset;
import java.io.InputStreamReader;
import java.io.FileInputStream;
import java.nio.charset.Charset;
import java.util.Locale;
    = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.getDefault());
    log.info("Parsing file: {}", input);
    BufferedReader reader = new BufferedReader(new InputStreamReader(
      new FileInputStream(input), Charset.defaultCharset()));
    log.info("finished parsing file. Total number of log lines: {}", lineCount);
    log.info("pageview with key: {} deleted", lineNum);
    log.info("pageviews with keys between {} and {} are deleted.", startKey, endKey);
      log.info("{} :", resultKey);
    log.info("Number of pageviews from the query: {}", result.getOffset());
      log.info("No result to show"); 
      log.info(pageview.toString());
      log.error(USAGE);
      log.info(USAGE);
import java.util.concurrent.ConcurrentNavigableMap;
import java.util.concurrent.ConcurrentSkipListMap;
  // This map behaves like DB, has to be static and concurrent collection
  @SuppressWarnings("rawtypes")
  public static ConcurrentSkipListMap map = new ConcurrentSkipListMap();
      return deletedRows;
    } catch (Exception e) {
  
  @SuppressWarnings("unchecked")
      startKey = (K) map.firstKey();
      endKey = (K) map.lastKey();
    ConcurrentNavigableMap<K,T> submap = map.subMap(startKey, true, endKey, true);
  
  @SuppressWarnings("unchecked")
    T obj = (T) map.get(key);
  
  @SuppressWarnings("unchecked")
      } catch (IOException e) {
        LOG.error(e.getMessage());
      LOG.error(ioe.getMessage());
        LOG.error(e.getMessage());
      LOG.error(e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(LicenseHeaders.class);

      LOG.error(e.getMessage());
      LOG.error(e.getMessage());
      LOG.error(e.getMessage());
      LOG.error(e.getMessage());

	    LOG.error("Error reading Gora records: {}", e.getMessage());
      LOG.warn("Exception at GoraRecordWriter.class while closing datastore: {}", e.getMessage());
      LOG.warn("Trace: {}", e.getStackTrace());
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore: {}", e.getMessage());
      LOG.warn("Trace: {}", e.getStackTrace());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

    private static final Logger LOG = LoggerFactory.getLogger(BeanFactoryWSImpl.class);

      LOG.error(e.getMessage());
      LOG.error(e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(WSBackedDataStoreBase.class);

      LOG.error(e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(ByteUtils.class);

      LOG.error(e.getMessage());
      LOG.error(e.getMessage());
      LOG.error("Failed to parse command line {}", e.getMessage());
      LOG.error("Failed to parse command line {}", e.getMessage());
      LOG.error(e.getMessage());
        LOG.error(e.getMessage());
        LOG.error(e.getMessage());
        LOG.error(e.getMessage());
      LOG.error(ioe.getMessage());
        LOG.error(e.getMessage());
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(GoraCompilerCLI.class);

      LOG.error("Must supply at least one source file and an output directory.");
      LOG.error("Must supply a directory for output");
        LOG.error("Input must be a file.");
      LOG.info("Compiler executed SUCCESSFULL.");
      LOG.error("Error while compiling schema files. Check that the schemas are properly formatted.");
      throw new RuntimeException(e);
    LOG.info("Usage: gora-compiler ( -h | --help ) | (<input> [<input>...] <output>)");
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(GoraCompiler.class);

      LOG.info("Compiling: {}", src.getAbsolutePath());
      LOG.info("Compiled into: {}", dest.getAbsolutePath());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(LicenseHeaders.class);

      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import java.nio.charset.Charset;
          page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes(Charset.defaultCharset())));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(QueryCounter.class);

      LOG.info("Usage QueryCounter <keyClass> <persistentClass> [dataStoreClass]");
    LOG.info("Number of result to the query:"  results);
import java.nio.charset.Charset;
        String content = new String(page.getContent().array(), Charset.defaultCharset());

	    LOG.error("Error reading Gora records: {}", e.getMessage());
	    throw new RuntimeException(e);
      LOG.warn("Exception at GoraRecordWriter.class while closing datastore: {}", e.getMessage());
      LOG.warn("Trace: {}", e.getStackTrace());
      throw new RuntimeException(e);
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore: {}", e.getMessage());
      LOG.warn("Trace: {}", e.getStackTrace());
      throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

    private static final Logger LOG = LoggerFactory.getLogger(BeanFactoryWSImpl.class);

      LOG.error(e.getMessage());
      LOG.error(e.getMessage());
import java.util.Locale;
      String fullKey = GORA  "."  org.apache.gora.util.StringUtils.getClassname(clazz).toLowerCase(Locale.getDefault())  "."  baseKey;
      key = key.toLowerCase(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(WSBackedDataStoreBase.class);

      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import java.util.Locale;
      value = getProperty(properties, fullKey.toLowerCase(Locale.getDefault()));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(ByteUtils.class);

      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import java.util.Locale;
        NumberFormat nf = NumberFormat.getInstance(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(VersionInfo.class);
    LOG.info("Gora "  getVersion());
    LOG.info("Subversion "  getUrl()  " -r "  getRevision());
    LOG.info("Compiled by "  getUser()  " on "  getDate());
    LOG.info("From source with checksum "  getSrcChecksum());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Delete.class);
      LOG.info("Usage : {} <node to delete>", Delete.class.getSimpleName());
    LOG.info("Delete returned {}", ret);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Generator.class);
      LOG.info("num {}", num);
      LOG.error("Failed to parse command line {}", e.getMessage());
    LOG.info("Running Generator with numMappers={}, numNodes={}", numMappers, numNodes);
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Loop.class); 
    LOG.info("Verify finished with succees. Total nodes={}", expectedNumNodes);
      LOG.error("Failed to parse command line {}", e.getMessage());
      throw new RuntimeException("Number of node per mapper is not a multiple of "  String.format(Locale.getDefault(), "%,d", Generator.WRAP));
      LOG.info("Starting iteration = {}", i);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Print.class);
      LOG.error("Failed to parse command line {}", e.getMessage());
      LOG.info("%016x:%016x:%012d:%s\n {} {} {} {}", new Object[] {rs.getKey(), 
        node.getPrev(), node.getCount(), node.getClient()});
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Verify.class);
          sb.append(String.format(Locale.getDefault(), "%016x", ref));
        context.write(new Text(String.format(Locale.getDefault(), "%016x", key.get())), new Text(sb.toString()));
      LOG.error("Failed to parse command line {}", e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Walker.class);
      LOG.error("Failed to parse command line {}", e.getMessage());
        LOG.info("CQ %d %016x \n {}", new Object [] {t2 - t1, prev});
        LOG.info("HQ %d %016x \n {}", new Object [] {t2 - t1, prev});
        LOG.info("FSR %d %016x\n {}", new Object[] {t2 - t1, rs.getKey()});
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
    LOG.info("FSR {}", new Object [] {(t2 - t1)});
  public static void main(String[] args) throws NoSuchElementException, InstantiationException, IllegalAccessException, IOException {
          throw new IOException(e);
          throw new IOException(e);
import java.nio.charset.Charset;
      BufferedReader br = new BufferedReader(new InputStreamReader(process.getInputStream(), Charset.defaultCharset()));
        LOG.info("Setting dfs.datanode.data.dir.perm to {}",  perms);
      LOG.warn("Couldn't get umask {}", e);
import java.util.Locale;
      newDocumentField(docFieldName, valueOf(fieldType.toUpperCase(Locale.getDefault())));
        Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.getDefault());

import java.util.Locale;
    if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("http")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("cloud")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("concurrent")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("loadbalance")) {
        LOG.error(e.getMessage());
        throw new RuntimeException(e);
        LOG.error(e.getMessage());
        throw new RuntimeException(e);
        LOG.warn("Invalid batch size '{}', using default {}", batchSizeString, DEFAULT_BATCH_SIZE);
        LOG.warn("Invalid commit within '{}' , using default {}", commitWithinString, DEFAULT_COMMIT_WITHIN);
        LOG.warn("Invalid results size '{}' , using default {}", resultsSizeString, DEFAULT_RESULTS_SIZE);
             " {} in mapping file.", classes.indexOf(classElement));
    log.info("Creating Hadoop Job: {}", job.getJobName());
    log.info("Log completed with {}", (success ? "success" : "failure"));
      log.info(USAGE);
import java.nio.charset.Charset;
import java.io.InputStreamReader;
import java.io.FileInputStream;
import java.nio.charset.Charset;
import java.util.Locale;
    = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.getDefault());
    log.info("Parsing file: {}", input);
    BufferedReader reader = new BufferedReader(new InputStreamReader(
      new FileInputStream(input), Charset.defaultCharset()));
    log.info("finished parsing file. Total number of log lines: {}", lineCount);
    log.info("pageview with key: {} deleted", lineNum);
    log.info("pageviews with keys between {} and {} are deleted.", startKey, endKey);
      log.info("{} :", resultKey);
    log.info("Number of pageviews from the query: {}", result.getOffset());
      log.info("No result to show"); 
      log.info(pageview.toString());
      log.error(USAGE);
      log.info(USAGE);
import java.nio.charset.Charset;

          String content = new String(webPage.getContent().array(), Charset.defaultCharset());
    log.info("Total Web page count: {}", count);
    log.info("SparkWordCount debug purpose TokenDatum print starts:");
      log.info(key);
      log.info(tokenDatumMap.get(key).toString());
    log.info("SparkWordCount debug purpose TokenDatum print ends:");
      log.info(USAGE);
      log.error(USAGE);
    log.info("Total Log Count: {}", count);
    log.info("MetricDatum count: {}", reducedGoraRdd.count());
        LOG.error(e.getMessage());
      LOG.error(ioe.getMessage());
        LOG.error(e.getMessage());
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(GoraCompilerCLI.class);

      LOG.error("Must supply at least one source file and an output directory.");
      LOG.error("Must supply a directory for output");
        LOG.error("Input must be a file.");
      LOG.info("Compiler executed SUCCESSFULL.");
      LOG.error("Error while compiling schema files. Check that the schemas are properly formatted.");
      throw new RuntimeException(e);
    LOG.info("Usage: gora-compiler ( -h | --help ) | (<input> [<input>...] <output>)");
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(GoraCompiler.class);

      LOG.info("Compiling: {}", src.getAbsolutePath());
      LOG.info("Compiled into: {}", dest.getAbsolutePath());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(LicenseHeaders.class);

      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import java.nio.charset.Charset;
          page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes(Charset.defaultCharset())));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(QueryCounter.class);

      LOG.info("Usage QueryCounter <keyClass> <persistentClass> [dataStoreClass]");
    LOG.info("Number of result to the query:"  results);
import java.nio.charset.Charset;
        String content = new String(page.getContent().array(), Charset.defaultCharset());

	    LOG.error("Error reading Gora records: {}", e.getMessage());
	    throw new RuntimeException(e);
      LOG.warn("Exception at GoraRecordWriter.class while closing datastore: {}", e.getMessage());
      LOG.warn("Trace: {}", e.getStackTrace());
      throw new RuntimeException(e);
      LOG.warn("Exception at GoraRecordWriter.class while writing to datastore: {}", e.getMessage());
      LOG.warn("Trace: {}", e.getStackTrace());
      throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

    private static final Logger LOG = LoggerFactory.getLogger(BeanFactoryWSImpl.class);

      LOG.error(e.getMessage());
      LOG.error(e.getMessage());
import java.util.Locale;
      String fullKey = GORA  "."  org.apache.gora.util.StringUtils.getClassname(clazz).toLowerCase(Locale.getDefault())  "."  baseKey;
      key = key.toLowerCase(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(WSBackedDataStoreBase.class);

      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import java.util.Locale;
      value = getProperty(properties, fullKey.toLowerCase(Locale.getDefault()));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(ByteUtils.class);

      LOG.error(e.getMessage());
      throw new RuntimeException(e);
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
import java.util.Locale;
        NumberFormat nf = NumberFormat.getInstance(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(VersionInfo.class);
    LOG.info("Gora "  getVersion());
    LOG.info("Subversion "  getUrl()  " -r "  getRevision());
    LOG.info("Compiled by "  getUser()  " on "  getDate());
    LOG.info("From source with checksum "  getSrcChecksum());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Delete.class);
      LOG.info("Usage : {} <node to delete>", Delete.class.getSimpleName());
    LOG.info("Delete returned {}", ret);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Generator.class);
      LOG.info("num {}", num);
      LOG.error("Failed to parse command line {}", e.getMessage());
    LOG.info("Running Generator with numMappers={}, numNodes={}", numMappers, numNodes);
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Loop.class); 
    LOG.info("Verify finished with succees. Total nodes={}", expectedNumNodes);
      LOG.error("Failed to parse command line {}", e.getMessage());
      throw new RuntimeException("Number of node per mapper is not a multiple of "  String.format(Locale.getDefault(), "%,d", Generator.WRAP));
      LOG.info("Starting iteration = {}", i);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Print.class);
      LOG.error("Failed to parse command line {}", e.getMessage());
      LOG.info("%016x:%016x:%012d:%s\n {} {} {} {}", new Object[] {rs.getKey(), 
        node.getPrev(), node.getCount(), node.getClient()});
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

  private static final Logger LOG = LoggerFactory.getLogger(Verify.class);
          sb.append(String.format(Locale.getDefault(), "%016x", ref));
        context.write(new Text(String.format(Locale.getDefault(), "%016x", key.get())), new Text(sb.toString()));
      LOG.error("Failed to parse command line {}", e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


  private static final Logger LOG = LoggerFactory.getLogger(Walker.class);
      LOG.error("Failed to parse command line {}", e.getMessage());
        LOG.info("CQ %d %016x \n {}", new Object [] {t2 - t1, prev});
        LOG.info("HQ %d %016x \n {}", new Object [] {t2 - t1, prev});
        LOG.info("FSR %d %016x\n {}", new Object[] {t2 - t1, rs.getKey()});
      LOG.error(e.getMessage());
      throw new RuntimeException(e);
    LOG.info("FSR {}", new Object [] {(t2 - t1)});
  public static void main(String[] args) throws NoSuchElementException, InstantiationException, IllegalAccessException, IOException {
          throw new IOException(e);
          throw new IOException(e);
import java.nio.charset.Charset;
      BufferedReader br = new BufferedReader(new InputStreamReader(process.getInputStream(), Charset.defaultCharset()));
        LOG.info("Setting dfs.datanode.data.dir.perm to {}",  perms);
      LOG.warn("Couldn't get umask {}", e);
import java.util.Locale;
      newDocumentField(docFieldName, valueOf(fieldType.toUpperCase(Locale.getDefault())));
        Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.getDefault());

import java.util.Locale;
    if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("http")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("cloud")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("concurrent")) {
    } else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("loadbalance")) {
        LOG.error(e.getMessage());
        throw new RuntimeException(e);
        LOG.error(e.getMessage());
        throw new RuntimeException(e);
        LOG.warn("Invalid batch size '{}', using default {}", batchSizeString, DEFAULT_BATCH_SIZE);
        LOG.warn("Invalid commit within '{}' , using default {}", commitWithinString, DEFAULT_COMMIT_WITHIN);
        LOG.warn("Invalid results size '{}' , using default {}", resultsSizeString, DEFAULT_RESULTS_SIZE);
             " {} in mapping file.", classes.indexOf(classElement));
    log.info("Creating Hadoop Job: {}", job.getJobName());
    log.info("Log completed with {}", (success ? "success" : "failure"));
      log.info(USAGE);
import java.nio.charset.Charset;
import java.io.InputStreamReader;
import java.io.FileInputStream;
import java.nio.charset.Charset;
import java.util.Locale;
    = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.getDefault());
    log.info("Parsing file: {}", input);
    BufferedReader reader = new BufferedReader(new InputStreamReader(
      new FileInputStream(input), Charset.defaultCharset()));
    log.info("finished parsing file. Total number of log lines: {}", lineCount);
    log.info("pageview with key: {} deleted", lineNum);
    log.info("pageviews with keys between {} and {} are deleted.", startKey, endKey);
      log.info("{} :", resultKey);
    log.info("Number of pageviews from the query: {}", result.getOffset());
      log.info("No result to show"); 
      log.info(pageview.toString());
      log.error(USAGE);
      log.info(USAGE);
    flush();
      return decodeByte(val) == 1;
    assert(job.isComplete());
    assert(job.isComplete());
    return limit > 0 && offset >= limit;
      if (!ret) {
    return limit > 0 && offset >= limit;
    return Arrays.equals(family, other.family) && Arrays.equals(qualifier, other.qualifier);
      return (K) Boolean.valueOf(val[0] != 0);
      return decodeByte(val) == 1;
    assert(job.isComplete());
    assert(job.isComplete());
    return limit > 0 && offset >= limit;
      if (!ret) {
    return limit > 0 && offset >= limit;
    return Arrays.equals(family, other.family) && Arrays.equals(qualifier, other.qualifier);
      return (K) Boolean.valueOf(val[0] != 0);
  public static final int DEFAULT_UNION_SCHEMA = 0;
  public static final String UNION_COL_SUFIX = "_UnionIndex";
  public static final int DEFAULT_UNION_SCHEMA = 0;
  public static final String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
    key = ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
          PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, location);
    SpecificDatumWriter writer = writerMap.get(schema.getFullName());
    SpecificDatumReader<?> reader = readerMap.get(schemaId);
    ByteBuffer byteBuffer = ByteBuffer.allocate(array.size() * size);
    int n = array.size();
    int n = map.size();
    int n = map.size();
    if (!field.name().contains(CassandraStore.UNION_COL_SUFIX)){
    GORA_RESERVED_NAMES.addAll(Arrays.asList(DIRTY_BYTES_FIELD_NAME));
    }
    }
    }

  }

  }

      query = IOUtils.deserialize(conf, in, null);
  public void executeQuery() throws Exception {
    T newObj = AvroUtils.deepClonePersistent(obj);
    for(int i = 0; i<otherFields.size(); i) {
      return persistentClass.newInstance();
  boolean next() throws Exception;
  public final boolean next() throws Exception {
  protected T getOrCreatePersistent(T persistent) throws Exception {
  protected T getOrCreatePersistent(T persistent) throws Exception {
  }
      Properties properties) throws Exception {
    return datumReader.read(object, decoder);
    return datumReader.read(object, decoder);
    return datumReader.read(object, decoder);
  public void start(Path outputDir, int numReducers, boolean concurrent) throws Exception {
      byte[] startKey, byte[] endKey, Call<T, R> callable) throws Throwable {
      byte[] endKey, Call<T, R> callable, Callback<R> callback) throws Throwable {
    getTable().coprocessorService(service, startKey, endKey, callable, callback);
      R responsePrototype) throws Throwable {
      throws Throwable {
      SpecificDatumReader<?> reader = readerMap.get(schemaId);
      SpecificDatumWriter writer = writerMap.get(schema.getFullName());
  }
    if (solrJServerType.toLowerCase(Locale.getDefault()).equals("http")) {
    } else if (solrJServerType.toLowerCase(Locale.getDefault()).equals("cloud")) {
    } else if (solrJServerType.toLowerCase(Locale.getDefault()).equals("concurrent")) {
    } else if (solrJServerType.toLowerCase(Locale.getDefault()).equals("loadbalance")) {
        sb.append("\\").append(c);
    SpecificDatumReader<?> reader = readerMap.get(schemaId);
    SpecificDatumWriter writer = writerMap.get(schemaId);
    }

    }

    /** Rolls up the given timestamp to the day cardinality, so that
    }
  private void parse(String input) throws Exception {
  private void storePageview(long key, Pageview pageview) throws Exception {
  private void get(long key) throws Exception {
  private void query(long key) throws Exception {
  private void query(long startKey, long endKey) throws Exception {
  private void deleteByQuery(long startKey, long endKey) throws Exception {
  private void printResult(Result<Long, Pageview> result) throws Exception {
  private void close() throws Exception {
      Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);
    deleteColumn(key, familyMap.values().iterator().next(), null);
    if (DIRTY_BYTES_FIELD_NAME.equals(field.name())) {
    if (DIRTY_BYTES_FIELD_NAME.equals(fieldName)) {
    System.arraycopy(URLS, 0, SORTED_URLS, 0, URLS.length);
import java.util.Collections;
    Collections.addAll(set, arr1);
    Collections.addAll(set, arr2);


            if (!memberName.contains(CassandraStore.UNION_COL_SUFIX)) {
      throw new IOException("Mapping file '"  MAPPING_FILE  "' could not be found!");
  public static final String PROP_MONGO_DB = "gora.mongodb.db";
  public static final String UNION_COL_SUFIX = "_UnionIndex";
  public static final int DEFAULT_UNION_SCHEMA = 0;
  public static final String DIRTY_BYTES_FIELD_NAME = "__g__dirty";

  public static final int DEFAULT_UNION_SCHEMA = 0;
    flush();
  public static final String UNION_COL_SUFIX = "_UnionIndex";
  public static final int DEFAULT_UNION_SCHEMA = 0;
  public static final String DIRTY_BYTES_FIELD_NAME = "__g__dirty";

  public static final int DEFAULT_UNION_SCHEMA = 0;
    flush();
    for (byte anA : a) {
      b |= fromChar(anA);
    for (byte aBin : bin) {
      hex[j] = chars[0x0f & (aBin >>> 4)];
      hex[j] = chars[0x0f & aBin];
  Map<String,Pair<Text,Text>> fieldMap = new HashMap<>();
  Map<Pair<Text,Text>,String> columnMap = new HashMap<>();
  Map<String,String> tableConfig = new HashMap<>();
        } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {
      } catch (AccumuloException | AccumuloSecurityException e) {
            Pair<Text,Text> col = new Pair<>(new Text(family), qualifier == null ? null : new Text(qualifier));
    } catch (AccumuloException | AccumuloSecurityException | TableExistsException e) {
    } catch (AccumuloException | AccumuloSecurityException | TableNotFoundException e) {
        currentMap = new DirtyMapWrapper<>(new HashMap<Utf8, Object>());
        currentArray = new DirtyListWrapper<>(new ArrayList<>());
          currentArray = new DirtyListWrapper<>(new ArrayList<>());
          currentMap = new DirtyMapWrapper<>(new HashMap<Utf8, Object>());
    String fieldName = mapping.columnMap.get(new Pair<>(entry.getKey().getColumnFamily(),
          SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
      return new AccumuloResult<>(this, query, scanner);
    return new AccumuloQuery<>(this);
      Map<String,Map<KeyExtent,List<Range>>> binnedRanges = new HashMap<>();
      List<PartitionQuery<K,T>> ret = new ArrayList<>();
      HashMap<String,String> hostNameCache = new HashMap<>();
          PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query, startKey, endKey, location);
    } catch (TableNotFoundException | AccumuloException | AccumuloSecurityException e) {

    for (Object currentPCassandraRow : pCassandraRow) {
      CassandraColumn cColumn = (CassandraColumn) currentPCassandraRow;
        if (!fieldName.contains(CassandraStore.UNION_COL_SUFIX)) {
  private HashMap<K, Integer> indexMap = new HashMap<>();
        List<Object> array = new ArrayList<>();
        Map<CharSequence, Object> map = new HashMap<>();
          if (!mapKey.toString().contains(CassandraStore.UNION_COL_SUFIX)) {
    for (Object currentHColumn : hColumns) {
      HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>) currentHColumn;
      new ThreadLocal<>();
      new ThreadLocal<>();
      new ThreadLocal<>();
      new ConcurrentHashMap<>();
      new ConcurrentHashMap<>();
  private static Map<Type, ListSerializer> elementTypeToSerializerMap = new HashMap<>();
  private static Map<Class, ListSerializer> fixedClassToSerializerMap = new HashMap<>();
    List<byte[]> list = new ArrayList<>(n);
    ArrayList<T> array = new ArrayList<>();
  private static Map<Type, MapSerializer> valueTypeToSerializerMap = new HashMap<>();
  private static Map<Class, MapSerializer> fixedClassToSerializerMap = new HashMap<>();
    List<byte[]> list = new ArrayList<>(n);
    List<byte[]> list = new ArrayList<>(n);
    Map<CharSequence, T> map = new HashMap<>();
  private static Map<Class, SpecificFixedSerializer> classToSerializerMap = new HashMap<>();
        accessMap = new HashMap<>();
    Map<String, HConsistencyLevel> clMap = new HashMap<>();
    Map<String, List<String>> map = new HashMap<>();
        list = new ArrayList<>();
    Map<String, String> map = new HashMap<>();
  private List<String> superFamilies = new ArrayList<>();
  private Map<String, String> familyMap = new HashMap<>();
  private Map<String, String> columnMap = new HashMap<>();
  private Map<String, String> columnAttrMap = new HashMap<>();
		  new HashMap<>();
    List<ColumnFamilyDefinition> list = new ArrayList<>();
    keyspaceMap = new HashMap<>();
    mappingMap  = new HashMap<>();
    catch (JDOMException | IOException e) {
  private CassandraClient<K, T> cassandraClient = new CassandraClient<>();
      new ThreadLocal<>();
      new ConcurrentHashMap<>();
    CassandraQuery<K, T> cassandraQuery = new CassandraQuery<>();
    CassandraResult<K, T> cassandraResult = new CassandraResult<>(this, query);
    CassandraResultSet<K> cassandraResultSet = new CassandraResultSet<>();
        cassandraRow = new CassandraRow<>();
        cassandraRow = new CassandraRow<>();
    CassandraQuery<K,T> query = new CassandraQuery<>();
    List<PartitionQuery<K,T>> partitions = new ArrayList<>();
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query);
    Query<K,T> query = new CassandraQuery<>(this);
              Map<CharSequence,Object> valueMap = new HashMap<>();
    for (Schema currentSchema : pUnionSchema.getTypes()) {
      Type schemaType = currentSchema.getType();
      unionSchemaPos;
  private static final Set<String> GORA_RESERVED_NAMES = new HashSet<>();
  private static final Set<String> GORA_HIDDEN_FIELD_NAMES = new HashSet<>();
      //Map<Schema,Schema> queue = new HashMap<>();
    List<Schema> newTypeSchemas = new ArrayList<>();
    for (Schema currentTypeSchema : schemaTypes) {
      newTypeSchemas.add(getSchemaWithDirtySupport(currentTypeSchema, queue));
    List<Field> newFields = new ArrayList<>();
    relatedLicenses = new HashMap<>();
    } catch (SecurityException | NoSuchFieldException | IllegalArgumentException | IllegalAccessException e) {
  public static HashMap<String, Integer> URL_INDEXES = new HashMap<>();
    return new AvroResult<>(this, (AvroQuery<K,T>)query,
    return new AvroQuery<>(this);
    return new SpecificDatumWriter<>(schema);
    return new SpecificDatumReader<>(schema);
      writer = new DataFileWriter<>(getDatumWriter());
      return new DataFileAvroResult<>(this, query
      return new DataFileAvroResult<>(this, query, reader, fsInput
    return new DataFileReader<>(fsInput, getDatumReader());
  private List<Filter<K, T>> filters = new ArrayList<>();
      filters = new ArrayList<>(size);
  protected List<Object> operands = new ArrayList<>();
    for (Object operand : operands) {
  protected List<Object> operands = new ArrayList<>();
    for (Object operand : operands) {
        operand = operand.toString();
    return new GoraRecordReader<>(partitionQuery, context);
    List<InputSplit> splits = new ArrayList<>(queries.size());
      datumReader = new SpecificDatumReader<>(schema);
    this.datumWriter = new SpecificDatumWriter<>();
    return new MemResult<>(this, query, submap);
    for (Field otherField : otherFields) {
      int index = otherField.pos();
    return new MemQuery<>(this);
    List<PartitionQuery<K, T>> list = new ArrayList<>();
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query);
    return new DirtyIteratorWrapper<>(delegate.iterator(), dirtyFlag);
    return new DirtyListIterator<>(getDelegate().listIterator(),
    return new DirtyListIterator<>(getDelegate().listIterator(index),
    return new DirtyListWrapper<>(getDelegate().subList(fromIndex, toIndex),
    return new DirtyCollectionWrapper<>(delegate.values(), dirtyFlag);
            return new DirtyEntryWrapper<>(input, dirtyFlag);
    } catch (InstantiationException | IllegalAccessException e) {
    new HashMap<>();
    new HashMap<>();
    HashMap<String, Integer> map = new HashMap<>(fieldsLength);
      this.beanFactory = new BeanFactoryImpl<>(keyClass, persistentClass);
    datumReader = new SpecificDatumReader<>(schema);
    datumWriter = new SpecificDatumWriter<>(schema);
    List<Field> list = new ArrayList<>();
    } catch (ClassNotFoundException | IOException ex) {
      queries = new ArrayList<>(splits.size());
        queries.add(new FileSplitPartitionQuery<>(query, (FileSplit) split));
    HashMap<String, Field> fieldMap = new HashMap<>(fields.size());
    SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<>(
    SpecificDatumReader<T> reader = new SpecificDatumReader<>(
    try (ByteBufferOutputStream os = new ByteBufferOutputStream()) {
    List<ByteBuffer> list = new ArrayList<>();
    try (ByteBufferInputStream is = new ByteBufferInputStream(list)) {
    List<ByteBuffer> buffers = new ArrayList<>(4);
    nodes = new Stack<>();
    HashSet<String> set = new HashSet<>();
    LinkedHashSet<Set<String>> power = new LinkedHashSet<>();
      LinkedHashSet<String> innerSet = new LinkedHashSet<>();
      new ArrayList<>();
      list.add(new PartitionQueryImpl<>(query, LOCATIONS[i]));
      ArrayList<InputSplit> splits = new ArrayList<>(numMappers);
        flushed = new HashMap<>();
    private ArrayList<Long> refs = new ArrayList<>();
    ArrayList<String> flushedEntries = new ArrayList<>();
      new HashMap<>();
      new HashMap<>();
        families = new HashMap<>();
      filterUtil = new HBaseFilterUtil<>(this.conf);
      ArrayList<Delete> deletes = new ArrayList<>();
    return new HBaseQuery<>(this);
    List<PartitionQuery<K,T>> partitions = new ArrayList<>(keys.getFirst().length);
        PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<>(
        return new HBaseGetResult<>(this, query, result);
            = new HBaseScannerResult<>(this, query, scanner);
      Map<Utf8, Object> map = new HashMap<>();
      ArrayList<Object> arrayList = new ArrayList<>();
      DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<>(arrayList);
  private final BlockingQueue<HTable> pool = new LinkedBlockingQueue<>();
    this.tables = new ThreadLocal<>();
    List<String> filters = new ArrayList<>();
      new ThreadLocal<>();
      new ThreadLocal<>();
      new ThreadLocal<>();
      new ConcurrentHashMap<>();
      new ConcurrentHashMap<>();
  private Map<String, FilterFactory<K, T>> factories = new LinkedHashMap<>();
    List<String> filters = new ArrayList<>();
    List<String> operands = new ArrayList<>(rawOperands.size());
  private Map<String, FilterFactory<K, T>> factories = new LinkedHashMap<>();
  private HashMap<String, String> classToDocument = new HashMap<>();
  private HashMap<String, String> documentToClass = new HashMap<>();
  private HashMap<String, DocumentFieldType> documentFields = new HashMap<>();
  private static ConcurrentHashMap<String, MongoClient> mapsOfClients = new ConcurrentHashMap<>();
      filterUtil = new MongoFilterUtil<>(getConf());
      MongoMappingBuilder<K, T> builder = new MongoMappingBuilder<>(this);
      credentials = new ArrayList<>();
    List<ServerAddress> addrs = new ArrayList<>();
    MongoDBResult<K, T> mongoResult = new MongoDBResult<>(this, query);
    MongoDBQuery<K, T> query = new MongoDBQuery<>(this);
    List<PartitionQuery<K, T>> partitions = new ArrayList<>();
    PartitionQueryImpl<K, T> partitionQuery = new PartitionQueryImpl<>(
    List<Object> rlist = new ArrayList<>();
    return new DirtyListWrapper<>(rlist);
    Map<Utf8, Object> rmap = new HashMap<>();
    return new DirtyMapWrapper<>(rmap);
  private static final Map<String, String> HADOOP_VERSION_TO_IMPL_MAP = new HashMap<>();
      HashSet<String> uniqFields = new HashSet<>(Arrays.asList(fields));
    mapping = new HashMap<>();
  public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<>();
  public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<>();
    batch = new ArrayList<>(batchSize);
    for (Schema currentSchema : pUnionSchema.getTypes()) {
      Type schemaType = currentSchema.getType();
      return new SolrResult<>(this, query, server, resultsSize);
    return new SolrQuery<>(this);
    ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<>();
    PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query);
      long lineCount = 0;
      try (BufferedReader reader = new BufferedReader(new InputStreamReader(
            new FileInputStream(input), Charset.defaultCharset()))) {
        String line = reader.readLine();
        do {
          Pageview pageview = parseLine(line);

          if (pageview != null) {
            //store the pageview
            storePageview(lineCount, pageview);
          }

          line = reader.readLine();
        } while (line != null);

      }
    throw new IOException("cannot read from DataInput of instance:"
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
  
  private static final Logger LOG = LoggerFactory.getLogger(MemStore.class); 

    return "MemStore";
    try{
      long deletedRows = 0;

  /**
   * An important feature of {@link MemStore#execute(Query)} is
   * that when specifying the {@link MemQuery} one should be aware 
   * that when fromKey and toKey are equal, the returned map is empty 
   * unless fromInclusive and toInclusive are both true. On the other hand
   * if either or both of fromKey and toKey are null we return no results.
   */
      if (!map.isEmpty()) {
        startKey = (K) map.firstKey();
      }
      if (!map.isEmpty()) {
        endKey = (K) map.lastKey();
      }
    ConcurrentNavigableMap<K,T> submap = null;
    try {
      submap =  map.subMap(startKey, true, endKey, true);
    } catch (NullPointerException npe){
      LOG.info("Either startKey || endKey || startKey and endKey value(s) is null. "
           "No results will be returned for query to MemStore.");
      return new MemResult<>(this, query, new ConcurrentSkipListMap<K, T>());
    }


  /**
   * As MemStore is basically an implementation of
   * {@link java.util.concurrent.ConcurrentSkipListMap}
   * it has no concept of a schema.
   */
    if (!map.isEmpty()) {
      map.clear();
    }
              Integer unionIndex = getUnionIndex(cc);
              Integer unionIndex = getUnionIndex(hc);
 private Integer getUnionIndex(HColumn<ByteBuffer, ByteBuffer> uc){
  public Job createJob(Query<K,T> query) throws IOException {
    GoraMapper.initMapperJob(job, query, NullWritable.class
  public long countQuery(Query<K,T> query) throws Exception {
    Job job = createJob(query);
    Job job = createJob(query);
  public Job createJob(Query<String,WebPage> query
    GoraMapper.initMapperJob(job, query, Text.class
    Job job = createJob(query, outStore);
  private void setInputPath(PartitionQuery<K, T> partitionQuery) throws IOException {
    setInputPath(partitionQuery);
      , Query<K1,V1> query, boolean reuseObjects)
    setInput(job, store.newQuery(), reuseObjects);
      Class<K2> outKeyClass,
    GoraInputFormat.setInput(job, query, reuseObjects);
    initMapperJob(job, dataStore.newQuery(),
      Class<K2> outKeyClass,
    initMapperJob(job, query, outKeyClass, outValueClass,
        DataStore<K, V> dataStore) {
    case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, (SpecificRecord)object);
    case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, (SpecificRecord)o);
      SpecificDatumWriter<T> datumWriter, T object)
      SpecificDatumWriter<T> datumWriter, T object)
      , T object) throws IOException {
    serialize(os, datumWriter, object);
      , T object) throws IOException {
    serialize(os, datumWriter, object);
      SpecificDatumReader<T> datumReader, T object)
      SpecificDatumReader<T> datumReader, T object)
      SpecificDatumReader<T> datumReader, T object)
    GoraMapper.initMapperJob(job, query, LongWritable.class, VLongWritable.class, VerifyMapper.class, true);
  public void addClassField(String classFieldName,
          .addClassField(field.getAttributeValue(ATT_NAME),
      fieldValue = IOUtils.deserialize((byte[]) solrValue, reader,
            persistent.get(field.pos()));
        data = IOUtils.serialize(writer, fieldValue);
          serilazeData = IOUtils.serialize(writer, fieldValue);
import org.apache.accumulo.core.security.Authorizations;
import org.apache.accumulo.core.security.Credentials;
  private Credentials credentials;
  private static byte[] getBytes(Text text) {
    byte[] bytes = text.getBytes();
    if (bytes.length != text.getLength()) {
      bytes = new byte[text.getLength()];
      System.arraycopy(text.getBytes(), 0, bytes, 0, bytes.length);
    }
    return bytes;
  }

        credentials = new Credentials(user, token);
    } catch (AccumuloException | AccumuloSecurityException e) {
    } catch (TableExistsException e) {
      LOG.debug(e.getMessage(), e);
      Scanner scanner = new IsolatedScanner(conn.createScanner(mapping.tableName, Authorizations.EMPTY));
    Scanner scanner = new IsolatedScanner(conn.createScanner(mapping.tableName, Authorizations.EMPTY));
        tl = TabletLocator.getLocator(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));
      while (tl.binRanges(credentials, Collections.singletonList(createRange(query)), binnedRanges).size() > 0) {
              startKey = followingKey(encoder, getKeyClass(), getBytes(ke.getPrevEndRow()));
            startKey = fromBytes(getKeyClass(), getBytes(startRow));
              endKey = lastPossibleKey(encoder, getKeyClass(), getBytes(ke.getEndRow()));
            endKey = fromBytes(getKeyClass(), getBytes(endRow));
import org.apache.gora.memory.store.MemStore;
 * <p>This is the main class for initiating Rackspace cloud
 * topography for use within the GoraCI job. A wealth of settings
 * are configurable from within <code>gora.properties</code>.</p> 
 * <p>For
 * further documentation on the Rackspace Orchestration please see the
 * <a href="http://gora.apache.org/current/index.html#goraci-integration-testsing-suite">
 * current documentation</a>.</p>
    String rsContinent = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), 
    String rsUser = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_USERNAME, "asf-gora");
    String rsApiKey = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_APIKEY, null);
    String rs_region = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_REGION, "DFW");
    String rs_flavourId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_FLAVORID, null);
    String rs_imageId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_IMAGEID, null);
    int num_servers = Integer.parseInt(DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_NUM_SERVERS, "10"));
    String serverName = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_SERVERNAME, "goraci_test_server");
    if (DataStoreFactory.findBooleanProperty(properties, MemStore.class.newInstance(), RS_PUBKEY, "true")) {


  "/**\n" 
      " *Licensed to the Apache Software Foundation (ASF) under one\n" 
      " *or more contributor license agreements.  See the NOTICE file\n" 
      " *distributed with this work for additional information\n" 
      " *regarding copyright ownership.  The ASF licenses this file\n" 
      " *to you under the Apache License, Version 2.0 (the\"\n" 
      " *License\"); you may not use this file except in compliance\n" 
      " *with the License.  You may obtain a copy of the License at\n" 
      " *\n " 
      " * http://www.apache.org/licenses/LICENSE-2.0\n" 
      " * \n" 
      " *Unless required by applicable law or agreed to in writing, software\n" 
      " *distributed under the License is distributed on an \"AS IS\" BASIS,\n" 
      " *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n" 
      " *See the License for the specific language governing permissions and\n" 
      " *limitations under the License.\n" 
      " */\n";

  "/**\n" 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the GNU Affero General Public License as published by\n" 
      " * the Free Software Foundation, either version 3 of the License, or\n" 
      " * (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Public License for more details.\n" 
      " */\n";

  "/**\n" 
      " * COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0\n" 
      " *\n " 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the Common Development and Distrubtion License as\n" 
      " * published by the Sun Microsystems, either version 1.0 of the\n" 
      " * License, or (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Lesser Public License for more details.\n" 
      " *\n " 
      " * You should have received a copy of the Common Development and Distrubtion\n" 
      " * License along with this program.  If not, see\n" 
      " * <http://www.gnu.org/licenses/gpl-1.0.html>.\n" 
      " */\n";

  "/**\n" 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the GNU Free Documentation License as published by\n" 
      " * the Free Software Foundation, either version 1.3 of the License, or\n" 
      " * (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Public License for more details.\n" 
      " *\n " 
      " * You should have received a copy of the GNU Free Documentation License\n" 
      " * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n" 
      " */\n";
  "/**\n" 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the GNU General Public License as\n" 
      " * published by the Free Software Foundation, either version 1 of the\n" 
      " * License, or (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Public License for more details.\n" 
      " *\n " 
      " * You should have received a copy of the GNU General Public\n" 
      " * License along with this program.  If not, see\n" 
      " * <http://www.gnu.org/licenses/gpl-1.0.html>.\n" 
      " */\n";
  "/**\n" 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the GNU General Public License as\n" 
      " * published by the Free Software Foundation, either version 2 of the\n"  
      " * License, or (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Public License for more details.\n" 
      " *\n " 
      " * You should have received a copy of the GNU General Public\n"  
      " * License along with this program.  If not, see\n" 
      " * <http://www.gnu.org/licenses/gpl-2.0.html>.\n" 
      " */\n";    
  "/**\n" 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the GNU General Public License as\n" 
      " * published by the Free Software Foundation, either version 3 of the\n"  
      " * License, or (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Public License for more details.\n" 
      " *\n " 
      " * You should have received a copy of the GNU General Public\n" 
      " * License along with this program.  If not, see\n" 
      " * <http://www.gnu.org/licenses/gpl-3.0.html>.\n" 
      " */\n";  
  "/**\n" 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the GNU Lesser General Public License as\n" 
      " * published by the Free Software Foundation, either version 2.1 of the\n"  
      " * License, or (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Public License for more details.\n" 
      " *\n " 
      " * You should have received a copy of the GNU Lesser General Public\n"  
      " * License along with this program.  If not, see\n" 
      " * <http://www.gnu.org/licenses/lgpl-2.1.html>.\n" 
      " */\n";  
  "/**\n" 
      " * This program is free software: you can redistribute it and/or modify\n" 
      " * it under the terms of the GNU Lesser General Public License as\n" 
      " * published by the Free Software Foundation, either version 3 of the\n" 
      " * License, or (at your option) any later version.\n" 
      " *\n " 
      " * This program is distributed in the hope that it will be useful,\n" 
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n" 
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n" 
      " * GNU General Public License for more details.\n" 
      " *\n " 
      " * You should have received a copy of the GNU Lesser General Public\n"  
      " * License along with this program.  If not, see\n" 
      " * <http://www.gnu.org/licenses/lgpl-3.0.html>.\n" 
      " */\n"; 

   * @param pLicenseName the license header you wish to create with this object 
      for (String licenseValue : supportedLicenses) {
        String var = (String) this.getClass().getDeclaredField(licenseValue).get(licenseValue);
        relatedLicenses.put(licenseValue,var);
      }
   *@param pLicenseName the license name to set the header to

   * @return get the license name (performing some checking to prevent NPE, etc.) you are advised to use this option

   * @return get the license name
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"CINode\",\"namespace\":\"org.apache.gora.goraci.generated\",\"fields\":[{\"name\":\"prev\",\"type\":\"long\",\"default\":0},{\"name\":\"client\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"count\",\"type\":\"long\",\"default\":0}]}");
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Flushed\",\"namespace\":\"org.apache.gora.goraci.generated\",\"fields\":[{\"name\":\"count\",\"type\":\"long\",\"default\":0}]}");
import java.util.List;
import java.util.Set;
import org.jclouds.chef.ChefContext;
import org.jclouds.chef.config.ChefProperties;
import org.jclouds.chef.domain.BootstrapConfig;
import org.jclouds.chef.domain.CookbookVersion;
import org.jclouds.chef.predicates.CookbookVersionPredicates;
import org.jclouds.chef.util.RunListBuilder;
import org.jclouds.compute.ComputeServiceContext;
import org.jclouds.compute.RunNodesException;
import org.jclouds.compute.domain.NodeMetadata;
import org.jclouds.compute.domain.TemplateBuilder;
import org.jclouds.domain.JsonBall;
import org.jclouds.scriptbuilder.domain.Statement;
import org.jclouds.sshj.config.SshjSshClientModule;
import static org.jclouds.compute.options.TemplateOptions.Builder.runScript; 
import com.google.common.collect.ImmutableSet;
import com.google.inject.Module;

import static com.google.common.collect.Iterables.any;
import static com.google.common.collect.Iterables.concat;

 * topography for use within GoraCI jobs. A wealth of settings
  
  private static NovaApi novaApi = null;
  
  private static String rsContinent = null;
  
  private static String rsUser = null;
  
  private static String rsApiKey = null;
  
  private static String rsRegion = null;
   * Right now the Rackspace Orchestration and Services Provisioning
   * requires no arguments. It can be invoked from
   * <code>$GORA_HOME/bin/gora goracisetup</code>
    performRackspaceOrchestration(properties);
  }
  
  private static void performRackspaceOrchestration(Properties properties) throws InstantiationException, IllegalAccessException, IOException {
    rsContinent = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), 
    rsUser = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_USERNAME, "asf-gora");
    rsApiKey = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_APIKEY, null);
    rsRegion = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_REGION, "DFW");
    String rsFlavourId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_FLAVORID, null);
    String rsImageId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_IMAGEID, null);
    int numServers = Integer.parseInt(DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_NUM_SERVERS, "10"));
    novaApi = ContextBuilder.newBuilder(rsContinent).credentials(rsUser, rsApiKey).buildApi(NovaApi.class);
    LOG.info("Defining Rackspace cloudserver continent as: {}, and region: {}.", rsContinent, rsRegion);
    ImageApi imageApi = novaApi.getImageApi(rsRegion);
    Image image = imageApi.get(rsImageId);
    FlavorApi flavorApi = novaApi.getFlavorApi(rsRegion);
    Flavor flavor = flavorApi.get(rsFlavourId);
    LOG.info("Defining Rackspace cloudserver flavors as: {}, with image id's {}", rsFlavourId, rsImageId);
    KeyPairApi keyPairApi = novaApi.getKeyPairApi(rsRegion).get();

    ServerApi serverApi = novaApi.getServerApi(rsRegion);
    for (int i = 0; i < numServers; i) {
        serverCreated = serverApi.create(serverName  i, rsImageId, rsFlavourId, options);
        serverCreated = serverApi.create("goraci_test_server"  i, image.getId(), flavor.getId(), options);
      LOG.info("Provisioned node {} of {} with name {}", new Object[]{i  1, numServers, serverName  i});
  }
/**
 * {@link FilterList} enables us to utilize conditional operands
 * for advanced filtering within the construction and 
 * execution of Gora queries.
 *
 * @param <K>
 * @param <T>
 */








    boolean filtered = false;
    //OR
    if (operator.equals(Operator.MUST_PASS_ONE)) {
      for (Filter<K, T> filter: filters) {
        if (!filter.filter(key, persistent)) {
          return !filtered;
        }
      }
      //AND
    } else if (operator.equals(Operator.MUST_PASS_ALL)) {
      for (Filter<K, T> filter: filters) {
        if (filter.filter(key, persistent)) {
          return !filtered;
        }
      }
    } else {
      throw new IllegalStateException(operator  " not yet implemented!");
    }
    return filtered;
 * TODO N.B. Currently only EQUALS and NOT_EQUALS are implemented.
  //just create empty conf needed for ObjectWritable
  private Configuration conf = new Configuration();
      //TODO Currently only EQUALS and NOT_EQUALS are implemented. 
      throw new IllegalStateException(filterOp  " not yet implemented!");
  //just create empty conf needed for ObjectWritable
  private Configuration conf = new Configuration();
  

    //.getIndexNamed(fieldName); throws org.apache.avro.AvroRuntimeException: Not a union:
    int fieldIndex = persistent.getSchema().getField(fieldName).pos();
      //TODO Currently only EQUALS and NOT_EQUALS are implemented. 
      throw new IllegalStateException(filterOp  " not yet implemented!");




          = (Class<? extends DataStore<K, T>>) ClassLoadingUtils.loadClass(dataStoreClass);
   * Note:
   *    consider that default dataStore is always visible
   *
    List<MongoCredential> credentials = new ArrayList<>();
    LOG.debug("Collection {} has been created for Mongo instance {}.",
    LOG.debug(
      LOG.debug("Forced synced of database for Mongo instance {}.",
      if (bin instanceof String) {
        ObjectId id = new ObjectId((String) bin);
        result = new Utf8(id.toString());
      } else {
        result = new Utf8(bin.toString());
      }
 * <p/>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p/>
import org.apache.avro.util.Utf8;
import org.bson.BSONObject;

import java.nio.ByteBuffer;
import java.util.Date;
   * @param fieldName fully qualified name of the field
   * {@link DBObject}, false otherwise
   * @param fieldName fully qualified name of the field to be accessed
            .get(getLeafName(fieldName));
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) ? parent.getBoolean(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) ? parent.getDouble(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) && parent.get(lf) != null ? parent.getInt(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) ? parent.getLong(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.getDate(lf);
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
   * @param value     value of the field
   * @param fieldName       fully qualified name of the field
   * @param createIfMissing create the intermediate fields if necessary
   * @throws IllegalAccessError if the field does not exist
                   "' does not exist: '"  fields[i]  "' is missing.");
   * @param fieldName fully qualified name of the target field
    switch (operator.toString()) {
    case "MUST_PASS_ALL":
      break;
    case "MUST_PASS_ONE":
      break;
    default:
          = (Class<? extends DataStore<K, T>>) ClassLoadingUtils.loadClass(dataStoreClass);
   * Note:
   *    consider that default dataStore is always visible
   *
    List<MongoCredential> credentials = new ArrayList<>();
    LOG.debug("Collection {} has been created for Mongo instance {}.",
    LOG.debug(
      LOG.debug("Forced synced of database for Mongo instance {}.",
      if (bin instanceof String) {
        ObjectId id = new ObjectId((String) bin);
        result = new Utf8(id.toString());
      } else {
        result = new Utf8(bin.toString());
      }
 * <p/>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p/>
import org.apache.avro.util.Utf8;
import org.bson.BSONObject;

import java.nio.ByteBuffer;
import java.util.Date;
   * @param fieldName fully qualified name of the field
   * {@link DBObject}, false otherwise
   * @param fieldName fully qualified name of the field to be accessed
            .get(getLeafName(fieldName));
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) ? parent.getBoolean(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) ? parent.getDouble(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) && parent.get(lf) != null ? parent.getInt(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.containsField(lf) ? parent.getLong(lf) : null;
   * @param fieldName fully qualified name of the field to be accessed
    String lf = getLeafName(fieldName);
    return parent.getDate(lf);
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
   * @param fieldName fully qualified name of the field to be accessed
   * @param value     value of the field
   * @param fieldName       fully qualified name of the field
   * @param createIfMissing create the intermediate fields if necessary
   * @throws IllegalAccessError if the field does not exist
                   "' does not exist: '"  fields[i]  "' is missing.");
   * @param fieldName fully qualified name of the target field
    if (table == null) {
      throw new IOException("No table was provided.");
    }


    DataOutputStream dos = null;
      dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
      dos.close();
    } finally {
      try {
        dos.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataInputStream dis = null;
      dis = new DataInputStream(new ByteArrayInputStream(a));
      dis.close();
    } finally {
      try {
        dis.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataOutputStream dos = null;
      dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
      dos.close();
    } finally {
      try {
        dos.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataInputStream dis = null;
      dis = new DataInputStream(new ByteArrayInputStream(a));
      dis.close();
    } finally {
      try {
        dis.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataOutputStream dos = null;
      dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
      dos.close();
    } finally {
      try {
        dos.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataInputStream dis = null;
      dis = new DataInputStream(new ByteArrayInputStream(a));
      dis.close();
    } finally {
      try {
        dis.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataOutputStream dos = null;
      dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
      dos.close();
    } finally {
      try {
        dos.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataInputStream dis = null;
      dis = new DataInputStream(new ByteArrayInputStream(a));
      dis.close();
    } finally {
      try {
        dis.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataOutputStream dos = null;
      dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
    } finally {
      try {
        dos.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataInputStream dis = null;
      dis = new DataInputStream(new ByteArrayInputStream(a));
    } finally {
      try {
        dis.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataInputStream dis = null;
      dis = new DataInputStream(new ByteArrayInputStream(a));
    } finally {
      try {
        dis.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    DataOutputStream dos = null;
      dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
    } finally {
      try {
        dos.close();
      } catch (IOException ioe) {
        throw new RuntimeException(ioe);
      }
    mappingFile.close();
import org.apache.hadoop.hbase.client.HTable;
      HTable hTable = htu.createTable(tableName, cfs);
      hTable.close();
      HTable hTable = htu.truncateTable(table.getName());
      hTable.close();
      PersistentBase persistent = (PersistentBase) fieldValue;
      PersistentBase newRecord = (PersistentBase) SpecificData.get().newRecord(persistent, persistent.getSchema());






      return result.getProgress();
    }
    catch(Exception e){
      return 0;
    }
      throws IOException, InterruptedException { }
    try{
      if (counter.isModulo()) {
        boolean firstBatch = (this.result == null);
        if (! firstBatch) {
          this.query.setStartKey(this.result.getKey());
          if (this.query.getLimit() == counter.getRecordsMax()) {
            this.query.setLimit(counter.getRecordsMax()  1);
          }
        }
        if (this.result != null) {
          this.result.close();
        }

        executeQuery();

        if (! firstBatch) {
          // skip first result
          this.result.next();
        }
      }

      counter.increment();
      return this.result.next();
    }
    catch(Exception e){
      LOG.error("Error reading Gora records: {}", e.getMessage());
      throw new RuntimeException(e);
    }
import org.apache.gora.persistency.impl.PersistentBase;
   implements Deserializer<PersistentBase> {
  private Class<? extends PersistentBase> persistentClass;
  private SpecificDatumReader<PersistentBase> datumReader;
  public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
      datumReader = new SpecificDatumReader<PersistentBase>(schema);
  public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerialization implements Serialization<PersistentBase> {
    return PersistentBase.class.isAssignableFrom(c);
  public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
  public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
  private SpecificDatumWriter<PersistentBase> datumWriter;
  public void serialize(PersistentBase persistent) throws IOException {
  private static<T extends PersistentBase> T getPersistent(T obj, String[] fields) {
public interface Persistent extends Dirtyable {
/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import org.apache.gora.store.DataStoreFactory;
  
  /** 
   * Determines if an schema will be automatically created. 
   */
  protected boolean autoCreateSchema;
  @Override
    autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
    this.properties = properties;
import org.apache.gora.persistency.impl.PersistentBase;
  public static Schema getSchema(Class<? extends PersistentBase> clazz)
  public static String[] getPersistentFieldNames(PersistentBase persistent) {
  public static <T extends PersistentBase> T deepClonePersistent(T persistent) {
    SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(
import org.apache.gora.persistency.impl.PersistentBase;

      throws SecurityException, NoSuchMethodException {

      throws SecurityException, NoSuchMethodException {
  public static <T> T newInstance(Class<T> clazz) 
      throws InstantiationException, IllegalAccessException, 
      SecurityException, NoSuchMethodException, IllegalArgumentException, 
      InvocationTargetException {



  public static Object newInstance(String classStr) 
      throws InstantiationException, IllegalAccessException, 
      ClassNotFoundException, SecurityException, IllegalArgumentException, 
      NoSuchMethodException, InvocationTargetException {

      throws IllegalArgumentException, SecurityException,
      IllegalAccessException, NoSuchFieldException {


  public static <T extends PersistentBase> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) 
      throws SecurityException, NoSuchMethodException, IllegalArgumentException, 
      IllegalAccessException, InvocationTargetException {

import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;

import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Locale;
import org.apache.commons.io.FilenameUtils;
import org.apache.gora.util.GoraException;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
/** 
 * Generate specific Java classes for defined schemas. 
 * Different from the {@link org.apache.gora.compiler.GoraCompiler}, 
 * which uses an .avsc or .json schema definition, this compiler 
 * expects an XML schema file as input.
 */
  private String packageName;
  /**
   * GoraDynamoDBCompiler
   * 
   * @param File
   *          where the data bean will be written.
   */
    this.dest = dest;
    log.info("Compiling {} to {}", src, dest);
    if (dynamoDBMap.getTables().isEmpty()) 
      throw new IllegalStateException("There are no tables defined.");
    for(String tableName : dynamoDBMap.getTables().keySet()) {
      compiler.compile(tableName, dynamoDBMap.getKeySchema(tableName), 
          dynamoDBMap.getItems(tableName));
      log.info("{} written without issues to {}", tableName, dest.getAbsolutePath());
   * Method in charge of compiling a specific table using a key schema and a set 
   * of attributes
   * @param dest2 
   * @param pTableNameTable 
   *          name
   * @param pKeySchemaKey 
   *          schema used
   * @param pItemsList 
   *          of items belonging to a specific table
  private void compile(String pTableName, ArrayList<KeySchemaElement> arrayList, Map<String, String> map){
      setHeaders(packageName);
      for (KeySchemaElement pKeySchema : arrayList) {
        setKeyAttributes(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
        setKeyMethods(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
        map.remove(pKeySchema.getAttributeName());
      }
      setItems(map, 2);
      setDefaultMethods(2, pTableName);
      log.error("Error while compiling table {}",pTableName, e.getMessage());
      throw new RuntimeException(e);

   * @param pItemsThe 
   *          items belonging to the table
   * @param pIdenThe 
   *          number of spaces used for identation
  private void setItems(Map<String, String> pItems, int pIden)
      throws IOException {
    for (String itemName : pItems.keySet()) {
      String itemType = "String";
      if (pItems.get(itemName).toString().equals("N"))
        itemType = "double";
      if (pItems.get(itemName).toString().equals("SS"))
        itemType = "Set<String>";
      if (pItems.get(itemName).toString().equals("SN"))
        itemType = "Set<double>";
      line(pIden, "private "  itemType  " "  itemName  ";");
      setItemMethods(itemName, itemType, pIden);

   * @param pItemNameItem
   *          's name
   * @param pItemTypeItem
   *          's type
   * @param pIdenNumber 
   *          of spaces used for indentation
  private void setItemMethods(String pItemName, String pItemType, int pIden)
      throws IOException {
    line(pIden, "@DynamoDBAttribute(attributeName = \""
         camelCasify(pItemName)  "\")");
    line(pIden, "public "  pItemType  " get"  camelCasify(pItemName)
     "() {  return "  pItemName  ";  }");
    line(pIden, "public void set"  camelCasify(pItemName)  "("  pItemType
         " p"  camelCasify(pItemName)  ") {  this."  pItemName  " = p"
         camelCasify(pItemName)  ";  }");

   * @param pKeySchemaThe 
   *          key schema for a specific table
   * @param pIdenNumber 
   *          of spaces used for indentation
  private void setKeyMethods(KeySchemaElement pKeySchema, String attType,
      int pIden) throws IOException {
    attType = attType.equals("S") ? "String" : "double";
    if (pKeySchema.getKeyType().equals(KeyType.HASH.toString())) {
      strBuilder.append("@DynamoDBHashKey(attributeName=\""
           pKeySchema.getAttributeName()  "\") \n");
      strBuilder.append("    public "  attType  " getHashKey() {  return "
           pKeySchema.getAttributeName()  "; } \n");
      strBuilder.append("    public void setHashKey("  attType  " ");
      strBuilder.append("p"  camelCasify(pKeySchema.getAttributeName())
       "){  this."  pKeySchema.getAttributeName());
      strBuilder.append(" = p"  camelCasify(pKeySchema.getAttributeName())
       "; }");
    if (pKeySchema.getKeyType().equals(KeyType.RANGE.toString())) {
      strBuilder.append("@DynamoDBRangeKey(attributeName=\""
           pKeySchema.getAttributeName()  "\") \n");
      strBuilder.append("    public "  attType  " getRangeKey() { return "
           pKeySchema.getAttributeName()  "; } \n");
      strBuilder.append("    public void setRangeKey("  attType  " ");
      strBuilder.append("p"  camelCasify(pKeySchema.getAttributeName())
       "){  this."  pKeySchema.getAttributeName());
      strBuilder.append(" = p"  camelCasify(pKeySchema.getAttributeName())
       "; }");

   * @param pKeySchema
   *          schema
   * @param attType
   *          attribute type
   * @param pIden
   *          of spaces used for indentation
  private void setKeyAttributes(KeySchemaElement pKeySchema, String attType,
      int pIden) throws IOException {
    attType = attType.equals("S") ? "String " : "double ";
    if (pKeySchema != null) {
      strBuilder.append("private "  attType);
      strBuilder.append(pKeySchema.getAttributeName()  ";");

   * @param sString 
   *          to be camelcasified
    return s.substring(0, 1).toUpperCase(Locale.getDefault())  s.substring(1);
   * @param nameClass 
   *          name
   *          spacing
    String fullDest = FilenameUtils.normalize
        (dest.getAbsolutePath()  File.separatorChar  packageName.replace('.', File.separatorChar));
    File dir = new File(fullDest);
    out = new OutputStreamWriter(new FileOutputStream(new File(dir, name)), Charset.defaultCharset());
    if (namespace != null) {
      line(0, "package "  namespace  ";\n");
    line(0, "import java.util.List;");
    line(0, "");
    line(0, "import org.apache.avro.Schema.Field;");
    line(0, "import org.apache.gora.persistency.Tombstone;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBAttribute;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBHashKey;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTable;");
   * @param pIden 
   *          of spaces used for indentation
   * @param tabName
   *          table name
  private void setDefaultMethods(int pIden, String tabName) throws IOException {
    line(pIden, "public "  tabName  " clone() { return null; }");
    line(pIden, "public Tombstone getTombstone() { return null; }");
    line(pIden, "public List<Field> getUnmanagedFields() { return null; }");
    line(pIden, "public Persistent newInstance() { return new "  tabName  "(); }");
   * @param indentNumber 
   *          of spaces used for indentation
   * @param textText 
   *          to be written
    for (int i = 0; i < indent; i) {
   * @param name 
   *          to be converted
    return name.substring(0,1).toUpperCase(Locale.getDefault())
         name.substring(1,name.length());
   * @param argsReceives 
   *          the schema file to be compiled and where this should be written
  public static void main(String[] args) {
    try {
      if (args.length < 2) {
        log.error("Usage: Compiler <schema file> <output dir>");
        System.exit(1);
      }
      compileSchema(new File(args[0]), new File(args[1]));
    } catch (Exception e) {
      log.error("Something went wrong. Please check the input file.", e.getMessage());
      throw new RuntimeException(e);
   * @param pMapFile
   *          schema file to be mapped into a table

      if (doc == null || doc.getRootElement() == null)
        throw new GoraException("Unable to load "  MAPPING_FILE
             ". Please check its existance!");

      boolean keys = false;
      for (Element tableElement : tableElements) {

        String tableName = tableElement.getAttributeValue("name");
        long readCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("readcunit"));
        long writeCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("writecunit"));
        this.packageName = tableElement.getAttributeValue("package");

        mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
            writeCapacUnits);
        log.debug("Table properties have been set for name, package and provisioned throughput.");

        // Retrieving attributes
        List<Element> fieldElements = tableElement.getChildren("attribute");
        for (Element fieldElement : fieldElements) {
          String key = fieldElement.getAttributeValue("key");
          String attributeName = fieldElement.getAttributeValue("name");
          mappingBuilder.addAttribute(tableName, attributeName, attributeType);
          // Retrieving key's features
          if (key != null) {
            mappingBuilder.setKeySchema(tableName, attributeName, key);
            keys = true;
          }
        log.debug("Attributes for table '{}' have been read.", tableName);
        if (!keys)
          log.warn("Keys for table '{}' have NOT been set.", tableName);
      log.error("Error while performing xml mapping.", ex.getMessage());
      throw new RuntimeException(ex);
      log.error("An error occured whilst reading the xml mapping file!", ex.getMessage());
/**
 * Class abstracting a composed DynamoDB key.
 * @param <H>
 * @param <R>
 */
public class DynamoDBKey<H, R> {  






  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append('[').append(hashKey != null? hashKey.toString():":");
    sb.append(rangeKey != null? ":"  rangeKey.toString():"");
    sb.append(']');
    return sb.toString();
  }
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;
import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBScanExpression;
import com.amazonaws.services.dynamodbv2.model.AttributeValue;
import com.amazonaws.services.dynamodbv2.model.ComparisonOperator;
import com.amazonaws.services.dynamodbv2.model.Condition;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
  public static final ComparisonOperator DEFAULT_SCAN_OP = ComparisonOperator.GE;

  private ArrayList<KeySchemaElement> keySchema;
  private Map<String, String> keyItems;

    super(null);

   * 
    super(dataStore);
  private void defineQueryParams() {
    if ((query.getStartKey() != null || query.getKey() != null)
        && query.getEndKey() != null) {
      DynamoDBQuery.setType(RANGE_QUERY);
    } else if (query.getKey() != null || query.getStartKey() != null) {
      DynamoDBQuery.setType(SCAN_QUERY);
    }
  }

  public void buildExpression() {
    defineQueryParams();
    if (DynamoDBQuery.getType().equals(RANGE_QUERY)) {
      buildRangeExpression();
    } else if (DynamoDBQuery.getType().equals(SCAN_QUERY)) {
      buildScanExpression();
    } else {
      throw new IllegalArgumentException("Query type not supported");
  }

  /**
   * Builds hash key attribute from generic query received.
   * 
   * @param qKey
   * 
   * @returnAttributeValue build from query
   */
  private Map<String, AttributeValue> buildHashKey(K qKey) {
    Map<String, AttributeValue> hashKey = new HashMap<>();
    for (KeySchemaElement key : getKeySchema()) {
      AttributeValue attr = new AttributeValue();
      if (key.getKeyType().equals(KeyType.HASH.toString())) {
        if (keyItems.get(key.getAttributeName()).equals("N")) {
          attr.withN(getHashKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("S")) {
          attr.withS(getHashKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("B")) {
          attr.withB(ByteBuffer.wrap(getHashKey(qKey).toString().getBytes(Charset.defaultCharset())));
        } else {
          throw new IllegalArgumentException("Data type not supported for "
               key.getAttributeName());
        }
        hashKey.put(key.getAttributeName(), attr);
      }
    }
    if (hashKey.isEmpty()) {
      throw new IllegalStateException("No key value has been defined.");
    }
    return hashKey;
  }

  /**
   * Builds range key attribute from generic query received.
   * 
   * @param qKey
   * 
   * @return
   */
  private Map<String, AttributeValue> buildRangeKey(K qKey) {
    Map<String, AttributeValue> kAttrs = new HashMap<>();
    for (KeySchemaElement key : getKeySchema()) {
      AttributeValue attr = new AttributeValue();
      if (key.getKeyType().equals(KeyType.RANGE.toString())) {
        if (keyItems.get(key.getAttributeName()).equals("N")) {
          attr.withN(getRangeKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("S")) {
          attr.withS(getRangeKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("B")) {
          attr.withB(ByteBuffer.wrap(getRangeKey(qKey).toString().getBytes(Charset.defaultCharset())));
        } else {
          throw new IllegalArgumentException("Data type not supported for "
               key.getAttributeName());
        }
        kAttrs.put(key.getAttributeName(), attr);
      }
    }
    return kAttrs;
   * 
   * @param pHashAttrValueHash
   *          attribute value where to start scanning
  public void buildScanExpression() {
    K qKey = getKey();
    if (qKey == null) {
      LOG.warn("No key defined. Trying with startKey.");
      qKey = query.getStartKey();
      if (qKey == null) {
        throw new IllegalStateException("No key has been defined please check");
      }
    }
    ComparisonOperator compOp = getScanCompOp() != null ? getScanCompOp()
        : DEFAULT_SCAN_OP;

    // hash key condition
    Map<String, AttributeValue> hashAttrVals = buildHashKey(qKey);
    for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
      Condition scanFilterHashCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue());
      newScanExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
    }
    // range key condition
    Map<String, AttributeValue> rangeAttrVals = buildRangeKey(qKey);
    for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
      Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue());
      newScanExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
    }
   * 
  public void buildRangeExpression() {
    DynamoDBScanExpression queryExpression = new DynamoDBScanExpression();
    ComparisonOperator compOp = ComparisonOperator.BETWEEN;
    // hash key range
    Map<String, AttributeValue> hashAttrVals = buildHashKey(query.getStartKey());
    Map<String, AttributeValue> endHashAttrVals = buildHashKey(query.getEndKey());
    for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
      Condition scanFilterHashCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue(), endHashAttrVals.get(en.getKey()));
      queryExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
    }
    // range key range
    Map<String, AttributeValue> rangeAttrVals = buildRangeKey(query.getStartKey());
    Map<String, AttributeValue> endRangeAttrVals = buildRangeKey(query.getEndKey());
    for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
      Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue(), endRangeAttrVals.get(en.getKey()));
      queryExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
    }
    dynamoDBExpression = queryExpression;
   * 
      // Our key may be have hash and range keys
      for (Method met : key.getClass().getDeclaredMethods()) {
        if (met.getName().equals("getHashKey")) {
          Object[] params = null;
          hashKey = met.invoke(key, params);
          break;
        }
    } catch (IllegalArgumentException e) {
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new IllegalArgumentException(e);
    } catch (IllegalAccessException e) {
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
    } catch (InvocationTargetException e) {
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
    return hashKey;
   * 
      // Our key may be have hash and range keys
        if(met.getName().equals("getRangeKey")){
          Object [] params = null;
          rangeKey = met.invoke(key, params);
          break;
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new IllegalArgumentException(e);
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
   * 
   * 
   * 
  public ArrayList<KeySchemaElement> getKeySchema(){
   * 
   * 
   * @param arrayList
  public void setKeySchema(ArrayList<KeySchemaElement> arrayList) {
    this.keySchema = arrayList;
   * 
    this.setStartKey(query.getStartKey());
    this.setEndKey(query.getEndKey());
   * 
   * 
   * 
   * 
   * 
   * 
  /**
   * Sets the keyItems that could be used.
   * 
   * @param items
   */
  public void setKeyItems(Map<String, String> items) {
    keyItems = items;
  }



import static org.apache.gora.dynamodb.store.DynamoDBUtils.DYNAMO_KEY_HASHRANGE;

import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;


  private final Map<String, Map<String, String>> tablesToItems;

  private final Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas;


   * Constructor for DynamoDBMapping
   * 
   * @param tablesToItems2
   *          Tables mapped.
   * @param tablesToKeySchemas
   *          KeySchemas used within tables mapped.
   * @param provisionedThroughput
   *          Provisioned throughput used within tables mapped.
  public DynamoDBMapping(Map<String, Map<String, String>> tablesToItems2,
      Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas,
      Map<String, ProvisionedThroughput> provisionedThroughput) {

    this.tablesToItems = tablesToItems2;
   * 
   * @return tablesToItem 
   *          HashMap 
  public Map<String, Map<String, String>> getTables() {

   * 
   * @param tableName
   *          table name to determine which attributes to get 
  public Map<String, String> getItems(String tableName) {
   * @param tableName
   *          Table name to determine which key schema to get
  public ArrayList<KeySchemaElement> getKeySchema(String tableName) {

   * 
   * @param tableName
   *          Table name to determine which provisioned throughput to get

    private Map<String, Map<String, String>> tablesToItems = 
        new HashMap<String, Map<String, String>>();
    private Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas = 
        new HashMap<String, ArrayList<KeySchemaElement>>();
    private Map<String, ProvisionedThroughput> tablesToPrTh = 
        new HashMap<String, ProvisionedThroughput>();

     * 

     * 
    public void setProvisionedThroughput(String tableName, long readCapUnits,
        long writeCapUnits) {
      ProvisionedThroughput ptDesc = new ProvisionedThroughput()
          .withReadCapacityUnits(readCapUnits).withWriteCapacityUnits(
              writeCapUnits);

     * 
    // public void setHashRangeKeySchema(String tableName, String rangeKeyName,
    // String rangeKeyType){
    // KeySchemaElement kSchema = tablesToKeySchemas.get(tableName);
    // if ( kSchema == null)
    // kSchema = new KeySchemaElement();

    // KeySchemaElement rangeKeyElement = new
    // KeySchemaElement().withAttributeName(rangeKeyName).withKeyType(KeyType.RANGE).withKeyType(rangeKeyType);
    // kSchema.
    // kSchema.setRangeKeyElement(rangeKeyElement);
    // tablesToKeySchemas.put(tableName, kSchema);
    // }

     * @param keyType2
    public void setKeySchema(String tableName, String keyName, String keyType) {
      ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
      if (kSchema == null) {
        kSchema = new ArrayList<KeySchemaElement>();
      }
      KeyType type = keyType.equals(DYNAMO_KEY_HASHRANGE) ? KeyType.RANGE : KeyType.HASH;
      kSchema.add(new KeySchemaElement().withAttributeName(keyName)
          .withKeyType(type));

     * Checks if a table exists, and if doesn't exist it creates the new table.
     * 
    private Map<String, String> getOrCreateTable(String tableName) {
      Map<String, String> items = tablesToItems.get(tableName);
        items = new HashMap<String, String>();

     * Gets the attribute for a specific item. The idea is to be able to get 
     * different items with different attributes.
     * TODO This method is incomplete because the itemNumber might not 
     * be present and this would be a problem
     * 
    /*private HashMap<String, String> getOrCreateItemAttribs(
        Map<String, String> items) {


      if (itemAttribs == null) {
        itemAttribs = new HashMap<String, String>();
      }

      items.add(itemAttribs);
      return null;
    }*/

     * 
    public void addAttribute(String tableName, String attributeName,
        String attrType) {
      // selecting table
      Map<String, String> items = getOrCreateTable(tableName);
      // add attribute to item
      //HashMap<String, String> itemAttribs = getOrCreateItemAttribs(items);
      //itemAttribs.put(attributeName, attrType);
      // add item to table
      items.put(attributeName, attrType);
      // tablesToItems.put(tableName, items);
    }

     * 
    private boolean verifyAllKeySchemas() {
      boolean rsl = true;
      if (tablesToItems.isEmpty() || tablesToKeySchemas.isEmpty())
        rsl = false;
      for (String tableName : tablesToItems.keySet()) {
        // if there are not schemas defined
        if (tablesToKeySchemas.get(tableName) == null) {
          LOG.error("No schema defined for DynamoDB table '"  tableName  '\'');
          rsl = false;
        rsl = verifyKeySchema(tableName);
      }
      return rsl;

     * 
    private boolean verifyKeySchema(String tableName) {
      ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
      boolean hashPk = false;
      if (kSchema == null) {
        LOG.error("No keys defined for '{}'. Please check your schema!", tableName);
        return hashPk;
      for (KeySchemaElement ks : kSchema) {
        if (ks.getKeyType().equals(KeyType.HASH.toString())) {
          hashPk = true;
        }
      }
      return hashPk;

     * 
      // verifying items for at least a table
      if (tablesToItems.isEmpty())
        throw new IllegalStateException("No tables were defined.");

      // verifying if key schemas have been properly defined
      if (!verifyAllKeySchemas())
        throw new IllegalStateException("no key schemas defined for table ");

      // Return the tableDescription and all the attributes needed
      return new DynamoDBMapping(tablesToItems, tablesToKeySchemas,
          tablesToPrTh);
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CLI_TYP_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS_TRUE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.ENDPOINT_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.PREF_SCH_NAME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SERIALIZATION_TYPE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SLEEP_DELETE_TIME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.WAIT_TIME;

import java.util.Map;
import org.apache.gora.store.DataStore;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
import com.amazonaws.services.dynamodbv2.model.DeleteTableRequest;
import com.amazonaws.services.dynamodbv2.model.DeleteTableResult;
import com.amazonaws.services.dynamodbv2.model.DescribeTableRequest;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
import com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException;
import com.amazonaws.services.dynamodbv2.model.TableDescription;
/**
 * Class for using DynamoDBStores
 * 
 * @param <K>
 * @param <T>
 */
public class DynamoDBStore<K, T extends Persistent> implements DataStore<K, T> {
  /** Handler for different serialization modes. */
  private IDynamoDB<K, T> dynamoDbStore;

  /** Helper to write useful information into the logs. */

   * Amazon DynamoDB client which can be asynchronous or not


  /** Specifies how the objects will be serialized inside DynamoDb. */
  private DynamoDBUtils.DynamoDBType serializationType;
   * Schema name which will be used from within the data store. If not set, all
   * the available schemas from the mapping file will be used.
  private String preferredSchema;

  @Override
  public void close() {
    dynamoDbStore.close();
   * Creates the table within the data store for a preferred schema or for a
   * group of schemas defined within the mapping file
   * @throws IOException
  @Override
  public void createSchema() {
    dynamoDbStore.createSchema();
  }

  @Override
  public boolean delete(K key) {
    return dynamoDbStore.delete(key);
  }

  @Override
  public long deleteByQuery(Query<K, T> query) {
    return dynamoDbStore.deleteByQuery(query);
  }

  @Override
  public void deleteSchema() {
    if (getDynamoDbMapping().getTables().isEmpty())
      throw new IllegalStateException("There are not tables defined.");
    if (preferredSchema == null) {
      LOG.debug("Delete schemas");
      if (getDynamoDbMapping().getTables().isEmpty())
        throw new IllegalStateException("There are not tables defined.");
      // read the mapping object
      for (String tableName : getDynamoDbMapping().getTables().keySet())
        executeDeleteTableRequest(tableName);
      LOG.debug("All schemas deleted successfully.");
    } else {
      LOG.debug("create schema "  preferredSchema);
      executeDeleteTableRequest(preferredSchema);

  @Override
  public Result<K, T> execute(Query<K, T> query) {
    return dynamoDbStore.execute(query);
  }

  @Override
  public void flush() {
    dynamoDbStore.flush();
  }

  @Override
  public T get(K key) {
    return dynamoDbStore.get(key);
  }

  @Override
  public T get(K key, String[] fields) {
    return dynamoDbStore.get(key, fields);
  }

  @Override
  public BeanFactory<K, T> getBeanFactory() {
    // TODO Auto-generated method stub

  @Override
  public Class<K> getKeyClass() {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> arg0)
      throws IOException {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public Class<T> getPersistentClass() {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public String getSchemaName() {
    return this.getPreferredSchema();
  }

  @Override
  public void initialize(Class<K> keyClass, Class<T> persistentClass,
      Properties properties) {
    try {
      LOG.debug("Initializing DynamoDB store");
      setDynamoDBProperties(properties);

      dynamoDbStore = DynamoDBFactory.buildDynamoDBStore(getSerializationType());
      dynamoDbStore.setDynamoDBStoreHandler(this);
      dynamoDbStore.initialize(keyClass, persistentClass, properties);
    } catch (Exception e) {
      LOG.error("Error while initializing DynamoDB store", e.getMessage());
      throw new RuntimeException(e);
    }
  }

  private void setDynamoDBProperties(Properties properties) throws IOException {
    setSerializationType(properties.getProperty(SERIALIZATION_TYPE));
    PropertiesCredentials creds = DynamoDBUtils.getCredentials(this.getClass());
    setPreferredSchema(properties.getProperty(PREF_SCH_NAME));
    setDynamoDBClient(DynamoDBUtils.getClient(
        properties.getProperty(CLI_TYP_PROP), creds));
    getDynamoDBClient().setEndpoint(properties.getProperty(ENDPOINT_PROP));
    setDynamoDbMapping(readMapping());
    setConsistency(properties.getProperty(CONSISTENCY_READS));
  }

  @Override
  public K newKey() {
    return dynamoDbStore.newKey();
  }

  @Override
  public T newPersistent() {
    return dynamoDbStore.newPersistent();
  }

  @Override
  public Query<K, T> newQuery() {
    return dynamoDbStore.newQuery();
  }

  @Override
  public void put(K key, T value) {
    dynamoDbStore.put(key, value);
  }



   * Verifies if the specified schemas exist
   * 
   * @throws IOException
   */
  @Override
  public boolean schemaExists() {
    LOG.info("Verifying schemas.");
    TableDescription success = null;
    if (getDynamoDbMapping().getTables().isEmpty())
      throw new IllegalStateException("There are not tables defined.");
    if (getPreferredSchema() == null) {
      LOG.debug("Verifying schemas");
      if (getDynamoDbMapping().getTables().isEmpty())
        throw new IllegalStateException("There are not tables defined.");
      // read the mapping object
      for (String tableName : getDynamoDbMapping().getTables().keySet()) {
        success = getTableSchema(tableName);
        if (success == null)
          return false;
      }
    } else {
      LOG.info("Verifying schema "  preferredSchema);
      success = getTableSchema(preferredSchema);
    }
    LOG.info("Finished verifying schemas.");
    return (success != null) ? true : false;
  }

  @Override
  public void setBeanFactory(BeanFactory<K, T> arg0) {
    // TODO Auto-generated method stub
  }

  @Override
  public void setKeyClass(Class<K> arg0) {
    dynamoDbStore.setKeyClass(arg0);
  }

  @Override
  public void setPersistentClass(Class<T> arg0) {
    dynamoDbStore.setPersistentClass(arg0);
  }

  @Override
  public void truncateSchema() {
    // TODO Auto-generated method stub
  }

  /** 
   * 
   * @param pMapFile
   *          The schema file to be mapped into a table
   * @return DynamoDBMapping Object containing all necessary information to
   *         create tables
      Document doc = builder.build(getClass().getClassLoader()
          .getResourceAsStream(MAPPING_FILE));
      if (doc == null || doc.getRootElement() == null)
        throw new GoraException("Unable to load "  MAPPING_FILE
             ". Please check its existance!");

      boolean keys = false;
      for (Element tableElement : tableElements) {

        long readCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("readcunit"));
        long writeCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("writecunit"));

        mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
            writeCapacUnits);

        List<Element> fieldElements = tableElement.getChildren("attribute");
        for (Element fieldElement : fieldElements) {
          String key = fieldElement.getAttributeValue("key");
          String attributeName = fieldElement.getAttributeValue("name");
          mappingBuilder.addAttribute(tableName, attributeName, attributeType);
          // Retrieving key's features
          if (key != null) {
            mappingBuilder.setKeySchema(tableName, attributeName, key);
            keys = true;
          }
        LOG.debug("Attributes for table '"  tableName  "' have been read.");
        if (!keys)
          LOG.warn("Keys for table '"  tableName  "' have NOT been set.");
    } catch (IOException ex) {
      LOG.error("Error while performing xml mapping.", ex.getMessage());
    } catch (Exception ex) {
      LOG.error("Error while performing xml mapping.", ex.getMessage());
      throw new RuntimeException(ex);
   * 
  public void executeDeleteTableRequest(String pTableName) {
    try {
      DeleteTableRequest deleteTableRequest = new DeleteTableRequest()
          .withTableName(pTableName);
      DeleteTableResult result = getDynamoDBClient().deleteTable(
          deleteTableRequest);
      LOG.debug("Schema: "  result.getTableDescription()
       " deleted successfully.");
    } catch (Exception e) {
      LOG.debug("Schema: {} deleted.", pTableName, e.getMessage());
      throw new RuntimeException(e);


   * 
  private void waitForTableToBeDeleted(String pTableName) {
    long endTime = startTime  WAIT_TIME;
        Thread.sleep(SLEEP_DELETE_TIME);
      } catch (Exception e) {
      }
      try {
        DescribeTableRequest request = new DescribeTableRequest()
            .withTableName(pTableName);
        TableDescription tableDescription = getDynamoDBClient().describeTable(
            request).getTable();
        LOG.error(ase.getMessage());
   * 
  private TableDescription getTableSchema(String tableName) {
    try {
      DescribeTableRequest describeTableRequest = new DescribeTableRequest()
          .withTableName(tableName);
      tableDescription = getDynamoDBClient()
          .describeTable(describeTableRequest).getTable();
    } catch (ResourceNotFoundException e) {

   * Gets a specific table key schema.
   * 
   * @param tableName
   *          from which key schema is to be obtained.
   * @return KeySchema from table.
  public ArrayList<KeySchemaElement> getTableKeySchema(String tableName) {
    return getDynamoDbMapping().getKeySchema(tableName);
   * Gets the provisioned throughput for a specific table.
   * 
   * @param tableName
   *          to get the ProvisionedThroughput.
   * @return ProvisionedThroughput for a specific table
  public ProvisionedThroughput getTableProvisionedThroughput(String tableName) {
    return getDynamoDbMapping().getProvisionedThroughput(tableName);
   * Returns a table attribues.
   * @param tableName
  public Map<String, String> getTableAttributes(String tableName) {
    return getDynamoDbMapping().getItems(tableName);
   * Gets consistency level for reads
   * 
   * @return True for strong consistency or false for eventual consistent reads
  public boolean getConsistencyReads() {
    if (getConsistency() != null)
      if (getConsistency().equals(CONSISTENCY_READS_TRUE))
        return true;
    return false;
  }


  /**
   * Set DynamoDBStore to be used.
   * 
   * @param iDynamoDB
   */
  public void setDynamoDbStore(IDynamoDB<K, T> iDynamoDB) {
    this.dynamoDbStore = iDynamoDB;
  }

  /**
   * @param serializationType
   *          the serializationType to set
   */
  private void setSerializationType(String serializationType) {
    if (serializationType == null || serializationType.isEmpty()
        || serializationType.equals(DynamoDBUtils.AVRO_SERIALIZATION)) {
      LOG.warn("Using AVRO serialization.");
      this.serializationType = DynamoDBUtils.DynamoDBType.AVRO;
    } else {
      LOG.warn("Using DynamoDB serialization.");
      this.serializationType = DynamoDBUtils.DynamoDBType.DYNAMO;
    }
  }

  /**
   * Gets serialization type used inside DynamoDB module.
   * 
   * @return
   */
  private DynamoDBUtils.DynamoDBType getSerializationType() {
    return serializationType;
  }

  /**
   * @return the preferredSchema
   */
  public String getPreferredSchema() {
    return preferredSchema;
  }

  /**
   * @param preferredSchema
   *          the preferredSchema to set
   */
  public void setPreferredSchema(String preferredSchema) {
    this.preferredSchema = preferredSchema;
  }

  /**
   * Gets DynamoDBClient.
   * 
   * @return
   */
  public AmazonDynamoDB getDynamoDbClient() {
    return getDynamoDBClient();
  }


  /**
   * @return the mapping
   */
  public DynamoDBMapping getDynamoDbMapping() {
    return mapping;
  }

  /**
   * @param mapping
   *          the mapping to set
   */
  public void setDynamoDbMapping(DynamoDBMapping mapping) {
    this.mapping = mapping;
  }

  /**
   * @return the consistency
   */
  public String getConsistency() {
    return consistency;
  }

  /**
   * @param consistency
   *          the consistency to set
   */
  public void setConsistency(String consistency) {
    this.consistency = consistency;
  }

  /**
   * @return the dynamoDBClient
   */
  public AmazonDynamoDB getDynamoDBClient() {
    return dynamoDBClient;
  }

  /**
   * @param dynamoDBClient
   *          the dynamoDBClient to set
   */
  public void setDynamoDBClient(AmazonDynamoDB dynamoDBClient) {
    this.dynamoDBClient = dynamoDBClient;
    PersistentBase record = (PersistentBase) new BeanFactoryImpl(keyClass, clazz).newPersistent();
    if ( unionSchema == null ) {
      unionSchema = pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
    }
      if (serializer != null) {
        List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
        value = genericArray;
      } else {
        LOG.warn("Field detected as type Array, however no serializer could be found!")
      }
    if ( unionSchema == null ) {
      unionSchema = pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
    }
        LOG.warn("Field detected as type Array, however no serializer could be found!");
      PersistentBase persistent = (PersistentBase) fieldValue;
      PersistentBase newRecord = (PersistentBase) SpecificData.get().newRecord(persistent, persistent.getSchema());






      return result.getProgress();
    }
    catch(Exception e){
      return 0;
    }
      throws IOException, InterruptedException { }
    try{
      if (counter.isModulo()) {
        boolean firstBatch = (this.result == null);
        if (! firstBatch) {
          this.query.setStartKey(this.result.getKey());
          if (this.query.getLimit() == counter.getRecordsMax()) {
            this.query.setLimit(counter.getRecordsMax()  1);
          }
        }
        if (this.result != null) {
          this.result.close();
        }

        executeQuery();

        if (! firstBatch) {
          // skip first result
          this.result.next();
        }
      }

      counter.increment();
      return this.result.next();
    }
    catch(Exception e){
      LOG.error("Error reading Gora records: {}", e.getMessage());
      throw new RuntimeException(e);
    }
import org.apache.gora.persistency.impl.PersistentBase;
   implements Deserializer<PersistentBase> {
  private Class<? extends PersistentBase> persistentClass;
  private SpecificDatumReader<PersistentBase> datumReader;
  public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
      datumReader = new SpecificDatumReader<PersistentBase>(schema);
  public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerialization implements Serialization<PersistentBase> {
    return PersistentBase.class.isAssignableFrom(c);
  public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
  public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
  private SpecificDatumWriter<PersistentBase> datumWriter;
  public void serialize(PersistentBase persistent) throws IOException {
  private static<T extends PersistentBase> T getPersistent(T obj, String[] fields) {
public interface Persistent extends Dirtyable {
/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements. See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership. The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/
import org.apache.gora.store.DataStoreFactory;
  
  /** 
   * Determines if an schema will be automatically created. 
   */
  protected boolean autoCreateSchema;
  @Override
    autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
    this.properties = properties;
import org.apache.gora.persistency.impl.PersistentBase;
  public static Schema getSchema(Class<? extends PersistentBase> clazz)
  public static String[] getPersistentFieldNames(PersistentBase persistent) {
  public static <T extends PersistentBase> T deepClonePersistent(T persistent) {
    SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(
import org.apache.gora.persistency.impl.PersistentBase;

      throws SecurityException, NoSuchMethodException {

      throws SecurityException, NoSuchMethodException {
  public static <T> T newInstance(Class<T> clazz) 
      throws InstantiationException, IllegalAccessException, 
      SecurityException, NoSuchMethodException, IllegalArgumentException, 
      InvocationTargetException {



  public static Object newInstance(String classStr) 
      throws InstantiationException, IllegalAccessException, 
      ClassNotFoundException, SecurityException, IllegalArgumentException, 
      NoSuchMethodException, InvocationTargetException {

      throws IllegalArgumentException, SecurityException,
      IllegalAccessException, NoSuchFieldException {


  public static <T extends PersistentBase> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) 
      throws SecurityException, NoSuchMethodException, IllegalArgumentException, 
      IllegalAccessException, InvocationTargetException {

import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;

import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Locale;
import org.apache.commons.io.FilenameUtils;
import org.apache.gora.util.GoraException;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
/** 
 * Generate specific Java classes for defined schemas. 
 * Different from the {@link org.apache.gora.compiler.GoraCompiler}, 
 * which uses an .avsc or .json schema definition, this compiler 
 * expects an XML schema file as input.
 */
  private String packageName;
  /**
   * GoraDynamoDBCompiler
   * 
   * @param File
   *          where the data bean will be written.
   */
    this.dest = dest;
    log.info("Compiling {} to {}", src, dest);
    if (dynamoDBMap.getTables().isEmpty()) 
      throw new IllegalStateException("There are no tables defined.");
    for(String tableName : dynamoDBMap.getTables().keySet()) {
      compiler.compile(tableName, dynamoDBMap.getKeySchema(tableName), 
          dynamoDBMap.getItems(tableName));
      log.info("{} written without issues to {}", tableName, dest.getAbsolutePath());
   * Method in charge of compiling a specific table using a key schema and a set 
   * of attributes
   * @param dest2 
   * @param pTableNameTable 
   *          name
   * @param pKeySchemaKey 
   *          schema used
   * @param pItemsList 
   *          of items belonging to a specific table
  private void compile(String pTableName, ArrayList<KeySchemaElement> arrayList, Map<String, String> map){
      setHeaders(packageName);
      for (KeySchemaElement pKeySchema : arrayList) {
        setKeyAttributes(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
        setKeyMethods(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
        map.remove(pKeySchema.getAttributeName());
      }
      setItems(map, 2);
      setDefaultMethods(2, pTableName);
      log.error("Error while compiling table {}",pTableName, e.getMessage());
      throw new RuntimeException(e);

   * @param pItemsThe 
   *          items belonging to the table
   * @param pIdenThe 
   *          number of spaces used for identation
  private void setItems(Map<String, String> pItems, int pIden)
      throws IOException {
    for (String itemName : pItems.keySet()) {
      String itemType = "String";
      if (pItems.get(itemName).toString().equals("N"))
        itemType = "double";
      if (pItems.get(itemName).toString().equals("SS"))
        itemType = "Set<String>";
      if (pItems.get(itemName).toString().equals("SN"))
        itemType = "Set<double>";
      line(pIden, "private "  itemType  " "  itemName  ";");
      setItemMethods(itemName, itemType, pIden);

   * @param pItemNameItem
   *          's name
   * @param pItemTypeItem
   *          's type
   * @param pIdenNumber 
   *          of spaces used for indentation
  private void setItemMethods(String pItemName, String pItemType, int pIden)
      throws IOException {
    line(pIden, "@DynamoDBAttribute(attributeName = \""
         camelCasify(pItemName)  "\")");
    line(pIden, "public "  pItemType  " get"  camelCasify(pItemName)
     "() {  return "  pItemName  ";  }");
    line(pIden, "public void set"  camelCasify(pItemName)  "("  pItemType
         " p"  camelCasify(pItemName)  ") {  this."  pItemName  " = p"
         camelCasify(pItemName)  ";  }");

   * @param pKeySchemaThe 
   *          key schema for a specific table
   * @param pIdenNumber 
   *          of spaces used for indentation
  private void setKeyMethods(KeySchemaElement pKeySchema, String attType,
      int pIden) throws IOException {
    attType = attType.equals("S") ? "String" : "double";
    if (pKeySchema.getKeyType().equals(KeyType.HASH.toString())) {
      strBuilder.append("@DynamoDBHashKey(attributeName=\""
           pKeySchema.getAttributeName()  "\") \n");
      strBuilder.append("    public "  attType  " getHashKey() {  return "
           pKeySchema.getAttributeName()  "; } \n");
      strBuilder.append("    public void setHashKey("  attType  " ");
      strBuilder.append("p"  camelCasify(pKeySchema.getAttributeName())
       "){  this."  pKeySchema.getAttributeName());
      strBuilder.append(" = p"  camelCasify(pKeySchema.getAttributeName())
       "; }");
    if (pKeySchema.getKeyType().equals(KeyType.RANGE.toString())) {
      strBuilder.append("@DynamoDBRangeKey(attributeName=\""
           pKeySchema.getAttributeName()  "\") \n");
      strBuilder.append("    public "  attType  " getRangeKey() { return "
           pKeySchema.getAttributeName()  "; } \n");
      strBuilder.append("    public void setRangeKey("  attType  " ");
      strBuilder.append("p"  camelCasify(pKeySchema.getAttributeName())
       "){  this."  pKeySchema.getAttributeName());
      strBuilder.append(" = p"  camelCasify(pKeySchema.getAttributeName())
       "; }");

   * @param pKeySchema
   *          schema
   * @param attType
   *          attribute type
   * @param pIden
   *          of spaces used for indentation
  private void setKeyAttributes(KeySchemaElement pKeySchema, String attType,
      int pIden) throws IOException {
    attType = attType.equals("S") ? "String " : "double ";
    if (pKeySchema != null) {
      strBuilder.append("private "  attType);
      strBuilder.append(pKeySchema.getAttributeName()  ";");

   * @param sString 
   *          to be camelcasified
    return s.substring(0, 1).toUpperCase(Locale.getDefault())  s.substring(1);
   * @param nameClass 
   *          name
   *          spacing
    String fullDest = FilenameUtils.normalize
        (dest.getAbsolutePath()  File.separatorChar  packageName.replace('.', File.separatorChar));
    File dir = new File(fullDest);
    out = new OutputStreamWriter(new FileOutputStream(new File(dir, name)), Charset.defaultCharset());
    if (namespace != null) {
      line(0, "package "  namespace  ";\n");
    line(0, "import java.util.List;");
    line(0, "");
    line(0, "import org.apache.avro.Schema.Field;");
    line(0, "import org.apache.gora.persistency.Tombstone;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBAttribute;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBHashKey;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey;");
    line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTable;");
   * @param pIden 
   *          of spaces used for indentation
   * @param tabName
   *          table name
  private void setDefaultMethods(int pIden, String tabName) throws IOException {
    line(pIden, "public "  tabName  " clone() { return null; }");
    line(pIden, "public Tombstone getTombstone() { return null; }");
    line(pIden, "public List<Field> getUnmanagedFields() { return null; }");
    line(pIden, "public Persistent newInstance() { return new "  tabName  "(); }");
   * @param indentNumber 
   *          of spaces used for indentation
   * @param textText 
   *          to be written
    for (int i = 0; i < indent; i) {
   * @param name 
   *          to be converted
    return name.substring(0,1).toUpperCase(Locale.getDefault())
         name.substring(1,name.length());
   * @param argsReceives 
   *          the schema file to be compiled and where this should be written
  public static void main(String[] args) {
    try {
      if (args.length < 2) {
        log.error("Usage: Compiler <schema file> <output dir>");
        System.exit(1);
      }
      compileSchema(new File(args[0]), new File(args[1]));
    } catch (Exception e) {
      log.error("Something went wrong. Please check the input file.", e.getMessage());
      throw new RuntimeException(e);
   * @param pMapFile
   *          schema file to be mapped into a table

      if (doc == null || doc.getRootElement() == null)
        throw new GoraException("Unable to load "  MAPPING_FILE
             ". Please check its existance!");

      boolean keys = false;
      for (Element tableElement : tableElements) {

        String tableName = tableElement.getAttributeValue("name");
        long readCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("readcunit"));
        long writeCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("writecunit"));
        this.packageName = tableElement.getAttributeValue("package");

        mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
            writeCapacUnits);
        log.debug("Table properties have been set for name, package and provisioned throughput.");

        // Retrieving attributes
        List<Element> fieldElements = tableElement.getChildren("attribute");
        for (Element fieldElement : fieldElements) {
          String key = fieldElement.getAttributeValue("key");
          String attributeName = fieldElement.getAttributeValue("name");
          mappingBuilder.addAttribute(tableName, attributeName, attributeType);
          // Retrieving key's features
          if (key != null) {
            mappingBuilder.setKeySchema(tableName, attributeName, key);
            keys = true;
          }
        log.debug("Attributes for table '{}' have been read.", tableName);
        if (!keys)
          log.warn("Keys for table '{}' have NOT been set.", tableName);
      log.error("Error while performing xml mapping.", ex.getMessage());
      throw new RuntimeException(ex);
      log.error("An error occured whilst reading the xml mapping file!", ex.getMessage());
/**
 * Class abstracting a composed DynamoDB key.
 * @param <H>
 * @param <R>
 */
public class DynamoDBKey<H, R> {  






  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append('[').append(hashKey != null? hashKey.toString():":");
    sb.append(rangeKey != null? ":"  rangeKey.toString():"");
    sb.append(']');
    return sb.toString();
  }
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;
import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBScanExpression;
import com.amazonaws.services.dynamodbv2.model.AttributeValue;
import com.amazonaws.services.dynamodbv2.model.ComparisonOperator;
import com.amazonaws.services.dynamodbv2.model.Condition;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
  public static final ComparisonOperator DEFAULT_SCAN_OP = ComparisonOperator.GE;

  private ArrayList<KeySchemaElement> keySchema;
  private Map<String, String> keyItems;

    super(null);

   * 
    super(dataStore);
  private void defineQueryParams() {
    if ((query.getStartKey() != null || query.getKey() != null)
        && query.getEndKey() != null) {
      DynamoDBQuery.setType(RANGE_QUERY);
    } else if (query.getKey() != null || query.getStartKey() != null) {
      DynamoDBQuery.setType(SCAN_QUERY);
    }
  }

  public void buildExpression() {
    defineQueryParams();
    if (DynamoDBQuery.getType().equals(RANGE_QUERY)) {
      buildRangeExpression();
    } else if (DynamoDBQuery.getType().equals(SCAN_QUERY)) {
      buildScanExpression();
    } else {
      throw new IllegalArgumentException("Query type not supported");
  }

  /**
   * Builds hash key attribute from generic query received.
   * 
   * @param qKey
   * 
   * @returnAttributeValue build from query
   */
  private Map<String, AttributeValue> buildHashKey(K qKey) {
    Map<String, AttributeValue> hashKey = new HashMap<>();
    for (KeySchemaElement key : getKeySchema()) {
      AttributeValue attr = new AttributeValue();
      if (key.getKeyType().equals(KeyType.HASH.toString())) {
        if (keyItems.get(key.getAttributeName()).equals("N")) {
          attr.withN(getHashKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("S")) {
          attr.withS(getHashKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("B")) {
          attr.withB(ByteBuffer.wrap(getHashKey(qKey).toString().getBytes(Charset.defaultCharset())));
        } else {
          throw new IllegalArgumentException("Data type not supported for "
               key.getAttributeName());
        }
        hashKey.put(key.getAttributeName(), attr);
      }
    }
    if (hashKey.isEmpty()) {
      throw new IllegalStateException("No key value has been defined.");
    }
    return hashKey;
  }

  /**
   * Builds range key attribute from generic query received.
   * 
   * @param qKey
   * 
   * @return
   */
  private Map<String, AttributeValue> buildRangeKey(K qKey) {
    Map<String, AttributeValue> kAttrs = new HashMap<>();
    for (KeySchemaElement key : getKeySchema()) {
      AttributeValue attr = new AttributeValue();
      if (key.getKeyType().equals(KeyType.RANGE.toString())) {
        if (keyItems.get(key.getAttributeName()).equals("N")) {
          attr.withN(getRangeKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("S")) {
          attr.withS(getRangeKey(qKey).toString());
        } else if (keyItems.get(key.getAttributeName()).equals("B")) {
          attr.withB(ByteBuffer.wrap(getRangeKey(qKey).toString().getBytes(Charset.defaultCharset())));
        } else {
          throw new IllegalArgumentException("Data type not supported for "
               key.getAttributeName());
        }
        kAttrs.put(key.getAttributeName(), attr);
      }
    }
    return kAttrs;
   * 
   * @param pHashAttrValueHash
   *          attribute value where to start scanning
  public void buildScanExpression() {
    K qKey = getKey();
    if (qKey == null) {
      LOG.warn("No key defined. Trying with startKey.");
      qKey = query.getStartKey();
      if (qKey == null) {
        throw new IllegalStateException("No key has been defined please check");
      }
    }
    ComparisonOperator compOp = getScanCompOp() != null ? getScanCompOp()
        : DEFAULT_SCAN_OP;

    // hash key condition
    Map<String, AttributeValue> hashAttrVals = buildHashKey(qKey);
    for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
      Condition scanFilterHashCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue());
      newScanExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
    }
    // range key condition
    Map<String, AttributeValue> rangeAttrVals = buildRangeKey(qKey);
    for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
      Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue());
      newScanExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
    }
   * 
  public void buildRangeExpression() {
    DynamoDBScanExpression queryExpression = new DynamoDBScanExpression();
    ComparisonOperator compOp = ComparisonOperator.BETWEEN;
    // hash key range
    Map<String, AttributeValue> hashAttrVals = buildHashKey(query.getStartKey());
    Map<String, AttributeValue> endHashAttrVals = buildHashKey(query.getEndKey());
    for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
      Condition scanFilterHashCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue(), endHashAttrVals.get(en.getKey()));
      queryExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
    }
    // range key range
    Map<String, AttributeValue> rangeAttrVals = buildRangeKey(query.getStartKey());
    Map<String, AttributeValue> endRangeAttrVals = buildRangeKey(query.getEndKey());
    for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
      Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
          compOp.toString()).withAttributeValueList(en.getValue(), endRangeAttrVals.get(en.getKey()));
      queryExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
    }
    dynamoDBExpression = queryExpression;
   * 
      // Our key may be have hash and range keys
      for (Method met : key.getClass().getDeclaredMethods()) {
        if (met.getName().equals("getHashKey")) {
          Object[] params = null;
          hashKey = met.invoke(key, params);
          break;
        }
    } catch (IllegalArgumentException e) {
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new IllegalArgumentException(e);
    } catch (IllegalAccessException e) {
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
    } catch (InvocationTargetException e) {
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
    return hashKey;
   * 
      // Our key may be have hash and range keys
        if(met.getName().equals("getRangeKey")){
          Object [] params = null;
          rangeKey = met.invoke(key, params);
          break;
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new IllegalArgumentException(e);
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
      LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
      throw new RuntimeException(e);
   * 
   * 
   * 
  public ArrayList<KeySchemaElement> getKeySchema(){
   * 
   * 
   * @param arrayList
  public void setKeySchema(ArrayList<KeySchemaElement> arrayList) {
    this.keySchema = arrayList;
   * 
    this.setStartKey(query.getStartKey());
    this.setEndKey(query.getEndKey());
   * 
   * 
   * 
   * 
   * 
   * 
  /**
   * Sets the keyItems that could be used.
   * 
   * @param items
   */
  public void setKeyItems(Map<String, String> items) {
    keyItems = items;
  }



import static org.apache.gora.dynamodb.store.DynamoDBUtils.DYNAMO_KEY_HASHRANGE;

import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;


  private final Map<String, Map<String, String>> tablesToItems;

  private final Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas;


   * Constructor for DynamoDBMapping
   * 
   * @param tablesToItems2
   *          Tables mapped.
   * @param tablesToKeySchemas
   *          KeySchemas used within tables mapped.
   * @param provisionedThroughput
   *          Provisioned throughput used within tables mapped.
  public DynamoDBMapping(Map<String, Map<String, String>> tablesToItems2,
      Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas,
      Map<String, ProvisionedThroughput> provisionedThroughput) {

    this.tablesToItems = tablesToItems2;
   * 
   * @return tablesToItem 
   *          HashMap 
  public Map<String, Map<String, String>> getTables() {

   * 
   * @param tableName
   *          table name to determine which attributes to get 
  public Map<String, String> getItems(String tableName) {
   * @param tableName
   *          Table name to determine which key schema to get
  public ArrayList<KeySchemaElement> getKeySchema(String tableName) {

   * 
   * @param tableName
   *          Table name to determine which provisioned throughput to get

    private Map<String, Map<String, String>> tablesToItems = 
        new HashMap<String, Map<String, String>>();
    private Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas = 
        new HashMap<String, ArrayList<KeySchemaElement>>();
    private Map<String, ProvisionedThroughput> tablesToPrTh = 
        new HashMap<String, ProvisionedThroughput>();

     * 

     * 
    public void setProvisionedThroughput(String tableName, long readCapUnits,
        long writeCapUnits) {
      ProvisionedThroughput ptDesc = new ProvisionedThroughput()
          .withReadCapacityUnits(readCapUnits).withWriteCapacityUnits(
              writeCapUnits);

     * 
    // public void setHashRangeKeySchema(String tableName, String rangeKeyName,
    // String rangeKeyType){
    // KeySchemaElement kSchema = tablesToKeySchemas.get(tableName);
    // if ( kSchema == null)
    // kSchema = new KeySchemaElement();

    // KeySchemaElement rangeKeyElement = new
    // KeySchemaElement().withAttributeName(rangeKeyName).withKeyType(KeyType.RANGE).withKeyType(rangeKeyType);
    // kSchema.
    // kSchema.setRangeKeyElement(rangeKeyElement);
    // tablesToKeySchemas.put(tableName, kSchema);
    // }

     * @param keyType2
    public void setKeySchema(String tableName, String keyName, String keyType) {
      ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
      if (kSchema == null) {
        kSchema = new ArrayList<KeySchemaElement>();
      }
      KeyType type = keyType.equals(DYNAMO_KEY_HASHRANGE) ? KeyType.RANGE : KeyType.HASH;
      kSchema.add(new KeySchemaElement().withAttributeName(keyName)
          .withKeyType(type));

     * Checks if a table exists, and if doesn't exist it creates the new table.
     * 
    private Map<String, String> getOrCreateTable(String tableName) {
      Map<String, String> items = tablesToItems.get(tableName);
        items = new HashMap<String, String>();

     * Gets the attribute for a specific item. The idea is to be able to get 
     * different items with different attributes.
     * TODO This method is incomplete because the itemNumber might not 
     * be present and this would be a problem
     * 
    /*private HashMap<String, String> getOrCreateItemAttribs(
        Map<String, String> items) {


      if (itemAttribs == null) {
        itemAttribs = new HashMap<String, String>();
      }

      items.add(itemAttribs);
      return null;
    }*/

     * 
    public void addAttribute(String tableName, String attributeName,
        String attrType) {
      // selecting table
      Map<String, String> items = getOrCreateTable(tableName);
      // add attribute to item
      //HashMap<String, String> itemAttribs = getOrCreateItemAttribs(items);
      //itemAttribs.put(attributeName, attrType);
      // add item to table
      items.put(attributeName, attrType);
      // tablesToItems.put(tableName, items);
    }

     * 
    private boolean verifyAllKeySchemas() {
      boolean rsl = true;
      if (tablesToItems.isEmpty() || tablesToKeySchemas.isEmpty())
        rsl = false;
      for (String tableName : tablesToItems.keySet()) {
        // if there are not schemas defined
        if (tablesToKeySchemas.get(tableName) == null) {
          LOG.error("No schema defined for DynamoDB table '"  tableName  '\'');
          rsl = false;
        rsl = verifyKeySchema(tableName);
      }
      return rsl;

     * 
    private boolean verifyKeySchema(String tableName) {
      ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
      boolean hashPk = false;
      if (kSchema == null) {
        LOG.error("No keys defined for '{}'. Please check your schema!", tableName);
        return hashPk;
      for (KeySchemaElement ks : kSchema) {
        if (ks.getKeyType().equals(KeyType.HASH.toString())) {
          hashPk = true;
        }
      }
      return hashPk;

     * 
      // verifying items for at least a table
      if (tablesToItems.isEmpty())
        throw new IllegalStateException("No tables were defined.");

      // verifying if key schemas have been properly defined
      if (!verifyAllKeySchemas())
        throw new IllegalStateException("no key schemas defined for table ");

      // Return the tableDescription and all the attributes needed
      return new DynamoDBMapping(tablesToItems, tablesToKeySchemas,
          tablesToPrTh);
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CLI_TYP_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS_TRUE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.ENDPOINT_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.PREF_SCH_NAME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SERIALIZATION_TYPE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SLEEP_DELETE_TIME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.WAIT_TIME;

import java.util.Map;
import org.apache.gora.store.DataStore;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
import com.amazonaws.services.dynamodbv2.model.DeleteTableRequest;
import com.amazonaws.services.dynamodbv2.model.DeleteTableResult;
import com.amazonaws.services.dynamodbv2.model.DescribeTableRequest;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
import com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException;
import com.amazonaws.services.dynamodbv2.model.TableDescription;
/**
 * Class for using DynamoDBStores
 * 
 * @param <K>
 * @param <T>
 */
public class DynamoDBStore<K, T extends Persistent> implements DataStore<K, T> {
  /** Handler for different serialization modes. */
  private IDynamoDB<K, T> dynamoDbStore;

  /** Helper to write useful information into the logs. */

   * Amazon DynamoDB client which can be asynchronous or not


  /** Specifies how the objects will be serialized inside DynamoDb. */
  private DynamoDBUtils.DynamoDBType serializationType;
   * Schema name which will be used from within the data store. If not set, all
   * the available schemas from the mapping file will be used.
  private String preferredSchema;

  @Override
  public void close() {
    dynamoDbStore.close();
   * Creates the table within the data store for a preferred schema or for a
   * group of schemas defined within the mapping file
   * @throws IOException
  @Override
  public void createSchema() {
    dynamoDbStore.createSchema();
  }

  @Override
  public boolean delete(K key) {
    return dynamoDbStore.delete(key);
  }

  @Override
  public long deleteByQuery(Query<K, T> query) {
    return dynamoDbStore.deleteByQuery(query);
  }

  @Override
  public void deleteSchema() {
    if (getDynamoDbMapping().getTables().isEmpty())
      throw new IllegalStateException("There are not tables defined.");
    if (preferredSchema == null) {
      LOG.debug("Delete schemas");
      if (getDynamoDbMapping().getTables().isEmpty())
        throw new IllegalStateException("There are not tables defined.");
      // read the mapping object
      for (String tableName : getDynamoDbMapping().getTables().keySet())
        executeDeleteTableRequest(tableName);
      LOG.debug("All schemas deleted successfully.");
    } else {
      LOG.debug("create schema "  preferredSchema);
      executeDeleteTableRequest(preferredSchema);

  @Override
  public Result<K, T> execute(Query<K, T> query) {
    return dynamoDbStore.execute(query);
  }

  @Override
  public void flush() {
    dynamoDbStore.flush();
  }

  @Override
  public T get(K key) {
    return dynamoDbStore.get(key);
  }

  @Override
  public T get(K key, String[] fields) {
    return dynamoDbStore.get(key, fields);
  }

  @Override
  public BeanFactory<K, T> getBeanFactory() {
    // TODO Auto-generated method stub

  @Override
  public Class<K> getKeyClass() {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> arg0)
      throws IOException {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public Class<T> getPersistentClass() {
    // TODO Auto-generated method stub
    return null;
  }

  @Override
  public String getSchemaName() {
    return this.getPreferredSchema();
  }

  @Override
  public void initialize(Class<K> keyClass, Class<T> persistentClass,
      Properties properties) {
    try {
      LOG.debug("Initializing DynamoDB store");
      setDynamoDBProperties(properties);

      dynamoDbStore = DynamoDBFactory.buildDynamoDBStore(getSerializationType());
      dynamoDbStore.setDynamoDBStoreHandler(this);
      dynamoDbStore.initialize(keyClass, persistentClass, properties);
    } catch (Exception e) {
      LOG.error("Error while initializing DynamoDB store", e.getMessage());
      throw new RuntimeException(e);
    }
  }

  private void setDynamoDBProperties(Properties properties) throws IOException {
    setSerializationType(properties.getProperty(SERIALIZATION_TYPE));
    PropertiesCredentials creds = DynamoDBUtils.getCredentials(this.getClass());
    setPreferredSchema(properties.getProperty(PREF_SCH_NAME));
    setDynamoDBClient(DynamoDBUtils.getClient(
        properties.getProperty(CLI_TYP_PROP), creds));
    getDynamoDBClient().setEndpoint(properties.getProperty(ENDPOINT_PROP));
    setDynamoDbMapping(readMapping());
    setConsistency(properties.getProperty(CONSISTENCY_READS));
  }

  @Override
  public K newKey() {
    return dynamoDbStore.newKey();
  }

  @Override
  public T newPersistent() {
    return dynamoDbStore.newPersistent();
  }

  @Override
  public Query<K, T> newQuery() {
    return dynamoDbStore.newQuery();
  }

  @Override
  public void put(K key, T value) {
    dynamoDbStore.put(key, value);
  }



   * Verifies if the specified schemas exist
   * 
   * @throws IOException
   */
  @Override
  public boolean schemaExists() {
    LOG.info("Verifying schemas.");
    TableDescription success = null;
    if (getDynamoDbMapping().getTables().isEmpty())
      throw new IllegalStateException("There are not tables defined.");
    if (getPreferredSchema() == null) {
      LOG.debug("Verifying schemas");
      if (getDynamoDbMapping().getTables().isEmpty())
        throw new IllegalStateException("There are not tables defined.");
      // read the mapping object
      for (String tableName : getDynamoDbMapping().getTables().keySet()) {
        success = getTableSchema(tableName);
        if (success == null)
          return false;
      }
    } else {
      LOG.info("Verifying schema "  preferredSchema);
      success = getTableSchema(preferredSchema);
    }
    LOG.info("Finished verifying schemas.");
    return (success != null) ? true : false;
  }

  @Override
  public void setBeanFactory(BeanFactory<K, T> arg0) {
    // TODO Auto-generated method stub
  }

  @Override
  public void setKeyClass(Class<K> arg0) {
    dynamoDbStore.setKeyClass(arg0);
  }

  @Override
  public void setPersistentClass(Class<T> arg0) {
    dynamoDbStore.setPersistentClass(arg0);
  }

  @Override
  public void truncateSchema() {
    // TODO Auto-generated method stub
  }

  /** 
   * 
   * @param pMapFile
   *          The schema file to be mapped into a table
   * @return DynamoDBMapping Object containing all necessary information to
   *         create tables
      Document doc = builder.build(getClass().getClassLoader()
          .getResourceAsStream(MAPPING_FILE));
      if (doc == null || doc.getRootElement() == null)
        throw new GoraException("Unable to load "  MAPPING_FILE
             ". Please check its existance!");

      boolean keys = false;
      for (Element tableElement : tableElements) {

        long readCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("readcunit"));
        long writeCapacUnits = Long.parseLong(tableElement
            .getAttributeValue("writecunit"));

        mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
            writeCapacUnits);

        List<Element> fieldElements = tableElement.getChildren("attribute");
        for (Element fieldElement : fieldElements) {
          String key = fieldElement.getAttributeValue("key");
          String attributeName = fieldElement.getAttributeValue("name");
          mappingBuilder.addAttribute(tableName, attributeName, attributeType);
          // Retrieving key's features
          if (key != null) {
            mappingBuilder.setKeySchema(tableName, attributeName, key);
            keys = true;
          }
        LOG.debug("Attributes for table '"  tableName  "' have been read.");
        if (!keys)
          LOG.warn("Keys for table '"  tableName  "' have NOT been set.");
    } catch (IOException ex) {
      LOG.error("Error while performing xml mapping.", ex.getMessage());
    } catch (Exception ex) {
      LOG.error("Error while performing xml mapping.", ex.getMessage());
      throw new RuntimeException(ex);
   * 
  public void executeDeleteTableRequest(String pTableName) {
    try {
      DeleteTableRequest deleteTableRequest = new DeleteTableRequest()
          .withTableName(pTableName);
      DeleteTableResult result = getDynamoDBClient().deleteTable(
          deleteTableRequest);
      LOG.debug("Schema: "  result.getTableDescription()
       " deleted successfully.");
    } catch (Exception e) {
      LOG.debug("Schema: {} deleted.", pTableName, e.getMessage());
      throw new RuntimeException(e);


   * 
  private void waitForTableToBeDeleted(String pTableName) {
    long endTime = startTime  WAIT_TIME;
        Thread.sleep(SLEEP_DELETE_TIME);
      } catch (Exception e) {
      }
      try {
        DescribeTableRequest request = new DescribeTableRequest()
            .withTableName(pTableName);
        TableDescription tableDescription = getDynamoDBClient().describeTable(
            request).getTable();
        LOG.error(ase.getMessage());
   * 
  private TableDescription getTableSchema(String tableName) {
    try {
      DescribeTableRequest describeTableRequest = new DescribeTableRequest()
          .withTableName(tableName);
      tableDescription = getDynamoDBClient()
          .describeTable(describeTableRequest).getTable();
    } catch (ResourceNotFoundException e) {

   * Gets a specific table key schema.
   * 
   * @param tableName
   *          from which key schema is to be obtained.
   * @return KeySchema from table.
  public ArrayList<KeySchemaElement> getTableKeySchema(String tableName) {
    return getDynamoDbMapping().getKeySchema(tableName);
   * Gets the provisioned throughput for a specific table.
   * 
   * @param tableName
   *          to get the ProvisionedThroughput.
   * @return ProvisionedThroughput for a specific table
  public ProvisionedThroughput getTableProvisionedThroughput(String tableName) {
    return getDynamoDbMapping().getProvisionedThroughput(tableName);
   * Returns a table attribues.
   * @param tableName
  public Map<String, String> getTableAttributes(String tableName) {
    return getDynamoDbMapping().getItems(tableName);
   * Gets consistency level for reads
   * 
   * @return True for strong consistency or false for eventual consistent reads
  public boolean getConsistencyReads() {
    if (getConsistency() != null)
      if (getConsistency().equals(CONSISTENCY_READS_TRUE))
        return true;
    return false;
  }


  /**
   * Set DynamoDBStore to be used.
   * 
   * @param iDynamoDB
   */
  public void setDynamoDbStore(IDynamoDB<K, T> iDynamoDB) {
    this.dynamoDbStore = iDynamoDB;
  }

  /**
   * @param serializationType
   *          the serializationType to set
   */
  private void setSerializationType(String serializationType) {
    if (serializationType == null || serializationType.isEmpty()
        || serializationType.equals(DynamoDBUtils.AVRO_SERIALIZATION)) {
      LOG.warn("Using AVRO serialization.");
      this.serializationType = DynamoDBUtils.DynamoDBType.AVRO;
    } else {
      LOG.warn("Using DynamoDB serialization.");
      this.serializationType = DynamoDBUtils.DynamoDBType.DYNAMO;
    }
  }

  /**
   * Gets serialization type used inside DynamoDB module.
   * 
   * @return
   */
  private DynamoDBUtils.DynamoDBType getSerializationType() {
    return serializationType;
  }

  /**
   * @return the preferredSchema
   */
  public String getPreferredSchema() {
    return preferredSchema;
  }

  /**
   * @param preferredSchema
   *          the preferredSchema to set
   */
  public void setPreferredSchema(String preferredSchema) {
    this.preferredSchema = preferredSchema;
  }

  /**
   * Gets DynamoDBClient.
   * 
   * @return
   */
  public AmazonDynamoDB getDynamoDbClient() {
    return getDynamoDBClient();
  }


  /**
   * @return the mapping
   */
  public DynamoDBMapping getDynamoDbMapping() {
    return mapping;
  }

  /**
   * @param mapping
   *          the mapping to set
   */
  public void setDynamoDbMapping(DynamoDBMapping mapping) {
    this.mapping = mapping;
  }

  /**
   * @return the consistency
   */
  public String getConsistency() {
    return consistency;
  }

  /**
   * @param consistency
   *          the consistency to set
   */
  public void setConsistency(String consistency) {
    this.consistency = consistency;
  }

  /**
   * @return the dynamoDBClient
   */
  public AmazonDynamoDB getDynamoDBClient() {
    return dynamoDBClient;
  }

  /**
   * @param dynamoDBClient
   *          the dynamoDBClient to set
   */
  public void setDynamoDBClient(AmazonDynamoDB dynamoDBClient) {
    this.dynamoDBClient = dynamoDBClient;
    PersistentBase record = (PersistentBase) new BeanFactoryImpl(keyClass, clazz).newPersistent();
    try {
      Result<K, T> result = query.execute();
      String[] fields = getFieldsToQuery(query.getFields());
      boolean isAllFields = Arrays.equals(fields, getFields());

      while (result.next()) {
        if (isAllFields) {
          if (delete(result.getKey())) {
            deletedRows;
            continue;
          }
        }
        for (String field : fields) {
          result.get().clearField(field);
        }
        deletedRows;
* Clears the inner state of the object based on field without any modification to the actual
* data on the data store. This method should be called before re-using the existing fields on
* object to hold the data for another result.
*/
  void clearField(String Field);

  /**
  public void clearField(String field) {
    Collection<Field> unmanagedFields = getUnmanagedFields();
    Field specificField = getSchema().getField(field);
    if (unmanagedFields.contains(specificField)) {
      put(specificField.pos(), PersistentData.get().deepCopy(specificField.schema(),
              PersistentData.get().getDefaultValue(specificField)));
    }
    clearDirynessIfFieldIsDirtyable(specificField.pos());
  }

  @Override
    public void clearField(String Field) { }
    @Override
    public void clearField(String Field) { }
    @Override
    line(pIden, "public void clearField(String Field) { }");
    line(pIden, "@Override");
import java.util.*;
          }
        } else {
          ArrayList<String> excludedFields = new ArrayList<>();
          for (String field : getFields()){
            if (!Arrays.asList(fields).contains(field)){
              excludedFields.add(field);
            }
          }
          T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
          if (delete(result.getKey())) {
            put(result.getKey(),newClonedObj);
            deletedRows;
    newObj.clear();
    for (String field : fields) {
      Field otherField = obj.getSchema().getField(field);
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;
import java.util.NavigableMap;
          for (String field : getFields()) {
            if (!Arrays.asList(fields).contains(field)) {
  @SuppressWarnings("unchecked")
  @SuppressWarnings("unused")
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]}");
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":null},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]},\"default\":null},{\"name\":\"byteData\",\"type\":{\"type\":\"map\",\"values\":\"bytes\"},\"default\":{}},{\"name\":\"stringData\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}],\"default\":null}");
    BYTE_DATA(6, "byteData"),
    STRING_DATA(7, "stringData"),
  "byteData",
  "stringData",
  private java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> byteData;
  private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> stringData;
    case 6: return byteData;
    case 7: return stringData;
    case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
    case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
  /**
   * Gets the value of the 'byteData' field.
   */
  public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
    return byteData;
  }

  /**
   * Sets the value of the 'byteData' field.
   * @param value the value to set.
   */
  public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
    this.byteData = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(6);
  }
  
  /**
   * Checks the dirty status of the 'byteData' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isByteDataDirty() {
    return isDirty(6);
  }

  /**
   * Gets the value of the 'stringData' field.
   */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
    return stringData;
  }

  /**
   * Sets the value of the 'stringData' field.
   * @param value the value to set.
   */
  public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
    this.stringData = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
    setDirty(7);
  }
  
  /**
   * Checks the dirty status of the 'stringData' field. A field is dirty if it represents a change that has not yet been written to the database.
   * @param value the value to set.
   */
  public boolean isStringDataDirty() {
    return isDirty(7);
  }

    private java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> byteData;
    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> stringData;
      if (isValidValue(fields()[6], other.byteData)) {
        this.byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>) data().deepCopy(fields()[6].schema(), other.byteData);
        fieldSetFlags()[6] = true;
      }
      if (isValidValue(fields()[7], other.stringData)) {
        this.stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[7].schema(), other.stringData);
        fieldSetFlags()[7] = true;
      }
    /** Gets the value of the 'byteData' field */
    public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
      return byteData;
    }
    
    /** Sets the value of the 'byteData' field */
    public org.apache.gora.examples.generated.WebPage.Builder setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
      validate(fields()[6], value);
      this.byteData = value;
      fieldSetFlags()[6] = true;
      return this; 
    }
    
    /** Checks whether the 'byteData' field has been set */
    public boolean hasByteData() {
      return fieldSetFlags()[6];
    }
    
    /** Clears the value of the 'byteData' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearByteData() {
      byteData = null;
      fieldSetFlags()[6] = false;
      return this;
    }
    
    /** Gets the value of the 'stringData' field */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
      return stringData;
    }
    
    /** Sets the value of the 'stringData' field */
    public org.apache.gora.examples.generated.WebPage.Builder setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      validate(fields()[7], value);
      this.stringData = value;
      fieldSetFlags()[7] = true;
      return this; 
    }
    
    /** Checks whether the 'stringData' field has been set */
    public boolean hasStringData() {
      return fieldSetFlags()[7];
    }
    
    /** Clears the value of the 'stringData' field */
    public org.apache.gora.examples.generated.WebPage.Builder clearStringData() {
      stringData = null;
      fieldSetFlags()[7] = false;
      return this;
    }
    
        record.byteData = fieldSetFlags()[6] ? this.byteData : (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[6]));
        record.stringData = fieldSetFlags()[7] ? this.stringData : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[7]));
				  /**
	   * Gets the value of the 'byteData' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'byteData' field.
		   * @param value the value to set.
	   */
	  public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'byteData' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isByteDataDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
				  /**
	   * Gets the value of the 'stringData' field.
		   */
	  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
	    throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
	  }
	
	  /**
	   * Sets the value of the 'stringData' field.
		   * @param value the value to set.
	   */
	  public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
	    throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
	  }
	  
	  /**
	   * Checks the dirty status of the 'stringData' field. A field is dirty if it represents a change that has not yet been written to the database.
		   * @param value the value to set.
	   */
	  public boolean isStringDataDirty() {
	    throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
	  }
	
  public static final ConcurrentHashMap<Schema, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<>();
  public static final ConcurrentHashMap<Schema, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<>();
  private SpecificDatumReader getDatumReader(Schema fieldSchema) {
    SpecificDatumReader<?> reader = readerMap.get(fieldSchema);
      if ((localReader = readerMap.putIfAbsent(fieldSchema, reader)) != null) {
  private SpecificDatumWriter getDatumWriter(Schema fieldSchema) {
    SpecificDatumWriter writer = writerMap.get(fieldSchema);
      writerMap.put(fieldSchema, writer);
      SpecificDatumReader reader = getDatumReader(fieldSchema);
        SpecificDatumReader unionReader = getDatumReader(fieldSchema);
        SpecificDatumWriter writer = getDatumWriter(fieldSchema);
          SpecificDatumWriter writer = getDatumWriter(fieldSchema);

  /** Fetches a single pageview object with required fields and prints it*/
  private void get(long key, String[] fields) throws Exception {
    Pageview pageview = dataStore.get(key, fields);
    printPageview(pageview);
  }
                                      "           -get <lineNum> <fieldList>\n" 
  		                              "           -delete <lineNum>\n" 
  		                              "           -deleteByQuery <startLineNum> <endLineNum>\n";
      if(args.length == 2) {
        manager.get(Long.parseLong(args[1]));
      } else {
        //field array should be input as comma ',' separated
        String[] fields = args[2].split(",");
        manager.get(Long.parseLong(args[1]), fields);
      }
 * {@link org.apache.gora.cassandra.store.CassandraStore#initialize} .

   * Given our key, persistentClass from
   * we make best efforts to dictate our data model.
   * We make a quick check within {@link org.apache.gora.cassandra.store.CassandraClient#checkKeyspace() }
   * to see if our keyspace has already been invented, this simple check prevents us from
   * recreating the keyspace if it already exists.
   *
   * @param keyClass        the Key by which we wish o assign a record object
   * @param persistentClass the generated {@link org.apache.gora.persistency.Persistent} bean representing the data.
   * @param properties      key value pairs from gora.properties

   * In this method, we also utilize Hector's
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel} logic.
   * It is set by passing a
   * {@link me.prettyprint.cassandra.model.ConfigurableConsistencyLevel} object right
   * when the {@link me.prettyprint.hector.api.Keyspace} is created.
   * If we cannot find a consistency level within <code>gora.properites</code>,
   * then column family consistency level is set to QUORUM (by default) which permits
   *
   * @see <a href="http://hector-client.github.io/hector/build/html/content/consistency_level.html">Consistency Level</a>
   *
   * @param familyName
 * {@link org.apache.gora.cassandra.store.CassandraStore} is the primary class
 * responsible for directing Gora CRUD operations into Cassandra. We (delegate) rely
 * such as initialization, creating and deleting schemas (Cassandra Keyspaces), etc.
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore}
   * {@link org.apache.gora.cassandra.store.CassandraClient#initialize}.
   *
   * @param keyClass
   * @param persistent
   * @param properties
   * When doing the
   * <li>Obtain a {@link java.util.List} of the {@link org.apache.avro.Schema}
   * <li>Iterate through the field {@link java.util.List}. This allows us to
   * <li>Check to see if the {@link org.apache.avro.Schema.Field} is NOT dirty.
   * <li>Obtain the element at the specified position in this list so we can
   * <li>Obtain the {@link org.apache.avro.Schema.Type} of the element obtained
   * RECORD or UNION, we shadow the checks in bullet point 5 above to infer that the
   * {@link org.apache.avro.Schema.Field} is either at
   * process this field. This is carried out in
   * <li>We then insert the Key and Object into the {@link java.util.LinkedHashMap} buffer
   * objects into a synchronized {@link java.util.LinkedHashMap}. This allows
   *
   * @param key   for the Avro Record (object).
   * @see org.apache.gora.store.DataStore#put(java.lang.Object,org.apache.gora.persistency.Persistent)
 * @see CassandraClient for safe usage.
  /**
   * Recognizes camel case
   *
   * @param s converts the given input string to camel case
   * @return the converted camel case string
   */
  /**
   * Construct given a path and a configuration.
   *
   * @param path a path on HDFS to construct
   * @param conf a Hadoop {@link org.apache.hadoop.conf.Configuration} object
   * @throws IOException if there is an error opening path, or obtaining a file status from the file system
   */
 *

   *
   * @return <code>true</code> if the row is filtered out (excluded),
 * for advanced filtering within the construction and
 *
 * Hadoop jobs can be either configured through static
 *
   *
   * @param job          the job to set the properties for
   * @param query        the query to get the inputs from
   * @param <K1>
   * @param <V1>

   *
   * @param job            the job to set the properties for
   * @param inKeyClass     Map input key class
   * @param inValueClass   Map input value class
   * @param reuseObjects   whether to reuse objects in serialization
   * @param <K1>
   * @param <V1>
   * Initializes the Mapper, and sets input parameters for the job. All of
   * the records in the dataStore are used as the input. If you want to
   *
   * @param job              the job to set the properties for
   * @param dataStoreClass   the datastore class
   * @param inKeyClass       Map input key class
   * @param inValueClass     Map input value class
   * @param outKeyClass      Map output key class
   * @param outValueClass    Map output value class
   * @param mapperClass      the mapper class extending GoraMapper
   * @param reuseObjects     whether to reuse objects in serialization
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>
   * @throws IOException

   * Initializes the Mapper, and sets input parameters for the job. All of
   * the records in the dataStore are used as the input. If you want to
   *
   * @param job            the job to set the properties for
   * @param inKeyClass     Map input key class
   * @param inValueClass   Map input value class
   * @param outKeyClass    Map output key class
   * @param outValueClass  Map output value class
   * @param mapperClass    the mapper class extending GoraMapper
   * @param reuseObjects   whether to reuse objects in serialization
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>
   * @throws IOException
   */

   *
   * @param job              the job to set the properties for
   * @param query            the query to get the inputs from
   * @param outKeyClass      Map output key class
   * @param outValueClass    Map output value class
   * @param mapperClass      the mapper class extending GoraMapper
   * @param reuseObjects     whether to reuse objects in serialization
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>
   * @throws IOException
   *
   * @param job           the job to set the properties for
   * @param dataStore     the datastore as the input
   * @param outKeyClass   Map output key class
   * @param mapperClass   the mapper class extending GoraMapper
   * @param reuseObjects  whether to reuse objects in serialization
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>
   * @throws IOException

   *
   * @param job           the job to set the properties for
   * @param query         the query to get the inputs from
   * @param outKeyClass   Map output key class
   * @param mapperClass   the mapper class extending GoraMapper
   * @param reuseObjects  whether to reuse objects in serialization
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>
   * @throws IOException
   *
   * @param job          the job to set the properties for
   * @param dataStore    the datastore as the output
   * @param <K>
   * @param <V>
   * Sets the output parameters for the job
   *
   * @param job             the job to set the properties for
   * @param dataStoreClass  the datastore class
   * @param keyClass        output key class
   * @param reuseObjects    whether to reuse objects in serialization
   * @param <K>
   * @param <V>

   * Initializes the Reducer, and sets output parameters for the job.
   *
   * @param job             the job to set the properties for
   * @param dataStoreClass  the datastore class
   * @param keyClass        output key class
   * @param reducerClass    the reducer class extending GoraReducer
   * @param reuseObjects    whether to reuse objects in serialization
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>

   * Initializes the Reducer, and sets output parameters for the job.
   *
   * @param job          the job to set the properties for
   * @param dataStore    the datastore as the output
   * @param reducerClass the reducer class extending GoraReducer* @param reducerClass
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>
   * Initializes the Reducer, and sets output parameters for the job.
   *
   * @param job          the job to set the properties for
   * @param dataStore    the datastore as the output
   * @param <K1>
   * @param <V1>
   * @param <K2>
   * @param <V2>
   * Do the serialization of the {@link PersistentBase} object
   *
   * @param persistent
   * @throws IOException
 * BeanFactory's enable contruction of keys and Persistent objects.
 *
 * @param <K>
 * @param <T>
   *
   * @throws Exception
   *
   * Returns an instance of the key object to be
   * used to access static fields of the object. Returned object MUST
   * be treated as read-only. No fields other than the static fields
   * of the object should be assumed to be readable.
   *

   * Returns an instance of the {@link Persistent} object to be
   * used to access static fields of the object. Returned object MUST
   * be treated as read-only. No fields other than the static fields
   * of the object should be assumed to be readable.
   *
   *
   *
 * A default implementation of the {@link BeanFactory} interface. Constructs
 * the keys using by reflection, {@link Persistent} objects by calling
 *
 * @param <K>
 * @param <T>

   *
   * @param keyClass
 * the keys using by reflection, {@link Persistent} objects by calling
 *
 * @param <K>
 * @param <T>

   *

   * @param filter Set a filter on this query.


   *
   * @param enable true to enable local {@link org.apache.gora.filter.Filter}

   * @param startKey an inclusive start key
   * @param endKey an inclusive end key
   *
   * @param startKey an inclusive start key
   * @param endKey   an inclusive end key

   *
   * @param limit long value for the limit permitted for a given query
   *

   *
   * @throws Exception if an error is encountered whilst advancing to next result

   *

   *

   *

   *

   *

   *
   * @return a float value representing progress of the job
   * @throws IOException          if there is an erro obtaining progress
   * @throws InterruptedException if progress stalls or is interrupted
implements Result<K, T> {













   * Returns whether the limit for the query is reached.
   * @return true if result limit is reached









   * actual results.
   * @return true if another result exists
   * @throws IOException if for some reason we reach a result which does not exist

    if(persistent != null) {
      return persistent;
    }
    return dataStore.newPersistent();

  /**
   * Creates a job and sets the output parameters for the conf that Spark will use
   *
   * @param dataStore the datastore as the output
   * @param <K>
   * @param <V>
   * @return
   * @throws IOException
   */
  /**
   * Sets the output parameters for the conf that Spark will use
   *
   * @param job the job to set the properties for
   * @param dataStore the datastore as the output
   * @param <K>
   * @param <V>
   * @return
   */
  /**
   * Sets the output parameters for the conf that Spark will use
   *
   * @param job             the job to set the properties for
   * @param dataStoreClass  the datastore class
   * @param keyClass        output key class
   * @param persistentClass output value class
   * @param <K>
   * @param <V>
   * @return
   */
 * <p> DataStores implementations should be thread safe.</p>
 * <p><a name="visibility"><b>Note:</b></a> Results of updates ({@link #put(Object, Persistent)},
 * </p>
 *
   *
   * @param key
   * @param obj
   *
   * @param query
   *
   * @param fields
   * @return String array
   *
   * @return String
  /** Opens an InputStream for the input Hadoop path
   *
   * @return
   * @throws IOException
   */
  /** Opens an OutputStream for the output Hadoop path
   *
   * @return
   */
   *
   * @param query
   * @return
   * @throws IOException
   *
   * @param query
   * @return
   * @throws IOException
   *
   * @param schema the schema object to get the fields names from
   * @return 0 if equal, {@literal <} 0 if left is less than right, etc.
   * @return 0 if equal, {@literal <} 0 if left is less than right, etc.
   *
   * @param args the schema file to be compiled and where this should be written
   *
   * @param dynamoHandler handler to main DynamoDB
     *
     *
   *
   * @param query matching records to this query will be deleted
   * @return
   *
   * @return
   *
   * @return
   *
   * @param key
   * @param obj
   *
   * @param key the key of the object
   *
   * @return
   * @param pTableName
   *
   * @param clazz
   * @return
   *
   * @param dynamoHandler handler to main DynamoDB
   * @param numRows the number of rows for caching {@literal >=} 0
 * Contains utility methods for byte[] {@literal <->} field conversions.
 * @author Damien Raude-Morvan draudemorvan@dictanova.com
 * @author Damien Raude-Morvan draudemorvan@dictanova.com
 * @author Damien Raude-Morvan draudemorvan@dictanova.com
 * @author Fabien Poulard fpoulard@dictanova.com
 * @author Fabien Poulard fpoulard@dictanova.com
 * @author Damien Raude-Morvan draudemorvan@dictanova.com
 * @author Fabien Poulard fpoulard@dictanova.com
      .info("Keyclass and nameclass match but mismatching table names "
           " mappingfile schema is '"  docNameFromMapping
           "' vs actual schema '"  collName
           "' , assuming they are the same.");
      .addClassField(field.getAttributeValue(ATT_NAME),
          field.getAttributeValue(ATT_FIELD),
          field.getAttributeValue(ATT_TYPE));
 * @author Fabien Poulard fpoulard@dictanova.com
 * @author Damien Raude-Morvan draudemorvan@dictanova.com
DataStoreBase<K, T> {
          parameters.getDbname(), parameters.getServers() });
        .dbEncoderFactory(GoraDBEncoder.FACTORY); // Utf8 serialization!
          String fieldPath = docf  "."  innerDocField;
          LOG.debug(
              "Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}",
              new Object[] { recField.name(), innerSchema.getType(), fieldPath,
                  innerStoreType });
          record.put(
              recField.pos(),
              fromDBObject(innerSchema, innerStoreType, recField, innerDocField,
                  innerBson));
      final BSONDecorator easybson, final Field f) {
      final BSONDecorator easybson, final Field f) {
      return new DirtyMapWrapper(rmap);
           ": Invalid string: unable to convert to ObjectId");
           ": Invalid date format '"  value  "'");
   * Ensure Key encoding -&gt; dots replaced with middle dots
   * Ensure Key decoding -&gt; middle dots replaced with dots
 * @author Fabien Poulard fpoulard@dictanova.com
   * @throws IOException if the job cannot be created successfully from given configuration

    /**
     * Checks the dirty status of the 'referrer' field. A field is dirty if it represents a change that has not yet been written to the database.
     */
    public boolean isReferrerDirty() {

    /**
     * Sets the value of the 'userAgent' field.
     * @param value the value to set.
      LOG.error("{}  is not found, please check the file.", DEFAULT_MAPPING_FILE);
      throw new RuntimeException(ex);
      LOG.error("{}  is not found, please check the file.", DEFAULT_MAPPING_FILE);
      throw new RuntimeException(ex);
    try {
      Result<K, T> result = query.execute();
      String[] fields = getFieldsToQuery(query.getFields());
      boolean isAllFields = Arrays.equals(fields, getFields());

      while (result.next()) {
        if (isAllFields) {
          if (delete(result.getKey())) {
            deletedRows;
          }
        } else {
          ArrayList<String> excludedFields = new ArrayList<>();
          for (String field : getFields()) {
            if (!Arrays.asList(fields).contains(field)) {
              excludedFields.add(field);
            }
          }
          T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
          if (delete(result.getKey())) {
            put(result.getKey(),newClonedObj);
            deletedRows;
          }
        }
    newObj.clear();
    for (String field : fields) {
      Field otherField = obj.getSchema().getField(field);
    try {
      Result<K, T> result = query.execute();
      String[] fields = getFieldsToQuery(query.getFields());
      boolean isAllFields = Arrays.equals(fields, getFields());

      while (result.next()) {
        if (isAllFields) {
          if (delete(result.getKey())) {
            deletedRows;
          }
        } else {
          ArrayList<String> excludedFields = new ArrayList<>();
          for (String field : getFields()) {
            if (!Arrays.asList(fields).contains(field)) {
              excludedFields.add(field);
            }
          }
          T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
          if (delete(result.getKey())) {
            put(result.getKey(),newClonedObj);
            deletedRows;
          }
        }
    newObj.clear();
    for (String field : fields) {
      Field otherField = obj.getSchema().getField(field);
/*

  @Override
  public byte[] encodeShort(short s) throws IOException {

  @Override
  public byte[] encodeShort(short s, byte[] ret) throws IOException {
    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){

  @Override
  public short decodeShort(byte[] a) throws IOException {
    try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){

  @Override
  public byte[] encodeInt(int i) throws IOException {

  @Override
  public byte[] encodeInt(int i, byte[] ret) throws IOException {
    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){

  @Override
  public int decodeInt(byte[] a) throws IOException {
    try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){

  @Override
  public byte[] encodeLong(long l) throws IOException {

  @Override
  public byte[] encodeLong(long l, byte[] ret) throws IOException {
    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){

  @Override
  public long decodeLong(byte[] a) throws IOException {
    try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){

  @Override
  public byte[] encodeDouble(double d) throws IOException {

  @Override
  public byte[] encodeDouble(double d, byte[] ret) throws IOException {
    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){

  @Override
  public double decodeDouble(byte[] a) throws IOException {
    try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){

  @Override
  public byte[] encodeFloat(float d) throws IOException {

  @Override
  public byte[] encodeFloat(float f, byte[] ret) throws IOException {
    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){

  @Override
  public float decodeFloat(byte[] a) throws IOException {
    try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){

  @Override

  @Override

  @Override

  @Override
  public boolean decodeBoolean(byte[] a) throws IOException {
    try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){

  @Override
  public byte[] encodeBoolean(boolean b) throws IOException {

  @Override
  public byte[] encodeBoolean(boolean b, byte[] ret) throws IOException {
    try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){


import java.io.IOException;

  public byte[] encodeShort(short s) throws IOException;
  public byte[] encodeShort(short s, byte[] ret) throws IOException;
  public short decodeShort(byte[] a) throws IOException;
  public byte[] encodeInt(int i) throws IOException;
  public byte[] encodeInt(int i, byte[] ret) throws IOException;
  public int decodeInt(byte[] a) throws IOException;
  public byte[] encodeLong(long l) throws IOException;
  public byte[] encodeLong(long l, byte[] ret) throws IOException;
  public long decodeLong(byte[] a) throws IOException;
  public byte[] encodeDouble(double d) throws IOException;
  public byte[] encodeDouble(double d, byte[] ret) throws IOException;
  public double decodeDouble(byte[] a) throws IOException;
  public byte[] encodeFloat(float d) throws IOException;
  public byte[] encodeFloat(float f, byte[] ret) throws IOException;
  public float decodeFloat(byte[] a) throws IOException;
  public boolean decodeBoolean(byte[] val) throws IOException;
  public byte[] encodeBoolean(boolean b) throws IOException;
  public byte[] encodeBoolean(boolean b, byte[] ret) throws IOException;
  private byte[] chars = new byte[] {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'};
      return b - '0';
      return b - 'a'  10;
import java.io.IOException;

  @Override
  public byte[] encodeShort(short s, byte[] ret) throws IOException{

  @Override
  public short decodeShort(byte[] a) throws IOException{

  @Override
  public byte[] encodeInt(int i, byte[] ret) throws IOException{

  @Override
  public int decodeInt(byte[] a) throws IOException{

  @Override
  public byte[] encodeLong(long l, byte[] ret) throws IOException{
    l = l ^ 0x8000000000000000L;

  @Override
  public long decodeLong(byte[] a) throws IOException {
    l = l ^ 0x8000000000000000L;

  @Override
  public byte[] encodeDouble(double d, byte[] ret) throws IOException {
      l = l ^ 0x8000000000000000L;

  @Override
  public double decodeDouble(byte[] a) throws IOException{
      l = l ^ 0x8000000000000000L;

  @Override
  public byte[] encodeFloat(float f, byte[] ret) throws IOException {



  @Override
  public float decodeFloat(byte[] a) throws IOException{

    byte[] ret = new byte[size];
      byte[] copy = new byte[ret.length - 1];
    byte[] copy = new byte[minLen];
  private final static String UNKOWN = "Unknown type ";

  public Object fromBytes(Schema schema, byte[] data) throws IOException {
  public static Object fromBytes(Encoder encoder, Schema schema, byte data[]) throws IOException {
    throw new IllegalArgumentException(UNKOWN  schema.getType());
      throw new IllegalArgumentException(UNKOWN  clazz.getName());
      byte[] copy = new byte[len];
    throw new IllegalArgumentException(UNKOWN  o.getClass().getName());
        batchWriterConfig.setMaxLatency(60000L, TimeUnit.MILLISECONDS);
      if (mapping.encoder == null || "".equals(mapping.encoder)) {
            if ("".equals(qualifier))
  private void setFetchColumns(Scanner scanner, String[] fields) {
        LOG.error("Mapping not found for field: {}", field);
          || !(o instanceof DirtyMapWrapper)) {
    throw new IllegalArgumentException(UNKOWN  clazz.getName());
    throw new IllegalArgumentException(UNKOWN  clazz.getName());
    try {
      Result<K, T> result = query.execute();
      String[] fields = getFieldsToQuery(query.getFields());
      boolean isAllFields = Arrays.equals(fields, getFields());

      while (result.next()) {
        if (isAllFields) {
          if (delete(result.getKey())) {
            deletedRows;
          }
        } else {
          ArrayList<String> excludedFields = new ArrayList<>();
          for (String field : getFields()) {
            if (!Arrays.asList(fields).contains(field)) {
              excludedFields.add(field);
            }
          }
          T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
          if (delete(result.getKey())) {
            put(result.getKey(),newClonedObj);
            deletedRows;
          }
        }
    newObj.clear();
    for (String field : fields) {
      Field otherField = obj.getSchema().getField(field);
     * @return a string of the form "XhYmZs" when the elapsed time is X hours, Y minutes and Z seconds or null if start {@literal >} end.
  /**
   * Default constructor
   */
  public HBaseStore() {//Empty Constrctor
    Marshaller<T> marshaller = new Marshaller<>(persistentClass);
  public static final Logger LOG = LoggerFactory.getLogger(InfinispanStore.class);
  private InfinispanClient<K, T> infinispanClient;
  private String primaryFieldName;
  private int primaryFieldPos;
  private int splitSize;
  /**
   * Default constructor
   */
  public InfinispanStore(){
    //Empty default constructor
  }
  @Override
  public synchronized void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) {
    try {
      if (primaryFieldName!=null) {
        LOG.info("Client already initialized; ignoring.");
        return;
      super.initialize(keyClass, persistentClass, properties);
      infinispanClient  = new InfinispanClient<>();
      infinispanClient.setConf(conf);
      LOG.info("key class: "
           keyClass.getCanonicalName()
           ", persistent class: "
           persistentClass.getCanonicalName());
      schema = persistentClass.newInstance().getSchema();
      splitSize = Integer.valueOf(
          properties.getProperty( BUFFER_LIMIT_READ_NAME,
              getConf().get(
                  BUFFER_LIMIT_READ_NAME,
                  Integer.toString(BUFFER_LIMIT_READ_VALUE))));
      LOG.info("split size: "splitSize);
      primaryFieldPos = 0;
      primaryFieldName = schema.getFields().get(0).name();
      this.infinispanClient.initialize(keyClass, persistentClass, properties);
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }
  @Override
  public void close() {
    LOG.debug("close()");
    infinispanClient.close();
  }
  @Override
  public void createSchema() {
    LOG.debug("createSchema()");
    this.infinispanClient.createCache();
  }

  @Override
  public boolean delete(K key) {
    LOG.debug("delete("  key")");
    this.infinispanClient.deleteByKey(key);
    return true;
  }

  @Override
  public long deleteByQuery(Query<K, T> query) {
    ((InfinispanQuery<K, T>) query).build();
    LOG.debug("deleteByQuery("query.toString()")");
    InfinispanQuery<K, T> q = (InfinispanQuery) query;
    q.build();
    for( T t : q.list()){
      infinispanClient.deleteByKey((K) t.get(primaryFieldPos));
    }
    return q.getResultSize();
  }

  @Override
  public void deleteSchema() {
    LOG.debug("deleteSchema()");
    this.infinispanClient.dropCache();
  }

  @Override
  public Result<K, T> execute(Query<K, T> query) {
    LOG.debug("execute()");
    ((InfinispanQuery<K,T>)query).build();
    InfinispanResult<K,T> result = new InfinispanResult<>(this, (InfinispanQuery<K,T>)query);
    LOG.trace("query: "  query.toString());
    LOG.trace("result size: "  result.size());
    return result;
  }

  @Override
  public T get(K key){
    LOG.debug("get("key")");
    return infinispanClient.get(key);
  }

  @Override
  public T get(K key, String[] fields) {
    LOG.debug("get("key","fields")");
    if (fields==null)
    InfinispanQuery<K, T> query = new InfinispanQuery<K, T>(this);
    query.setKey(key);
    query.setFields(fields);
    query.build();
    Result<K,T> result = query.execute();
    try {
      result.next();
      return result.get();
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }
  /**
   *
   * Split the query per infinispan node resulting in a list of queries.
   * For each Infinispan server, this function returns a set of qeuries
   * using pagination of the originial query. The size of each query
   * in this pagination equals <i>gora.buffer.read.limit</i>.
   *
   * @param query the base query to create the partitions for. If the query
   * is null, then the data store returns the partitions for the default query
   * (returning every object)
   * @return
   * @throws IOException
   */
  @Override
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
      throws IOException {
    LOG.debug("getPartitions()");
    // 1 - split the query per location
    List<PartitionQuery<K,T>> locations = ((InfinispanQuery<K,T>)query).split();
    // 2 -split each location
    List<PartitionQuery<K,T>> splitLocations = new ArrayList<>();
    for(PartitionQuery<K,T> location : locations) {
      LOG.trace("location: " ((InfinispanQuery<K, T>)location).getLocation().toString());
      // 2.1 - compute the result size
      InfinispanQuery<K,T> sizeQuery = (InfinispanQuery<K, T>) ((InfinispanQuery<K, T>) location).clone();
      sizeQuery.setFields(primaryFieldName);
      sizeQuery.setLimit(1);
      sizeQuery.rebuild();
      // 2.2 - check if splitting is necessary
      int resultSize = sizeQuery.getResultSize();
      long queryLimit = query.getLimit();
      long splitLimit = queryLimit>0 ? Math.min((long)resultSize,queryLimit) : resultSize;
      LOG.trace("split limit: " splitLimit);
      LOG.trace("split size: " splitSize);
      if (splitLimit <= splitSize) {
        LOG.trace("location returned");
        splitLocations.add(location);
        continue;
      // 2.3 - compute the splits
      for(int i=0; i<Math.ceil((double)splitLimit/(double)splitSize); i) {
        InfinispanQuery<K, T> split = (InfinispanQuery<K, T>) ((InfinispanQuery<K, T>) location).clone();
        split.setOffset(i * splitSize);
        split.setLimit(splitSize);
        split.rebuild();
        splitLocations.add(split);
      }
    }
    return splitLocations;
  }
  @Override
  public void flush() {
    LOG.debug("flush()");
    infinispanClient.flush();
  }
  /**
   * In Infinispan, Schemas are referred to as caches.
   *
   * @return Cache
   */
  @Override
  public String getSchemaName() {
    LOG.debug("getSchemaName()");
    return this.infinispanClient.getCacheName();
  }
  @Override
  public Query<K, T> newQuery() {
    LOG.debug("newQuery()");
    Query<K, T> query = new InfinispanQuery<>(this);
    query.setFields(getFieldsToQuery(null));
    return query;
  }
  @Override
  public void put(K key, T obj) {
    LOG.debug("put(" key.toString()")");
    LOG.trace(obj.toString());
    if (obj.get(primaryFieldPos)==null)
      obj.put(primaryFieldPos,key);
    if (!obj.get(primaryFieldPos).equals(key) )
      LOG.warn("Invalid or different primary field :"key"<->"obj.get(primaryFieldPos));
    this.infinispanClient.put(key, obj);
  }
  @Override
  public boolean schemaExists() {
    LOG.debug("schemaExists()");
    return infinispanClient.cacheExists();
  }
  public InfinispanClient<K, T> getClient() {
    LOG.debug("getClient()");
    return infinispanClient;
  }
  public String getPrimaryFieldName() {
    LOG.debug("getPrimaryField()");
    return primaryFieldName;
  }
  public void setPrimaryFieldName(String name){
    LOG.debug("getPrimaryFieldName()");
    primaryFieldName = name;
  }
  public int getPrimaryFieldPos(){
    LOG.debug("getPrimaryFieldPos()");
    return primaryFieldPos;
  }

  public void setPrimaryFieldPos(int p){
    LOG.debug("setPrimaryFieldPos()");
    primaryFieldPos = p;
  }
  public static final String GORA_DEFAULT_CACHE_DATASTORE_KEY = "gora.cache.datastore.default";


  /**
   * Instantiate <i>the default</i> {@link DataStore} wrapped over JCache datastore which provides caching
   * abstraction over any GORA persistence dataStore.
   * Uses default properties. Uses 'null' schema.
   *
   * Note:
   *    consider that default dataStore is always visible
   *
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param isCacheEnabled caching enable
   * @return A new store instance.
   * @throws GoraException
   */
  @SuppressWarnings("unchecked")
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
          Class<K> keyClass, Class<T> persistent, Configuration conf, boolean isCacheEnabled) throws GoraException {
    Properties createProps = createProps();
    Class<? extends DataStore<K, T>> c;
    try {
      if (isCacheEnabled) {
        c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultCacheDataStore(createProps));
      } else {
        c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
      }
    } catch (Exception ex) {
      throw new GoraException(ex);
    }
    return createDataStore(c, keyClass, persistent, conf, createProps, null);
  }

  private static String getDefaultCacheDataStore(Properties properties) {
    return getProperty(properties, GORA_DEFAULT_CACHE_DATASTORE_KEY);
  }

import java.util.concurrent.ConcurrentSkipListSet;
import javax.cache.Cache;
import javax.cache.CacheManager;
import javax.cache.Caching;
import javax.cache.configuration.MutableCacheEntryListenerConfiguration;
import javax.cache.configuration.MutableConfiguration;
import javax.cache.spi.CachingProvider;

  private Cache<K, T> cache;
  private CacheManager manager;
  private ConcurrentSkipListSet<K> cacheEntryList;
  private static final String GORA_DEFAULT_JCACHE_PROVIDER_KEY = "gora.datastore.jcache.provider";
  private static final Logger LOG = LoggerFactory.getLogger(JCacheStore.class);
    CachingProvider cachingProvider = Caching.getCachingProvider(
           properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY)
    );
    manager = cachingProvider.getCacheManager();
    cacheEntryList = new ConcurrentSkipListSet<>();
    MutableConfiguration<K, T> config = new MutableConfiguration<K, T>();
    config.setTypes(keyClass, persistentClass);
    config.setReadThrough(true);
    config.setWriteThrough(true);
    config.setCacheLoaderFactory(JCacheCacheFactoryBuilder.factoryOfCacheLoader(keyClass,persistentClass));
    config.setCacheWriterFactory(JCacheCacheFactoryBuilder.factoryOfCacheWriter(keyClass,persistentClass));
    config.addCacheEntryListenerConfiguration(
            new MutableCacheEntryListenerConfiguration<>(
                    JCacheCacheFactoryBuilder.factoryOfEntryListener(new JCacheCacheEntryListener<K,T>(cacheEntryList)),
                    null, true, true
            )
    );
    cache = manager.createCache(persistentClass.getSimpleName(),config);
  public T get(K key){
    return cache.get(key);
  }

  @Override
    cache.put(key,val);
    return cache.remove(key);
    Collection<T>, java.io.Serializable {
final class DirtyFlag implements Dirtyable, java.io.Serializable {
final class DirtyIteratorWrapper<T> implements Iterator<T>, java.io.Serializable {
public class DirtyMapWrapper<K, V> implements Map<K, V>, Dirtyable, java.io.Serializable {
import java.io.Serializable;
    Persistent, java.io.Serializable {
  private byte[] __g__dirty;
    __g__dirty = new byte[getFieldsCount()];
    //__g__dirty = java.nio.ByteBuffer.wrap(new byte[getFieldsCount()]);
    return java.nio.ByteBuffer.wrap(__g__dirty);
public class JCacheQuery<K, T extends PersistentBase> extends QueryBase<K, T> {

  public JCacheQuery(DataStore<K, T> dataStore) {

import java.util.Iterator;
import java.util.NavigableSet;
public class JCacheResult<K, T extends PersistentBase> extends ResultBase<K, T> {
  private NavigableSet<K> cacheKeySet;
  private Iterator<K> iterator;

  public JCacheResult(DataStore<K, T> dataStore, Query<K, T> query) {

  public JCacheResult(DataStore<K, T> dataStore, Query<K, T> query, NavigableSet<K> cacheKeySet) {
    super(dataStore, query);
    this.cacheKeySet = cacheKeySet;
    this.iterator = cacheKeySet.iterator();
  }

  public JCacheStore<K, T> getDataStore() {
    return (JCacheStore<K, T>) super.getDataStore();
  }




    if (!iterator.hasNext()) {
      return false;
    }
    key = iterator.next();
    persistent = dataStore.get(key);

  private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheEntryListenerFactory.class);
import org.apache.gora.store.DataStore;
  factoryOfCacheLoader(DataStore<K, T> dataStore) {
    return new JCacheCacheLoaderFactory<>(new JCacheCacheLoader<>(dataStore));
  factoryOfCacheWriter(DataStore<K, T> dataStore) {
    return new JCacheCacheWriterFactory<>(new JCacheCacheWriter<>(dataStore));
  public JCacheCacheLoader(DataStore<K, T> dataStore) {
      this.dataStore = dataStore;
  private transient JCacheCacheLoader<K, T> instance;
  public JCacheCacheLoaderFactory(JCacheCacheLoader<K, T> instance) {
    this.instance = instance;
  public JCacheCacheLoader<K, T> create() {
    return (JCacheCacheLoader<K, T>) this.instance;
  public JCacheCacheWriter(DataStore<K, T> dataStore) {
    this.dataStore = dataStore;
  private transient JCacheCacheWriter<K,T> instance;
  public JCacheCacheWriterFactory(JCacheCacheWriter<K,T> instance) {
    this.instance = instance;
    return (JCacheCacheWriter<K,T>)this.instance;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Arrays;
import java.util.ArrayList;
import com.hazelcast.cache.HazelcastCachingProvider;
import com.hazelcast.cache.ICache;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.Member;
import com.hazelcast.core.Partition;
import org.apache.avro.Schema;
import org.apache.gora.jcache.query.JCacheResult;
import org.apache.gora.query.impl.PartitionQueryImpl;
import org.apache.gora.store.DataStore;
import org.apache.gora.store.DataStoreFactory;
import org.apache.gora.util.AvroUtils;
import org.apache.gora.util.GoraException;
import org.apache.hadoop.conf.Configuration;
  private ICache<K, T> cache;
  private static final String GORA_DEFAULT_JCACHE_NAMESPACE = "gora.jcache.namespace";
  private DataStore<K, T> persistentDataStore;
  private MutableConfiguration<K, T> cacheConfig;
  private HazelcastInstance hazelcastInstance;
    try {
      this.persistentDataStore = DataStoreFactory.getDataStore(keyClass, persistentClass,
              new Configuration());
    } catch (GoraException ex) {
      LOG.error("Couldn't initialize persistent DataStore");
    }
    hazelcastInstance = Hazelcast.newHazelcastInstance();
    Properties providerProperties = new Properties();
    providerProperties.setProperty( HazelcastCachingProvider.HAZELCAST_INSTANCE_NAME,
            hazelcastInstance.getName());
    try {
      manager = cachingProvider.getCacheManager(new URI(GORA_DEFAULT_JCACHE_NAMESPACE), null, providerProperties);
    } catch (URISyntaxException ex) {
      LOG.error("Couldn't initialize cache manager to a bounded hazelcast instance");
      manager = cachingProvider.getCacheManager();
    }
    cacheConfig = new MutableConfiguration<K, T>();
    cacheConfig.setTypes(keyClass, persistentClass);
    cacheConfig.setReadThrough(true);
    cacheConfig.setWriteThrough(true);
    cacheConfig.setStoreByValue(true);
    cacheConfig.setCacheLoaderFactory(JCacheCacheFactoryBuilder
            .factoryOfCacheLoader(this.persistentDataStore));
    cacheConfig.setCacheWriterFactory(JCacheCacheFactoryBuilder
            .factoryOfCacheWriter(this.persistentDataStore));
    cacheConfig.addCacheEntryListenerConfiguration(
                    JCacheCacheFactoryBuilder
                            .factoryOfEntryListener(new JCacheCacheEntryListener<K,T>(cacheEntryList)),
    cache = manager.createCache(persistentClass.getSimpleName(),
            cacheConfig).unwrap(ICache.class);
    return super.persistentClass.getSimpleName();
    if (manager.getCache(super.getPersistentClass().getSimpleName()) == null) {
      cache = manager.createCache(persistentClass.getSimpleName(),
              cacheConfig).unwrap(ICache.class);
    }
    persistentDataStore.createSchema();
    manager.destroyCache(super.getPersistentClass().getSimpleName());
    persistentDataStore.deleteSchema();
    return (manager.getCache(super.getPersistentClass().getSimpleName()) != null)
            && persistentDataStore.schemaExists();
    T persitent = (T) cache.get(key);
    if (persitent == null) {
      return null;
    }
    return getPersistent(persitent, fields);
  }

  private static <T extends PersistentBase> T getPersistent(T persitent, String[] fields) {
    List<Schema.Field> otherFields = persitent.getSchema().getFields();
    String[] otherFieldStrings = new String[otherFields.size()];
    for (int i = 0; i < otherFields.size(); i) {
      otherFieldStrings[i] = otherFields.get(i).name();
    }
    if (Arrays.equals(fields, otherFieldStrings)) {
      return persitent;
    }
    T clonedPersistent = AvroUtils.deepClonePersistent(persitent);
    clonedPersistent.clear();
    for (String field : fields) {
      Schema.Field otherField = persitent.getSchema().getField(field);
      int index = otherField.pos();
      clonedPersistent.put(index, persitent.get(index));
    }
    return clonedPersistent;
  public long deleteByQuery(Query<K, T> query) {
    try {
      long deletedRows = 0;
      Result<K, T> result = query.execute();
      String[] fields = getFieldsToQuery(query.getFields());
      boolean isAllFields = Arrays.equals(fields, getFields());
      while (result.next()) {
        if (isAllFields) {
          if (delete(result.getKey())) {
            deletedRows;
          }
        } else {
          ArrayList<String> excludedFields = new ArrayList<>();
          for (String field : getFields()) {
            if (!Arrays.asList(fields).contains(field)) {
              excludedFields.add(field);
            }
          }
          T newClonedObj = getPersistent(result.get(),
                  excludedFields.toArray(new String[excludedFields.size()]));
          if (delete(result.getKey())) {
            put(result.getKey(), newClonedObj);
            deletedRows;
          }
        }
      }
      return deletedRows;
    } catch (Exception e) {
      return 0;
    }
  public Result<K, T> execute(Query<K, T> query) {
    K startKey = query.getStartKey();
    K endKey = query.getEndKey();
    if (startKey == null) {
      if (!cacheEntryList.isEmpty()) {
        startKey = (K) cacheEntryList.first();
      }
    }
    if (endKey == null) {
      if (!cacheEntryList.isEmpty()) {
        endKey = (K) cacheEntryList.last();
      }
    }
    query.setFields(getFieldsToQuery(query.getFields()));
    ConcurrentSkipListSet<K> cacheEntrySubList = null;
    try {
      cacheEntrySubList = (ConcurrentSkipListSet<K>) cacheEntryList.subSet(startKey, true, endKey, true);
    } catch (NullPointerException npe) {
      LOG.error("NPE occurred while executing the query for JCacheStore");
      return new JCacheResult<>(this, query, new ConcurrentSkipListSet<K>());
    }
    return new JCacheResult<>(this, query, cacheEntrySubList);
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query) throws IOException {
    List<PartitionQuery<K, T>> partitions = new ArrayList<>();
    try {
      Member[] clusterMembers = new Member[hazelcastInstance.getCluster().getMembers().size()];
      this.hazelcastInstance.getCluster().getMembers().toArray(clusterMembers);
      for (Member member : clusterMembers) {
        JCacheResult<K, T> result = ((JCacheResult<K, T>) query.execute());
        ConcurrentSkipListSet<K> memberOwnedCacheEntries = new ConcurrentSkipListSet<>();
        while (result.next()) {
          K key = result.getKey();
          Partition partition = hazelcastInstance.getPartitionService().getPartition(key);
          if (partition.getOwner().getUuid().equals(member.getUuid())) {
            memberOwnedCacheEntries.add(key);
          }
        }
        PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<>(
                query, memberOwnedCacheEntries.first(),
                memberOwnedCacheEntries.last(), member.getSocketAddress().getHostString());
        partitions.add(partition);
      }
    } catch (java.lang.Exception ex) {
      LOG.error("Exception occurred while partitioning the query based on Hazelcast partitions.");
      return null;
    }
    return partitions;
    persistentDataStore.flush();
    flush();
    if (!cache.isDestroyed()) {
      cache.destroy();
    }
    if (!manager.isClosed()) {
      manager.close();
    }
    persistentDataStore.close();
import java.util.concurrent.TimeUnit;
import com.hazelcast.config.CacheConfig;
import com.hazelcast.config.EvictionConfig;
import com.hazelcast.config.EvictionPolicy;
import javax.cache.configuration.FactoryBuilder;
import javax.cache.expiry.AccessedExpiryPolicy;
import javax.cache.expiry.ModifiedExpiryPolicy;
import javax.cache.expiry.CreatedExpiryPolicy;
import javax.cache.expiry.TouchedExpiryPolicy;
import javax.cache.expiry.Duration;
  private static final String GORA_DEFAULT_JCACHE_PROVIDER_KEY = "gora.datastore.jcache.provider";
  private static final String JCACHE_READ_THROUGH_PROPERTY_KEY = "jcache.read.through.enable";
  private static final String JCACHE_WRITE_THROUGH_PROPERTY_KEY = "jcache.write.through.enable";
  private static final String JCACHE_STORE_BY_VALUE_PROPERTY_KEY = "jcache.store.by.value.enable";
  private static final String JCACHE_STATISTICS_PROPERTY_KEY = "jcache.statistics.enable";
  private static final String JCACHE_MANAGEMENT_PROPERTY_KEY = "jcache.management.enable";
  private static final String JCACHE_CACHE_NAMESPACE_PROPERTY_KEY = "jcache.cache.namespace";
  private static final String JCACHE_EVICTION_POLICY_PROPERTY_KEY = "jcache.eviction.policy";
  private static final String JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY = "jcache.eviction.max.size.policy";
  private static final String JCACHE_EVICTION_SIZE_PROPERTY_KEY = "jcache.eviction.size";
  private static final String JCACHE_EXPIRE_POLICY_PROPERTY_KEY = "jcache.expire.policy";
  private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy";
  private static final String JCACHE_ACCESSED_EXPIRY_IDENTIFIER = "ACCESSED";
  private static final String JCACHE_CREATED_EXPIRY_IDENTIFIER = "CREATED";
  private static final String JCACHE_MODIFIED_EXPIRY_IDENTIFIER = "MODIFIED";
  private static final String JCACHE_TOUCHED_EXPIRY_IDENTIFIER = "TOUCHED";
  private String goraCacheNamespace = GORA_DEFAULT_JCACHE_NAMESPACE;
  private CacheConfig<K, T> cacheConfig;
    if (properties.getProperty(JCACHE_CACHE_NAMESPACE_PROPERTY_KEY) != null) {
      goraCacheNamespace = properties.getProperty(JCACHE_CACHE_NAMESPACE_PROPERTY_KEY);
    }
      manager = cachingProvider.getCacheManager(new URI(goraCacheNamespace), null, providerProperties);
      LOG.error("Couldn't initialize cache manager to bounded hazelcast instance");
    cacheConfig = new CacheConfig<K, T>();
    if (properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY) != null) {
      cacheConfig.setReadThrough(Boolean.valueOf(properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY) != null) {
      cacheConfig.setWriteThrough(Boolean.valueOf(properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY) != null) {
      cacheConfig.setStoreByValue(Boolean.valueOf(properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY) != null) {
      cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY) != null) {
      cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY) != null) {
      cacheConfig.getEvictionConfig()
              .setEvictionPolicy(EvictionPolicy.valueOf(properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY) != null) {
      cacheConfig.getEvictionConfig()
              .setMaximumSizePolicy(EvictionConfig.MaxSizePolicy
                      .valueOf(properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY) != null) {
      cacheConfig.getEvictionConfig()
              .setSize(Integer.valueOf(properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY)));
    }
    if (properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY) != null) {
      String expiryPolicyIdentifier = properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY);
      if (expiryPolicyIdentifier.equals(JCACHE_ACCESSED_EXPIRY_IDENTIFIER)){
        cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                new AccessedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                        Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
        ));
      } else if (expiryPolicyIdentifier.equals(JCACHE_CREATED_EXPIRY_IDENTIFIER)){
        cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                new CreatedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                        Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
        ));
      } else if (expiryPolicyIdentifier.equals(JCACHE_MODIFIED_EXPIRY_IDENTIFIER)){
        cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                new ModifiedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                        Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
        ));
      } else if (expiryPolicyIdentifier.equals(JCACHE_TOUCHED_EXPIRY_IDENTIFIER)){
        cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                new TouchedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                        Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
        ));
      }
    }
import org.apache.hadoop.mapreduce.task.JobContextImpl;
        conf.getStrings("io.serializations"),
      Job job = Job.getInstance(conf);
      return new JobContextImpl(job.getConfiguration(), null);
    return new JobContextImpl(conf, null);
  private static final String DEFAULT_TEMPLATES_PATH = "/org/apache/gora/compiler/templates/" ;
  
      compiler.setTemplateDir(DEFAULT_TEMPLATES_PATH);
  /**
   * Compiles a single schema. Any subschemas must be included in the own schema.
   * 
   * @param sourceSchema String with the schema definition in json (avro)
   * @param dest Path where .java classes will be written
   * @param templatesPath Path where Gora's velocity templates are. If null, will use DEFAULT_TEMPLATES_PATH
   * @return The compiled resulting Schema
   * @throws IOException
   */
  public static Schema compileSchema(String sourceSchema, File dest, String templatesPath) throws IOException {
    Schema.Parser parser = new Schema.Parser();

    if (templatesPath == null) {
      templatesPath = DEFAULT_TEMPLATES_PATH ;
    }
    
    LOG.info("Compiling source schema from String into {} using templates in {}", dest.getPath(), templatesPath);
    Schema newSchema = parser.parse(sourceSchema);
    GoraCompiler compiler = new GoraCompiler(newSchema);
    compiler.setTemplateDir(templatesPath);
    compiler.compileToDestination(null, dest); // Will always write to destination
    LOG.info("Compiled avro into: {}", dest.getAbsolutePath());
    return newSchema;
  }
  
   * Instantiate a new {@link DataStore}. Uses default properties. Uses 'null' schema.
   * 
   * @param dataStoreClass The datastore implementation class <i>as string</i>.
   * @param keyClass The key class <i>as string</i>.
   * @param persistentClass The value class <i>as string</i>.
   * @param props Gora properties configuration
   * @param conf {@link Configuration} to be used be the store.
   * @return A new store instance.
   * @throws GoraException
   */
  @SuppressWarnings({ "unchecked" })
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
      String dataStoreClass, String keyClass, String persistentClass, Properties props, Configuration conf)
          throws GoraException {

    try {
      Class<? extends DataStore<K,T>> c
          = (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
      Class<K> k = (Class<K>) ClassLoadingUtils.loadClass(keyClass);
      Class<T> p = (Class<T>) ClassLoadingUtils.loadClass(persistentClass);
      return createDataStore(c, k, p, conf, props, null);
    } catch(GoraException ex) {
      throw ex;
    } catch (Exception ex) {
      throw new GoraException(ex);
    }
  }
  
  /**
import java.io.InputStream;
import org.apache.commons.io.IOUtils;
  public static final String XML_MAPPING_DEFINITION = "gora.mapping" ;  
  
      
      InputStream mappingInputStream ;
      // If there is a mapping definition in the configuration, use it.
      if (getConf().get(XML_MAPPING_DEFINITION, null) != null) {
           mappingInputStream = IOUtils.toInputStream(getConf().get(XML_MAPPING_DEFINITION, null)) ;
      }

      // Otherwise use the configuration from de default file gora-hbase-mapping.xml or whatever
      // configured in the key "gora.hbase.mapping.file"
      else {
          mappingInputStream = getClass().getClassLoader().getResourceAsStream(getConf().get(PARSE_MAPPING_FILE_KEY, DEFAULT_MAPPING_FILE)) ;
      }
      
      mapping = readMapping(mappingInputStream);
  private HBaseMapping readMapping(InputStream mappingStream) throws IOException {
      Document doc = builder.build(mappingStream);
              mappingStream) ;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import javax.cache.event.CacheEntryExpiredListener;
import javax.cache.event.CacheEntryUpdatedListener;
import javax.cache.event.CacheEntryRemovedListener;
        CacheEntryRemovedListener<K, T>, CacheEntryUpdatedListener<K, T>, CacheEntryExpiredListener<K, T> {
  private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheEntryListener.class);
      LOG.info("Cache entry added on key "  event.getKey().toString());
      LOG.info("Cache entry removed on key "  event.getKey().toString());

  @Override
  public void onUpdated(Iterable<CacheEntryEvent<? extends K, ? extends T>> cacheEntryEvents)
          throws CacheEntryListenerException {
    for (CacheEntryEvent<? extends K, ? extends T> event : cacheEntryEvents) {
      LOG.info("Cache entry updated set on key "  event.getKey().toString());
    }
  }

  @Override
  public void onExpired(Iterable<CacheEntryEvent<? extends K, ? extends T>> cacheEntryEvents)
          throws CacheEntryListenerException {
    for (CacheEntryEvent<? extends K, ? extends T> event : cacheEntryEvents) {
      LOG.warn("Cache entry expired on key "  event.getKey().toString());
    }
  }

  public static final long serialVersionUID = 201305101634L;
  private transient JCacheCacheEntryListener<K, T> instance;
    LOG.info("JCache cache entry listener factory initialized successfully.");
      LOG.info("Loaded data bean from persistent datastore on key "  key.toString());
      LOG.info("Loaded data bean from persistent datastore on key "  key.toString());
  public static final long serialVersionUID = 201305101626L;
    LOG.info("JCache cache entry loader factory initialized successfully.");
    LOG.info("Written data bean to persistent datastore on key "  entry.getKey().toString());
    LOG.info("Deleted data bean from persistent datastore on key "  key.toString());
  public static final long serialVersionUID = 201205101621L;
    LOG.info("JCache entry writer factory initialized successfully.");
import com.hazelcast.config.InMemoryFormat;
  private static final String HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY = "jcache.cache.inmemory.format";
  private static final String HAZELCAST_CACHE_BINARY_IN_MEMORY_FORMAT_IDENTIFIER = "BINARY";
  private static final String HAZELCAST_CACHE_OBJECT_IN_MEMORY_FORMAT_IDENTIFIER = "OBJECT";
  private static final String HAZELCAST_CACHE_NATIVE_IN_MEMORY_FORMAT_IDENTIFIER = "NATIVE";
    if (properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY) != null) {
      String inMemoryFormat = properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY);
      if (inMemoryFormat.equals(HAZELCAST_CACHE_BINARY_IN_MEMORY_FORMAT_IDENTIFIER) ||
              inMemoryFormat.equals(HAZELCAST_CACHE_OBJECT_IN_MEMORY_FORMAT_IDENTIFIER) ||
              inMemoryFormat.equals(HAZELCAST_CACHE_NATIVE_IN_MEMORY_FORMAT_IDENTIFIER)) {
        cacheConfig.setInMemoryFormat(InMemoryFormat.valueOf(inMemoryFormat));
      }
    }
    LOG.info("JCache Gora datastore initialized successfully.");
    LOG.info("Created schema on persistent store and initialized cache for persistent bean "
             super.getPersistentClass().getSimpleName());
    LOG.info("Deleted schema on persistent store and destroyed cache for persistent bean "
             super.getPersistentClass().getSimpleName());
      LOG.info("JCache Gora datastore deleled "  deletedRows  " rows from Persistent datastore");
      LOG.error("Exception occured while deleting entries from JCache Gora datastore. Hence returning 0");
      LOG.error("NPE occurred while executing the query for JCacheStore. Hence returning empty entry set.");
    LOG.info("JCache Gora datastore flushed successfully.");
    LOG.info("JCache Gora datastore destroyed successfully.");
import org.apache.avro.SchemaNormalization;
  public static long fingerprint64(Schema schema) {
    return SchemaNormalization.parsingFingerprint64(schema);
  }

  private static final long serialVersionUID = -6468893522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }
  private static final long serialVersionUID = -6468893522296148698L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }

  private static final long serialVersionUID = -6468893522296178608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }

  private static final long serialVersionUID = -6468894522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }
  private static final long serialVersionUID = -6468893522496148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }

  private static final long serialVersionUID = -6468893522236148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

        Collection<T> {
final class DirtyFlag implements Dirtyable {
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

final class DirtyIteratorWrapper<T> implements Iterator<T> {
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

public class DirtyMapWrapper<K, V> implements Map<K, V>, Dirtyable {
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

    Persistent, java.io.Externalizable {
  private ByteBuffer __g__dirty;
    __g__dirty = ByteBuffer.wrap(new byte[getFieldsCount()]);
  public ByteBuffer getDirtyBytes() {
    return __g__dirty;
  }

  public void setDirtyBytes(ByteBuffer __g__dirty) {
    this.__g__dirty = __g__dirty;
  private final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(getSchema());
  private final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(getSchema());

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }

  private static final String JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY ="jcache.auto.create.cache";
    if (properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY) != null) {
      Boolean createCache = Boolean.valueOf(properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY));
      if (createCache) {
        cache = manager.createCache(persistentClass.getSimpleName(),
                cacheConfig).unwrap(ICache.class);
      }
    } else {
      cache = manager.createCache(persistentClass.getSimpleName(),
              cacheConfig).unwrap(ICache.class);
    }
    if (manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) == null) {
      cacheEntryList.clear();
    cacheEntryList.clear();
    return (manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) != null);
    if (fields != null && fields.length > 0) {
      for (String field : fields) {
        Schema.Field otherField = persitent.getSchema().getField(field);
        int index = otherField.pos();
        clonedPersistent.put(index, persitent.get(index));
      }
    } else {
      for (String field : otherFieldStrings) {
        Schema.Field otherField = persitent.getSchema().getField(field);
        int index = otherField.pos();
        clonedPersistent.put(index, persitent.get(index));
      }
    cacheEntryList.clear();
    if (!cache.isDestroyed() && !manager.isClosed()) {
  private static final long serialVersionUID = -6468793522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }
package org.apache.gora.tutorial.log.generated;

  private static final long serialVersionUID = -6468893522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }

public class JCacheStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
  private static final String JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY = "jcache.auto.create.cache";
  private ICache<K, T> cache;
  private CacheManager manager;
  private ConcurrentSkipListSet<K> cacheEntryList;
  private String goraCacheNamespace = GORA_DEFAULT_JCACHE_NAMESPACE;
  private static <T extends PersistentBase> T getPersistent(T persitent, String[] fields) {
    List<Schema.Field> otherFields = persitent.getSchema().getFields();
    String[] otherFieldStrings = new String[otherFields.size()];
    for (int i = 0; i < otherFields.size(); i) {
      otherFieldStrings[i] = otherFields.get(i).name();
    }
    if (Arrays.equals(fields, otherFieldStrings)) {
      return persitent;
    }
    T clonedPersistent = AvroUtils.deepClonePersistent(persitent);
    clonedPersistent.clear();
    if (fields != null && fields.length > 0) {
      for (String field : fields) {
        Schema.Field otherField = persitent.getSchema().getField(field);
        int index = otherField.pos();
        clonedPersistent.put(index, persitent.get(index));
      }
    } else {
      for (String field : otherFieldStrings) {
        Schema.Field otherField = persitent.getSchema().getField(field);
        int index = otherField.pos();
        clonedPersistent.put(index, persitent.get(index));
      }
    }
    return clonedPersistent;
  }

            properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY)
    providerProperties.setProperty(HazelcastCachingProvider.HAZELCAST_INSTANCE_NAME,
      if (expiryPolicyIdentifier.equals(JCACHE_ACCESSED_EXPIRY_IDENTIFIER)) {
      } else if (expiryPolicyIdentifier.equals(JCACHE_CREATED_EXPIRY_IDENTIFIER)) {
      } else if (expiryPolicyIdentifier.equals(JCACHE_MODIFIED_EXPIRY_IDENTIFIER)) {
      } else if (expiryPolicyIdentifier.equals(JCACHE_TOUCHED_EXPIRY_IDENTIFIER)) {
                            .factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
  public T get(K key) {
    cache.put(key, val);
  public Query<K, T> newQuery() {
    LOG.info("JCache cache writer factory initialized successfully.");
    } else {
      cacheConfig.setReadThrough(true);
    } else {
      cacheConfig.setWriteThrough(true);
  private static final long serialVersionUID = -6468893532296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }
  private static final long serialVersionUID = -6468883532296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }
  private java.nio.ByteBuffer __g__dirty;
    __g__dirty = java.nio.ByteBuffer.wrap(new byte[getFieldsCount()]);
  private static final long serialVersionUID = -7468893532296148608L;
  public static final org.apache.avro.Schema SCHEMA$ =
          new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
  /**
   * Utility method by velocity templates to generate serialVersionUID on AVRO beans.
   *
   * @param schema Data bean AVRO schema.
   */
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
import java.io.ObjectInput;
import java.io.ObjectOutput;
  /**
   * Exposing dirty bytes over public method. Purpose is to preserve dirty bytes content
   * while transporting AVRO data beans over TCP wire in serialized form.
   * Since {@link org.apache.gora.persistency.impl.PersistentBase} implements {@link java.io.Externalizable},
   * this method can be used to retrieve the dirty bytes as {@link java.nio.ByteBuffer} and and get the content
   * as bytes[] and write byte stream to the TCP wire.
   * See {@link java.io.Externalizable#writeExternal(ObjectOutput)} abstract method implementation
   * on velocity template record.vm.
   * <p>
   * Note {@link java.nio.ByteBuffer} is not itself not in serializable form.
   */

  /**
   * Setter method for assign dirty bytes when deserializing AVRO bean from dirty bytes
   * preserved in serialized bytes form.
   * Since {@link org.apache.gora.persistency.impl.PersistentBase} implements {@link java.io.Externalizable}
   * and when actual deserialization happens for {@link org.apache.gora.persistency.impl.PersistentBase}
   * new instance, acquire byte stream from TCP wire, extracting specific byte[] from byte stream
   * and create {@link java.nio.ByteBuffer} instance and set using this public method.
   * See {@link java.io.Externalizable#readExternal(ObjectInput)} abstract method implementation
   * on velocity template record.vm.
   * <p>
   * Note {@link java.io.Externalizable} extending means it is mandatory to have default public constructor.
   *
   * @param __g__dirty dirty bytes
   */
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
import org.apache.avro.SchemaNormalization;
  /**
   * Utility method by velocity templates to generate serialVersionUID on AVRO beans.
   *
   * @param schema Data bean AVRO schema.
   */
  public static long fingerprint64(Schema schema) {
    return SchemaNormalization.parsingFingerprint64(schema);
  }

  private static final long serialVersionUID = -6468893522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }
  private static final long serialVersionUID = -6468893522296148698L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

  private static final long serialVersionUID = -6468893522296178608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

  private static final long serialVersionUID = -6468894522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }
  private static final long serialVersionUID = -6468893522496148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

  private static final long serialVersionUID = -6468893522236148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

        Collection<T> {
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.ObjectInput;
import java.io.ObjectOutput;
    Persistent, java.io.Externalizable {
  /**
   * Exposing dirty bytes over public method. Purpose is to preserve dirty bytes content
   * while transporting AVRO data beans over TCP wire in serialized form.
   * Since {@link org.apache.gora.persistency.impl.PersistentBase} implements {@link java.io.Externalizable},
   * this method can be used to retrieve the dirty bytes as {@link java.nio.ByteBuffer} and and get the content
   * as bytes[] and write byte stream to the TCP wire.
   * See {@link java.io.Externalizable#writeExternal(ObjectOutput)} abstract method implementation
   * on velocity template record.vm.
   * <p>
   * Note {@link java.nio.ByteBuffer} is not itself not in serializable form.
   */
  public ByteBuffer getDirtyBytes() {

  /**
   * Setter method for assign dirty bytes when deserializing AVRO bean from dirty bytes
   * preserved in serialized bytes form.
   * Since {@link org.apache.gora.persistency.impl.PersistentBase} implements {@link java.io.Externalizable}
   * and when actual deserialization happens for {@link org.apache.gora.persistency.impl.PersistentBase}
   * new instance, acquire byte stream from TCP wire, extracting specific byte[] from byte stream
   * and create {@link java.nio.ByteBuffer} instance and set using this public method.
   * See {@link java.io.Externalizable#readExternal(ObjectInput)} abstract method implementation
   * on velocity template record.vm.
   * <p>
   * Note {@link java.io.Externalizable} extending means it is mandatory to have default public constructor.
   *
   * @param __g__dirty dirty bytes
   */
  public void setDirtyBytes(ByteBuffer __g__dirty) {
    this.__g__dirty = __g__dirty;
  }

  public static final String GORA_DEFAULT_CACHE_DATASTORE_KEY = "gora.cache.datastore.default";


  /**
   * Instantiate <i>the default</i> {@link DataStore} wrapped over JCache datastore which provides caching
   * abstraction over any GORA persistence dataStore.
   * Uses default properties. Uses 'null' schema.
   *
   * Note:
   *    consider that default dataStore is always visible
   *
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} to be used be the store.
   * @param isCacheEnabled caching enable
   * @return A new store instance.
   * @throws GoraException
   */
  @SuppressWarnings("unchecked")
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
          Class<K> keyClass, Class<T> persistent, Configuration conf, boolean isCacheEnabled) throws GoraException {
    Properties createProps = createProps();
    Class<? extends DataStore<K, T>> c;
    try {
      if (isCacheEnabled) {
        c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultCacheDataStore(createProps));
      } else {
        c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
      }
    } catch (Exception ex) {
      throw new GoraException(ex);
    }
    return createDataStore(c, keyClass, persistent, conf, createProps, null);
  }

  private static String getDefaultCacheDataStore(Properties properties) {
    return getProperty(properties, GORA_DEFAULT_CACHE_DATASTORE_KEY);
  }

  private static final long serialVersionUID = -7468893532296148608L;
  public static final org.apache.avro.Schema SCHEMA$ =
          new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write
            (this, org.apache.avro.io.EncoderFactory.get()
                    .directBinaryEncoder((java.io.OutputStream) out,
                            null));
  }

  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read
            (this, org.apache.avro.io.DecoderFactory.get()
                    .directBinaryDecoder((java.io.InputStream) in,
                            null));
  }

  private static final long serialVersionUID = -6468893532296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }
  private static final long serialVersionUID = -6468883532296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }
  private static final long serialVersionUID = -6468793522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }
package org.apache.gora.tutorial.log.generated;

  private static final long serialVersionUID = -6468893522296148608L;

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  private int current;
    this.current = 0;
    if (cacheKeySet.size() == 0) {
      return 1;
    }
    float progress = ((float) current / (float) cacheKeySet.size());
    return progress;
    this.current;
  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  private int current;
    this.current = 0;
    if (cacheKeySet.size() == 0) {
      return 1;
    }
    float progress = ((float) current / (float) cacheKeySet.size());
    return progress;
    this.current;
   *
   * @return __g__dirty dirty bytes
  /*This selects the default caching dataStore which wraps any GORA persistency dataStore*/
   * Instantiate <i>the default</i> {@link DataStore} wrapped over caching dataStore which provides caching
   * abstraction over the GORA persistence dataStore.
   * @param conf {@link Configuration} To be used be the store.
   * @param isCacheEnabled Caching enable or not.
   * @throws GoraException If cache or persistency dataStore initialization interrupted.
/**
 * {@link org.apache.gora.jcache.query.JCacheQuery} is the primary class
 * responsible for representing a cache manipulation query.
 */
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
/**
 * {@link org.apache.gora.jcache.query.JCacheResult} is the primary class
 * responsible for representing result set of a cache manipulation query
 * {@link org.apache.gora.jcache.query.JCacheQuery}
 */
  private static final Logger LOG = LoggerFactory.getLogger(JCacheResult.class);
    LOG.info("Results set pointer is now moved to key {}.", key);
/**
 * {@link org.apache.gora.jcache.store.JCacheCacheEntryListener} is the primary class
 * responsible for listening on {@link javax.cache.event.CacheEntryEvent} cache entry events
 * EG:- Creation, Removal, Expiry etc of entries on caches and trigger actions as specified.
 */
      LOG.info("Cache entry added on key {}.", event.getKey().toString());
      LOG.info("Cache entry removed on key {}.", event.getKey().toString());
      LOG.info("Cache entry updated set on key {}.", event.getKey().toString());
      LOG.warn("Cache entry expired on key {}.", event.getKey().toString());
/**
 * {@link org.apache.gora.jcache.store.JCacheCacheEntryListenerFactory} is the primary class
 * responsible for creating cache entry listeners which listens on {@link javax.cache.event.CacheEntryEvent}
 * cache entry events EG:- Creation, Removal, etc of keys on caches and trigger actions as specified.
 */
/**
 * {@link org.apache.gora.jcache.store.JCacheCacheFactoryBuilder} is a Generic Factory
 * builder that creates Factory instances which extends {@link javax.cache.configuration.Factory}
 */
/**
 * {@link org.apache.gora.jcache.store.JCacheCacheLoader} is the primary class
 * responsible for loading data beans from persistency dataStore to in memory cache.
 */
      LOG.info("Loaded data bean from persistent datastore on key {}.", key.toString());
      LOG.info("Loaded data bean from persistent datastore on key {}.", key.toString());
/**
 * {@link org.apache.gora.jcache.store.JCacheCacheLoaderFactory} is the primary class
 * responsible for creating cache loader {@link javax.cache.integration.CacheLoader} instances which itself
 * loads data beans from persistency dataStore to in memory cache.
 */
/**
 * {@link org.apache.gora.jcache.store.JCacheCacheWriter} is the primary class
 * responsible for writing data beans to persistency dataStore from in memory cache.
 */
    LOG.info("Written data bean to persistent datastore on key {}.", entry.getKey().toString());
    LOG.info("Deleted data bean from persistent datastore on key {}.", key.toString());
/**
 * {@link org.apache.gora.jcache.store.JCacheCacheWriterFactory} is the primary class
 * responsible for creating cache writer {@link javax.cache.integration.CacheWriter} instances which itself
 * writes data beans to persistency dataStore from in memory cache.
 */
import java.util.Iterator;
import java.util.Properties;
import com.hazelcast.client.HazelcastClient;
import com.hazelcast.client.config.ClientConfig;
import com.hazelcast.client.config.XmlClientConfigBuilder;
import com.hazelcast.config.Config;
import com.hazelcast.config.ClasspathXmlConfig;
import com.hazelcast.config.EvictionConfig;
import javax.cache.Cache;
/**
 * {@link org.apache.gora.jcache.store.JCacheStore} is the primary class
 * responsible for GORA CRUD operations on Hazelcast Caches. This class can be think
 * of as caching layer that can is wrapped over any persistency dataStore implementations
 * which extends {@link org.apache.gora.store.DataStore}.  This class delegates
 * most operations to it s persistency dataStore. Hazelcast cache implementation is based on
 * JCache JSR 107 specification.
 */
  private static final String GORA_DEFAULT_JCACHE_HAZELCAST_CONFIG_KEY = "gora.datastore.jcache.hazelcast.config";
  private static final String HAZELCAST_SERVER_CACHE_PROVIDER_IDENTIFIER = "Server";
      LOG.error("Couldn't initialize persistent DataStore.", ex);
    if (properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY)
            .contains(HAZELCAST_SERVER_CACHE_PROVIDER_IDENTIFIER)) {
      Config config = new ClasspathXmlConfig(properties.getProperty(GORA_DEFAULT_JCACHE_HAZELCAST_CONFIG_KEY));
      hazelcastInstance = Hazelcast.newHazelcastInstance(config);
    } else {
      try {
        ClientConfig config =
                new XmlClientConfigBuilder(properties.getProperty(GORA_DEFAULT_JCACHE_HAZELCAST_CONFIG_KEY)).build();
        hazelcastInstance = HazelcastClient.newHazelcastClient(config);
      } catch (IOException ex) {
        LOG.error("Couldn't locate the client side cache provider configuration.", ex);
      }
    }
      LOG.error("Couldn't initialize cache manager to bounded hazelcast instance.", ex);
                    null, true, true));
      if (manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) == null) {
        cache = manager.createCache(persistentClass.getSimpleName(),
                cacheConfig).unwrap(ICache.class);
      } else {
        cache = manager.getCache(super.getPersistentClass().getSimpleName(),
                keyClass, persistentClass).unwrap(ICache.class);
        this.populateLocalCacheEntrySet(cache.iterator());
      }
    LOG.info("Created schema on persistent store and initialized cache for persistent bean {}."
            , super.getPersistentClass().getSimpleName());
    LOG.info("Deleted schema on persistent store and destroyed cache for persistent bean {}."
            , super.getPersistentClass().getSimpleName());
      LOG.info("JCache Gora datastore deleled {} rows from Persistent datastore.", deletedRows);
      LOG.error("Exception occurred while deleting entries from JCache Gora datastore. Hence returning 0.", e);
      LOG.error("NPE occurred while executing the query for JCacheStore. Hence returning empty entry set.", npe);
      LOG.error("Exception occurred while partitioning the query based on Hazelcast partitions.", ex);
    LOG.info("Query is partitioned to {} number of partitions.", partitions.size());
      cache.close();
    hazelcastInstance.shutdown();
  private void populateLocalCacheEntrySet(Iterator<Cache.Entry<K, T>> cacheEntryIterator) {
    cacheEntryList.clear();
    while (cacheEntryIterator.hasNext()) {
      cacheEntryList.add(cacheEntryIterator.next().getKey());
    }
    LOG.info("Populated local cache entry set with respect to remote cache provider.");
  }

        CacheEntryRemovedListener<K, T>, CacheEntryUpdatedListener<K, T>,
        CacheEntryExpiredListener<K, T>, java.io.Serializable {
  private transient ConcurrentSkipListSet<K> cacheEntryList;
    //get rid execution of listener chain/executing only one initialized
    if (cacheEntryList == null) {
      return;
    }
    //get rid execution of listener chain/executing only one initialized
    if (cacheEntryList == null) {
      return;
    }
    //get rid execution of listener chain/executing only one initialized
    if (cacheEntryList == null) {
      return;
    }
    //get rid execution of listener chain/executing only one initialized
    if (cacheEntryList == null) {
      return;
    }
  public void setCacheEntryList(ConcurrentSkipListSet<K> cacheEntryList) {
    this.cacheEntryList = cacheEntryList;
  }

public class JCacheCacheEntryListenerFactory<K, T extends PersistentBase>
  private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheEntryListenerFactory.class);
  private JCacheCacheEntryListener<K, T> instance;
    if (this == other) {
    } else if (other != null && this.getClass() == other.getClass()) {
      JCacheCacheEntryListenerFactory that = (JCacheCacheEntryListenerFactory) other;

  public static <K, T extends PersistentBase> Factory<JCacheCacheLoader<K, T>>
  public static <K, T extends PersistentBase> Factory<JCacheCacheWriter<K, T>>
  public static <K, T extends PersistentBase> Factory<JCacheCacheEntryListener<K, T>>
public class JCacheCacheLoader<K, T extends PersistentBase> implements CacheLoader<K, T>, java.io.Serializable {
  private transient DataStore<K, T> dataStore;
    this.dataStore = dataStore;
  public void setDataStore(DataStore<K, T> dataStore) {
    this.dataStore = dataStore;
  }

        implements Factory<JCacheCacheLoader<K, T>> {
  private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheLoaderFactory.class);
  private JCacheCacheLoader<K, T> instance;
public class JCacheCacheWriter<K, T extends PersistentBase> implements CacheWriter<K, T>, java.io.Serializable {
  private transient DataStore<K, T> dataStore;
  public void setDataStore(DataStore<K, T> dataStore) {
    this.dataStore = dataStore;
  }

public class JCacheCacheWriterFactory<K, T extends PersistentBase> implements Factory<JCacheCacheWriter<K, T>> {
  private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheWriterFactory.class);
  private JCacheCacheWriter<K, T> instance;
  public JCacheCacheWriterFactory(JCacheCacheWriter<K, T> instance) {
  public JCacheCacheWriter<K, T> create() {
    return (JCacheCacheWriter<K, T>) this.instance;
import java.util.List;
import java.util.Properties;
import javax.cache.configuration.CacheEntryListenerConfiguration;
    CachingProvider cachingProvider = Caching.getCachingProvider
            (properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY));
    if (((properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY) != null) &&
            Boolean.valueOf(properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY)))
            || ((manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) == null))) {
      cacheEntryList = new ConcurrentSkipListSet<>();
      cacheConfig = new CacheConfig<K, T>();
      cacheConfig.setTypes(keyClass, persistentClass);
      if (properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY) != null) {
        cacheConfig.setReadThrough(Boolean.valueOf(properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY)));
        cacheConfig.setReadThrough(true);
      if (properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY) != null) {
        cacheConfig.setWriteThrough(Boolean.valueOf(properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY)));
      } else {
        cacheConfig.setWriteThrough(true);
      }
      if (properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY) != null) {
        cacheConfig.setStoreByValue(Boolean.valueOf(properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY)));
      }
      if (properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY) != null) {
        cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY)));
      }
      if (properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY) != null) {
        cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY)));
      }
      if (properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY) != null) {
        cacheConfig.getEvictionConfig()
                .setEvictionPolicy(EvictionPolicy.valueOf(properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY)));
      }
      if (properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY) != null) {
        cacheConfig.getEvictionConfig()
                .setMaximumSizePolicy(EvictionConfig.MaxSizePolicy
                        .valueOf(properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY)));
      }
      if (properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY) != null) {
        cacheConfig.getEvictionConfig()
                .setSize(Integer.valueOf(properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY)));
      }
      if (properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY) != null) {
        String expiryPolicyIdentifier = properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY);
        if (expiryPolicyIdentifier.equals(JCACHE_ACCESSED_EXPIRY_IDENTIFIER)) {
          cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                  new AccessedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                          Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
          ));
        } else if (expiryPolicyIdentifier.equals(JCACHE_CREATED_EXPIRY_IDENTIFIER)) {
          cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                  new CreatedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                          Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
          ));
        } else if (expiryPolicyIdentifier.equals(JCACHE_MODIFIED_EXPIRY_IDENTIFIER)) {
          cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                  new ModifiedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                          Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
          ));
        } else if (expiryPolicyIdentifier.equals(JCACHE_TOUCHED_EXPIRY_IDENTIFIER)) {
          cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
                  new TouchedExpiryPolicy(new Duration(TimeUnit.SECONDS,
                          Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
          ));
        }
      }
      if (properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY) != null) {
        String inMemoryFormat = properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY);
        if (inMemoryFormat.equals(HAZELCAST_CACHE_BINARY_IN_MEMORY_FORMAT_IDENTIFIER) ||
                inMemoryFormat.equals(HAZELCAST_CACHE_OBJECT_IN_MEMORY_FORMAT_IDENTIFIER) ||
                inMemoryFormat.equals(HAZELCAST_CACHE_NATIVE_IN_MEMORY_FORMAT_IDENTIFIER)) {
          cacheConfig.setInMemoryFormat(InMemoryFormat.valueOf(inMemoryFormat));
        }
      }
      cacheConfig.setCacheLoaderFactory(JCacheCacheFactoryBuilder
              .factoryOfCacheLoader(this.persistentDataStore));
      cacheConfig.setCacheWriterFactory(JCacheCacheFactoryBuilder
              .factoryOfCacheWriter(this.persistentDataStore));
      cacheConfig.addCacheEntryListenerConfiguration(
              new MutableCacheEntryListenerConfiguration<>(
                      JCacheCacheFactoryBuilder
                              .factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
                      null, true, true));
      cache = manager.createCache(persistentClass.getSimpleName(),
              cacheConfig).unwrap(ICache.class);
    } else {
      cache = manager.getCache(super.getPersistentClass().getSimpleName(),
              keyClass, persistentClass).unwrap(ICache.class);
      this.populateLocalCacheEntrySet(cache);
      this.populateLocalCacheConfig(cache);
    cache.removeAll();
  private void populateLocalCacheEntrySet(ICache<K, T> cache) {
    cacheEntryList = new ConcurrentSkipListSet<>();
    Iterator<Cache.Entry<K, T>> cacheEntryIterator = cache.iterator();
    cacheConfig = cache.getConfiguration(CacheConfig.class);
    Iterator<CacheEntryListenerConfiguration<K, T>> itr =
            cacheConfig.getCacheEntryListenerConfigurations().iterator();
    while (itr.hasNext()) {
      JCacheCacheEntryListenerFactory<K, T> listenerFac = (JCacheCacheEntryListenerFactory<K, T>)
              ((MutableCacheEntryListenerConfiguration) itr.next()).getCacheEntryListenerFactory();
      //populate transient field in Cache Entry Listener
      listenerFac.create().setCacheEntryList(cacheEntryList);
      //register exactly one listener on each local node either client/server
      break;
    }
  private void populateLocalCacheConfig(ICache<K, T> cache) {
    cacheConfig = cache.getConfiguration(CacheConfig.class);
    //populate transient fields in Cache Loader/Cache Listener
    ((JCacheCacheLoaderFactory) cacheConfig.getCacheLoaderFactory())
            .create().setDataStore(this.persistentDataStore);
    ((JCacheCacheWriterFactory) cacheConfig.getCacheWriterFactory())
            .create().setDataStore(this.persistentDataStore);
    LOG.info("Populated transient cache loader/writer in local cache configuration.");
  }

 *      direct retrieved from persistent data store. 
 *      direct retrieved from persistent data store.
        CacheEntryExpiredListener<K, T> {
  private ConcurrentSkipListSet<K> cacheEntryList;
  private transient JCacheCacheEntryListener<K, T> instance;
  factoryOfCacheLoader(DataStore<K, T> dataStore, Class<K> keyClass, Class<T> persistentClass) {
    return new JCacheCacheLoaderFactory<>(new JCacheCacheLoader<>(dataStore), keyClass, persistentClass);
  factoryOfCacheWriter(DataStore<K, T> dataStore, Class<K> keyClass, Class<T> persistentClass) {
    return new JCacheCacheWriterFactory<>(new JCacheCacheWriter<>(dataStore), keyClass, persistentClass);
public class JCacheCacheLoader<K, T extends PersistentBase> implements CacheLoader<K, T> {
  private DataStore<K, T> dataStore;
import org.apache.gora.store.DataStoreFactory;
import org.apache.gora.util.GoraException;
import org.apache.hadoop.conf.Configuration;
  private transient JCacheCacheLoader<K, T> instance;
  private Class<K> keyClass;
  private Class<T> persistentClass;
  public JCacheCacheLoaderFactory(JCacheCacheLoader<K, T> instance,
                                  Class<K> keyClass,
                                  Class<T> persistentClass) {
    this.keyClass = keyClass;
    this.persistentClass = persistentClass;
    if (this.instance != null) {
      return (JCacheCacheLoader<K, T>) this.instance;
    } else {
      try {
        this.instance = new JCacheCacheLoader<>(DataStoreFactory
                .getDataStore(keyClass, persistentClass, new Configuration()));
      } catch (GoraException ex) {
        LOG.error("Couldn't initialize persistent dataStore for cache loader.", ex);
        return null;
      }
      return (JCacheCacheLoader<K, T>) this.instance;
    }
public class JCacheCacheWriter<K, T extends PersistentBase> implements CacheWriter<K, T> {
  private DataStore<K, T> dataStore;
import org.apache.gora.store.DataStoreFactory;
import org.apache.gora.util.GoraException;
import org.apache.hadoop.conf.Configuration;
  private transient JCacheCacheWriter<K, T> instance;
  private Class<K> keyClass;
  private Class<T> persistentClass;
  public JCacheCacheWriterFactory(JCacheCacheWriter<K, T> instance,
                                  Class<K> keyClass,
                                  Class<T> persistentClass) {
    this.keyClass = keyClass;
    this.persistentClass = persistentClass;
    if (this.instance != null) {
      return (JCacheCacheWriter<K, T>) this.instance;
    } else {
      try {
        this.instance = new JCacheCacheWriter<>(DataStoreFactory
                .getDataStore(keyClass, persistentClass, new Configuration()));
      } catch (GoraException ex) {
        LOG.error("Couldn't initialize persistent dataStore for cache writer.", ex);
        return null;
      }
      return (JCacheCacheWriter<K, T>) this.instance;
    }
              .factoryOfCacheLoader(this.persistentDataStore, keyClass, persistentClass));
              .factoryOfCacheWriter(this.persistentDataStore, keyClass, persistentClass));
    cache.registerCacheEntryListener(new MutableCacheEntryListenerConfiguration<>(
            JCacheCacheFactoryBuilder
                    .factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
            null, true, true));
    cache.registerCacheEntryListener(new MutableCacheEntryListenerConfiguration<>(
            JCacheCacheFactoryBuilder
                    .factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
            null, true, true));
 * exposed over the JCache store/distributed caching layer.
 * <p>
 * <p>
 * 1. Server mode - Participate in Hazelcast cluster as a member. ( DATA GRID ) and communicates directly
 * <p>
 * <p>
 * <b>gora.cache.datastore.default=org.apache.gora.jcache.store.JCacheStore</b>
 * <br><b>gora.datastore.jcache.provider=com.hazelcast.cache.impl.HazelcastServerCachingProvider</b>
 * <br><b>gora.datastore.jcache.hazelcast.config=hazelcast.xml</b>
 * <p>
 * official documentation</a> for more information.</p>
 * <p>
 * 2. Client mode - DOES not participate in Hazelcast cluster as a member. ( DATA GRID ) and For cache
 * <p>
 * <p>
 * <b>gora.cache.datastore.default=org.apache.gora.jcache.store.JCacheStore</b>
 * <br><b>gora.datastore.jcache.provider=com.hazelcast.client.cache.impl.HazelcastClientCachingProvider</b>
 * <br><b>gora.datastore.jcache.hazelcast.config=hazelcast-client.xml</b>
 * <p>
 * official documentation</a> for more information.</p>
 * <p>
 * <p>
 * 1. Start DistributedLogManager in SERVER mode for two or higher instances. ( separate JVMs )
 * <br><b>Members [2] {
 * }</b>
 * <p>
 * 2. Start DistributedLogManager in CLIENT mode for one instance.
 * <br><b>Members [2] {
 * }</b>
 * <br><b>INFO: HazelcastClient[hz.client_0_dev][3.6.4] is CLIENT_CONNECTED</b>
 * <p>
 * <p>
 *  (a) <b>-parse cache <-input_log_file-></b> - This will parse logs from logs file and put Pageview data beans to
 *      persistent store via the cache. Notice following logs
 *      <br><b>INFO 19:46:34,833 Written data bean to persistent datastore on key 45.</b>
 *      <br>on SERVER instance of DistributedLogManager.
 *      <br>Notice the persistent data bean writes are LOAD BALANCED among SERVER instances.
 * <p>
 *  (b) <b>-parse persistent <-input_log_file-></b> - This will write parsed log data beans directly to persistent store.
 * <p>
 * <p>
 *      <br><b>get <-lineNum-></b>
 *      <br>Data will be first loaded from persistent store to cache from one of SERVER instances. Then cache
 *      <br><b>INFO 17:13:22,652 Loaded data bean from persistent datastore on key 4.</b>
 *      <br>Notice the cache entry creation on ALL SERVER/CLIENT instances
 *      <br><b>INFO 17:13:22,656 Cache entry added on key 4.</b>
 *      <br>Once the cache entry is created, data bean is now available to be retrieved from cache without reaching the
 *      <br><b>-get <-lineNum-></b>
 *      <br><b>INFO 17:13:22,652 Loaded data bean from persistent datastore on key 4.</b>
 *      <br>Since there will be no data bean load from persistent data store and the data bean is now loaded from
 * <p>
 *      <br><b>-benchmark <-startLineNum-> <-endLineNum-> <-iterations-></b>
 *      <br>to compare data beans read for two cases. ( Cache layer is present and Not present when executing
 *      <br><b>INFO 17:13:22,652 Direct Backend took 1973 ms</b>
 *      <br><b>INFO 17:18:49,252 Via Cache took 1923 ms</b>
 * <p>
 *  (f) For standalone/single node DistributedLogManager usage, Start the DistributedLogManager in SERVER mode.
 *      Follow the the commands over the command line console of SERVER instance.
 * <p>
 * In the data model, keys are the line numbers in the log file,
 * web site</a> for more information.</p>
 *  (f) For standalone/single node DistributedLogManager usage, start the DistributedLogManager in SERVER mode.
 *      Follow the same commands as above over the command line console of SERVER instance.
import org.apache.gora.util.IOUtils;
          final SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
          final byte[] byteData = IOUtils.serialize(writer,o);
          m.put(col.getFirst(), col.getSecond(), new Value(byteData));
    final SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(persistent.getSchema());
    final byte[] byteData;
      byteData = IOUtils.serialize(writer, persistent);

    final SpecificDatumReader<T> reader = new SpecificDatumReader<>((Class<T>) persistent.getClass());
      return IOUtils.deserialize(byteData, reader, null);
  private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy.duration";
  private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy.duration";
 * This class is implemantation of {@link org.ektorp.impl.ObjectMapperFactory}.
 * Created a object mapper instance.
   *
 *  Accumulo specific implementation of the {@link org.apache.gora.query.Query} interface.

  /**
   * Constructor for the query
   */
  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */
 * Accumulo specific implementation of the {@link org.apache.gora.query.Result} interface.
  /**
   * Gets the data store used
   */

  /**
   * Gets the items reading progress
   */

  /**
   * Gets the next item
   */
/**
 * Mapping definitions for Accumulo.
 */

  /**
   * A map of field names to Field objects containing schema's fields
   */

  /**
   * Look up the column associated to the Avro field.
   */

 * Implementation of a Accumulo data store to be used by gora.
 *
 * @param <K> class to be used for the key
 * @param <T> class to be persisted within the store
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
  /**
   * Execute the query and return the result.
   */
/**
 * It is a implementation of {@link java.io.OutputStream} must always provide at least a method that writes one byte of output.
 */
/**
 * Cassandra specific implementation of the {@link Query} interface.
 */
  /**
   * Cassandra specific implementation of the {@link Query} interface.
   */

  /**
   * Constructor for the query
   */

  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */

  /**
   * Setter of familyMap.
   */

  /**
   * Getter of familyMap.
   */

  /**
   *
   * @return get {@link org.apache.gora.cassandra.query.CassandraQuery}
   */

  /**
   *
   * @return set a {@link org.apache.gora.cassandra.query.CassandraQuery}
   */
/**
 * CassandraResult specific implementation of the {@link org.apache.gora.query.Result}
 * interface.
 */
  /**
   * Result set containing query results
   */

  /**
   * Constructor for the result set
   *
   * @param dataStore Data store used
   * @param query     Query used
   */
  /**
   * Gets the next item
   */
  /**
   * Gets the items reading progress
   */
  /**
   * Set the Result set containing query results
   *
   * @param cassandraResultSet
   */

/**
 * Mapping definitions for CouchDB.
 */
/**
 * A builder for creating the mapper.
 */
   * @param dest where the data bean will be written.
   * Method in charge of compiling a specific table using a key schema and a set
   *
   * @param pTableName
   * @param arrayList
   * @param map
   *
   * @param pItems The items belonging to the table
   * @param pIden  The number of spaces used for identation
   *
   * @param pItemName Item's name
   * @param pItemType Item's type
   * @param pIden     Number of spaces used for indentation
   * Creates key getters and setters
   *
   * @param pKeySchema The key schema for a specific table
   * @param pIden      Number of spaces used for indentation
   *
   * @param s String to be camelcasified
   *
   * @param name  Class name
   * @param space spacing
   *
   * @param indent Number of spaces used for indentation
   * @param text   Text to be written
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
   *
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
import org.apache.gora.query.Query;
/**
 * Infinispan specific implementation of the {@link Query} interface.
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
  /**
   * Execute the query and return the result.
   */
   * @param params This value should specify the host:port (at least one) for
   *               connecting to remote MongoDB.
 * Configuration properties
 *
/**
 * BSON encoder for BSONObject instances.
 */
import org.apache.gora.query.Query;
/**
 * Solr specific implementation of the {@link Query} interface.
 */
  /**
   * Constructor for the query
   */

  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */

  /**
   * Create a solr query
   *
   * @return the solr query string
   */
/**
 * SolrResult specific implementation of the {@link org.apache.gora.query.Result}
 * interface.
 */
  /**
   * Constructor for the result set
   *
   * @param dataStore Data store used
   * @param query     Query used
   * @param server    A client that talks directly to a Solr server
   * @param resultsSize  The number of rows to be returned
   */
  /**
   * Gets the next item
   */
  /**
   * Gets the items reading progress
   */
/**
 * Mapping definitions for Solr.
 */
/**
 * Implementation of a Solr data store to be used by gora.
 *
 * @param <K> class to be used for the key
 * @param <T> class to be persisted within the store
 */
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
    final Map<String, Object>  result;
        partition.setConf(this.getConf());
        partition.setConf(this.getConf());
 * Base class implementing common functionality for Web services
 * This package contains all the unit tests for basic CRUD operations
 * functionality of the CouchDB dataStore.
 * and all the unit tests for basic CRUD operations functionality of the
 * DynamoDB dataStore.
   * Utility method used by velocity templates to generate serialVersionUID on AVRO beans.
   * @return serialVersionUID for Serializable AVRO databeans.
 *  (a) <b>-parse cache #input_log_file</b> - This will parse logs from logs file and put Pageview data beans to
 *  (b) <b>-parse persistent #input_log_file</b> - This will write parsed log data beans directly to persistent store.
 *      <br><b>-get #lineNum</b>
 *      <br><b>-get #lineNum</b>
 *      <br><b>-benchmark #startLineNum #endLineNum #iterations</b>
   * @param query set a {@link org.apache.gora.cassandra.query.CassandraQuery}

   * Method to maintain backward compatibility with earlier versions.
   * @param keyClass
   * @param persistentClass
   * @throws Exception
   */
   *
   * @return if keyspace already exists return true.
 * Base class implementing common functionality for Web services
 * This package contains all the unit tests for basic CRUD operations
 * functionality of the CouchDB dataStore.
 * and all the unit tests for basic CRUD operations functionality of the
 * DynamoDB dataStore.
   * Utility method used by velocity templates to generate serialVersionUID on AVRO beans.
   * @return serialVersionUID for Serializable AVRO databeans.
 *  (a) <b>-parse cache #input_log_file</b> - This will parse logs from logs file and put Pageview data beans to
 *  (b) <b>-parse persistent #input_log_file</b> - This will write parsed log data beans directly to persistent store.
 *      <br><b>-get #lineNum</b>
 *      <br><b>-get #lineNum</b>
 *      <br><b>-benchmark #startLineNum #endLineNum #iterations</b>
   * @param query set a {@link org.apache.gora.cassandra.query.CassandraQuery}

   * Method to maintain backward compatibility with earlier versions.
   * @param keyClass
   * @param persistentClass
   * @throws Exception
   */
   *
   * @return if keyspace already exists return true.
 * Different from the @see org.apache.gora.compiler.GoraCompiler,
 * Different from the @see org.apache.gora.compiler.GoraCompiler,
import org.apache.hadoop.hbase.client.*;
  private volatile Admin admin;

      admin = ConnectionFactory.createConnection(getConf()).getAdmin();
      admin.disableTable(mapping.getTable().getTableName());
      admin.deleteTable(mapping.getTable().getTableName());
      return admin.tableExists(mapping.getTable().getTableName());

//        table.delete(delete);
//        table.delete(delete); // HBase sometimes does not delete arbitrarily
      }
      if (put.size() > 0) {
        table.put(put);
//          delete.deleteFamily(hcol.getFamily());
          delete.addFamily(hcol.getFamily());
//          delete.deleteColumn(hcol.getFamily(), qualifier);
          delete.addColumn(hcol.getFamily(), qualifier);
          put.addColumn(hcol.getFamily(), qualifier, serializedBytes);
        //delete.deleteFamily(hcol.getFamily());
        delete.addFamily(hcol.getFamily());
        //delete.deleteColumn(hcol.getFamily(), qualifier);
        delete.addColumn(hcol.getFamily(), qualifier);
      put.addColumn(hcol.getFamily(), qualifier, serializedBytes);
import java.util.ArrayList;
import java.util.concurrent.ConcurrentLinkedDeque;
import java.util.concurrent.ConcurrentLinkedQueue;
import org.apache.hadoop.hbase.client.*;
public class HBaseTableConnection {
   * HTablePool for maintaining a tPool of tables, but there are still some
   *

  private final Connection connection;
  private final RegionLocator regionLocator;
  // BufferedMutator used for doing async flush i.e. autoflush = false
  private final ThreadLocal<ConcurrentLinkedQueue<Mutation>> buffers;
  private final ThreadLocal<Table> tables;

  private final BlockingQueue<Table> tPool = new LinkedBlockingQueue<>();
  private final BlockingQueue<ConcurrentLinkedQueue<Mutation>> bPool = new LinkedBlockingQueue<>();

//  public class MutationPair {
//    private Mutation mutation;
//    private boolean type;
//
//    public void MutationPair(Mutation m, boolean t) {
//      this.mutation = m;
//      this.type = t;
//    }
//
//    public boolean isType() {
//      return type;
//    }
//
//    public Mutation getMutation() {
//      return mutation;
//    }
//  }
   *

    this.buffers = new ThreadLocal<>();
    this.connection = ConnectionFactory.createConnection(conf);
    this.regionLocator = this.connection.getRegionLocator(this.tableName);


  private Table getTable() throws IOException {
    Table table = tables.get();
      table = connection.getTable(tableName);
//      table.setAutoFlushTo(autoFlush);
      tPool.add(table); //keep track

  private ConcurrentLinkedQueue<Mutation> getBuffer() throws IOException {
    ConcurrentLinkedQueue<Mutation> buffer = buffers.get();
    if (buffer == null) {
//      BufferedMutatorParams params = new BufferedMutatorParams(this.tableName).listener(listener);
//      buffer = connection.getBufferedMutator(this.tableName);
      buffer = new ConcurrentLinkedQueue<>();
      bPool.add(buffer);
      buffers.set(buffer);
    }
    return buffer;
  }

  public void flushCommits() throws IOException {
    BufferedMutator bufMutator = connection.getBufferedMutator(this.tableName);
    for (ConcurrentLinkedQueue<Mutation> buffer : bPool) {
      for (Mutation m: buffer) {
        bufMutator.mutate(m);
        bufMutator.flush();
      }
    }
    bufMutator.close();
  }

    flushCommits();

    for (Table table : tPool) {
   * getStartEndKeys provided by {@link HRegionLocation}.
   * @see RegionLocator#getStartEndKeys()
    return regionLocator.getStartEndKeys();
   * getRegionLocation provided by {@link HRegionLocation}
   * @see RegionLocator#getRegionLocation(byte[])
    return regionLocator.getRegionLocation(bs);
  public boolean[] existsAll(List<Get> list) throws IOException {
    return getTable().existsAll(list);
  }

    getBuffer().add(put);
//    getBuffer().flush();
//    getTable().put(put);
//  @Override
//    getTable().put(puts);
    getBuffer().addAll(puts);
//    getBuffer().flush();
//  @Override
    getBuffer().add(delete);
//    getBuffer().flush();
//    getTable().delete(delete);
//  @Override
//    getTable().delete(deletes);
    getBuffer().addAll(deletes);
//    getBuffer().flush();
 *  Accumulo specific implementation of the {@link org.apache.gora.query.Query} interface.

  /**
   * Constructor for the query
   */
  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */
 * Accumulo specific implementation of the {@link org.apache.gora.query.Result} interface.
  /**
   * Gets the data store used
   */

  /**
   * Gets the items reading progress
   */

  /**
   * Gets the next item
   */
/**
 * Mapping definitions for Accumulo.
 */

  /**
   * A map of field names to Field objects containing schema's fields
   */

  /**
   * Look up the column associated to the Avro field.
   */

 * Implementation of a Accumulo data store to be used by gora.
 *
 * @param <K> class to be used for the key
 * @param <T> class to be persisted within the store
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
  /**
   * Execute the query and return the result.
   */
/**
 * It is a implementation of {@link java.io.OutputStream} must always provide at least a method that writes one byte of output.
 */
/**
 * Cassandra specific implementation of the {@link Query} interface.
 */
  /**
   * Cassandra specific implementation of the {@link Query} interface.
   */

  /**
   * Constructor for the query
   */

  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */

  /**
   * Setter of familyMap.
   */

  /**
   * Getter of familyMap.
   */

  /**
   *
   * @return get {@link org.apache.gora.cassandra.query.CassandraQuery}
   */

  /**
   *
   * @param query set a {@link org.apache.gora.cassandra.query.CassandraQuery}
   */
/**
 * CassandraResult specific implementation of the {@link org.apache.gora.query.Result}
 * interface.
 */
  /**
   * Result set containing query results
   */

  /**
   * Constructor for the result set
   *
   * @param dataStore Data store used
   * @param query     Query used
   */
  /**
   * Gets the next item
   */
  /**
   * Gets the items reading progress
   */
  /**
   * Set the Result set containing query results
   *
   * @param cassandraResultSet
   */


   * Method to maintain backward compatibility with earlier versions.
   * @param keyClass
   * @param persistentClass
   * @throws Exception
   */
   *
   * @return if keyspace already exists return true.
/**
 * Mapping definitions for CouchDB.
 */
/**
 * A builder for creating the mapper.
 */
   * Utility method used by velocity templates to generate serialVersionUID on AVRO beans.
   * @return serialVersionUID for Serializable AVRO databeans.
 * Base class implementing common functionality for Web services
 * Different from the @see org.apache.gora.compiler.GoraCompiler,
   * @param dest where the data bean will be written.
   * Method in charge of compiling a specific table using a key schema and a set
   *
   * @param pTableName
   * @param arrayList
   * @param map
   *
   * @param pItems The items belonging to the table
   * @param pIden  The number of spaces used for identation
   *
   * @param pItemName Item's name
   * @param pItemType Item's type
   * @param pIden     Number of spaces used for indentation
   * Creates key getters and setters
   *
   * @param pKeySchema The key schema for a specific table
   * @param pIden      Number of spaces used for indentation
   *
   * @param s String to be camelcasified
   *
   * @param name  Class name
   * @param space spacing
   *
   * @param indent Number of spaces used for indentation
   * @param text   Text to be written
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
   *
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
import org.apache.gora.query.Query;
/**
 * Infinispan specific implementation of the {@link Query} interface.
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
  /**
   * Execute the query and return the result.
   */
  private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy.duration";
        partition.setConf(this.getConf());
   * @param params This value should specify the host:port (at least one) for
   *               connecting to remote MongoDB.
 * Configuration properties
 *
/**
 * BSON encoder for BSONObject instances.
 */
import org.apache.gora.query.Query;
/**
 * Solr specific implementation of the {@link Query} interface.
 */
  /**
   * Constructor for the query
   */

  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */

  /**
   * Create a solr query
   *
   * @return the solr query string
   */
/**
 * SolrResult specific implementation of the {@link org.apache.gora.query.Result}
 * interface.
 */
  /**
   * Constructor for the result set
   *
   * @param dataStore Data store used
   * @param query     Query used
   * @param server    A client that talks directly to a Solr server
   * @param resultsSize  The number of rows to be returned
   */
  /**
   * Gets the next item
   */
  /**
   * Gets the items reading progress
   */
/**
 * Mapping definitions for Solr.
 */
/**
 * Implementation of a Solr data store to be used by gora.
 *
 * @param <K> class to be used for the key
 * @param <T> class to be persisted within the store
 */
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
 *  (a) <b>-parse cache #input_log_file</b> - This will parse logs from logs file and put Pageview data beans to
 *  (b) <b>-parse persistent #input_log_file</b> - This will write parsed log data beans directly to persistent store.
 *      <br><b>-get #lineNum</b>
 *      <br><b>-get #lineNum</b>
 *      <br><b>-benchmark #startLineNum #endLineNum #iterations</b>
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
      LOG.error("Can not load {} from gora.properties. Setting to default value: {}.", SCANNER_CACHING_PROPERTIES_KEY, SCANNER_CACHING_PROPERTIES_DEFAULT);
    closeHBaseAdmin();
    closeHBaseAdmin();
    closeHBaseAdmin();
      return newInstance(result, fields);
      delete.addFamily(col.family);
      delete.addColumn(col.family, col.qualifier);
            LOG.warn("Mismatching schema's names. Mappingfile schema: '{}'. PersistentClass schema's name: '{}'. Assuming they are the same.", tableNameFromMapping, tableName);
      LOG.warn("Invalid Scanner Caching optimization value. Cannot set to: {}.", numRows) ;
  
  private void closeHBaseAdmin(){
    try {
      admin.close();
    } catch (IOException ioe) {
      LOG.error("An error occured whilst closing HBase Admin", ioe);
    }
  }
import org.apache.hadoop.hbase.client.BufferedMutator;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Mutation;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.RegionLocator;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.Table;
  @SuppressWarnings("unused")
    bufMutator.flush();
    try (DataInputBuffer buffer = new DataInputBuffer()) {
      buffer.reset(in, in.length);
      return deserialize(conf, buffer, obj);
    }
  public synchronized void createSchema() {
 * Autogenerated by Avro
 * DO NOT EDIT DIRECTLY
package org.apache.gora.examples.generated;
@SuppressWarnings("all")
          "name",
          "dateOfBirth",
          "ssn",
          "salary",
          "boss",
          "webpage",
  // Used by DatumWriter.  Applications should not call.
      case 0: return name;
      case 1: return dateOfBirth;
      case 2: return ssn;
      case 3: return salary;
      case 4: return boss;
      case 5: return webpage;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
  @SuppressWarnings(value="unchecked")
      case 0: name = (java.lang.CharSequence)(value); break;
      case 1: dateOfBirth = (java.lang.Long)(value); break;
      case 2: ssn = (java.lang.CharSequence)(value); break;
      case 3: salary = (java.lang.Integer)(value); break;
      case 4: boss = (java.lang.Object)(value); break;
      case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");









          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Employee> {


      super(org.apache.gora.examples.generated.Employee.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'name' field.
     */
    public java.lang.CharSequence getName() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'name' field.
     * @param value the value to set.
     */
    public void setName(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isNameDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'dateOfBirth' field.
     */
    public java.lang.Long getDateOfBirth() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'dateOfBirth' field.
     * @param value the value to set.
     */
    public void setDateOfBirth(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isDateOfBirthDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'ssn' field.
     */
    public java.lang.CharSequence getSsn() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'ssn' field.
     * @param value the value to set.
     */
    public void setSsn(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isSsnDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'salary' field.
     */
    public java.lang.Integer getSalary() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'salary' field.
     * @param value the value to set.
     */
    public void setSalary(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isSalaryDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'boss' field.
     */
    public java.lang.Object getBoss() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'boss' field.
     * @param value the value to set.
     */
    public void setBoss(java.lang.Object value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isBossDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'webpage' field.
     */
    public org.apache.gora.examples.generated.WebPage getWebpage() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'webpage' field.
     * @param value the value to set.
     */
    public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isWebpageDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
package org.apache.gora.examples.generated;
  private static final long serialVersionUID = -7621464515167921131L;
          "v1",
          "v2",
  // Used by DatumWriter.  Applications should not call.
      case 0: return v1;
      case 1: return v2;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: v1 = (java.lang.Integer)(value); break;
      case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");





          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<ImmutableFields> {


      super(org.apache.gora.examples.generated.ImmutableFields.SCHEMA$);

      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'v1' field.
     */
    public java.lang.Integer getV1() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v1' field.
     * @param value the value to set.
     */
    public void setV1(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v1' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV1Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'v2' field.
     */
    public org.apache.gora.examples.generated.V2 getV2() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v2' field.
     * @param value the value to set.
     */
    public void setV2(org.apache.gora.examples.generated.V2 value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v2' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV2Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 *
package org.apache.gora.examples.generated;
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");
  private static final long serialVersionUID = -7097391446015721734L;
          "version",
          "data",
  // Used by DatumWriter.  Applications should not call.
      case 0: return version;
      case 1: return data;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: version = (java.lang.Integer)(value); break;
      case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");





          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Metadata> {


      super(org.apache.gora.examples.generated.Metadata.SCHEMA$);

      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'version' field.
     */
    public java.lang.Integer getVersion() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'version' field.
     * @param value the value to set.
     */
    public void setVersion(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isVersionDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'data' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'data' field.
     * @param value the value to set.
     */
    public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


package org.apache.gora.examples.generated;
  private static final long serialVersionUID = -4481652577902636424L;
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: count = (java.lang.Integer)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");




          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<TokenDatum> {


      super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);

      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Integer getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
package org.apache.gora.examples.generated;
  private static final long serialVersionUID = -6538763924317658547L;
          "v3",
  // Used by DatumWriter.  Applications should not call.
      case 0: return v3;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: v3 = (java.lang.Integer)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");




          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<V2> {


      super(org.apache.gora.examples.generated.V2.SCHEMA$);

      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'v3' field.
     */
    public java.lang.Integer getV3() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v3' field.
     * @param value the value to set.
     */
    public void setV3(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v3' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV3Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 *
package org.apache.gora.examples.generated;
  private static final long serialVersionUID = -2829100587222969501L;
          "url",
          "content",
          "parsedContent",
          "outlinks",
          "headers",
          "metadata",
          "byteData",
          "stringData",
  // Used by DatumWriter.  Applications should not call.
      case 0: return url;
      case 1: return content;
      case 2: return parsedContent;
      case 3: return outlinks;
      case 4: return headers;
      case 5: return metadata;
      case 6: return byteData;
      case 7: return stringData;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: url = (java.lang.CharSequence)(value); break;
      case 1: content = (java.nio.ByteBuffer)(value); break;
      case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
      case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
      case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
      case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");











          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<WebPage> {


      super(org.apache.gora.examples.generated.WebPage.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'url' field.
     */
    public java.lang.CharSequence getUrl() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'url' field.
     * @param value the value to set.
     */
    public void setUrl(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUrlDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'content' field.
     */
    public java.nio.ByteBuffer getContent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'content' field.
     * @param value the value to set.
     */
    public void setContent(java.nio.ByteBuffer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isContentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'parsedContent' field.
     */
    public java.util.List<java.lang.CharSequence> getParsedContent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'parsedContent' field.
     * @param value the value to set.
     */
    public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isParsedContentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'outlinks' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'outlinks' field.
     * @param value the value to set.
     */
    public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isOutlinksDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'headers' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'headers' field.
     * @param value the value to set.
     */
    public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHeadersDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'metadata' field.
     */
    public org.apache.gora.examples.generated.Metadata getMetadata() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metadata' field.
     * @param value the value to set.
     */
    public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetadataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'byteData' field.
     */
    public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'byteData' field.
     * @param value the value to set.
     */
    public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'byteData' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isByteDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'stringData' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'stringData' field.
     * @param value the value to set.
     */
    public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'stringData' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isStringDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 *
package org.apache.gora.goraci.generated;
  private static final long serialVersionUID = 1014651356631895518L;
          "prev",
          "client",
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return prev;
      case 1: return client;
      case 2: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: prev = (java.lang.Long)(value); break;
      case 1: client = (java.lang.CharSequence)(value); break;
      case 2: count = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<CINode> {


      super(org.apache.gora.goraci.generated.CINode.SCHEMA$);

      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'prev' field.
     */
    public java.lang.Long getPrev() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'prev' field.
     * @param value the value to set.
     */
    public void setPrev(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'prev' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isPrevDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'client' field.
     */
    public java.lang.CharSequence getClient() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'client' field.
     * @param value the value to set.
     */
    public void setClient(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'client' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isClientDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Long getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
 *
package org.apache.gora.goraci.generated;
  private static final long serialVersionUID = -8888031915401438521L;
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: count = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Flushed> {


      super(org.apache.gora.goraci.generated.Flushed.SCHEMA$);

      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Long getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
 *
package org.apache.gora.tutorial.log.generated;
@SuppressWarnings("all")
  private static final long serialVersionUID = 8278557845311856507L;
          "metricDimension",
          "timestamp",
          "metric",
  // Used by DatumWriter.  Applications should not call.
      case 0: return metricDimension;
      case 1: return timestamp;
      case 2: return metric;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: metricDimension = (java.lang.CharSequence)(value); break;
      case 1: timestamp = (java.lang.Long)(value); break;
      case 2: metric = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<MetricDatum> {


      super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);

      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'metricDimension' field.
     */
    public java.lang.CharSequence getMetricDimension() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metricDimension' field.
     * @param value the value to set.
     */
    public void setMetricDimension(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetricDimensionDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'timestamp' field.
     */
    public java.lang.Long getTimestamp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'timestamp' field.
     * @param value the value to set.
     */
    public void setTimestamp(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isTimestampDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'metric' field.
     */
    public java.lang.Long getMetric() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metric' field.
     * @param value the value to set.
     */
    public void setMetric(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetricDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
 *
  private static final long serialVersionUID = -6136058768384995982L;
          "url",
          "timestamp",
          "ip",
          "httpMethod",
          "httpStatusCode",
          "responseSize",
          "referrer",
          "userAgent",
  // Used by DatumWriter.  Applications should not call.
      case 0: return url;
      case 1: return timestamp;
      case 2: return ip;
      case 3: return httpMethod;
      case 4: return httpStatusCode;
      case 5: return responseSize;
      case 6: return referrer;
      case 7: return userAgent;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: url = (java.lang.CharSequence)(value); break;
      case 1: timestamp = (java.lang.Long)(value); break;
      case 2: ip = (java.lang.CharSequence)(value); break;
      case 3: httpMethod = (java.lang.CharSequence)(value); break;
      case 4: httpStatusCode = (java.lang.Integer)(value); break;
      case 5: responseSize = (java.lang.Integer)(value); break;
      case 6: referrer = (java.lang.CharSequence)(value); break;
      case 7: userAgent = (java.lang.CharSequence)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Pageview> {


      super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'url' field.
     */
    public java.lang.CharSequence getUrl() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'url' field.
     * @param value the value to set.
     */
    public void setUrl(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUrlDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'timestamp' field.
     */
    public java.lang.Long getTimestamp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'timestamp' field.
     * @param value the value to set.
     */
    public void setTimestamp(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isTimestampDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'ip' field.
     */
    public java.lang.CharSequence getIp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'ip' field.
     * @param value the value to set.
     */
    public void setIp(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isIpDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'httpMethod' field.
     */
    public java.lang.CharSequence getHttpMethod() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'httpMethod' field.
     * @param value the value to set.
     */
    public void setHttpMethod(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHttpMethodDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'httpStatusCode' field.
     */
    public java.lang.Integer getHttpStatusCode() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'httpStatusCode' field.
     * @param value the value to set.
     */
    public void setHttpStatusCode(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHttpStatusCodeDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'responseSize' field.
     */
    public java.lang.Integer getResponseSize() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'responseSize' field.
     * @param value the value to set.
     */
    public void setResponseSize(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isResponseSizeDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'referrer' field.
     */
    public java.lang.CharSequence getReferrer() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'referrer' field.
     * @param value the value to set.
     */
    public void setReferrer(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
     * @param value the value to set.
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'userAgent' field.
     */
    public java.lang.CharSequence getUserAgent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
     */
    public void setUserAgent(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUserAgentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
    persistent = datumReader.read(reuseObjects ? persistent : null, decoder);
    byte[] __g__dirty = new byte[persistent.getFieldsCount()];
    decoder.readFixed(__g__dirty);
    persistent.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    return persistent;
    encoder.writeFixed(persistent.getDirtyBytes().array());
            throws IOException, InterruptedException {
      page.setUrl("hola");
      context.write(new Text(key), page);
    }
            throws IOException, InterruptedException {
        LOG.info(key.toString());
        LOG.info(val.toString());
        LOG.info(String.valueOf(val.isDirty()));
    }
   *
   * @param inStore  input store on MR jobs runs on
   * @param query    query to select input set run MR
   * @param outStore output store which stores results of MR jobs
   * @return job MR job definition
  public Job createJob(DataStore<String, WebPage> inStore, Query<String, WebPage> query
          , DataStore<String, WebPage> outStore) throws IOException {
  public int mapReduceSerialization(DataStore<String, WebPage> inStore,
                                    DataStore<String, WebPage> outStore)
          throws IOException, InterruptedException, ClassNotFoundException {
    Query<String, WebPage> query = inStore.newQuery();
    query.setFields("url");
    DataStore<String, WebPage> inStore;
    DataStore<String, WebPage> outStore;
    if (args.length > 0) {
      if (args.length > 1) {
import org.apache.avro.Schema;
  
  /**
   * Returns the avro's data schema
   * @return the parsed schema definition
   */
  public Schema getSchema() ;
  
import org.apache.avro.Schema;
    @Override
    public Schema getSchema() { return null; }
import org.apache.avro.Schema;

    @Override
    public Schema getSchema() { return null; }



  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]}");







 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
package org.apache.gora.examples.generated;

          "name",
          "dateOfBirth",
          "ssn",
          "salary",
          "boss",
          "webpage",
  // Used by DatumWriter.  Applications should not call.
      case 0: return name;
      case 1: return dateOfBirth;
      case 2: return ssn;
      case 3: return salary;
      case 4: return boss;
      case 5: return webpage;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
  @SuppressWarnings(value="unchecked")
      case 0: name = (java.lang.CharSequence)(value); break;
      case 1: dateOfBirth = (java.lang.Long)(value); break;
      case 2: ssn = (java.lang.CharSequence)(value); break;
      case 3: salary = (java.lang.Integer)(value); break;
      case 4: boss = (java.lang.Object)(value); break;
      case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");









          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Employee> {


      super(org.apache.gora.examples.generated.Employee.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'name' field.
     */
    public java.lang.CharSequence getName() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'name' field.
     * @param value the value to set.
     */
    public void setName(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isNameDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'dateOfBirth' field.
     */
    public java.lang.Long getDateOfBirth() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'dateOfBirth' field.
     * @param value the value to set.
     */
    public void setDateOfBirth(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isDateOfBirthDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'ssn' field.
     */
    public java.lang.CharSequence getSsn() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'ssn' field.
     * @param value the value to set.
     */
    public void setSsn(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isSsnDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'salary' field.
     */
    public java.lang.Integer getSalary() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'salary' field.
     * @param value the value to set.
     */
    public void setSalary(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isSalaryDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'boss' field.
     */
    public java.lang.Object getBoss() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'boss' field.
     * @param value the value to set.
     */
    public void setBoss(java.lang.Object value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isBossDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'webpage' field.
     */
    public org.apache.gora.examples.generated.WebPage getWebpage() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'webpage' field.
     * @param value the value to set.
     */
    public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isWebpageDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -7621464515167921131L;
          "v1",
          "v2",
  // Used by DatumWriter.  Applications should not call.
      case 0: return v1;
      case 1: return v2;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: v1 = (java.lang.Integer)(value); break;
      case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");





          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<ImmutableFields> {


      super(org.apache.gora.examples.generated.ImmutableFields.SCHEMA$);

      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'v1' field.
     */
    public java.lang.Integer getV1() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v1' field.
     * @param value the value to set.
     */
    public void setV1(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v1' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV1Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'v2' field.
     */
    public org.apache.gora.examples.generated.V2 getV2() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v2' field.
     * @param value the value to set.
     */
    public void setV2(org.apache.gora.examples.generated.V2 value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v2' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV2Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -7097391446015721734L;
          "version",
          "data",
  // Used by DatumWriter.  Applications should not call.
      case 0: return version;
      case 1: return data;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: version = (java.lang.Integer)(value); break;
      case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");





          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Metadata> {


      super(org.apache.gora.examples.generated.Metadata.SCHEMA$);

      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'version' field.
     */
    public java.lang.Integer getVersion() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'version' field.
     * @param value the value to set.
     */
    public void setVersion(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isVersionDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'data' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'data' field.
     * @param value the value to set.
     */
    public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -4481652577902636424L;
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: count = (java.lang.Integer)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");




          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<TokenDatum> {


      super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);

      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Integer getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -6538763924317658547L;
          "v3",
  // Used by DatumWriter.  Applications should not call.
      case 0: return v3;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: v3 = (java.lang.Integer)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");




          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<V2> {


      super(org.apache.gora.examples.generated.V2.SCHEMA$);

      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'v3' field.
     */
    public java.lang.Integer getV3() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v3' field.
     * @param value the value to set.
     */
    public void setV3(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v3' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV3Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -2829100587222969501L;
          "url",
          "content",
          "parsedContent",
          "outlinks",
          "headers",
          "metadata",
          "byteData",
          "stringData",
  // Used by DatumWriter.  Applications should not call.
      case 0: return url;
      case 1: return content;
      case 2: return parsedContent;
      case 3: return outlinks;
      case 4: return headers;
      case 5: return metadata;
      case 6: return byteData;
      case 7: return stringData;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: url = (java.lang.CharSequence)(value); break;
      case 1: content = (java.nio.ByteBuffer)(value); break;
      case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
      case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
      case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
      case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");











          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<WebPage> {


      super(org.apache.gora.examples.generated.WebPage.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'url' field.
     */
    public java.lang.CharSequence getUrl() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'url' field.
     * @param value the value to set.
     */
    public void setUrl(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUrlDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'content' field.
     */
    public java.nio.ByteBuffer getContent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'content' field.
     * @param value the value to set.
     */
    public void setContent(java.nio.ByteBuffer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isContentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'parsedContent' field.
     */
    public java.util.List<java.lang.CharSequence> getParsedContent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'parsedContent' field.
     * @param value the value to set.
     */
    public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isParsedContentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'outlinks' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'outlinks' field.
     * @param value the value to set.
     */
    public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isOutlinksDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'headers' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'headers' field.
     * @param value the value to set.
     */
    public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHeadersDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'metadata' field.
     */
    public org.apache.gora.examples.generated.Metadata getMetadata() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metadata' field.
     * @param value the value to set.
     */
    public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetadataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'byteData' field.
     */
    public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'byteData' field.
     * @param value the value to set.
     */
    public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'byteData' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isByteDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'stringData' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'stringData' field.
     * @param value the value to set.
     */
    public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'stringData' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isStringDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.goraci.generated;

  private static final long serialVersionUID = 1014651356631895518L;
          "prev",
          "client",
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return prev;
      case 1: return client;
      case 2: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: prev = (java.lang.Long)(value); break;
      case 1: client = (java.lang.CharSequence)(value); break;
      case 2: count = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<CINode> {


      super(org.apache.gora.goraci.generated.CINode.SCHEMA$);

      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'prev' field.
     */
    public java.lang.Long getPrev() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'prev' field.
     * @param value the value to set.
     */
    public void setPrev(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'prev' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isPrevDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'client' field.
     */
    public java.lang.CharSequence getClient() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'client' field.
     * @param value the value to set.
     */
    public void setClient(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'client' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isClientDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Long getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.goraci.generated;

  private static final long serialVersionUID = -8888031915401438521L;
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: count = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Flushed> {


      super(org.apache.gora.goraci.generated.Flushed.SCHEMA$);

      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Long getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.tutorial.log.generated;

  private static final long serialVersionUID = 8278557845311856507L;
          "metricDimension",
          "timestamp",
          "metric",
  // Used by DatumWriter.  Applications should not call.
      case 0: return metricDimension;
      case 1: return timestamp;
      case 2: return metric;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: metricDimension = (java.lang.CharSequence)(value); break;
      case 1: timestamp = (java.lang.Long)(value); break;
      case 2: metric = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<MetricDatum> {


      super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);

      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'metricDimension' field.
     */
    public java.lang.CharSequence getMetricDimension() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metricDimension' field.
     * @param value the value to set.
     */
    public void setMetricDimension(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetricDimensionDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'timestamp' field.
     */
    public java.lang.Long getTimestamp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'timestamp' field.
     * @param value the value to set.
     */
    public void setTimestamp(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isTimestampDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'metric' field.
     */
    public java.lang.Long getMetric() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metric' field.
     * @param value the value to set.
     */
    public void setMetric(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetricDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


}
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
  private static final long serialVersionUID = -6136058768384995982L;
          "url",
          "timestamp",
          "ip",
          "httpMethod",
          "httpStatusCode",
          "responseSize",
          "referrer",
          "userAgent",
  // Used by DatumWriter.  Applications should not call.
      case 0: return url;
      case 1: return timestamp;
      case 2: return ip;
      case 3: return httpMethod;
      case 4: return httpStatusCode;
      case 5: return responseSize;
      case 6: return referrer;
      case 7: return userAgent;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: url = (java.lang.CharSequence)(value); break;
      case 1: timestamp = (java.lang.Long)(value); break;
      case 2: ip = (java.lang.CharSequence)(value); break;
      case 3: httpMethod = (java.lang.CharSequence)(value); break;
      case 4: httpStatusCode = (java.lang.Integer)(value); break;
      case 5: responseSize = (java.lang.Integer)(value); break;
      case 6: referrer = (java.lang.CharSequence)(value); break;
      case 7: userAgent = (java.lang.CharSequence)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Pageview> {


      super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'url' field.
     */
    public java.lang.CharSequence getUrl() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'url' field.
     * @param value the value to set.
     */
    public void setUrl(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUrlDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'timestamp' field.
     */
    public java.lang.Long getTimestamp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'timestamp' field.
     * @param value the value to set.
     */
    public void setTimestamp(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isTimestampDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'ip' field.
     */
    public java.lang.CharSequence getIp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'ip' field.
     * @param value the value to set.
     */
    public void setIp(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isIpDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'httpMethod' field.
     */
    public java.lang.CharSequence getHttpMethod() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'httpMethod' field.
     * @param value the value to set.
     */
    public void setHttpMethod(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHttpMethodDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'httpStatusCode' field.
     */
    public java.lang.Integer getHttpStatusCode() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'httpStatusCode' field.
     * @param value the value to set.
     */
    public void setHttpStatusCode(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHttpStatusCodeDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'responseSize' field.
     */
    public java.lang.Integer getResponseSize() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'responseSize' field.
     * @param value the value to set.
     */
    public void setResponseSize(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isResponseSizeDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'referrer' field.
     */
    public java.lang.CharSequence getReferrer() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'referrer' field.
     * @param value the value to set.
     */
    public void setReferrer(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
     * @param value the value to set.
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'userAgent' field.
     */
    public java.lang.CharSequence getUserAgent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
     */
    public void setUserAgent(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUserAgentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


 *  Accumulo specific implementation of the {@link org.apache.gora.query.Query} interface.

  /**
   * Constructor for the query
   */
  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */
 * Accumulo specific implementation of the {@link org.apache.gora.query.Result} interface.
  /**
   * Gets the data store used
   */

  /**
   * Gets the items reading progress
   */

  /**
   * Gets the next item
   */
/**
 * Mapping definitions for Accumulo.
 */

  /**
   * A map of field names to Field objects containing schema's fields
   */

  /**
   * Look up the column associated to the Avro field.
   */

import org.apache.gora.util.IOUtils;
 * Implementation of a Accumulo data store to be used by gora.
 *
 * @param <K> class to be used for the key
 * @param <T> class to be persisted within the store
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
          final SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
          final byte[] byteData = IOUtils.serialize(writer,o);
          m.put(col.getFirst(), col.getSecond(), new Value(byteData));
  /**
   * Execute the query and return the result.
   */
/**
 * It is a implementation of {@link java.io.OutputStream} must always provide at least a method that writes one byte of output.
 */
/**
 * Cassandra specific implementation of the {@link Query} interface.
 */
  /**
   * Cassandra specific implementation of the {@link Query} interface.
   */

  /**
   * Constructor for the query
   */

  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */

  /**
   * Setter of familyMap.
   */

  /**
   * Getter of familyMap.
   */

  /**
   *
   * @return get {@link org.apache.gora.cassandra.query.CassandraQuery}
   */

  /**
   *
   * @param query set a {@link org.apache.gora.cassandra.query.CassandraQuery}
   */
/**
 * CassandraResult specific implementation of the {@link org.apache.gora.query.Result}
 * interface.
 */
  /**
   * Result set containing query results
   */

  /**
   * Constructor for the result set
   *
   * @param dataStore Data store used
   * @param query     Query used
   */
  /**
   * Gets the next item
   */
  /**
   * Gets the items reading progress
   */
  /**
   * Set the Result set containing query results
   *
   * @param cassandraResultSet
   */


   * Method to maintain backward compatibility with earlier versions.
   * @param keyClass
   * @param persistentClass
   * @throws Exception
   */
   *
   * @return if keyspace already exists return true.
/**
 * Mapping definitions for CouchDB.
 */
/**
 * A builder for creating the mapper.
 */
import org.apache.avro.SchemaNormalization;
  /**
   * Utility method used by velocity templates to generate serialVersionUID on AVRO beans.
   *
   * @param schema Data bean AVRO schema.
   * @return serialVersionUID for Serializable AVRO databeans.
   */
  public static long fingerprint64(Schema schema) {
    return SchemaNormalization.parsingFingerprint64(schema);
  }

 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -6468893522296148608L;
          "name",
          "dateOfBirth",
          "ssn",
          "salary",
          "boss",
          "webpage",
  // Used by DatumWriter.  Applications should not call.
      case 0: return name;
      case 1: return dateOfBirth;
      case 2: return ssn;
      case 3: return salary;
      case 4: return boss;
      case 5: return webpage;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
  @SuppressWarnings(value="unchecked")
      case 0: name = (java.lang.CharSequence)(value); break;
      case 1: dateOfBirth = (java.lang.Long)(value); break;
      case 2: ssn = (java.lang.CharSequence)(value); break;
      case 3: salary = (java.lang.Integer)(value); break;
      case 4: boss = (java.lang.Object)(value); break;
      case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");









          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Employee> {


      super(org.apache.gora.examples.generated.Employee.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;
  public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'name' field.
     */
    public java.lang.CharSequence getName() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'name' field.
     * @param value the value to set.
     */
    public void setName(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'name' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isNameDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'dateOfBirth' field.
     */
    public java.lang.Long getDateOfBirth() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'dateOfBirth' field.
     * @param value the value to set.
     */
    public void setDateOfBirth(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'dateOfBirth' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isDateOfBirthDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'ssn' field.
     */
    public java.lang.CharSequence getSsn() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'ssn' field.
     * @param value the value to set.
     */
    public void setSsn(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'ssn' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isSsnDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'salary' field.
     */
    public java.lang.Integer getSalary() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'salary' field.
     * @param value the value to set.
     */
    public void setSalary(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'salary' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isSalaryDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'boss' field.
     */
    public java.lang.Object getBoss() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'boss' field.
     * @param value the value to set.
     */
    public void setBoss(java.lang.Object value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'boss' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isBossDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'webpage' field.
     */
    public org.apache.gora.examples.generated.WebPage getWebpage() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'webpage' field.
     * @param value the value to set.
     */
    public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'webpage' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isWebpageDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -7621464515167921131L;
          "v1",
          "v2",
  // Used by DatumWriter.  Applications should not call.
      case 0: return v1;
      case 1: return v2;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: v1 = (java.lang.Integer)(value); break;
      case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");





          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<ImmutableFields> {


      super(org.apache.gora.examples.generated.ImmutableFields.SCHEMA$);

      return this;




      return this;




    return TOMBSTONE;
  public static final class Tombstone extends ImmutableFields implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'v1' field.
     */
    public java.lang.Integer getV1() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v1' field.
     * @param value the value to set.
     */
    public void setV1(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v1' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV1Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'v2' field.
     */
    public org.apache.gora.examples.generated.V2 getV2() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v2' field.
     * @param value the value to set.
     */
    public void setV2(org.apache.gora.examples.generated.V2 value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v2' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV2Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -7097391446015721734L;
          "version",
          "data",
  // Used by DatumWriter.  Applications should not call.
      case 0: return version;
      case 1: return data;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: version = (java.lang.Integer)(value); break;
      case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");





          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Metadata> {


      super(org.apache.gora.examples.generated.Metadata.SCHEMA$);

      return this;




      return this;




    return TOMBSTONE;
  public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'version' field.
     */
    public java.lang.Integer getVersion() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'version' field.
     * @param value the value to set.
     */
    public void setVersion(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'version' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isVersionDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'data' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'data' field.
     * @param value the value to set.
     */
    public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'data' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -4481652577902636424L;
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: count = (java.lang.Integer)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");




          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<TokenDatum> {


      super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);

      return this;




    return TOMBSTONE;
  public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Integer getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -6538763924317658547L;
          "v3",
  // Used by DatumWriter.  Applications should not call.
      case 0: return v3;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: v3 = (java.lang.Integer)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");




          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<V2> {


      super(org.apache.gora.examples.generated.V2.SCHEMA$);

      return this;




    return TOMBSTONE;
  public static final class Tombstone extends V2 implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'v3' field.
     */
    public java.lang.Integer getV3() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'v3' field.
     * @param value the value to set.
     */
    public void setV3(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'v3' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isV3Dirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.examples.generated;

  private static final long serialVersionUID = -2829100587222969501L;
          "url",
          "content",
          "parsedContent",
          "outlinks",
          "headers",
          "metadata",
          "byteData",
          "stringData",
  // Used by DatumWriter.  Applications should not call.
      case 0: return url;
      case 1: return content;
      case 2: return parsedContent;
      case 3: return outlinks;
      case 4: return headers;
      case 5: return metadata;
      case 6: return byteData;
      case 7: return stringData;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: url = (java.lang.CharSequence)(value); break;
      case 1: content = (java.nio.ByteBuffer)(value); break;
      case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
      case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
      case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
      case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");











          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<WebPage> {


      super(org.apache.gora.examples.generated.WebPage.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;
  public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'url' field.
     */
    public java.lang.CharSequence getUrl() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'url' field.
     * @param value the value to set.
     */
    public void setUrl(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUrlDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'content' field.
     */
    public java.nio.ByteBuffer getContent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'content' field.
     * @param value the value to set.
     */
    public void setContent(java.nio.ByteBuffer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'content' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isContentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'parsedContent' field.
     */
    public java.util.List<java.lang.CharSequence> getParsedContent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'parsedContent' field.
     * @param value the value to set.
     */
    public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'parsedContent' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isParsedContentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'outlinks' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'outlinks' field.
     * @param value the value to set.
     */
    public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'outlinks' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isOutlinksDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'headers' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'headers' field.
     * @param value the value to set.
     */
    public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'headers' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHeadersDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'metadata' field.
     */
    public org.apache.gora.examples.generated.Metadata getMetadata() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metadata' field.
     * @param value the value to set.
     */
    public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metadata' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetadataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'byteData' field.
     */
    public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'byteData' field.
     * @param value the value to set.
     */
    public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'byteData' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isByteDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'stringData' field.
     */
    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'stringData' field.
     * @param value the value to set.
     */
    public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'stringData' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isStringDataDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
import org.apache.hadoop.mapreduce.task.JobContextImpl;
        conf.getStrings("io.serializations"),
      Job job = Job.getInstance(conf);
      return new JobContextImpl(job.getConfiguration(), null);
    return new JobContextImpl(conf, null);
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

        Collection<T> {
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.ObjectInput;
import java.io.ObjectOutput;
    Persistent, java.io.Externalizable {
  /**
   * Exposing dirty bytes over public method. Purpose is to preserve dirty bytes content
   * while transporting AVRO data beans over TCP wire in serialized form.
   * Since {@link org.apache.gora.persistency.impl.PersistentBase} implements {@link java.io.Externalizable},
   * this method can be used to retrieve the dirty bytes as {@link java.nio.ByteBuffer} and and get the content
   * as bytes[] and write byte stream to the TCP wire.
   * See {@link java.io.Externalizable#writeExternal(ObjectOutput)} abstract method implementation
   * on velocity template record.vm.
   * <p>
   * Note {@link java.nio.ByteBuffer} is not itself not in serializable form.
   *
   * @return __g__dirty dirty bytes
   */
  public ByteBuffer getDirtyBytes() {
  /**
   * Setter method for assign dirty bytes when deserializing AVRO bean from dirty bytes
   * preserved in serialized bytes form.
   * Since {@link org.apache.gora.persistency.impl.PersistentBase} implements {@link java.io.Externalizable}
   * and when actual deserialization happens for {@link org.apache.gora.persistency.impl.PersistentBase}
   * new instance, acquire byte stream from TCP wire, extracting specific byte[] from byte stream
   * and create {@link java.nio.ByteBuffer} instance and set using this public method.
   * See {@link java.io.Externalizable#readExternal(ObjectInput)} abstract method implementation
   * on velocity template record.vm.
   * <p>
   * Note {@link java.io.Externalizable} extending means it is mandatory to have default public constructor.
   *
   * @param __g__dirty dirty bytes
   */
  public void setDirtyBytes(ByteBuffer __g__dirty) {
    this.__g__dirty = __g__dirty;
  }

 * Base class implementing common functionality for Web services
  /*This selects the default caching dataStore which wraps any GORA persistency dataStore*/
  public static final String GORA_DEFAULT_CACHE_DATASTORE_KEY = "gora.cache.datastore.default";

  /**
   * Instantiate <i>the default</i> {@link DataStore} wrapped over caching dataStore which provides caching
   * abstraction over the GORA persistence dataStore.
   * Uses default properties. Uses 'null' schema.
   *
   * Note:
   *    consider that default dataStore is always visible
   *
   * @param keyClass The key class.
   * @param persistent The value class.
   * @param conf {@link Configuration} To be used be the store.
   * @param isCacheEnabled Caching enable or not.
   * @return A new store instance.
   * @throws GoraException If cache or persistency dataStore initialization interrupted.
   */
  @SuppressWarnings("unchecked")
  public static <K, T extends Persistent> DataStore<K, T> getDataStore(
          Class<K> keyClass, Class<T> persistent, Configuration conf, boolean isCacheEnabled) throws GoraException {
    Properties createProps = createProps();
    Class<? extends DataStore<K, T>> c;
    try {
      if (isCacheEnabled) {
        c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultCacheDataStore(createProps));
      } else {
        c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
      }
    } catch (Exception ex) {
      throw new GoraException(ex);
    }
    return createDataStore(c, keyClass, persistent, conf, createProps, null);
  }

  private static String getDefaultCacheDataStore(Properties properties) {
    return getProperty(properties, GORA_DEFAULT_CACHE_DATASTORE_KEY);
  }

    final SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(persistent.getSchema());
    final byte[] byteData;
      byteData = IOUtils.serialize(writer, persistent);

    final SpecificDatumReader<T> reader = new SpecificDatumReader<>((Class<T>) persistent.getClass());
      return IOUtils.deserialize(byteData, reader, null);
    try (DataInputBuffer buffer = new DataInputBuffer()) {
      buffer.reset(in, in.length);
      return deserialize(conf, buffer, obj);
    }
  private static final long serialVersionUID = -7468893532296148608L;
  public static final org.apache.avro.Schema SCHEMA$ =
          new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

 * Different from the @see org.apache.gora.compiler.GoraCompiler,
   * @param dest where the data bean will be written.
   * Method in charge of compiling a specific table using a key schema and a set
   *
   * @param pTableName
   * @param arrayList
   * @param map
   *
   * @param pItems The items belonging to the table
   * @param pIden  The number of spaces used for identation
   *
   * @param pItemName Item's name
   * @param pItemType Item's type
   * @param pIden     Number of spaces used for indentation
   * Creates key getters and setters
   *
   * @param pKeySchema The key schema for a specific table
   * @param pIden      Number of spaces used for indentation
   *
   * @param s String to be camelcasified
   *
   * @param name  Class name
   * @param space spacing
   *
   * @param indent Number of spaces used for indentation
   * @param text   Text to be written
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
   *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.goraci.generated;

  private static final long serialVersionUID = 1014651356631895518L;
          "prev",
          "client",
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return prev;
      case 1: return client;
      case 2: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: prev = (java.lang.Long)(value); break;
      case 1: client = (java.lang.CharSequence)(value); break;
      case 2: count = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<CINode> {


      super(org.apache.gora.goraci.generated.CINode.SCHEMA$);

      return this;




      return this;




      return this;




    return TOMBSTONE;
  public static final class Tombstone extends CINode implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'prev' field.
     */
    public java.lang.Long getPrev() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'prev' field.
     * @param value the value to set.
     */
    public void setPrev(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'prev' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isPrevDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'client' field.
     */
    public java.lang.CharSequence getClient() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'client' field.
     * @param value the value to set.
     */
    public void setClient(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'client' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isClientDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Long getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.goraci.generated;

  private static final long serialVersionUID = -8888031915401438521L;
          "count",
  // Used by DatumWriter.  Applications should not call.
      case 0: return count;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: count = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Flushed> {


      super(org.apache.gora.goraci.generated.Flushed.SCHEMA$);

      return this;




    return TOMBSTONE;
  public static final class Tombstone extends Flushed implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'count' field.
     */
    public java.lang.Long getCount() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'count' field.
     * @param value the value to set.
     */
    public void setCount(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'count' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isCountDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
import org.apache.gora.query.Query;
/**
 * Infinispan specific implementation of the {@link Query} interface.
  public synchronized void createSchema() {
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
  /**
   * Execute the query and return the result.
   */
   * @param params This value should specify the host:port (at least one) for
   *               connecting to remote MongoDB.
 * Configuration properties
 *
/**
 * BSON encoder for BSONObject instances.
 */
import org.apache.gora.query.Query;
/**
 * Solr specific implementation of the {@link Query} interface.
 */
  /**
   * Constructor for the query
   */

  /**
   * Constructor for the query
   *
   * @param dataStore Data store used
   *
   */

  /**
   * Create a solr query
   *
   * @return the solr query string
   */
/**
 * SolrResult specific implementation of the {@link org.apache.gora.query.Result}
 * interface.
 */
  /**
   * Constructor for the result set
   *
   * @param dataStore Data store used
   * @param query     Query used
   * @param server    A client that talks directly to a Solr server
   * @param resultsSize  The number of rows to be returned
   */
  /**
   * Gets the next item
   */
  /**
   * Gets the items reading progress
   */
/**
 * Mapping definitions for Solr.
 */
/**
 * Implementation of a Solr data store to be used by gora.
 *
 * @param <K> class to be used for the key
 * @param <T> class to be persisted within the store
 */
  /**
   * Initialize the data store by reading the credentials, setting the client's properties up and
   * reading the mapping file. Initialize is called when then the call to
   * {@link org.apache.gora.store.DataStoreFactory#createDataStore} is made.
   *
   * @param keyClass
   * @param persistentClass
   * @param properties
   */
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.tutorial.log.generated;

  private static final long serialVersionUID = 8278557845311856507L;
          "metricDimension",
          "timestamp",
          "metric",
  // Used by DatumWriter.  Applications should not call.
      case 0: return metricDimension;
      case 1: return timestamp;
      case 2: return metric;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: metricDimension = (java.lang.CharSequence)(value); break;
      case 1: timestamp = (java.lang.Long)(value); break;
      case 2: metric = (java.lang.Long)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<MetricDatum> {


      super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);

      return this;




      return this;




      return this;




    return TOMBSTONE;
  public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {

    private Tombstone() { }

    /**
     * Gets the value of the 'metricDimension' field.
     */
    public java.lang.CharSequence getMetricDimension() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metricDimension' field.
     * @param value the value to set.
     */
    public void setMetricDimension(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metricDimension' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetricDimensionDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'timestamp' field.
     */
    public java.lang.Long getTimestamp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'timestamp' field.
     * @param value the value to set.
     */
    public void setTimestamp(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isTimestampDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'metric' field.
     */
    public java.lang.Long getMetric() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'metric' field.
     * @param value the value to set.
     */
    public void setMetric(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'metric' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isMetricDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
package org.apache.gora.tutorial.log.generated;

  private static final long serialVersionUID = -6136058768384995982L;
          "url",
          "timestamp",
          "ip",
          "httpMethod",
          "httpStatusCode",
          "responseSize",
          "referrer",
          "userAgent",
  // Used by DatumWriter.  Applications should not call.
      case 0: return url;
      case 1: return timestamp;
      case 2: return ip;
      case 3: return httpMethod;
      case 4: return httpStatusCode;
      case 5: return responseSize;
      case 6: return referrer;
      case 7: return userAgent;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

  // Used by DatumReader.  Applications should not call.
      case 0: url = (java.lang.CharSequence)(value); break;
      case 1: timestamp = (java.lang.Long)(value); break;
      case 2: ip = (java.lang.CharSequence)(value); break;
      case 3: httpMethod = (java.lang.CharSequence)(value); break;
      case 4: httpStatusCode = (java.lang.Integer)(value); break;
      case 5: responseSize = (java.lang.Integer)(value); break;
      case 6: referrer = (java.lang.CharSequence)(value); break;
      case 7: userAgent = (java.lang.CharSequence)(value); break;
      default: throw new org.apache.avro.AvroRuntimeException("Bad index");

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.

   * @param value the value to set.



          java.nio.ByteBuffer input) {

          implements org.apache.avro.data.RecordBuilder<Pageview> {


      super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);

      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




      return this;




    return TOMBSTONE;


    private Tombstone() { }

    /**
     * Gets the value of the 'url' field.
     */
    public java.lang.CharSequence getUrl() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'url' field.
     * @param value the value to set.
     */
    public void setUrl(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'url' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUrlDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'timestamp' field.
     */
    public java.lang.Long getTimestamp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'timestamp' field.
     * @param value the value to set.
     */
    public void setTimestamp(java.lang.Long value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'timestamp' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isTimestampDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'ip' field.
     */
    public java.lang.CharSequence getIp() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'ip' field.
     * @param value the value to set.
     */
    public void setIp(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'ip' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isIpDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'httpMethod' field.
     */
    public java.lang.CharSequence getHttpMethod() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'httpMethod' field.
     * @param value the value to set.
     */
    public void setHttpMethod(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'httpMethod' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHttpMethodDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'httpStatusCode' field.
     */
    public java.lang.Integer getHttpStatusCode() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'httpStatusCode' field.
     * @param value the value to set.
     */
    public void setHttpStatusCode(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'httpStatusCode' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isHttpStatusCodeDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'responseSize' field.
     */
    public java.lang.Integer getResponseSize() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'responseSize' field.
     * @param value the value to set.
     */
    public void setResponseSize(java.lang.Integer value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }

    /**
     * Checks the dirty status of the 'responseSize' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isResponseSizeDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'referrer' field.
     */
    public java.lang.CharSequence getReferrer() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }

    /**
     * Sets the value of the 'referrer' field.
     * @param value the value to set.
     */
    public void setReferrer(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
     * @param value the value to set.
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }

    /**
     * Gets the value of the 'userAgent' field.
     */
    public java.lang.CharSequence getUserAgent() {
      throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
    }
     */
    public void setUserAgent(java.lang.CharSequence value) {
      throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
    }
    /**
     * Checks the dirty status of the 'userAgent' field. A field is dirty if it represents a change that has not yet been written to the database.
     * @param value the value to set.
     */
    public boolean isUserAgentDirty() {
      throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
    }


  }

  private static final org.apache.avro.io.DatumWriter
          DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
  private static final org.apache.avro.io.DatumReader
          DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);

  /**
   * Writes AVRO data bean to output stream in the form of AVRO Binary encoding format. This will transform
   * AVRO data bean from its Java object form to it s serializable form.
   *
   * @param out java.io.ObjectOutput output stream to write data bean in serializable form
   */
  @Override
  public void writeExternal(java.io.ObjectOutput out)
          throws java.io.IOException {
    out.write(super.getDirtyBytes().array());
    DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
            .directBinaryEncoder((java.io.OutputStream) out,
                    null));
  }

  /**
   * Reads AVRO data bean from input stream in it s AVRO Binary encoding format to Java object format.
   * This will transform AVRO data bean from it s serializable form to deserialized Java object form.
   *
   * @param in java.io.ObjectOutput input stream to read data bean in serializable form
   */
  @Override
  public void readExternal(java.io.ObjectInput in)
          throws java.io.IOException {
    byte[] __g__dirty = new byte[getFieldsCount()];
    in.read(__g__dirty);
    super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
            .directBinaryDecoder((java.io.InputStream) in,
                    null));
  }

}
    persistent = datumReader.read(reuseObjects ? persistent : null, decoder);
    byte[] __g__dirty = new byte[persistent.getFieldsCount()];
    decoder.readFixed(__g__dirty);
    persistent.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
    return persistent;
    encoder.writeFixed(persistent.getDirtyBytes().array());
import static com.mongodb.AuthenticationMechanism.*;
    if (params.getAuthenticationType() != null) {
      credentials.add(createCredential(params.getAuthenticationType(), params.getLogin(), params.getDbname(), params.getSecret()));
  private MongoCredential createCredential(String authenticationType, String username, String database, String password) {
    MongoCredential credential = null;
    if (authenticationType.equals(PLAIN.getMechanismName())) {
      credential = MongoCredential.createPlainCredential(username, database, password.toCharArray());
    } else if (authenticationType.equals(SCRAM_SHA_1.getMechanismName())) {
      credential = MongoCredential.createScramSha1Credential(username, database, password.toCharArray());
    } else if (authenticationType.equals(MONGODB_CR.getMechanismName())) {
      credential = MongoCredential.createMongoCRCredential(username, database, password.toCharArray());
    } else if (authenticationType.equals(GSSAPI.getMechanismName())) {
      credential = MongoCredential.createGSSAPICredential(username);
    } else if (authenticationType.equals(MONGODB_X509.getMechanismName())) {
      credential = MongoCredential.createMongoX509Credential(username);
    } else {
      LOG.error("Error while initializing MongoDB store: Invalid Authentication type.");
      throw new RuntimeException("Error while initializing MongoDB store: Invalid Authentication type.");
    }
    return credential;
  }


  /**
   * Property pointing to the authentication type to connect to the server
   */
  public static final String PROP_MONGO_AUTHENTICATION_TYPE = "gora.mongodb.authentication.type";

  private final String authenticationType;
   * @param authenticationType Authentication type to login
  private MongoStoreParameters(String mappingFile, String servers, String dbname, String authenticationType, String login, String secret, String readPreference, String writeConcern) {
    this.authenticationType = authenticationType;
  public String getAuthenticationType() {
    return authenticationType;
  }

    String vPropMongoAuthenticationType = properties.getProperty(PROP_MONGO_AUTHENTICATION_TYPE);
      vPropMongoAuthenticationType = conf.get(PROP_MONGO_AUTHENTICATION_TYPE, vPropMongoAuthenticationType);
    return new MongoStoreParameters(vPropMappingFile, vPropMongoServers, vPropMongoDb, vPropMongoAuthenticationType, vPropMongoLogin, vPropMongoSecret, vPropMongoRead, vPropMongoWrite);
