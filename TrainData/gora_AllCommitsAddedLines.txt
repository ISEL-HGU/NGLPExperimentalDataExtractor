import org.apache.gora.store.DataStoreFactory;
public static <K1, V1 extends Persistent> void setInput(
Job job,
Class<? extends DataStore<K1,V1>> dataStoreClass,
Class<K1> inKeyClass,
Class<V1> inValueClass,
boolean reuseObjects)
throws IOException {
DataStore<K1,V1> store = DataStoreFactory.getDataStore(dataStoreClass
, inKeyClass, inValueClass);
setInput(job, store.newQuery(), store, reuseObjects);
}
}
extends Mapper<K1, V1, K2, V2> {
void initMapperJob(
Job job,
Class<? extends DataStore<K1,V1>> dataStoreClass,
Class<K1> inKeyClass,
Class<V1> inValueClass,
Class<K2> outKeyClass,
Class<V2> outValueClass,
Class<? extends Partitioner> partitionerClass,
boolean reuseObjects)
throws IOException {
GoraInputFormat.setInput(job, dataStoreClass, inKeyClass, inValueClass, reuseObjects);
job.setMapperClass(mapperClass);
job.setMapOutputKeyClass(outKeyClass);
job.setMapOutputValueClass(outValueClass);
if (partitionerClass != null) {
job.setPartitionerClass(partitionerClass);
}
}
@SuppressWarnings("rawtypes")
public static <K1, V1 extends Persistent, K2, V2>
void initMapperJob(
Job job,
Class<? extends DataStore<K1,V1>> dataStoreClass,
Class<K1> inKeyClass,
Class<V1> inValueClass,
Class<K2> outKeyClass,
Class<V2> outValueClass,
Class<? extends GoraMapper> mapperClass,
boolean reuseObjects)
throws IOException {
initMapperJob(job, dataStoreClass, inKeyClass, inValueClass, outKeyClass
, outValueClass, mapperClass, null, reuseObjects);
}
@SuppressWarnings("rawtypes")
public static <K1, V1 extends Persistent, K2, V2>
void initMapperJob(
Job job,
Query<K1,V1> query,
DataStore<K1,V1> dataStore,
Class<K2> outKeyClass,
Class<V2> outValueClass,
Class<? extends GoraMapper> mapperClass,
Class<? extends Partitioner> partitionerClass,
boolean reuseObjects)
void initMapperJob(
Job job,
DataStore<K1,V1> dataStore,
Class<K2> outKeyClass,
Class<V2> outValueClass,
Class<? extends GoraMapper> mapperClass,
boolean reuseObjects)
throws IOException {
initMapperJob(job, dataStore.newQuery(), dataStore,
outKeyClass, outValueClass, mapperClass, reuseObjects);
}
@SuppressWarnings({ "rawtypes" })
public static <K1, V1 extends Persistent, K2, V2>
void initMapperJob(
Job job,
Query<K1,V1> query,
DataStore<K1,V1> dataStore,
Class<K2> outKeyClass,
Class<V2> outValueClass,
Class<? extends GoraMapper> mapperClass,
boolean reuseObjects)
setOutput(job, dataStore.getClass(), dataStore.getKeyClass()
, dataStore.getPersistentClass(), reuseObjects);
Class<K> keyClass, Class<V> persistentClass,
job.setOutputKeyClass(keyClass);
job.setOutputValueClass(persistentClass);
extends Reducer<K1, V1, K2, V2> {
void initReducerJob(
Job job,
Class<? extends DataStore<K2,V2>> dataStoreClass,
Class<K2> keyClass,
Class<V2> persistentClass,
Class<? extends GoraReducer<K1, V1, K2, V2>> reducerClass,
boolean reuseObjects) {
GoraOutputFormat.setOutput(job, dataStoreClass, keyClass, persistentClass, reuseObjects);
job.setReducerClass(reducerClass);
}
public static <K1, V1, K2, V2 extends Persistent>
void initReducerJob(
Job job,
DataStore<K2,V2> dataStore,
void initReducerJob(
Job job,
DataStore<K2,V2> dataStore,
Class<? extends GoraReducer<K1, V1, K2, V2>> reducerClass,
boolean reuseObjects) {
import org.apache.gora.util.GoraException;
, Class<K> keyClass, Class<T> persistent) throws GoraException {
D createDataStore(Class<D> dataStoreClass , Class<K> keyClass,
Class<T> persistent, String schemaName) throws GoraException {
, Class<T> persistent, Properties properties, String schemaName)
throws GoraException {
} catch (GoraException ex) {
throw ex;
} catch(Exception ex) {
throw new GoraException(ex);
, Class<K> keyClass, Class<T> persistent, Properties properties)
throws GoraException {
Class<T> persistentClass) throws GoraException {
throws GoraException {
try {
Class<? extends DataStore<K,T>> c
return getDataStore(c, keyClass, persistentClass);
} catch(GoraException ex) {
throw ex;
} catch (Exception ex) {
throw new GoraException(ex);
}
throws GoraException {
try {
Class k = Class.forName(keyClass);
Class p = Class.forName(persistentClass);
return getDataStore(dataStoreClass, k, p);
} catch(GoraException ex) {
throw ex;
} catch (Exception ex) {
throw new GoraException(ex);
}
Class<K> keyClass, Class<T> persistent) throws GoraException {
return getDataStore(defaultDataStoreClass, keyClass, persistent);
import org.apache.gora.util.GoraException;
MockDataStore dataStore;
try {
dataStore = DataStoreFactory.getDataStore(MockDataStore.class
, String.class, MockPersistent.class);
return dataStore;
} catch (GoraException ex) {
throw new RuntimeException(ex);
}
if(ReflectionUtils.hasConstructor(keyClass)) {
this.keyConstructor = ReflectionUtils.getConstructor(keyClass);
this.key = keyConstructor.newInstance(ReflectionUtils.EMPTY_OBJECT_ARRAY);
}
else if(keyConstructor == null) {
throw new RuntimeException("Key class does not have a no-arg constructor");
}
public static boolean hasConstructor(Class<?> clazz)
throws SecurityException, NoSuchMethodException {
if(clazz == null) {
throw new IllegalArgumentException("class cannot be null");
}
Constructor<?>[] consts = clazz.getConstructors();
boolean found = false;
for(Constructor<?> cons : consts) {
if(cons.getParameterTypes().length == 0) {
found = true;
}
}
return found;
}
}
if(!ex.getMessage().contains("closed")) {
throw new IOException(ex);
}
return new GoraRecordWriter(store, context);
return new GoraRecordReader<K, T>(partitionQuery, context);
import org.apache.hadoop.conf.Configuration;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class GoraRecordReader<K, T extends Persistent> extends RecordReader<K,T> {
public static final Logger LOG = LoggerFactory.getLogger(GoraRecordReader.class);
private static final String BUFFER_LIMIT_READ_NAME = "gora.buffer.read.limit";
private static final int BUFFER_LIMIT_READ_VALUE = 10000;
private GoraRecordCounter counter = new GoraRecordCounter();
public GoraRecordReader(Query<K,T> query, TaskAttemptContext context) {
Configuration configuration = context.getConfiguration();
int recordsMax = configuration.getInt(BUFFER_LIMIT_READ_NAME, BUFFER_LIMIT_READ_VALUE);
if (recordsMax <= 1) {
recordsMax = BUFFER_LIMIT_READ_VALUE;
}
counter.setRecordsMax(recordsMax);
this.query.setLimit(recordsMax);
if (counter.isModulo()) {
boolean firstBatch = (this.result == null);
if (! firstBatch) {
this.query.setStartKey(this.result.getKey());
if (this.query.getLimit() == counter.getRecordsMax()) {
}
}
if (! firstBatch) {
this.result.next();
}
counter.increment();
return this.result.next();
private static final String BUFFER_LIMIT_WRITE_NAME = "gora.buffer.write.limit";
private static final int BUFFER_LIMIT_WRITE_VALUE = 10000;
private GoraRecordCounter counter = new GoraRecordCounter();
int recordsMax = configuration.getInt(BUFFER_LIMIT_WRITE_NAME, BUFFER_LIMIT_WRITE_VALUE);
counter.setRecordsMax(recordsMax);
counter.increment();
if (counter.isModulo()) {
import org.apache.hadoop.hbase.regionserver.StoreFile.BloomType;
, String maxVersions, String timeToLive, String inMemory) {
columnDescriptor.setBloomFilterType(BloomType.valueOf(bloomFilter));
, bloomFilter, maxVersions, timeToLive, inMemory);
void setKeySpace(String keySpace);
void setConsistencyLevel(ConsistencyLevel level);
Row get(String key, Select select) throws IOException;
RowIterable getRange(String startKey, String endKey, int rowCount, Select select)
throws IOException;
RowIterable getTokenRange(String startToken, String endToken,
int rowCount, Select select) throws IOException;
void mutate(String key, Mutate mutation) throws IOException;
Map<String, Map<String, String>> describeKeySpace() throws IOException;
List<TokenRange> describeRing() throws IOException;
List<String> describeSplits(String startToken, String endToken, int size)
throws IOException;
void close();
K newKey() throws Exception;
T newPersistent();
K getCachedKey();
T getCachedPersistent();
Class<K> getKeyClass();
Class<T> getPersistentClass();
StateManager getStateManager();
Persistent newInstance(StateManager stateManager);
String[] getFields();
String getField(int index);
int getFieldIndex(String field);
void clear();
boolean isNew();
void setNew();
void clearNew();
boolean isDirty();
boolean isDirty(int fieldIndex);
boolean isDirty(String field);
void setDirty();
void setDirty(int fieldIndex);
void setDirty(String field);
void clearDirty(int fieldIndex);
void clearDirty(String field);
void clearDirty();
boolean isReadable(int fieldIndex);
boolean isReadable(String field);
void setReadable(int fieldIndex);
void setReadable(String field);
void clearReadable(int fieldIndex);
void clearReadable(String field);
void clearReadable();
Persistent clone();
void setManagedPersistent(Persistent persistent);
boolean isNew(Persistent persistent);
void setNew(Persistent persistent);
void clearNew(Persistent persistent);
boolean isDirty(Persistent persistent);
boolean isDirty(Persistent persistent, int fieldIndex);
void setDirty(Persistent persistent);
void setDirty(Persistent persistent, int fieldIndex);
void clearDirty(Persistent persistent, int fieldIndex);
void clearDirty(Persistent persistent);
boolean isReadable(Persistent persistent, int fieldIndex);
void setReadable(Persistent persistent, int fieldIndex);
void clearReadable(Persistent persistent, int fieldIndex);
void clearReadable(Persistent persistent);
State getState(K key);
void putState(K key, State state);
Map<K, State> states();
void clearStates();
String[] getLocations();
void setDataStore(DataStore<K,T> dataStore);
DataStore<K,T> getDataStore();
Result<K,T> execute() throws IOException;
void setFields(String... fieldNames);
String[] getFields();
void setKey(K key);
void setStartKey(K startKey);
void setEndKey(K endKey);
void setKeyRange(K startKey, K endKey);
K getKey();
K getStartKey();
K getEndKey();
void setTimestamp(long timestamp);
void setStartTime(long startTime);
void setEndTime(long endTime);
void setTimeRange(long startTime, long endTime);
long getTimestamp();
long getStartTime();
long getEndTime();
void setLimit(long limit);
long getLimit();
DataStore<K,T> getDataStore();
Query<K, T> getQuery();
boolean next() throws IOException;
K getKey();
T get();
Class<K> getKeyClass();
Class<T> getPersistentClass();
long getOffset();
float getProgress() throws IOException;
void close() throws IOException;
void initialize(Class<K> keyClass, Class<T> persistentClass,
void setKeyClass(Class<K> keyClass);
Class<K> getKeyClass();
void setPersistentClass(Class<T> persistentClass);
Class<T> getPersistentClass();
String getSchemaName();
void createSchema() throws IOException;
void deleteSchema() throws IOException;
void truncateSchema() throws IOException;
boolean schemaExists() throws IOException;
K newKey() throws IOException;
T newPersistent() throws IOException;
T get(K key) throws IOException;
T get(K key, String[] fields) throws IOException;
void put(K key, T obj) throws IOException;
boolean delete(K key) throws IOException;
long deleteByQuery(Query<K, T> query) throws IOException;
Result<K,T> execute(Query<K, T> query) throws IOException;
Query<K, T> newQuery();
List<PartitionQuery<K,T>> getPartitions(Query<K,T> query)
void flush() throws IOException;
void setBeanFactory(BeanFactory<K,T> beanFactory);
BeanFactory<K,T> getBeanFactory();
void close() throws IOException;
Configuration getConf();
void setConf(Configuration conf);
void readFields(DataInput in) throws IOException;
void write(DataOutput out) throws IOException;
void setInputPath(String inputPath);
void setOutputPath(String outputPath);
String getInputPath();
String getOutputPath();
void setInputStream(InputStream inputStream);
void setOutputStream(OutputStream outputStream);
InputStream getInputStream();
OutputStream getOutputStream();
inStore = DataStoreFactory.getDataStore(dataStoreClass, Long.class, Pageview.class);
String fieldType = type(fieldSchema);
fieldType = type(fieldSchema.getValueType());
fieldType = type(fieldSchema.getValueType());
import java.util.List;
import java.util.Map;
import org.apache.gora.query.Query;
public class CassandraQuery<K, T extends Persistent> extends QueryBase<K, T> {
private Query<K, T> query;
private Map<String, List<String>> familyMap;
public void setFamilyMap(Map<String, List<String>> familyMap) {
this.familyMap = familyMap;
}
public Map<String, List<String>> getFamilyMap() {
return familyMap;
}
public String[] getColumns(String family) {
List<String> columnList = familyMap.get(family);
String[] columns = new String[columnList.size()];
columns[i] = columnList.get(i);
}
return columns;
}
public Query<K, T> getQuery() {
return query;
}
public void setQuery(Query<K, T> query) {
this.query = query;
}
import java.util.List;
import java.util.Map;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Field;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class CassandraResult<K, T extends Persistent> extends ResultBase<K, T> {
public static final Logger LOG = LoggerFactory.getLogger(CassandraResult.class);
private int rowNumber;
private CassandraResultSet cassandraResultSet;
private Map<String, String> reverseMap;
public CassandraResult(DataStore<K, T> dataStore, Query<K, T> query) {
if (this.rowNumber < this.cassandraResultSet.size()) {
updatePersistent();
this.rowNumber;
return (this.rowNumber <= this.cassandraResultSet.size());
private void updatePersistent() throws IOException {
CassandraRow cassandraRow = this.cassandraResultSet.get(this.rowNumber);
this.key = (K) cassandraRow.getKey();
Schema schema = this.persistent.getSchema();
List<Field> fields = schema.getFields();
for (CassandraColumn cassandraColumn: cassandraRow) {
String family = cassandraColumn.getFamily();
int pos = this.persistent.getFieldIndex(fieldName);
Field field = fields.get(pos);
cassandraColumn.setField(field);
Object value = cassandraColumn.getValue();
this.persistent.put(pos, value);
this.persistent.clearDirty(pos);
public void close() throws IOException {
@Override
public float getProgress() throws IOException {
return (((float) this.rowNumber) / this.cassandraResultSet.size());
}
public void setResultSet(CassandraResultSet cassandraResultSet) {
this.cassandraResultSet = cassandraResultSet;
}
public void setReverseMap(Map<String, String> reverseMap) {
this.reverseMap = reverseMap;
}
}
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import me.prettyprint.cassandra.model.BasicColumnFamilyDefinition;
import me.prettyprint.cassandra.service.ThriftCfDef;
import me.prettyprint.hector.api.ddl.ColumnFamilyDefinition;
import me.prettyprint.hector.api.ddl.ColumnType;
import me.prettyprint.hector.api.ddl.ComparatorType;
import org.jdom.Document;
import org.jdom.Element;
import org.jdom.JDOMException;
import org.jdom.input.SAXBuilder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static final Logger LOG = LoggerFactory.getLogger(CassandraMapping.class);
private static final String MAPPING_FILE = "gora-cassandra-mapping.xml";
private static final String KEYSPACE_ELEMENT = "keyspace";
private static final String NAME_ATTRIBUTE = "name";
private static final String MAPPING_ELEMENT = "class";
private static final String COLUMN_ATTRIBUTE = "qualifier";
private static final String FAMILY_ATTRIBUTE = "family";
private static final String SUPER_ATTRIBUTE = "type";
private static final String CLUSTER_ATTRIBUTE = "cluster";
private static final String HOST_ATTRIBUTE = "host";
private String hostName;
private String clusterName;
private String keyspaceName;
private List<String> superFamilies = new ArrayList<String>();
private Map<String, String> familyMap = new HashMap<String, String>();
private Map<String, String> columnMap = new HashMap<String, String>();
private Map<String, BasicColumnFamilyDefinition> columnFamilyDefinitions = new HashMap<String, BasicColumnFamilyDefinition>();
public String getHostName() {
return this.hostName;
public String getClusterName() {
return this.clusterName;
public String getKeyspaceName() {
return this.keyspaceName;
@SuppressWarnings("unchecked")
public void loadConfiguration() throws JDOMException, IOException {
SAXBuilder saxBuilder = new SAXBuilder();
Document document = saxBuilder.build(getClass().getClassLoader().getResourceAsStream(MAPPING_FILE));
Element root = document.getRootElement();
Element keyspace = root.getChild(KEYSPACE_ELEMENT);
this.keyspaceName = keyspace.getAttributeValue(NAME_ATTRIBUTE);
this.clusterName = keyspace.getAttributeValue(CLUSTER_ATTRIBUTE);
this.hostName = keyspace.getAttributeValue(HOST_ATTRIBUTE);
List<Element> elements = keyspace.getChildren();
for (Element element: elements) {
BasicColumnFamilyDefinition cfDef = new BasicColumnFamilyDefinition();
String familyName = element.getAttributeValue(NAME_ATTRIBUTE);
String superAttribute = element.getAttributeValue(SUPER_ATTRIBUTE);
if (superAttribute != null) {
this.superFamilies.add(familyName);
cfDef.setColumnType(ColumnType.SUPER);
cfDef.setSubComparatorType(ComparatorType.UTF8TYPE);
}
cfDef.setKeyspaceName(this.keyspaceName);
cfDef.setName(familyName);
cfDef.setComparatorType(ComparatorType.UTF8TYPE);
cfDef.setDefaultValidationClass(ComparatorType.UTF8TYPE.getClassName());
this.columnFamilyDefinitions.put(familyName, cfDef);
}
Element mapping = root.getChild(MAPPING_ELEMENT);
elements = mapping.getChildren();
for (Element element: elements) {
String fieldName = element.getAttributeValue(NAME_ATTRIBUTE);
String familyName = element.getAttributeValue(FAMILY_ATTRIBUTE);
String columnName = element.getAttributeValue(COLUMN_ATTRIBUTE);
BasicColumnFamilyDefinition columnFamilyDefinition = this.columnFamilyDefinitions.get(familyName);
if (columnFamilyDefinition == null) {
}
this.familyMap.put(fieldName, familyName);
this.columnMap.put(fieldName, columnName);
}
public String getFamily(String name) {
return this.familyMap.get(name);
public String getColumn(String name) {
return this.columnMap.get(name);
}
public boolean isSuper(String family) {
return this.superFamilies.indexOf(family) != -1;
}
public List<ColumnFamilyDefinition> getColumnFamilyDefinitions() {
List<ColumnFamilyDefinition> list = new ArrayList<ColumnFamilyDefinition>();
for (String key: this.columnFamilyDefinitions.keySet()) {
ColumnFamilyDefinition columnFamilyDefinition = this.columnFamilyDefinitions.get(key);
ThriftCfDef thriftCfDef = new ThriftCfDef(columnFamilyDefinition);
list.add(thriftCfDef);
}
return list;
}
import me.prettyprint.hector.api.beans.ColumnSlice;
import me.prettyprint.hector.api.beans.HColumn;
import me.prettyprint.hector.api.beans.HSuperColumn;
import me.prettyprint.hector.api.beans.Row;
import me.prettyprint.hector.api.beans.SuperRow;
import me.prettyprint.hector.api.beans.SuperSlice;
import org.apache.gora.cassandra.query.CassandraResultSet;
import org.apache.gora.cassandra.query.CassandraRow;
import org.apache.gora.cassandra.query.CassandraSubColumn;
import org.apache.gora.cassandra.query.CassandraSuperColumn;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.persistency.impl.StateManagerImpl;
import org.apache.gora.query.impl.PartitionQueryImpl;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class CassandraStore<K, T extends Persistent> extends DataStoreBase<K, T> {
public static final Logger LOG = LoggerFactory.getLogger(CassandraStore.class);
private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
private Map<K, T> buffer = new HashMap<K, T>();
public CassandraStore() throws Exception {
this.cassandraClient.init();
public void close() throws IOException {
LOG.debug("close");
flush();
public void createSchema() {
LOG.debug("create schema");
this.cassandraClient.checkKeyspace();
return false;
public long deleteByQuery(Query<K, T> query) throws IOException {
return 0;
}
public void deleteSchema() throws IOException {
LOG.debug("delete schema");
this.cassandraClient.dropKeyspace();
}
@Override
public Result<K, T> execute(Query<K, T> query) throws IOException {
Map<String, List<String>> familyMap = this.cassandraClient.getFamilyMap(query);
Map<String, String> reverseMap = this.cassandraClient.getReverseMap(query);
CassandraQuery<K, T> cassandraQuery = new CassandraQuery<K, T>();
cassandraQuery.setQuery(query);
cassandraQuery.setFamilyMap(familyMap);
CassandraResult<K, T> cassandraResult = new CassandraResult<K, T>(this, query);
cassandraResult.setReverseMap(reverseMap);
CassandraResultSet cassandraResultSet = new CassandraResultSet();
for (String family : familyMap.keySet()) {
if (this.cassandraClient.isSuper(family)) {
addSuperColumns(family, cassandraQuery, cassandraResultSet);
} else {
addSubColumns(family, cassandraQuery, cassandraResultSet);
}
}
cassandraResult.setResultSet(cassandraResultSet);
return cassandraResult;
}
private void addSubColumns(String family, CassandraQuery<K, T> cassandraQuery,
CassandraResultSet cassandraResultSet) {
List<Row<String, String, String>> rows = this.cassandraClient.execute(cassandraQuery, family);
for (Row<String, String, String> row : rows) {
String key = row.getKey();
CassandraRow cassandraRow = cassandraResultSet.getRow(key);
if (cassandraRow == null) {
cassandraRow = new CassandraRow();
cassandraResultSet.putRow(key, cassandraRow);
cassandraRow.setKey(key);
}
ColumnSlice<String, String> columnSlice = row.getColumnSlice();
for (HColumn<String, String> hColumn : columnSlice.getColumns()) {
CassandraSubColumn cassandraSubColumn = new CassandraSubColumn();
cassandraSubColumn.setValue(hColumn);
cassandraSubColumn.setFamily(family);
cassandraRow.add(cassandraSubColumn);
}
}
}
private void addSuperColumns(String family, CassandraQuery<K, T> cassandraQuery,
CassandraResultSet cassandraResultSet) {
List<SuperRow<String, String, String, String>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
for (SuperRow<String, String, String, String> superRow: superRows) {
String key = superRow.getKey();
CassandraRow cassandraRow = cassandraResultSet.getRow(key);
if (cassandraRow == null) {
cassandraRow = new CassandraRow();
cassandraResultSet.putRow(key, cassandraRow);
cassandraRow.setKey(key);
}
SuperSlice<String, String, String> superSlice = superRow.getSuperSlice();
for (HSuperColumn<String, String, String> hSuperColumn: superSlice.getSuperColumns()) {
CassandraSuperColumn cassandraSuperColumn = new CassandraSuperColumn();
cassandraSuperColumn.setValue(hSuperColumn);
cassandraSuperColumn.setFamily(family);
cassandraRow.add(cassandraSuperColumn);
}
}
}
@Override
public void flush() throws IOException {
for (K key: this.buffer.keySet()) {
T value = this.buffer.get(key);
Schema schema = value.getSchema();
for (Field field: schema.getFields()) {
if (value.isDirty(field.pos())) {
addOrUpdateField((String) key, field, value.get(field.pos()));
}
}
}
this.buffer.clear();
}
@Override
public T get(K key, String[] fields) throws IOException {
return null;
}
@Override
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
throws IOException {
List<PartitionQuery<K,T>> partitions = new ArrayList<PartitionQuery<K,T>>();
partitions.add(new PartitionQueryImpl<K,T>(query));
return partitions;
}
@Override
public String getSchemaName() {
LOG.info("get schema name");
return null;
public void put(K key, T value) throws IOException {
T p = (T) value.newInstance(new StateManagerImpl());
Schema schema = value.getSchema();
for (Field field: schema.getFields()) {
if (value.isDirty(field.pos())) {
Object fieldValue = value.get(field.pos());
Schema fieldSchema = field.schema();
Type type = fieldSchema.getType();
switch(type) {
case RECORD:
Persistent persistent = (Persistent) fieldValue;
Persistent newRecord = persistent.newInstance(new StateManagerImpl());
for (Field member: fieldSchema.getFields()) {
newRecord.put(member.pos(), persistent.get(member.pos()));
fieldValue = newRecord;
break;
case MAP:
StatefulHashMap<?, ?> map = (StatefulHashMap<?, ?>) fieldValue;
StatefulHashMap<?, ?> newMap = new StatefulHashMap(map);
fieldValue = newMap;
break;
p.put(field.pos(), fieldValue);
}
this.buffer.put(key, p);
}
private void addOrUpdateField(String key, Field field, Object value) {
Schema schema = field.schema();
Type type = schema.getType();
switch (type) {
case STRING:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case INT:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case LONG:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case BYTES:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case FLOAT:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
for (Field member: schema.getFields()) {
Object memberValue = persistentBase.get(member.pos());
if (memberValue instanceof GenericArray<?>) {
GenericArray<String> array = (GenericArray<String>) memberValue;
if (array.size() == 0) {
continue;
}
}
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
}
} else {
}
}
break;
case MAP:
if (value != null) {
if (value instanceof StatefulHashMap<?, ?>) {
Map<Utf8, Object> map = (Map<Utf8, Object>) value;
for (Utf8 mapKey: map.keySet()) {
Object keyValue = map.get(mapKey);
if (keyValue instanceof GenericArray<?>) {
GenericArray<String> array = (GenericArray<String>) keyValue;
if (array.size() == 0) {
continue;
}
}
this.cassandraClient.addSubColumn(key, field.name(), mapKey.toString(), keyValue);
}
} else {
}
}
break;
default:
@Override
public boolean schemaExists() throws IOException {
LOG.info("schema exists");
return false;
}
}
public static final String BUFFER_LIMIT_READ_NAME = "gora.buffer.read.limit";
public static final int BUFFER_LIMIT_READ_VALUE = 10000;
}
import org.apache.hadoop.conf.Configuration;
= DataStoreFactory.getDataStore(dataStoreClass, String.class, WebPage.class, new Configuration());
Configuration conf = new Configuration();
dataStore = DataStoreFactory.getDataStore(dataStoreClass, keyClass, persistentClass, conf);
dataStore = DataStoreFactory.getDataStore(keyClass, persistentClass, conf);
Configuration conf = new Configuration();
String.class, WebPage.class, conf);
String.class, TokenDatum.class, conf);
inStore = DataStoreFactory.getDataStore(String.class, WebPage.class, conf);
outStore = DataStoreFactory.getDataStore(String.class, TokenDatum.class, conf);
, inKeyClass, inValueClass, job.getConfiguration());
DataStoreFactory.createDataStore(dataStoreClass, keyClass, rowClass, context.getConfiguration());
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
DataStore<K, T> dataStore, Class<K> keyClass, Class<T> persistent,
Properties properties) throws IOException {
, Class<K> keyClass, Class<T> persistent, Configuration conf) throws GoraException {
return createDataStore(dataStoreClass, keyClass, persistent, conf, properties);
Class<T> persistent, Configuration conf, String schemaName) throws GoraException {
return createDataStore(dataStoreClass, keyClass, persistent, conf, properties, schemaName);
, Class<T> persistent, Configuration conf, Properties properties, String schemaName)
if ((dataStore instanceof Configurable) && conf != null) {
((Configurable)dataStore).setConf(conf);
}
, Class<K> keyClass, Class<T> persistent, Configuration conf, Properties properties)
return createDataStore(dataStoreClass, keyClass, persistent, conf, properties, null);
Class<T> persistentClass, Configuration conf) throws GoraException {
dataStore = createDataStore(dataStoreClass, keyClass, persistentClass,
conf, properties);
String dataStoreClass, Class<K> keyClass, Class<T> persistentClass, Configuration conf)
return getDataStore(c, keyClass, persistentClass, conf);
String dataStoreClass, String keyClass, String persistentClass, Configuration conf)
return getDataStore(dataStoreClass, k, p, conf);
Class<K> keyClass, Class<T> persistent, Configuration conf) throws GoraException {
return getDataStore(defaultDataStoreClass, keyClass, persistent, conf);
import org.apache.hadoop.conf.Configuration;
, String.class, MockPersistent.class, new Configuration());
this.conf = HBaseConfiguration.create(getConf());
admin = new HBaseAdmin(this.conf);
import org.apache.hadoop.conf.Configuration;
Configuration conf = new Configuration();
inStore = DataStoreFactory.
getDataStore(dataStoreClass, Long.class, Pageview.class, conf);
outStore = DataStoreFactory.
getDataStore(dataStoreClass,
String.class, MetricDatum.class, conf);
inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, conf);
outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, conf);
import org.apache.hadoop.conf.Configuration;
dataStore = DataStoreFactory.getDataStore(Long.class, Pageview.class,
new Configuration());
import java.util.LinkedHashMap;
import java.util.Set;
private Map<K, T> buffer = new LinkedHashMap<K, T>();
Set<K> keys = this.buffer.keySet();
K[] keyArray = (K[]) keys.toArray();
for (K key: keyArray) {
if (value == null) {
continue;
}
for (K key: keyArray) {
this.buffer.remove(key);
}
table = new HTable(conf, mapping.getTableName());
final String tableName;
final byte[] family;
final byte[] qualifier;
this.family = family==null ? null : Arrays.copyOf(family, family.length);
this.qualifier = qualifier==null ? null :
Arrays.copyOf(qualifier, qualifier.length);
private final Map<String, HTableDescriptor> tableDescriptors;
private final String tableName;
private final Map<String, HBaseColumn> columnMap;
public HBaseMapping(Map<String, HTableDescriptor> tableDescriptors,
String tableName, Map<String, HBaseColumn> columnMap) {
super();
this.tableDescriptors = tableDescriptors;
this.columnMap = columnMap;
public static class HBaseMappingBuilder {
private Map<String, HTableDescriptor> tableDescriptors
= new HashMap<String, HTableDescriptor>();
private String tableName;
private Map<String, HBaseColumn> columnMap =
new HashMap<String, HBaseColumn>();
public void addTable(String tableName) {
if(!tableDescriptors.containsKey(tableName)) {
tableDescriptors.put(tableName, new HTableDescriptor(tableName));
}
}
public String getTableName() {
return tableName;
}
public void setTableName(String tableName) {
this.tableName = tableName;
}
public void addColumnFamily(String tableName, String familyName,
String compression, String blockCache, String blockSize,
String bloomFilter ,String maxVersions, String timeToLive,
String inMemory) {
HColumnDescriptor columnDescriptor = addColumnFamily(tableName,
familyName);
if(compression != null)
columnDescriptor.setCompressionType(Algorithm.valueOf(compression));
if(blockCache != null)
columnDescriptor.setBlockCacheEnabled(Boolean.parseBoolean(blockCache));
if(blockSize != null)
columnDescriptor.setBlocksize(Integer.parseInt(blockSize));
if(bloomFilter != null)
columnDescriptor.setBloomFilterType(BloomType.valueOf(bloomFilter));
if(maxVersions != null)
columnDescriptor.setMaxVersions(Integer.parseInt(maxVersions));
if(timeToLive != null)
columnDescriptor.setTimeToLive(Integer.parseInt(timeToLive));
if(inMemory != null)
columnDescriptor.setInMemory(Boolean.parseBoolean(inMemory));
getTable(tableName).addFamily(columnDescriptor);
}
public HTableDescriptor getTable(String tableName) {
return tableDescriptors.get(tableName);
}
public HColumnDescriptor addColumnFamily(String tableName,
String familyName) {
HTableDescriptor tableDescriptor = getTable(tableName);
HColumnDescriptor columnDescriptor =  tableDescriptor.getFamily(
Bytes.toBytes(familyName));
if(columnDescriptor == null) {
columnDescriptor = new HColumnDescriptor(familyName);
tableDescriptor.addFamily(columnDescriptor);
}
return columnDescriptor;
}
public void addField(String fieldName, String tableName, String family,
String qualifier) {
byte[] familyBytes = Bytes.toBytes(family);
byte[] qualifierBytes = qualifier == null ? null :
Bytes.toBytes(qualifier);
HBaseColumn column = new HBaseColumn(tableName, familyBytes,
qualifierBytes);
columnMap.put(fieldName, column);
}
public HBaseMapping build() {
return new HBaseMapping(tableDescriptors, tableName, columnMap);
}
}
import org.apache.gora.hbase.store.HBaseMapping.HBaseMappingBuilder;
private volatile HBaseAdmin admin;
private volatile HBaseTableConnection table;
private volatile Configuration conf;
private final boolean autoCreateSchema = true;
private volatile HBaseMapping mapping;
table = new HBaseTableConnection(getConf(), mapping.getTableName(), true);
HBaseMappingBuilder mappingBuilder = new HBaseMappingBuilder();
mappingBuilder.addTable(tableName);
mappingBuilder.addColumnFamily(tableName, familyName, compression,
blockCache, blockSize, bloomFilter, maxVersions, timeToLive,
inMemory);
if(classElement.getAttributeValue("keyClass").equals(
keyClass.getCanonicalName())
String tableName = getSchemaName(
classElement.getAttributeValue("table"), persistentClass);
mappingBuilder.addTable(tableName);
mappingBuilder.setTableName(tableName);
mappingBuilder.addField(fieldName, mappingBuilder.getTableName(),
family, qualifier);
mappingBuilder.addColumnFamily(mappingBuilder.getTableName(),
return mappingBuilder.build();
table.close();
List<Element> fieldElements = tableElement.getChildren("family");
import org.apache.gora.util.ClassLoadingUtils;
Class<K> keyClass = (Class<K>) ClassLoadingUtils.loadClass(args[0]);
Class<T> persistentClass = (Class<T>) ClassLoadingUtils.loadClass(args[1]);
import org.apache.gora.util.ClassLoadingUtils;
dataStore = (DataStore<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(dataStoreClass), conf);
import org.apache.gora.util.ClassLoadingUtils;
Class k = ClassLoadingUtils.loadClass(keyClass);
Class p = ClassLoadingUtils.loadClass(persistentClass);
import org.apache.gora.util.ClassLoadingUtils;
Class<K> keyClass = (Class<K>) ClassLoadingUtils.loadClass(Text.readString(in));
Class<T> persistentClass = (Class<T>)ClassLoadingUtils.loadClass(Text.readString(in));
Class<T> c = (Class<T>) ClassLoadingUtils.loadClass(objClass);
Class<T> c = (Class<T>)ClassLoadingUtils.loadClass(clazz);
T obj = (T) DefaultStringifier.load(conf, dataKey, ClassLoadingUtils.loadClass(className));
Class<?> clazz = ClassLoadingUtils.loadClass(classStr);
import org.apache.gora.util.ClassLoadingUtils;
ClassLoadingUtils.loadClass(jdbcDriverClass);
private StringSerializer stringSerializer = new StringSerializer();
@SuppressWarnings("unchecked")
public void addSubColumn(String key, String fieldName, String memberName, Object value) {
private StringSerializer stringSerializer = new StringSerializer();
@SuppressWarnings("unchecked")
public void addSubColumn(String key, String fieldName, String memberName, Object value) {
this.cluster.addKeyspace(keyspaceDefinition, true);
this.cluster.addKeyspace(keyspaceDefinition, true);
import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import me.prettyprint.hector.api.HConsistencyLevel;
ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
clmap.put("ColumnFamily", HConsistencyLevel.ONE);
configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);
import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import me.prettyprint.hector.api.HConsistencyLevel;
ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
clmap.put("ColumnFamily", HConsistencyLevel.ONE);
configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);
CassandraQuery<K,T> query = new CassandraQuery<K,T>();
query.setDataStore(this);
query.setKeyRange(key, key);
query.setFields(fields);
query.setLimit(1);
Result<K,T> result = execute(query);
boolean hasResult = result.next();
return hasResult ? result.get() : null;
public T get(K key) throws IOException {
return get(key, getFieldsToQuery(null));
CassandraQuery<K,T> query = new CassandraQuery<K,T>();
query.setDataStore(this);
query.setKeyRange(key, key);
query.setFields(fields);
query.setLimit(1);
Result<K,T> result = execute(query);
boolean hasResult = result.next();
return hasResult ? result.get() : null;
public T get(K key) throws IOException {
return get(key, getFieldsToQuery(null));
import java.io.OutputStream;
import java.util.HashMap;
import java.util.Map;
import org.apache.avro.io.DecoderFactory;
public static final ThreadLocal<BinaryDecoder> decoders =
new ThreadLocal<BinaryDecoder>();
public static final ThreadLocal<BinaryEncoderWithStream> encoders =
new ThreadLocal<BinaryEncoderWithStream>();
public static final class BinaryEncoderWithStream extends BinaryEncoder {
public BinaryEncoderWithStream(OutputStream out) {
super(out);
}
protected OutputStream getOut() {
return out;
}
}
public static final ThreadLocal<Map<String, SpecificDatumReader<?>>>
readerMaps = new ThreadLocal<Map<String, SpecificDatumReader<?>>>() {
protected Map<String,SpecificDatumReader<?>> initialValue() {
return new HashMap<String, SpecificDatumReader<?>>();
};
};
public static final ThreadLocal<Map<String, SpecificDatumWriter<?>>>
writerMaps = new ThreadLocal<Map<String, SpecificDatumWriter<?>>>() {
protected Map<String,SpecificDatumWriter<?>> initialValue() {
return new HashMap<String, SpecificDatumWriter<?>>();
};
};
Map<String, SpecificDatumReader<?>> readerMap = readerMaps.get();
SpecificDatumReader<?> reader = readerMap.get(schema.getFullName());
if (reader == null) {
reader = new SpecificDatumReader(schema);
readerMap.put(schema.getFullName(), reader);
}
BinaryDecoder decoderFromCache = decoders.get();
BinaryDecoder decoder=DecoderFactory.defaultFactory().
createBinaryDecoder(val, decoderFromCache);
if (decoderFromCache==null) {
decoders.set(decoder);
}
@SuppressWarnings({ "rawtypes", "unchecked" })
Map<String, SpecificDatumWriter<?>> writerMap = writerMaps.get();
SpecificDatumWriter writer = writerMap.get(schema.getFullName());
if (writer == null) {
writer = new SpecificDatumWriter(schema);
writerMap.put(schema.getFullName(),writer);
}
BinaryEncoderWithStream encoder = encoders.get();
if (encoder == null) {
encoder = new BinaryEncoderWithStream(new ByteArrayOutputStream());
encoders.set(encoder);
}
ByteArrayOutputStream os = (ByteArrayOutputStream) encoder.getOut();
os.reset();
import java.io.OutputStream;
import java.util.HashMap;
import java.util.Map;
import org.apache.avro.io.DecoderFactory;
public static final ThreadLocal<BinaryDecoder> decoders =
new ThreadLocal<BinaryDecoder>();
public static final ThreadLocal<BinaryEncoderWithStream> encoders =
new ThreadLocal<BinaryEncoderWithStream>();
public static final class BinaryEncoderWithStream extends BinaryEncoder {
public BinaryEncoderWithStream(OutputStream out) {
super(out);
}
protected OutputStream getOut() {
return out;
}
}
public static final ThreadLocal<Map<String, SpecificDatumReader<?>>>
readerMaps = new ThreadLocal<Map<String, SpecificDatumReader<?>>>() {
protected Map<String,SpecificDatumReader<?>> initialValue() {
return new HashMap<String, SpecificDatumReader<?>>();
};
};
public static final ThreadLocal<Map<String, SpecificDatumWriter<?>>>
writerMaps = new ThreadLocal<Map<String, SpecificDatumWriter<?>>>() {
protected Map<String,SpecificDatumWriter<?>> initialValue() {
return new HashMap<String, SpecificDatumWriter<?>>();
};
};
Map<String, SpecificDatumReader<?>> readerMap = readerMaps.get();
SpecificDatumReader<?> reader = readerMap.get(schema.getFullName());
if (reader == null) {
reader = new SpecificDatumReader(schema);
readerMap.put(schema.getFullName(), reader);
}
BinaryDecoder decoderFromCache = decoders.get();
BinaryDecoder decoder=DecoderFactory.defaultFactory().
createBinaryDecoder(val, decoderFromCache);
if (decoderFromCache==null) {
decoders.set(decoder);
}
@SuppressWarnings({ "rawtypes", "unchecked" })
Map<String, SpecificDatumWriter<?>> writerMap = writerMaps.get();
SpecificDatumWriter writer = writerMap.get(schema.getFullName());
if (writer == null) {
writer = new SpecificDatumWriter(schema);
writerMap.put(schema.getFullName(),writer);
}
BinaryEncoderWithStream encoder = encoders.get();
if (encoder == null) {
encoder = new BinaryEncoderWithStream(new ByteArrayOutputStream());
encoders.set(encoder);
}
ByteArrayOutputStream os = (ByteArrayOutputStream) encoder.getOut();
os.reset();
public HBaseColumn(byte[] family, byte[] qualifier) {
@Override
public String toString() {
}
private final HTableDescriptor tableDescriptor;
public HBaseMapping(HTableDescriptor tableDescriptor,
Map<String, HBaseColumn> columnMap) {
this.tableDescriptor = tableDescriptor;
return tableDescriptor.getNameAsString();
return tableDescriptor;
public static class HBaseMappingBuilder {
private Map<String, Map<String, HColumnDescriptor>> tableToFamilies =
new HashMap<String, Map<String, HColumnDescriptor>>();
private String tableName;
public void addFamilyProps(String tableName, String familyName,
Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);;
HColumnDescriptor columnDescriptor = getOrCreateFamily(familyName, families);
}
public void addColumnFamily(String tableName, String familyName) {
Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);
getOrCreateFamily(familyName, families);
public void addField(String fieldName, String family, String qualifier) {
HBaseColumn column = new HBaseColumn(familyBytes, qualifierBytes);
private HColumnDescriptor getOrCreateFamily(String familyName,
Map<String, HColumnDescriptor> families) {
HColumnDescriptor columnDescriptor = families.get(familyName);
if (columnDescriptor == null) {
columnDescriptor=new HColumnDescriptor(familyName);
families.put(familyName, columnDescriptor);
}
return columnDescriptor;
}
private Map<String, HColumnDescriptor> getOrCreateFamilies(String tableName) {
Map<String, HColumnDescriptor> families;
families = tableToFamilies.get(tableName);
if (families == null) {
families = new HashMap<String, HColumnDescriptor>();
tableToFamilies.put(tableName, families);
}
return families;
}
public void renameTable(String oldName, String newName) {
Map<String, HColumnDescriptor> families = tableToFamilies.remove(oldName);
tableToFamilies.put(newName, families);
}
if (tableName == null) throw new IllegalStateException("tableName is not specified");
Map<String, HColumnDescriptor> families = tableToFamilies.get(tableName);
HTableDescriptor tableDescriptors = new HTableDescriptor(tableName);
for (HColumnDescriptor desc : families.values()) {
tableDescriptors.addFamily(desc);
}
return new HBaseMapping(tableDescriptors, columnMap);
table = new HBaseTableConnection(getConf(), getSchemaName(), true);
if(schemaExists()) {
if(!schemaExists()) {
admin.disableTable(getSchemaName());
admin.deleteTable(getSchemaName());
mappingBuilder.addFamilyProps(tableName, familyName, compression,
String tableNameFromMapping = classElement.getAttributeValue("table");
mappingBuilder.addField(fieldName, family, qualifier);
mappingBuilder.addColumnFamily(tableNameFromMapping, family);
String tableName = getSchemaName(tableNameFromMapping, persistentClass);
if (!tableNameFromMapping.equals(tableName)) {
log.info("Keyclass and nameclass match but mismatching table names "
mappingBuilder.renameTable(tableNameFromMapping, tableName);
}
mappingBuilder.setTableName(tableName);
public HBaseColumn(byte[] family, byte[] qualifier) {
@Override
public String toString() {
}
private final HTableDescriptor tableDescriptor;
public HBaseMapping(HTableDescriptor tableDescriptor,
Map<String, HBaseColumn> columnMap) {
this.tableDescriptor = tableDescriptor;
return tableDescriptor.getNameAsString();
return tableDescriptor;
public static class HBaseMappingBuilder {
private Map<String, Map<String, HColumnDescriptor>> tableToFamilies =
new HashMap<String, Map<String, HColumnDescriptor>>();
private String tableName;
public void addFamilyProps(String tableName, String familyName,
Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);;
HColumnDescriptor columnDescriptor = getOrCreateFamily(familyName, families);
}
public void addColumnFamily(String tableName, String familyName) {
Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);
getOrCreateFamily(familyName, families);
public void addField(String fieldName, String family, String qualifier) {
HBaseColumn column = new HBaseColumn(familyBytes, qualifierBytes);
private HColumnDescriptor getOrCreateFamily(String familyName,
Map<String, HColumnDescriptor> families) {
HColumnDescriptor columnDescriptor = families.get(familyName);
if (columnDescriptor == null) {
columnDescriptor=new HColumnDescriptor(familyName);
families.put(familyName, columnDescriptor);
}
return columnDescriptor;
}
private Map<String, HColumnDescriptor> getOrCreateFamilies(String tableName) {
Map<String, HColumnDescriptor> families;
families = tableToFamilies.get(tableName);
if (families == null) {
families = new HashMap<String, HColumnDescriptor>();
tableToFamilies.put(tableName, families);
}
return families;
}
public void renameTable(String oldName, String newName) {
Map<String, HColumnDescriptor> families = tableToFamilies.remove(oldName);
tableToFamilies.put(newName, families);
}
if (tableName == null) throw new IllegalStateException("tableName is not specified");
Map<String, HColumnDescriptor> families = tableToFamilies.get(tableName);
HTableDescriptor tableDescriptors = new HTableDescriptor(tableName);
for (HColumnDescriptor desc : families.values()) {
tableDescriptors.addFamily(desc);
}
return new HBaseMapping(tableDescriptors, columnMap);
table = new HBaseTableConnection(getConf(), getSchemaName(), true);
if(schemaExists()) {
if(!schemaExists()) {
admin.disableTable(getSchemaName());
admin.deleteTable(getSchemaName());
mappingBuilder.addFamilyProps(tableName, familyName, compression,
String tableNameFromMapping = classElement.getAttributeValue("table");
mappingBuilder.addField(fieldName, family, qualifier);
mappingBuilder.addColumnFamily(tableNameFromMapping, family);
String tableName = getSchemaName(tableNameFromMapping, persistentClass);
if (!tableNameFromMapping.equals(tableName)) {
log.info("Keyclass and nameclass match but mismatching table names "
mappingBuilder.renameTable(tableNameFromMapping, tableName);
}
mappingBuilder.setTableName(tableName);
String tableName = getSchemaName(tableNameFromMapping, persistentClass);
if (!tableName.equals(tableNameFromMapping)) {
log.info("Keyclass and nameclass match but mismatching table names "
if (tableNameFromMapping != null) {
mappingBuilder.renameTable(tableNameFromMapping, tableName);
}
}
mappingBuilder.setTableName(tableName);
mappingBuilder.addColumnFamily(tableName, family);
String tableName = getSchemaName(tableNameFromMapping, persistentClass);
if (!tableName.equals(tableNameFromMapping)) {
log.info("Keyclass and nameclass match but mismatching table names "
if (tableNameFromMapping != null) {
mappingBuilder.renameTable(tableNameFromMapping, tableName);
}
}
mappingBuilder.setTableName(tableName);
mappingBuilder.addColumnFamily(tableName, family);
public void initialize() throws IOException {
private void setColumnConstraintForQuery() throws IOException {
private void getColumnConstraint() throws IOException {
return false;
return false;
return 0;
return null;
return null;
private void constructWhereClause() throws IOException {
private void setParametersForPreparedStatement() throws SQLException, IOException {
return null;
protected byte[] getBytes() throws SQLException, IOException {
protected Object readField() throws SQLException, IOException {
return null;
return null;
protected void setBytes() throws SQLException   {
protected void setField() throws IOException, SQLException {
return null;
protected String getIdentifier() {
return null;
private void addColumn() {
protected void createSqlTable() {
private void addField() throws IOException {
protected SqlMapping readMapping() throws IOException {
return null;
public void initialize() throws IOException {
private void setColumnConstraintForQuery() throws IOException {
private void getColumnConstraint() throws IOException {
return false;
return false;
return 0;
return null;
return null;
private void constructWhereClause() throws IOException {
private void setParametersForPreparedStatement() throws SQLException, IOException {
return null;
protected byte[] getBytes() throws SQLException, IOException {
protected Object readField() throws SQLException, IOException {
return null;
return null;
protected void setBytes() throws SQLException   {
protected void setField() throws IOException, SQLException {
return null;
protected String getIdentifier() {
return null;
private void addColumn() {
protected void createSqlTable() {
private void addField() throws IOException {
protected SqlMapping readMapping() throws IOException {
return null;
public static final String SCHEMA_NAME = "schema.name";
@Deprecated()
public static final Properties properties = createProps();
public static Properties createProps() {
Properties properties = new Properties();
InputStream stream = DataStoreFactory.class.getClassLoader()
.getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
if(stream != null) {
try {
properties.load(stream);
return properties;
} finally {
stream.close();
}
} else {
}
return properties;
} catch(Exception e) {
throw new RuntimeException(e);
return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), null);
return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), schemaName);
return createDataStore(dataStoreClass, keyClass, persistentClass, conf, createProps(), null);
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
return createDataStore(c, keyClass, persistentClass, conf, createProps(), null);
@SuppressWarnings({ "unchecked" })
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
Class<? extends DataStore<K,T>> c
= (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
Class<K> k = (Class<K>) ClassLoadingUtils.loadClass(keyClass);
Class<T> p = (Class<T>) ClassLoadingUtils.loadClass(persistentClass);
return createDataStore(c, k, p, conf, createProps(), null);
@SuppressWarnings("unchecked")
Properties createProps = createProps();
Class<? extends DataStore<K, T>> c;
try {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
} catch (Exception ex) {
throw new GoraException(ex);
return createDataStore(c, keyClass, persistent, conf, createProps, null);
private static String getDefaultDataStore(Properties properties) {
return getProperty(properties, GORA_DEFAULT_DATASTORE_KEY);
if(value != null) {
}
void setProperty(Properties properties, Class<D> dataStoreClass, String baseKey, String value) {
if (schemaName != null) {
setProperty(properties, SCHEMA_NAME, schemaName);
}
import java.util.Map.Entry;
import org.apache.gora.util.WritableUtils;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
Properties props = WritableUtils.readProperties(in);
initialize(keyClass, persistentClass, props);
WritableUtils.writeProperties(out, properties);
public static final String SCHEMA_NAME = "schema.name";
@Deprecated()
public static final Properties properties = createProps();
public static Properties createProps() {
Properties properties = new Properties();
InputStream stream = DataStoreFactory.class.getClassLoader()
.getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
if(stream != null) {
try {
properties.load(stream);
return properties;
} finally {
stream.close();
}
} else {
}
return properties;
} catch(Exception e) {
throw new RuntimeException(e);
return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), null);
return createDataStore(dataStoreClass, keyClass, persistent, conf, createProps(), schemaName);
return createDataStore(dataStoreClass, keyClass, persistentClass, conf, createProps(), null);
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
return createDataStore(c, keyClass, persistentClass, conf, createProps(), null);
@SuppressWarnings({ "unchecked" })
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
Class<? extends DataStore<K,T>> c
= (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
Class<K> k = (Class<K>) ClassLoadingUtils.loadClass(keyClass);
Class<T> p = (Class<T>) ClassLoadingUtils.loadClass(persistentClass);
return createDataStore(c, k, p, conf, createProps(), null);
@SuppressWarnings("unchecked")
Properties createProps = createProps();
Class<? extends DataStore<K, T>> c;
try {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
} catch (Exception ex) {
throw new GoraException(ex);
return createDataStore(c, keyClass, persistent, conf, createProps, null);
private static String getDefaultDataStore(Properties properties) {
return getProperty(properties, GORA_DEFAULT_DATASTORE_KEY);
if(value != null) {
}
void setProperty(Properties properties, Class<D> dataStoreClass, String baseKey, String value) {
if (schemaName != null) {
setProperty(properties, SCHEMA_NAME, schemaName);
}
import java.util.Map.Entry;
import org.apache.gora.util.WritableUtils;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
Properties props = WritableUtils.readProperties(in);
initialize(keyClass, persistentClass, props);
WritableUtils.writeProperties(out, properties);
public void initialize() throws Exception {
this.cassandraClient.initialize();
public void initialize() throws Exception {
this.cassandraClient.initialize();
boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
for (HTable table : pool) {
table.flushCommits();
}
for (HTable table : pool) {
table.flushCommits();
}
import java.util.HashMap;
import java.util.Map.Entry;
}
else {
persistent.clearDirty(i);
Map<Utf8, State> tempStates = null;
tempStates = new HashMap<Utf8, State>();
tempStates.put(key, state);
super.readMap(map, expected, in);
map.clearStates();
if (readDirtyBits) {
for (Entry<Utf8, State> entry : tempStates.entrySet()) {
map.putState(entry.getKey(), entry.getValue());
}
}
return map;
((StatefulHashMap)old).reuse();
for (java.util.Map.Entry<K, V> entry : m.entrySet()) {
put(entry.getKey(), entry.getValue());
}
clearStates();
keyStates.remove(key);
V old = super.put(key, value);
if (!value.equals(old)) {
keyStates.put(key, State.DIRTY);
}
return old;
keyStates.put((K) key, State.DELETED);
return null;
public void reuse() {
super.clear();
clearStates();
}
void reuse();
import org.apache.gora.persistency.StatefulHashMap;
case MAP:
if(get(i) != null) {
if (get(i) instanceof StatefulHashMap) {
((StatefulHashMap)get(i)).reuse();
} else {
((Map)get(i)).clear();
}
}
break;
import java.util.HashMap;
import java.util.Map.Entry;
}
else {
persistent.clearDirty(i);
Map<Utf8, State> tempStates = null;
tempStates = new HashMap<Utf8, State>();
tempStates.put(key, state);
super.readMap(map, expected, in);
map.clearStates();
if (readDirtyBits) {
for (Entry<Utf8, State> entry : tempStates.entrySet()) {
map.putState(entry.getKey(), entry.getValue());
}
}
return map;
((StatefulHashMap)old).reuse();
for (java.util.Map.Entry<K, V> entry : m.entrySet()) {
put(entry.getKey(), entry.getValue());
}
clearStates();
keyStates.remove(key);
V old = super.put(key, value);
if (!value.equals(old)) {
keyStates.put(key, State.DIRTY);
}
return old;
keyStates.put((K) key, State.DELETED);
return null;
public void reuse() {
super.clear();
clearStates();
}
void reuse();
import org.apache.gora.persistency.StatefulHashMap;
case MAP:
if(get(i) != null) {
if (get(i) instanceof StatefulHashMap) {
((StatefulHashMap)get(i)).reuse();
} else {
((Map)get(i)).clear();
}
}
break;
public static final Log LOG = LogFactory.getLog(HBaseStore.class);
LOG.info("Keyclass and nameclass match but mismatching table names "
public static final Log LOG = LogFactory.getLog(HBaseStore.class);
LOG.info("Keyclass and nameclass match but mismatching table names "
if (this.result != null) {
this.result.close();
}
if (result != null) {
result.close();
}
if (this.result != null) {
this.result.close();
}
if (result != null) {
result.close();
}
tl.invalidateCache();
tl.invalidateCache();
tl.invalidateCache();
tl.invalidateCache();
manager.get(Long.parseLong(args[1]));
manager.get(Long.parseLong(args[1]));
fieldType = type(fieldSchema.getElementType());
fieldType = type(fieldSchema.getElementType());
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log log = LogFactory.getLog(GoraCompiler.class);
System.err.println("Usage: Compiler <schema file> <output dir>");
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log log = LogFactory.getLog(GoraCompiler.class);
System.err.println("Usage: Compiler <schema file> <output dir>");
private Map<String, BasicColumnFamilyDefinition> columnFamilyDefinitions =
new HashMap<String, BasicColumnFamilyDefinition>();
if (document == null) {
}
if (keyspace == null) {
LOG.warn("Error locating Cassandra Keyspace element!");
} else {
}
if (this.keyspaceName == null) {
LOG.warn("Error locating Cassandra Keyspace name attribute!");
} else {
}
if (this.clusterName == null) {
LOG.warn("Error locating Cassandra Keyspace cluster attribute!");
} else {
}
if (this.hostName == null) {
LOG.warn("Error locating Cassandra Keyspace host attribute!");
} else {
}
if (familyName == null) {
LOG.warn("Error locating column family name attribute!");
} else {
}
LOG.info("Located super column family");
private Map<String, BasicColumnFamilyDefinition> columnFamilyDefinitions =
new HashMap<String, BasicColumnFamilyDefinition>();
if (document == null) {
}
if (keyspace == null) {
LOG.warn("Error locating Cassandra Keyspace element!");
} else {
}
if (this.keyspaceName == null) {
LOG.warn("Error locating Cassandra Keyspace name attribute!");
} else {
}
if (this.clusterName == null) {
LOG.warn("Error locating Cassandra Keyspace cluster attribute!");
} else {
}
if (this.hostName == null) {
LOG.warn("Error locating Cassandra Keyspace host attribute!");
} else {
}
if (familyName == null) {
LOG.warn("Error locating column family name attribute!");
} else {
}
LOG.info("Located super column family");
return this.list.hashCode();
return this.list.hashCode();
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(WebPageDataCreator.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(GoraCompiler.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static final Logger log = LoggerFactory.getLogger(DataStoreFactory.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static final Logger LOG = LoggerFactory.getLogger(HBaseStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(SqlStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(LogAnalytics.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(LogManager.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(WebPageDataCreator.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(GoraCompiler.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static final Logger log = LoggerFactory.getLogger(DataStoreFactory.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static final Logger LOG = LoggerFactory.getLogger(HBaseStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(SqlStore.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(LogAnalytics.class);
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(LogManager.class);
private CassandraResultSet<K> cassandraResultSet;
CassandraRow<K> cassandraRow = this.cassandraResultSet.get(this.rowNumber);
this.key = cassandraRow.getKey();
public void setResultSet(CassandraResultSet<K> cassandraResultSet) {
public class CassandraResultSet<K> extends ArrayList<CassandraRow<K>> {
private HashMap<K, Integer> indexMap = new HashMap<K, Integer>();
public CassandraRow<K> getRow(K key) {
public void putRow(K key, CassandraRow<K> cassandraRow) {
public class CassandraRow<K> extends ArrayList<CassandraColumn> {
private K key;
public K getKey() {
public void setKey(K key) {
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
private HColumn<String, ByteBuffer> hColumn;
ByteBuffer valueByteBuffer = hColumn.getValue();
value = new Utf8(StringSerializer.get().fromByteBuffer(valueByteBuffer));
value = valueByteBuffer;
value = IntegerSerializer.get().fromByteBuffer(valueByteBuffer);
value = LongSerializer.get().fromByteBuffer(valueByteBuffer);
value = FloatSerializer.get().fromByteBuffer(valueByteBuffer);
break;
case DOUBLE:
value = DoubleSerializer.get().fromByteBuffer(valueByteBuffer);
String valueString = StringSerializer.get().fromByteBuffer(valueByteBuffer);
public void setValue(HColumn<String, ByteBuffer> hColumn) {
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
private HSuperColumn<String, String, ByteBuffer> hSuperColumn;
for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
ByteBuffer memberByteBuffer = hColumn.getValue();
memberValue = new Utf8(StringSerializer.get().fromByteBuffer(memberByteBuffer));
memberValue = memberByteBuffer;
break;
case INT:
memberValue = IntegerSerializer.get().fromByteBuffer(memberByteBuffer);
break;
case LONG:
memberValue = LongSerializer.get().fromByteBuffer(memberByteBuffer);
break;
case FLOAT:
memberValue = FloatSerializer.get().fromByteBuffer(memberByteBuffer);
break;
case DOUBLE:
memberValue = DoubleSerializer.get().fromByteBuffer(memberByteBuffer);
for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
public void setValue(HSuperColumn<String, String, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.SerializerTypeInferer;
import me.prettyprint.hector.api.Serializer;
import org.apache.avro.util.Utf8;
private Mutator<K> mutator;
private Class<K> keyClass;
private ByteBufferSerializer byteBufferSerializer = new ByteBufferSerializer();
private Serializer<K> keySerializer;
public void initialize(Class<K> keyClass) throws Exception {
this.keyClass = keyClass;
this.keySerializer = SerializerTypeInferer.getSerializer(keyClass);
this.mutator = HFactory.createMutator(this.keyspace, this.keySerializer);
public void addColumn(K key, String fieldName, Object value) {
ByteBuffer byteBuffer = null;
byteBuffer = (ByteBuffer) value;
}
else if (value instanceof Utf8) {
byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
}
else if (value instanceof Float) {
byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
}
else if (value instanceof Double) {
byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
}
else {
byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, stringSerializer, byteBufferSerializer));
public void addSubColumn(K key, String fieldName, String memberName, Object value) {
ByteBuffer byteBuffer = null;
byteBuffer = (ByteBuffer) value;
}
else if (value instanceof Utf8) {
byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
}
else if (value instanceof Float) {
byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
}
else if (value instanceof Double) {
byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
}
else {
byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(memberName, byteBuffer, stringSerializer, byteBufferSerializer)), this.stringSerializer, this.stringSerializer, this.byteBufferSerializer));
public List<Row<K, String, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
if (limit < 1) {
limit = Integer.MAX_VALUE;
K startKey = query.getStartKey();
K endKey = query.getEndKey();
RangeSlicesQuery<K, String, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, stringSerializer, byteBufferSerializer);
QueryResult<OrderedRows<K, String, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
OrderedRows<K, String, ByteBuffer> orderedRows = queryResult.get();
public List<SuperRow<K, String, String, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
if (limit < 1) {
limit = Integer.MAX_VALUE;
K startKey = query.getStartKey();
K endKey = query.getEndKey();
RangeSuperSlicesQuery<K, String, String, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, this.stringSerializer, this.stringSerializer, this.byteBufferSerializer);
QueryResult<OrderedSuperRows<K, String, String, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
OrderedSuperRows<K, String, String, ByteBuffer> orderedRows = queryResult.get();
import java.nio.ByteBuffer;
import java.util.Properties;
}
public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) throws IOException {
super.initialize(keyClass, persistent, properties);
try {
this.cassandraClient.initialize(keyClass);
}
catch (Exception e) {
throw new IOException(e.getMessage(), e);
}
List<Row<K, String, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
for (Row<K, String, ByteBuffer> row : rows) {
K key = row.getKey();
CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
cassandraRow = new CassandraRow<K>();
ColumnSlice<String, ByteBuffer> columnSlice = row.getColumnSlice();
for (HColumn<String, ByteBuffer> hColumn : columnSlice.getColumns()) {
List<SuperRow<K, String, String, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
for (SuperRow<K, String, String, ByteBuffer> superRow: superRows) {
K key = superRow.getKey();
CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
SuperSlice<String, String, ByteBuffer> superSlice = superRow.getSuperSlice();
for (HSuperColumn<String, String, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
addOrUpdateField(key, field, value.get(field.pos()));
Query<K,T> query = new CassandraQuery<K, T>(this);
query.setFields(getFieldsToQuery(null));
return query;
private void addOrUpdateField(K key, Field field, Object value) {
case DOUBLE:
if (memberValue instanceof Utf8) {
memberValue = memberValue.toString();
}
if (keyValue instanceof Utf8) {
keyValue = keyValue.toString();
}
private CassandraResultSet<K> cassandraResultSet;
CassandraRow<K> cassandraRow = this.cassandraResultSet.get(this.rowNumber);
this.key = cassandraRow.getKey();
public void setResultSet(CassandraResultSet<K> cassandraResultSet) {
public class CassandraResultSet<K> extends ArrayList<CassandraRow<K>> {
private HashMap<K, Integer> indexMap = new HashMap<K, Integer>();
public CassandraRow<K> getRow(K key) {
public void putRow(K key, CassandraRow<K> cassandraRow) {
public class CassandraRow<K> extends ArrayList<CassandraColumn> {
private K key;
public K getKey() {
public void setKey(K key) {
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
private HColumn<String, ByteBuffer> hColumn;
ByteBuffer valueByteBuffer = hColumn.getValue();
value = new Utf8(StringSerializer.get().fromByteBuffer(valueByteBuffer));
value = valueByteBuffer;
value = IntegerSerializer.get().fromByteBuffer(valueByteBuffer);
value = LongSerializer.get().fromByteBuffer(valueByteBuffer);
value = FloatSerializer.get().fromByteBuffer(valueByteBuffer);
break;
case DOUBLE:
value = DoubleSerializer.get().fromByteBuffer(valueByteBuffer);
String valueString = StringSerializer.get().fromByteBuffer(valueByteBuffer);
public void setValue(HColumn<String, ByteBuffer> hColumn) {
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
private HSuperColumn<String, String, ByteBuffer> hSuperColumn;
for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
ByteBuffer memberByteBuffer = hColumn.getValue();
memberValue = new Utf8(StringSerializer.get().fromByteBuffer(memberByteBuffer));
memberValue = memberByteBuffer;
break;
case INT:
memberValue = IntegerSerializer.get().fromByteBuffer(memberByteBuffer);
break;
case LONG:
memberValue = LongSerializer.get().fromByteBuffer(memberByteBuffer);
break;
case FLOAT:
memberValue = FloatSerializer.get().fromByteBuffer(memberByteBuffer);
break;
case DOUBLE:
memberValue = DoubleSerializer.get().fromByteBuffer(memberByteBuffer);
for (HColumn<String, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
public void setValue(HSuperColumn<String, String, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.SerializerTypeInferer;
import me.prettyprint.hector.api.Serializer;
import org.apache.avro.util.Utf8;
private Mutator<K> mutator;
private Class<K> keyClass;
private ByteBufferSerializer byteBufferSerializer = new ByteBufferSerializer();
private Serializer<K> keySerializer;
public void initialize(Class<K> keyClass) throws Exception {
this.keyClass = keyClass;
this.keySerializer = SerializerTypeInferer.getSerializer(keyClass);
this.mutator = HFactory.createMutator(this.keyspace, this.keySerializer);
public void addColumn(K key, String fieldName, Object value) {
ByteBuffer byteBuffer = null;
byteBuffer = (ByteBuffer) value;
}
else if (value instanceof Utf8) {
byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
}
else if (value instanceof Float) {
byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
}
else if (value instanceof Double) {
byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
}
else {
byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, stringSerializer, byteBufferSerializer));
public void addSubColumn(K key, String fieldName, String memberName, Object value) {
ByteBuffer byteBuffer = null;
byteBuffer = (ByteBuffer) value;
}
else if (value instanceof Utf8) {
byteBuffer = stringSerializer.toByteBuffer(((Utf8)value).toString());
}
else if (value instanceof Float) {
byteBuffer = FloatSerializer.get().toByteBuffer((Float)value);
}
else if (value instanceof Double) {
byteBuffer = DoubleSerializer.get().toByteBuffer((Double)value);
}
else {
byteBuffer = SerializerTypeInferer.getSerializer(value).toByteBuffer(value);
this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(memberName, byteBuffer, stringSerializer, byteBufferSerializer)), this.stringSerializer, this.stringSerializer, this.byteBufferSerializer));
public List<Row<K, String, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
if (limit < 1) {
limit = Integer.MAX_VALUE;
K startKey = query.getStartKey();
K endKey = query.getEndKey();
RangeSlicesQuery<K, String, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, stringSerializer, byteBufferSerializer);
QueryResult<OrderedRows<K, String, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
OrderedRows<K, String, ByteBuffer> orderedRows = queryResult.get();
public List<SuperRow<K, String, String, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
if (limit < 1) {
limit = Integer.MAX_VALUE;
K startKey = query.getStartKey();
K endKey = query.getEndKey();
RangeSuperSlicesQuery<K, String, String, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, this.stringSerializer, this.stringSerializer, this.byteBufferSerializer);
QueryResult<OrderedSuperRows<K, String, String, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
OrderedSuperRows<K, String, String, ByteBuffer> orderedRows = queryResult.get();
import java.nio.ByteBuffer;
import java.util.Properties;
}
public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) throws IOException {
super.initialize(keyClass, persistent, properties);
try {
this.cassandraClient.initialize(keyClass);
}
catch (Exception e) {
throw new IOException(e.getMessage(), e);
}
List<Row<K, String, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
for (Row<K, String, ByteBuffer> row : rows) {
K key = row.getKey();
CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
cassandraRow = new CassandraRow<K>();
ColumnSlice<String, ByteBuffer> columnSlice = row.getColumnSlice();
for (HColumn<String, ByteBuffer> hColumn : columnSlice.getColumns()) {
List<SuperRow<K, String, String, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
for (SuperRow<K, String, String, ByteBuffer> superRow: superRows) {
K key = superRow.getKey();
CassandraRow<K> cassandraRow = cassandraResultSet.getRow(key);
SuperSlice<String, String, ByteBuffer> superSlice = superRow.getSuperSlice();
for (HSuperColumn<String, String, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
addOrUpdateField(key, field, value.get(field.pos()));
Query<K,T> query = new CassandraQuery<K, T>(this);
query.setFields(getFieldsToQuery(null));
return query;
private void addOrUpdateField(K key, Field field, Object value) {
case DOUBLE:
if (memberValue instanceof Utf8) {
memberValue = memberValue.toString();
}
if (keyValue instanceof Utf8) {
keyValue = keyValue.toString();
}
cfDef.setSubComparatorType(ComparatorType.BYTESTYPE);
cfDef.setComparatorType(ComparatorType.BYTESTYPE);
cfDef.setDefaultValidationClass(ComparatorType.BYTESTYPE.getClassName());
cfDef.setSubComparatorType(ComparatorType.BYTESTYPE);
cfDef.setComparatorType(ComparatorType.BYTESTYPE);
cfDef.setDefaultValidationClass(ComparatorType.BYTESTYPE.getClassName());
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.util.Utf8;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static final Logger LOG = LoggerFactory.getLogger(CassandraColumn.class);
public abstract ByteBuffer getName();
protected Object fromByteBuffer(Type type, ByteBuffer byteBuffer) {
Object value = null;
switch (type) {
case STRING:
value = new Utf8(StringSerializer.get().fromByteBuffer(byteBuffer));
break;
case BYTES:
value = byteBuffer;
break;
case INT:
value = IntegerSerializer.get().fromByteBuffer(byteBuffer);
break;
case LONG:
value = LongSerializer.get().fromByteBuffer(byteBuffer);
break;
case FLOAT:
value = FloatSerializer.get().fromByteBuffer(byteBuffer);
break;
case DOUBLE:
value = DoubleSerializer.get().fromByteBuffer(byteBuffer);
break;
default:
}
return value;
}
import me.prettyprint.cassandra.serializers.StringSerializer;
private HColumn<ByteBuffer, ByteBuffer> hColumn;
public ByteBuffer getName() {
ByteBuffer byteBuffer = hColumn.getValue();
if (type == Type.ARRAY) {
String valueString = StringSerializer.get().fromByteBuffer(byteBuffer);
valueString = valueString.substring(1, valueString.length()-1);
String[] elements = valueString.split(", ");
Type elementType = fieldSchema.getElementType().getType();
if (elementType == Schema.Type.STRING) {
GenericArray<String> genericArray = new GenericData.Array<String>(elements.length, Schema.createArray(Schema.create(Schema.Type.STRING)));
for (String element: elements) {
genericArray.add(element);
}
value = genericArray;
} else {
}
}
else {
value = fromByteBuffer(type, byteBuffer);
}
return value;
public void setValue(HColumn<ByteBuffer, ByteBuffer> hColumn) {
import org.apache.avro.generic.GenericArray;
import org.apache.gora.persistency.ListGenericArray;
private HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn;
public ByteBuffer getName() {
return StringSerializer.get().toByteBuffer(hSuperColumn.getName());
case ARRAY:
Type elementType = fieldSchema.getElementType().getType();
GenericArray array = new ListGenericArray(Schema.create(elementType));
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
ByteBuffer memberByteBuffer = hColumn.getValue();
Object memberValue = fromByteBuffer(elementType, hColumn.getValue());
array.add(memberValue);
}
value = array;
break;
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
Object memberValue = fromByteBuffer(valueType, hColumn.getValue());
map.put(new Utf8(StringSerializer.get().fromByteBuffer(hColumn.getName())), memberValue);
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
String memberName = StringSerializer.get().fromByteBuffer(hColumn.getName());
Field memberField = fieldSchema.getField(memberName);
record.put(record.getFieldIndex(memberName), cassandraColumn.getValue());
public void setValue(HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.IntegerSerializer;
ByteBuffer byteBuffer = toByteBuffer(value);
this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, StringSerializer.get(), ByteBufferSerializer.get()));
public void addSubColumn(K key, String fieldName, ByteBuffer columnName, Object value) {
ByteBuffer byteBuffer = toByteBuffer(value);
String columnFamily = this.cassandraMapping.getFamily(fieldName);
String superColumnName = this.cassandraMapping.getColumn(fieldName);
this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(columnName, byteBuffer, ByteBufferSerializer.get(), ByteBufferSerializer.get())), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get()));
}
@SuppressWarnings("unchecked")
public ByteBuffer toByteBuffer(Object value) {
if (value == null) {
return null;
}
byteBuffer = StringSerializer.get().toByteBuffer(((Utf8)value).toString());
return byteBuffer;
public List<Row<K, ByteBuffer, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
ByteBuffer[] columnNameByteBuffers = new ByteBuffer[columnNames.length];
columnNameByteBuffers[i] = StringSerializer.get().toByteBuffer(columnNames[i]);
}
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
rangeSlicesQuery.setRange(ByteBuffer.wrap(new byte[0]), ByteBuffer.wrap(new byte[0]), false, GoraRecordReader.BUFFER_LIMIT_READ_VALUE);
rangeSlicesQuery.setColumnNames(columnNameByteBuffers);
QueryResult<OrderedRows<K, ByteBuffer, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
OrderedRows<K, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
public List<SuperRow<K, String, ByteBuffer, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
QueryResult<OrderedSuperRows<K, String, ByteBuffer, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
OrderedSuperRows<K, String, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
import java.util.Iterator;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.ListGenericArray;
if (family == null) {
continue;
}
List<Row<K, ByteBuffer, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
for (Row<K, ByteBuffer, ByteBuffer> row : rows) {
ColumnSlice<ByteBuffer, ByteBuffer> columnSlice = row.getColumnSlice();
for (HColumn<ByteBuffer, ByteBuffer> hColumn : columnSlice.getColumns()) {
List<SuperRow<K, String, ByteBuffer, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
for (SuperRow<K, String, ByteBuffer, ByteBuffer> superRow: superRows) {
SuperSlice<String, ByteBuffer, ByteBuffer> superSlice = superRow.getSuperSlice();
for (HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
case ARRAY:
GenericArray array = (GenericArray) fieldValue;
Type elementType = fieldSchema.getElementType().getType();
GenericArray newArray = new ListGenericArray(Schema.create(elementType));
Iterator iter = array.iterator();
while (iter.hasNext()) {
newArray.add(iter.next());
}
fieldValue = newArray;
break;
this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(member.name()), memberValue);
this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(mapKey.toString()), keyValue);
case ARRAY:
if (value != null) {
if (value instanceof GenericArray<?>) {
GenericArray<Object> array = (GenericArray<Object>) value;
int i= 0;
for (Object itemValue: array) {
if (itemValue instanceof Utf8) {
itemValue = itemValue.toString();
}
}
} else {
}
}
break;
import java.nio.ByteBuffer;
import me.prettyprint.cassandra.serializers.FloatSerializer;
import me.prettyprint.cassandra.serializers.DoubleSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.LongSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.util.Utf8;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static final Logger LOG = LoggerFactory.getLogger(CassandraColumn.class);
public abstract ByteBuffer getName();
protected Object fromByteBuffer(Type type, ByteBuffer byteBuffer) {
Object value = null;
switch (type) {
case STRING:
value = new Utf8(StringSerializer.get().fromByteBuffer(byteBuffer));
break;
case BYTES:
value = byteBuffer;
break;
case INT:
value = IntegerSerializer.get().fromByteBuffer(byteBuffer);
break;
case LONG:
value = LongSerializer.get().fromByteBuffer(byteBuffer);
break;
case FLOAT:
value = FloatSerializer.get().fromByteBuffer(byteBuffer);
break;
case DOUBLE:
value = DoubleSerializer.get().fromByteBuffer(byteBuffer);
break;
default:
}
return value;
}
import me.prettyprint.cassandra.serializers.StringSerializer;
private HColumn<ByteBuffer, ByteBuffer> hColumn;
public ByteBuffer getName() {
ByteBuffer byteBuffer = hColumn.getValue();
if (type == Type.ARRAY) {
String valueString = StringSerializer.get().fromByteBuffer(byteBuffer);
valueString = valueString.substring(1, valueString.length()-1);
String[] elements = valueString.split(", ");
Type elementType = fieldSchema.getElementType().getType();
if (elementType == Schema.Type.STRING) {
GenericArray<String> genericArray = new GenericData.Array<String>(elements.length, Schema.createArray(Schema.create(Schema.Type.STRING)));
for (String element: elements) {
genericArray.add(element);
}
value = genericArray;
} else {
}
}
else {
value = fromByteBuffer(type, byteBuffer);
}
return value;
public void setValue(HColumn<ByteBuffer, ByteBuffer> hColumn) {
import org.apache.avro.generic.GenericArray;
import org.apache.gora.persistency.ListGenericArray;
private HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn;
public ByteBuffer getName() {
return StringSerializer.get().toByteBuffer(hSuperColumn.getName());
case ARRAY:
Type elementType = fieldSchema.getElementType().getType();
GenericArray array = new ListGenericArray(Schema.create(elementType));
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
ByteBuffer memberByteBuffer = hColumn.getValue();
Object memberValue = fromByteBuffer(elementType, hColumn.getValue());
array.add(memberValue);
}
value = array;
break;
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
Object memberValue = fromByteBuffer(valueType, hColumn.getValue());
map.put(new Utf8(StringSerializer.get().fromByteBuffer(hColumn.getName())), memberValue);
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
String memberName = StringSerializer.get().fromByteBuffer(hColumn.getName());
Field memberField = fieldSchema.getField(memberName);
record.put(record.getFieldIndex(memberName), cassandraColumn.getValue());
public void setValue(HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn) {
import me.prettyprint.cassandra.serializers.IntegerSerializer;
ByteBuffer byteBuffer = toByteBuffer(value);
this.mutator.insert(key, columnFamily, HFactory.createColumn(columnName, byteBuffer, StringSerializer.get(), ByteBufferSerializer.get()));
public void addSubColumn(K key, String fieldName, ByteBuffer columnName, Object value) {
ByteBuffer byteBuffer = toByteBuffer(value);
String columnFamily = this.cassandraMapping.getFamily(fieldName);
String superColumnName = this.cassandraMapping.getColumn(fieldName);
this.mutator.insert(key, columnFamily, HFactory.createSuperColumn(superColumnName, Arrays.asList(HFactory.createColumn(columnName, byteBuffer, ByteBufferSerializer.get(), ByteBufferSerializer.get())), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get()));
}
@SuppressWarnings("unchecked")
public ByteBuffer toByteBuffer(Object value) {
if (value == null) {
return null;
}
byteBuffer = StringSerializer.get().toByteBuffer(((Utf8)value).toString());
return byteBuffer;
public List<Row<K, ByteBuffer, ByteBuffer>> execute(CassandraQuery<K, T> cassandraQuery, String family) {
ByteBuffer[] columnNameByteBuffers = new ByteBuffer[columnNames.length];
columnNameByteBuffers[i] = StringSerializer.get().toByteBuffer(columnNames[i]);
}
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
rangeSlicesQuery.setRange(ByteBuffer.wrap(new byte[0]), ByteBuffer.wrap(new byte[0]), false, GoraRecordReader.BUFFER_LIMIT_READ_VALUE);
rangeSlicesQuery.setColumnNames(columnNameByteBuffers);
QueryResult<OrderedRows<K, ByteBuffer, ByteBuffer>> queryResult = rangeSlicesQuery.execute();
OrderedRows<K, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
public List<SuperRow<K, String, ByteBuffer, ByteBuffer>> executeSuper(CassandraQuery<K, T> cassandraQuery, String family) {
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
QueryResult<OrderedSuperRows<K, String, ByteBuffer, ByteBuffer>> queryResult = rangeSuperSlicesQuery.execute();
OrderedSuperRows<K, String, ByteBuffer, ByteBuffer> orderedRows = queryResult.get();
import java.util.Iterator;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.ListGenericArray;
if (family == null) {
continue;
}
List<Row<K, ByteBuffer, ByteBuffer>> rows = this.cassandraClient.execute(cassandraQuery, family);
for (Row<K, ByteBuffer, ByteBuffer> row : rows) {
ColumnSlice<ByteBuffer, ByteBuffer> columnSlice = row.getColumnSlice();
for (HColumn<ByteBuffer, ByteBuffer> hColumn : columnSlice.getColumns()) {
List<SuperRow<K, String, ByteBuffer, ByteBuffer>> superRows = this.cassandraClient.executeSuper(cassandraQuery, family);
for (SuperRow<K, String, ByteBuffer, ByteBuffer> superRow: superRows) {
SuperSlice<String, ByteBuffer, ByteBuffer> superSlice = superRow.getSuperSlice();
for (HSuperColumn<String, ByteBuffer, ByteBuffer> hSuperColumn: superSlice.getSuperColumns()) {
case ARRAY:
GenericArray array = (GenericArray) fieldValue;
Type elementType = fieldSchema.getElementType().getType();
GenericArray newArray = new ListGenericArray(Schema.create(elementType));
Iterator iter = array.iterator();
while (iter.hasNext()) {
newArray.add(iter.next());
}
fieldValue = newArray;
break;
this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(member.name()), memberValue);
this.cassandraClient.addSubColumn(key, field.name(), StringSerializer.get().toByteBuffer(mapKey.toString()), keyValue);
case ARRAY:
if (value != null) {
if (value instanceof GenericArray<?>) {
GenericArray<Object> array = (GenericArray<Object>) value;
int i= 0;
for (Object itemValue: array) {
if (itemValue instanceof Utf8) {
itemValue = itemValue.toString();
}
}
} else {
}
}
break;
line(0, "import org.apache.avro.specific.FixedSize;");
case FIXED:
line(0, "import org.apache.avro.specific.FixedSize;");
case FIXED:
byte[] splitStart = startRow.length == 0 ||
Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ?
byte[] splitStop = (stopRow.length == 0 ||
Bytes.compareTo(keys.getSecond()[i], stopRow) <= 0) &&
keys.getSecond()[i].length > 0 ? keys.getSecond()[i] : stopRow;
byte[] splitStart = startRow.length == 0 ||
Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ?
byte[] splitStop = (stopRow.length == 0 ||
Bytes.compareTo(keys.getSecond()[i], stopRow) <= 0) &&
keys.getSecond()[i].length > 0 ? keys.getSecond()[i] : stopRow;
table = new HTable(conf, tableName) {
@Override
public synchronized void flushCommits() throws IOException {
super.flushCommits();
}
};
table = new HTable(conf, tableName) {
@Override
public synchronized void flushCommits() throws IOException {
super.flushCommits();
}
};
private Class<T> persistentClass;
private CassandraMapping cassandraMapping = null;
public void initialize(Class<K> keyClass, Class<T> persistentClass) throws Exception {
this.persistentClass = persistentClass;
this.cassandraMapping = CassandraMappingManager.getManager().get(persistentClass);
public CassandraMapping(Element keyspace, Element mapping) {
this.cassandraClient.initialize(keyClass, persistent);
private Class<T> persistentClass;
private CassandraMapping cassandraMapping = null;
public void initialize(Class<K> keyClass, Class<T> persistentClass) throws Exception {
this.persistentClass = persistentClass;
this.cassandraMapping = CassandraMappingManager.getManager().get(persistentClass);
public CassandraMapping(Element keyspace, Element mapping) {
this.cassandraClient.initialize(keyClass, persistent);
import me.prettyprint.hector.api.Serializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
protected Object fromByteBuffer(Schema schema, ByteBuffer byteBuffer) {
Serializer serializer = GoraSerializerTypeInferer.getSerializer(schema);
if (serializer == null) {
} else {
value = serializer.fromByteBuffer(byteBuffer);
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.TypeUtils;
if (byteBuffer == null) {
return null;
}
GenericArraySerializer serializer = GenericArraySerializer.get(fieldSchema.getElementType());
GenericArray genericArray = serializer.fromByteBuffer(byteBuffer);
value = genericArray;
} else {
value = fromByteBuffer(fieldSchema, byteBuffer);
import org.apache.gora.cassandra.serializers.Utf8Serializer;
ListGenericArray array = new ListGenericArray(fieldSchema.getElementType());
Object memberValue = fromByteBuffer(fieldSchema.getElementType(), hColumn.getValue());
Object memberValue = null;
memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());
map.put(Utf8Serializer.get().fromByteBuffer(hColumn.getName()), memberValue);
if (memberName == null || memberName.length() == 0) {
LOG.warn("member name is null or empty.");
continue;
}
import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
import org.apache.gora.cassandra.serializers.TypeUtils;
this.keySerializer = GoraSerializerTypeInferer.getSerializer(keyClass);
if (columnName == null) {
return;
}
HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);
HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
}
public void addSubColumn(K key, String fieldName, String columnName, Object value) {
addSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName), value);
}
public void addSubColumn(K key, String fieldName, Integer columnName, Object value) {
addSubColumn(key, fieldName, IntegerSerializer.get().toByteBuffer(columnName), value);
}
@SuppressWarnings("unchecked")
public void addGenericArray(K key, String fieldName, GenericArray array) {
if (isSuper( cassandraMapping.getFamily(fieldName) )) {
int i= 0;
for (Object itemValue: array) {
if (itemValue instanceof GenericArray<?>) {
if (((GenericArray)itemValue).size() == 0) {
continue;
}
}
}
}
else {
addColumn(key, fieldName, array);
}
Serializer serializer = GoraSerializerTypeInferer.getSerializer(value);
if (serializer == null) {
byteBuffer = serializer.toByteBuffer(value);
}
if (byteBuffer == null) {
import org.apache.avro.specific.SpecificFixed;
ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
case BOOLEAN:
case FIXED:
if (((GenericArray)memberValue).size() == 0) {
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
if (((GenericArray)keyValue).size() == 0) {
this.cassandraClient.addSubColumn(key, field.name(), mapKey.toString(), keyValue);
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
import me.prettyprint.hector.api.Serializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
protected Object fromByteBuffer(Schema schema, ByteBuffer byteBuffer) {
Serializer serializer = GoraSerializerTypeInferer.getSerializer(schema);
if (serializer == null) {
} else {
value = serializer.fromByteBuffer(byteBuffer);
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.TypeUtils;
if (byteBuffer == null) {
return null;
}
GenericArraySerializer serializer = GenericArraySerializer.get(fieldSchema.getElementType());
GenericArray genericArray = serializer.fromByteBuffer(byteBuffer);
value = genericArray;
} else {
value = fromByteBuffer(fieldSchema, byteBuffer);
import org.apache.gora.cassandra.serializers.Utf8Serializer;
ListGenericArray array = new ListGenericArray(fieldSchema.getElementType());
Object memberValue = fromByteBuffer(fieldSchema.getElementType(), hColumn.getValue());
Object memberValue = null;
memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());
map.put(Utf8Serializer.get().fromByteBuffer(hColumn.getName()), memberValue);
if (memberName == null || memberName.length() == 0) {
LOG.warn("member name is null or empty.");
continue;
}
import me.prettyprint.cassandra.model.ConfigurableConsistencyLevel;
import org.apache.avro.Schema;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericArray;
import org.apache.gora.cassandra.serializers.GenericArraySerializer;
import org.apache.gora.cassandra.serializers.GoraSerializerTypeInferer;
import org.apache.gora.cassandra.serializers.TypeUtils;
this.keySerializer = GoraSerializerTypeInferer.getSerializer(keyClass);
if (columnName == null) {
return;
}
HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);
HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
}
public void addSubColumn(K key, String fieldName, String columnName, Object value) {
addSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName), value);
}
public void addSubColumn(K key, String fieldName, Integer columnName, Object value) {
addSubColumn(key, fieldName, IntegerSerializer.get().toByteBuffer(columnName), value);
}
@SuppressWarnings("unchecked")
public void addGenericArray(K key, String fieldName, GenericArray array) {
if (isSuper( cassandraMapping.getFamily(fieldName) )) {
int i= 0;
for (Object itemValue: array) {
if (itemValue instanceof GenericArray<?>) {
if (((GenericArray)itemValue).size() == 0) {
continue;
}
}
}
}
else {
addColumn(key, fieldName, array);
}
Serializer serializer = GoraSerializerTypeInferer.getSerializer(value);
if (serializer == null) {
byteBuffer = serializer.toByteBuffer(value);
}
if (byteBuffer == null) {
import org.apache.avro.specific.SpecificFixed;
ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
case BOOLEAN:
case FIXED:
if (((GenericArray)memberValue).size() == 0) {
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
if (((GenericArray)keyValue).size() == 0) {
this.cassandraClient.addSubColumn(key, field.name(), mapKey.toString(), keyValue);
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
DataStoreBase that = (DataStoreBase) obj;
String confSchemaName = getOrCreateConf().get("preferred.schema.name");
if (confSchemaName != null) {
return confSchemaName;
}
DataStoreBase that = (DataStoreBase) obj;
String confSchemaName = getOrCreateConf().get("preferred.schema.name");
if (confSchemaName != null) {
return confSchemaName;
}
import org.apache.gora.cassandra.serializers.StatefulHashMapSerializer;
import org.apache.gora.persistency.StatefulHashMap;
} else if (type == Type.MAP) {
StatefulHashMapSerializer serializer = StatefulHashMapSerializer.get(fieldSchema.getValueType());
StatefulHashMap map = serializer.fromByteBuffer(byteBuffer);
value = map;
import org.apache.gora.persistency.StatefulHashMap;
} else if (value instanceof StatefulHashMap) {
StatefulHashMap map = (StatefulHashMap)value;
if (map.size() == 0) {
serializer = ByteBufferSerializer.get();
}
else {
Object value0 = map.values().iterator().next();
Schema schema = TypeUtils.getSchema(value0);
serializer = StatefulHashMapSerializer.get(schema);
}
} else if (type == Type.MAP) {
serializer = StatefulHashMapSerializer.get(schema.getValueType());
} else if (type == Type.MAP) {
serializer = StatefulHashMapSerializer.get(elementType);
import org.apache.gora.persistency.StatefulHashMap;
} else if (itemValue instanceof StatefulHashMap<?,?>) {
if (((StatefulHashMap)itemValue).size() == 0) {
continue;
}
@SuppressWarnings("unchecked")
public void addStatefulHashMap(K key, String fieldName, StatefulHashMap<Utf8,Object> map) {
if (isSuper( cassandraMapping.getFamily(fieldName) )) {
int i= 0;
for (Utf8 mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
if (mapValue instanceof GenericArray<?>) {
if (((GenericArray)mapValue).size() == 0) {
continue;
}
} else if (mapValue instanceof StatefulHashMap<?,?>) {
if (((StatefulHashMap)mapValue).size() == 0) {
continue;
}
}
addSubColumn(key, fieldName, mapKey.toString(), mapValue);
}
}
else {
addColumn(key, fieldName, map);
}
}
int fieldPos = field.pos();
if (value.isDirty(fieldPos)) {
Object fieldValue = value.get(fieldPos);
p.put(fieldPos, fieldValue);
} else if (memberValue instanceof StatefulHashMap<?,?>) {
if (((StatefulHashMap)memberValue).size() == 0) {
continue;
}
this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
import org.apache.gora.cassandra.serializers.StatefulHashMapSerializer;
import org.apache.gora.persistency.StatefulHashMap;
} else if (type == Type.MAP) {
StatefulHashMapSerializer serializer = StatefulHashMapSerializer.get(fieldSchema.getValueType());
StatefulHashMap map = serializer.fromByteBuffer(byteBuffer);
value = map;
import org.apache.gora.persistency.StatefulHashMap;
} else if (value instanceof StatefulHashMap) {
StatefulHashMap map = (StatefulHashMap)value;
if (map.size() == 0) {
serializer = ByteBufferSerializer.get();
}
else {
Object value0 = map.values().iterator().next();
Schema schema = TypeUtils.getSchema(value0);
serializer = StatefulHashMapSerializer.get(schema);
}
} else if (type == Type.MAP) {
serializer = StatefulHashMapSerializer.get(schema.getValueType());
} else if (type == Type.MAP) {
serializer = StatefulHashMapSerializer.get(elementType);
import org.apache.gora.persistency.StatefulHashMap;
} else if (itemValue instanceof StatefulHashMap<?,?>) {
if (((StatefulHashMap)itemValue).size() == 0) {
continue;
}
@SuppressWarnings("unchecked")
public void addStatefulHashMap(K key, String fieldName, StatefulHashMap<Utf8,Object> map) {
if (isSuper( cassandraMapping.getFamily(fieldName) )) {
int i= 0;
for (Utf8 mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
if (mapValue instanceof GenericArray<?>) {
if (((GenericArray)mapValue).size() == 0) {
continue;
}
} else if (mapValue instanceof StatefulHashMap<?,?>) {
if (((StatefulHashMap)mapValue).size() == 0) {
continue;
}
}
addSubColumn(key, fieldName, mapKey.toString(), mapValue);
}
}
else {
addColumn(key, fieldName, map);
}
}
int fieldPos = field.pos();
if (value.isDirty(fieldPos)) {
Object fieldValue = value.get(fieldPos);
p.put(fieldPos, fieldValue);
} else if (memberValue instanceof StatefulHashMap<?,?>) {
if (((StatefulHashMap)memberValue).size() == 0) {
continue;
}
this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
import java.util.Map.Entry;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
DataStoreBase that = (DataStoreBase) obj;
import java.util.Map.Entry;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
DataStoreBase that = (DataStoreBase) obj;
DataStoreBase that = (DataStoreBase) obj;
String prefix = getOrCreateConf().get("schema.prefix","");
DataStoreBase that = (DataStoreBase) obj;
String prefix = getOrCreateConf().get("schema.prefix","");
public boolean keyspaceExists() {
KeyspaceDefinition keyspaceDefinition = this.cluster.describeKeyspace(this.cassandraMapping.getKeyspaceName());
return (keyspaceDefinition != null);
}
return cassandraClient.keyspaceExists();
public boolean keyspaceExists() {
KeyspaceDefinition keyspaceDefinition = this.cluster.describeKeyspace(this.cassandraMapping.getKeyspaceName());
return (keyspaceDefinition != null);
}
return cassandraClient.keyspaceExists();
import org.apache.gora.persistency.State;
List<byte[]> list = new ArrayList<byte[]>(map.size());
int n = 0;
if (map.getState(key) == State.DELETED) {
continue;
}
List<byte[]> list = new ArrayList<byte[]>(map.size());
int n = 0;
if (map.getState(key) == State.DELETED) {
continue;
}
import org.apache.gora.persistency.State;
@SuppressWarnings("unchecked")
public void deleteSubColumn(K key, String fieldName, ByteBuffer columnName) {
String columnFamily = this.cassandraMapping.getFamily(fieldName);
String superColumnName = this.cassandraMapping.getColumn(fieldName);
HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
}
public void deleteSubColumn(K key, String fieldName, String columnName) {
deleteSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName));
}
if (map.getState(mapKey) == State.DELETED) {
deleteSubColumn(key, fieldName, mapKey.toString());
continue;
}
public static<K> void deleteSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName) {
mutator.subDelete(key, columnFamily, superColumnName, columnName, StringSerializer.get(), ByteBufferSerializer.get());
}
import org.apache.gora.persistency.State;
List<byte[]> list = new ArrayList<byte[]>(map.size());
int n = 0;
if (map.getState(key) == State.DELETED) {
continue;
}
List<byte[]> list = new ArrayList<byte[]>(map.size());
int n = 0;
if (map.getState(key) == State.DELETED) {
continue;
}
import org.apache.gora.persistency.State;
@SuppressWarnings("unchecked")
public void deleteSubColumn(K key, String fieldName, ByteBuffer columnName) {
String columnFamily = this.cassandraMapping.getFamily(fieldName);
String superColumnName = this.cassandraMapping.getColumn(fieldName);
HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
}
public void deleteSubColumn(K key, String fieldName, String columnName) {
deleteSubColumn(key, fieldName, StringSerializer.get().toByteBuffer(columnName));
}
if (map.getState(mapKey) == State.DELETED) {
deleteSubColumn(key, fieldName, mapKey.toString());
continue;
}
public static<K> void deleteSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName) {
mutator.subDelete(key, columnFamily, superColumnName, columnName, StringSerializer.get(), ByteBufferSerializer.get());
}
public String getKeyspaceName() {
return this.cassandraMapping.getKeyspaceName();
}
LOG.debug("creating Cassandra keyspace");
return this.cassandraClient.getKeyspaceName();
public String getKeyspaceName() {
return this.cassandraMapping.getKeyspaceName();
}
LOG.debug("creating Cassandra keyspace");
return this.cassandraClient.getKeyspaceName();
private static final String USAGE = "LogAnalytics <input_data_store> <output_data_store>";
if(args.length < 2) {
System.err.println(USAGE);
System.exit(1);
}
private static final String USAGE = "LogAnalytics <input_data_store> <output_data_store>";
if(args.length < 2) {
System.err.println(USAGE);
System.exit(1);
}
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloQuery<K,T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloResult<K,T extends PersistentBase> extends ResultBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class AccumuloStore<K,T extends PersistentBase> extends DataStoreBase<K,T> {
public static final Logger LOG = LoggerFactory.getLogger(AccumuloStore.class);
public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) {
try{
super.initialize(keyClass, persistentClass, properties);
String mock = DataStoreFactory.findProperty(properties, this, MOCK_PROPERTY, null);
String mappingFile = DataStoreFactory.getMappingFile(properties, this, DEFAULT_MAPPING_FILE);
String user = DataStoreFactory.findProperty(properties, this, USERNAME_PROPERTY, null);
String password = DataStoreFactory.findProperty(properties, this, PASSWORD_PROPERTY, null);
mapping = readMapping(mappingFile);
if (mapping.encoder == null || mapping.encoder.equals("")) {
encoder = new org.apache.gora.accumulo.encoders.BinaryEncoder();
try {
encoder = (Encoder) getClass().getClassLoader().loadClass(mapping.encoder).newInstance();
} catch (InstantiationException e) {
throw new IOException(e);
} catch (IllegalAccessException e) {
throw new IOException(e);
} catch (ClassNotFoundException e) {
throw new IOException(e);
}
try {
if (mock == null || !mock.equals("true")) {
String instance = DataStoreFactory.findProperty(properties, this, INSTANCE_NAME_PROPERTY, null);
String zookeepers = DataStoreFactory.findProperty(properties, this, ZOOKEEPERS_NAME_PROPERTY, null);
conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, password);
authInfo = new AuthInfo(user, ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());
} else {
conn = new MockInstance().getConnector(user, password);
}
if (autoCreateSchema)
createSchema();
} catch (AccumuloException e) {
throw new IOException(e);
} catch (AccumuloSecurityException e) {
throw new IOException(e);
}
}catch(IOException e){
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void createSchema() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void deleteSchema() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
public boolean schemaExists() {
public T get(K key, String[] fields) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return null;
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void put(K key, T val) {
try{
Mutation m = new Mutation(new Text(toBytes(key)));
Schema schema = val.getSchema();
StateManager stateManager = val.getStateManager();
Iterator<Field> iter = schema.getFields().iterator();
int count = 0;
Field field = iter.next();
if (!stateManager.isDirty(val, i)) {
continue;
}
Object o = val.get(i);
Pair<Text,Text> col = mapping.fieldMap.get(field.name());
switch (field.schema().getType()) {
case MAP:
if (o instanceof StatefulMap) {
StatefulMap map = (StatefulMap) o;
Set<?> es = map.states().entrySet();
for (Object entry : es) {
Object mapKey = ((Entry) entry).getKey();
State state = (State) ((Entry) entry).getValue();
switch (state) {
case NEW:
case DIRTY:
m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(map.get(mapKey))));
count;
break;
case DELETED:
m.putDelete(col.getFirst(), new Text(toBytes(mapKey)));
count;
break;
}
} else {
Map map = (Map) o;
Set<?> es = map.entrySet();
for (Object entry : es) {
Object mapKey = ((Entry) entry).getKey();
Object mapVal = ((Entry) entry).getValue();
m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(mapVal)));
count;
}
break;
case ARRAY:
GenericArray array = (GenericArray) o;
int j = 0;
for (Object item : array) {
break;
case RECORD:
SpecificDatumWriter writer = new SpecificDatumWriter(field.schema());
ByteArrayOutputStream os = new ByteArrayOutputStream();
BinaryEncoder encoder = new BinaryEncoder(os);
writer.write(o, encoder);
encoder.flush();
m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
break;
default:
m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
}
if (count > 0)
try {
getBatchWriter().addMutation(m);
} catch (MutationsRejectedException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public boolean delete(K key) {
public long deleteByQuery(Query<K,T> query) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return 0;
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return 0;
} catch (IOException e){
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return 0;
public Result<K,T> execute(Query<K,T> query) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return null;
}
public void flush() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
public void close() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraClient<K, T extends PersistentBase> {
public class CassandraStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) {
super.initialize(keyClass, persistent, properties);
} catch (Exception e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void close() {
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
public void deleteSchema() {
public Result<K, T> execute(Query<K, T> query) {
public void flush() {
public T get(K key, String[] fields) {
boolean hasResult = false;
try {
hasResult = result.next();
} catch (Exception e) {
e.printStackTrace();
}
return this.cassandraClient.getKeyspaceName();
public void put(K key, T value) {
PersistentBase persistent = (PersistentBase) fieldValue;
PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
public boolean schemaExists() {
try{
WebPage page;
log.info("creating web page data");
page = new WebPage();
page.setUrl(new Utf8(URLS[i]));
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
for(String token : CONTENTS[i].split(" ")) {
page.addToParsedContent(new Utf8(token));
}
page.putToOutlinks(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
}
Metadata metadata = new Metadata();
metadata.setVersion(1);
metadata.putToData(new Utf8("metakey"), new Utf8("metavalue"));
page.setMetadata(metadata);
dataStore.put(URLS[i], page);
}
dataStore.flush();
log.info("finished creating web page data");
}
catch(Exception e){
log.info("error creating web page data");
}
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumReader<T extends PersistentBase>
Persistent cloned = (PersistentBase)persistent.newInstance(new StateManagerImpl());
case STRING : ((PersistentBase)cloned).put(pos, cloneObject(
field.schema(), ((PersistentBase)persistent).get(pos), ((PersistentBase)cloned).get(pos))); break;
default     : ((PersistentBase)cloned).put(pos, ((PersistentBase)persistent).get(pos)); break;
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumWriter<T extends PersistentBase>
import java.io.IOException;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.Result;
public class AvroQuery<K, T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class DataFileAvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
if(reader != null)
reader.close();
reader = null;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class AvroStore<K, T extends PersistentBase>
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
Properties properties) {
super.initialize(keyClass, persistentClass, properties);
if(properties != null) {
if(this.codecType == null) {
String codecType = DataStoreFactory.findProperty(
properties, this, CODEC_TYPE_KEY, "BINARY");
this.codecType = CodecType.valueOf(codecType);
}
public void close() {
try{
super.close();
if(encoder != null) {
encoder.flush();
}
encoder = null;
decoder = null;
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
public void flush() {
try{
super.flush();
if(encoder != null)
encoder.flush();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public T get(K key, String[] fields) {
public void put(K key, T obj) {
try{
getDatumWriter().write(obj, getEncoder());
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void write(DataOutput out) {
public void readFields(DataInput in) {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class DataFileAvroStore<K, T extends PersistentBase> extends AvroStore<K, T> {
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
public T get(K key, String[] fields) {
public void put(K key, T obj) {
try{
getWriter().append(obj);
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
protected Result<K, T> executeQuery(Query<K, T> query) {
try{
return new DataFileAvroResult<K, T>(this, query
, createReader(createFsInput()));
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
}
protected Result<K,T> executePartial(FileSplitPartitionQuery<K,T> query) {
try{
FsInput fsInput = createFsInput();
DataFileReader<T> reader = createReader(fsInput);
return new DataFileAvroResult<K, T>(this, query, reader, fsInput
, query.getStart(), query.getLength());
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
}
public void flush() {
try{
super.flush();
if(writer != null) {
writer.flush();
}
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
public void close() {
try{
if(writer != null)
writer = null;
super.close();
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
import org.apache.gora.persistency.impl.PersistentBase;
public class GoraInputFormat<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.impl.ResultBase;
public class GoraRecordReader<K, T extends PersistentBase> extends RecordReader<K,T> {
public void executeQuery() throws IOException, Exception {
try{
return result.getProgress();
}
catch(Exception e){
return 0;
}
try{
if (counter.isModulo()) {
boolean firstBatch = (this.result == null);
if (! firstBatch) {
this.query.setStartKey(this.result.getKey());
if (this.query.getLimit() == counter.getRecordsMax()) {
}
}
if (this.result != null) {
this.result.close();
}
executeQuery();
if (! firstBatch) {
this.result.next();
}
}
counter.increment();
return this.result.next();
}
catch(Exception e){
return false;
}
try{
store.close();
}catch(Exception e){
}
try{
store.put(key, (Persistent) value);
counter.increment();
if (counter.isModulo()) {
store.flush();
}
}catch(Exception e){
}
import org.apache.gora.persistency.impl.PersistentBase;
implements Deserializer<PersistentBase> {
private Class<? extends PersistentBase> persistentClass;
private PersistentDatumReader<PersistentBase> datumReader;
public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
datumReader = new PersistentDatumReader<PersistentBase>(schema, true);
public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
private PersistentDatumWriter<PersistentBase> datumWriter;
this.datumWriter = new PersistentDatumWriter<PersistentBase>();
public void serialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
public class MemStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
public static class MemQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
public static class MemResult<K, T extends PersistentBase> extends ResultBase<K, T> {
public void close() { }
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
try{
long deletedRows = 0;
Result<K,T> result = query.execute();
while(result.next()) {
if(delete(result.getKey()))
}
return 0;
}
catch(Exception e){
return 0;
}
public Result<K, T> execute(Query<K, T> query) {
public T get(K key, String[] fields) {
((PersistentBase)newObj).put(index, ((PersistentBase)obj).get(index));
public void put(K key, T obj) {
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
public void close() {
public void createSchema() { }
public void deleteSchema() {
public boolean schemaExists() {
public void flush() { }
public interface BeanFactory<K, T>{
public interface Persistent extends Cloneable{
public interface StateManager{
public abstract class PersistentBase implements Persistent, SpecificRecord {
protected static final PersistentDatumReader<PersistentBase> datumReader =
new PersistentDatumReader<PersistentBase>();
dirtyBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
readableBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
public interface PartitionQuery<K, T> extends Query<K, T> {
public interface Query<K, T> {
Result<K,T> execute() throws Exception, IOException;
public interface Result<K,T> {
boolean next() throws Exception, IOException;
float getProgress() throws IOException, InterruptedException, Exception;
import org.apache.gora.persistency.impl.PersistentBase;
public class FileSplitPartitionQuery<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
public class PartitionQueryImpl<K, T extends PersistentBase>
this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.io.Writable;
public abstract class QueryBase<K, T extends PersistentBase>
implements Query<K,T>, Writable, Configurable {
protected DataStoreBase<K,T> dataStore;
this.dataStore = (DataStoreBase<K, T>)dataStore;
public Result<K,T> execute() throws Exception, IOException {
this.dataStore = (DataStoreBase<K, T>)dataStore;
dataStore = (DataStoreBase<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(dataStoreClass), conf);
public final boolean next() throws Exception, IOException {
if(isLimitReached()) {
return false;
}
clear();
persistent = getOrCreatePersistent(persistent);
boolean ret = nextInner();
return ret;
protected T getOrCreatePersistent(T persistent) throws Exception, IOException {
if(persistent != null) {
return persistent;
}
return dataStore.newPersistent();
}
@Override
public void close() throws IOException{
public interface DataStore<K, T> {
Properties properties);
void createSchema();
void deleteSchema();
void truncateSchema();
boolean schemaExists();
K newKey();
T newPersistent();
T get(K key);
T get(K key, String[] fields);
void put(K key, T obj);
boolean delete(K key);
long deleteByQuery(Query<K, T> query);
Result<K,T> execute(Query<K, T> query);
List<PartitionQuery<K,T>> getPartitions(Query<K,T> query) throws IOException;
void flush();
void close();
public class DataStoreFactory{
import java.io.Closeable;
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.io.Writable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public abstract class DataStoreBase<K, T extends PersistentBase>
implements DataStore<K, T>, Configurable, Writable, Closeable {
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
Properties properties) {
setKeyClass(keyClass);
setPersistentClass(persistentClass);
if(this.beanFactory == null)
this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
schema = this.beanFactory.getCachedPersistent().getSchema();
fieldMap = AvroUtils.getFieldMap(schema);
autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
this.properties = properties;
datumReader = new PersistentDatumReader<T>(schema, false);
datumWriter = new PersistentDatumWriter<T>(schema, false);
public K newKey() {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
public T newPersistent() {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
public T get(K key) {
public void readFields(DataInput in) {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void write(DataOutput out) {
try {
Text.writeString(out, getKeyClass().getCanonicalName());
Text.writeString(out, getPersistentClass().getCanonicalName());
WritableUtils.writeProperties(out, properties);
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
public void truncateSchema() {
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public abstract class FileBackedDataStoreBase<K, T extends PersistentBase>
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
Properties properties) {
protected OutputStream createOutputStream() {
OutputStream conf = null;
try{
Path path = new Path(outputPath);
FileSystem fs = path.getFileSystem(getConf());
conf = fs.create(path);
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
return conf;
try{
if(inputStream == null) {
inputStream = createInputStream();
}
return inputStream;
}catch(IOException ex){
throw new IOException(ex);
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
List<InputSplit> splits = null;
List<PartitionQuery<K, T>> queries = null;
try{
splits = GoraMapReduceUtils.getSplits(getConf(), inputPath);
queries = new ArrayList<PartitionQuery<K,T>>(splits.size());
for(InputSplit split : splits) {
queries.add(new FileSplitPartitionQuery<K, T>(query, (FileSplit) split));
}
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
public Result<K, T> execute(Query<K, T> query) {
Result<K, T> results = null;
try{
if(query instanceof FileSplitPartitionQuery) {
results = executePartial((FileSplitPartitionQuery<K, T>) query);
} else {
results = executeQuery(query);
}
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return results;
public void flush() {
try{
if(outputStream != null)
outputStream.flush();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void createSchema() {
public void deleteSchema() {
public boolean schemaExists() {
public void write(DataOutput out) {
try{
super.write(out);
org.apache.gora.util.IOUtils.writeNullFieldsInfo(out, inputPath, outputPath);
if(inputPath != null)
Text.writeString(out, inputPath);
if(outputPath != null)
Text.writeString(out, outputPath);
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void readFields(DataInput in) {
try{
super.readFields(in);
boolean[] nullFields = org.apache.gora.util.IOUtils.readNullFieldsInfo(in);
if(!nullFields[0])
inputPath = Text.readString(in);
if(!nullFields[1])
outputPath = Text.readString(in);
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void close() {
IOUtils.closeStream(inputStream);
IOUtils.closeStream(outputStream);
inputStream = null;
outputStream = null;
import org.apache.gora.persistency.impl.PersistentBase;
public static<T extends PersistentBase> void serialize(OutputStream os,
public static<T extends PersistentBase> byte[] serialize(PersistentDatumWriter<T> datumWriter
public static<K, T extends PersistentBase> K deserialize(InputStream is,
public static<K, T extends PersistentBase> K deserialize(byte[] bytes,
public static<T extends PersistentBase> byte[] deserialize(PersistentDatumWriter<T> datumWriter
public void close() {
public void createSchema() {
public void deleteSchema() {
public void truncateSchema() {
public boolean schemaExists() {
public boolean delete(String key) {
public long deleteByQuery(Query<String, MockPersistent> query) {
public Result<String, MockPersistent> execute(Query<String, MockPersistent> query) {
public void flush() {
public MockPersistent get(String key, String[] fields) {
public void put(String key, MockPersistent obj) {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseGetResult<K, T extends PersistentBase> extends HBaseResult<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class HBaseResult<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseScannerResult<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseStore<K, T extends PersistentBase> extends DataStoreBase<K, T>
Properties properties) {
super.initialize(keyClass, persistentClass, properties);
this.conf = HBaseConfiguration.create(getConf());
admin = new HBaseAdmin(this.conf);
LOG.error(ex1.getMessage());
LOG.error(ex1.getStackTrace().toString());
}
try{
boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
}
public void createSchema() {
try{
if(schemaExists()) {
return;
}
HTableDescriptor tableDesc = mapping.getTable();
admin.createTable(tableDesc);
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
public void deleteSchema() {
try{
if(!schemaExists()) {
return;
}
admin.disableTable(getSchemaName());
admin.deleteTable(getSchemaName());
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
public boolean schemaExists() {
try{
return admin.tableExists(mapping.getTableName());
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
return false;
}
public T get(K key, String[] fields) {
try{
fields = getFieldsToQuery(fields);
Get get = new Get(toBytes(key));
addFields(get, fields);
Result result = table.get(get);
return newInstance(result, fields);
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
return null;
}
public void put(K key, T persistent) {
try{
Schema schema = persistent.getSchema();
StateManager stateManager = persistent.getStateManager();
byte[] keyRaw = toBytes(key);
Put put = new Put(keyRaw);
Delete delete = new Delete(keyRaw);
boolean hasPuts = false;
boolean hasDeletes = false;
Iterator<Field> iter = schema.getFields().iterator();
Field field = iter.next();
if (!stateManager.isDirty(persistent, i)) {
continue;
}
Type type = field.schema().getType();
Object o = persistent.get(i);
HBaseColumn hcol = mapping.getColumn(field.name());
switch(type) {
case MAP:
if(o instanceof StatefulMap) {
StatefulHashMap<Utf8, ?> map = (StatefulHashMap<Utf8, ?>) o;
for (Entry<Utf8, State> e : map.states().entrySet()) {
Utf8 mapKey = e.getKey();
switch (e.getValue()) {
case DIRTY:
byte[] qual = Bytes.toBytes(mapKey.toString());
byte[] val = toBytes(map.get(mapKey), field.schema().getValueType());
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
break;
case DELETED:
qual = Bytes.toBytes(mapKey.toString());
hasDeletes = true;
delete.deleteColumn(hcol.getFamily(), qual);
break;
}
}
} else {
Set<Map.Entry> set = ((Map)o).entrySet();
for(Entry entry: set) {
byte[] qual = toBytes(entry.getKey());
byte[] val = toBytes(entry.getValue());
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
break;
case ARRAY:
if(o instanceof GenericArray) {
GenericArray arr = (GenericArray) o;
int j=0;
for(Object item : arr) {
byte[] val = toBytes(item);
hasPuts = true;
}
break;
default:
put.add(hcol.getFamily(), hcol.getQualifier(), toBytes(o, field.schema()));
hasPuts = true;
break;
}
if (hasPuts) {
table.put(put);
}
if (hasDeletes) {
table.delete(delete);
}
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
public boolean delete(K key) {
try{
table.delete(new Delete(toBytes(key)));
return true;
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
return false;
public long deleteByQuery(Query<K, T> query) {
try {
String[] fields = getFieldsToQuery(query.getFields());
boolean isAllFields = Arrays.equals(fields
, getBeanFactory().getCachedPersistent().getFields());
org.apache.gora.query.Result<K, T> result = null;
result = query.execute();
ArrayList<Delete> deletes = new ArrayList<Delete>();
while(result.next()) {
Delete delete = new Delete(toBytes(result.getKey()));
deletes.add(delete);
if(!isAllFields) {
addFields(delete, query);
}
}
table.delete(deletes);
return deletes.size();
} catch (Exception e) {
e.printStackTrace();
return -1;
}
}
@Override
public void flush() {
try{
table.flushCommits();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public org.apache.gora.query.Result<K, T> execute(Query<K, T> query){
try{
query.setFields(getFieldsToQuery(query.getFields()));
if(query.getStartKey() != null && query.getStartKey().equals(
query.getEndKey())) {
Get get = new Get(toBytes(query.getStartKey()));
addFields(get, query.getFields());
addTimeRange(get, query);
Result result = table.get(get);
return new HBaseGetResult<K,T>(this, query, result);
} else {
ResultScanner scanner = createScanner(query);
org.apache.gora.query.Result<K,T> result
= new HBaseScannerResult<K,T>(this,query, scanner);
return result;
}
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
public void close() {
try{
table.close();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HSqlInsertUpdateStatement<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class InsertUpdateStatement<K, V extends PersistentBase> {
import org.apache.gora.persistency.impl.PersistentBase;
public static <K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class MySqlInsertUpdateStatement<K, V extends PersistentBase> extends InsertUpdateStatement<K, V> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
public void close() {
public void createSchema() {
public void deleteSchema() {
public boolean schemaExists() {
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
public void flush() {
public T get(K key, String[] requestFields) {
public Result<K, T> execute(Query<K, T> query) {
public void put(K key, T persistent) {
private void parse(String input) throws IOException, ParseException, Exception {
private void storePageview(long key, Pageview pageview) throws IOException, Exception {
private void get(long key) throws IOException, Exception {
private void query(long key) throws IOException, Exception {
private void query(long startKey, long endKey) throws IOException, Exception {
private void deleteByQuery(long startKey, long endKey) throws IOException, Exception {
private void printResult(Result<Long, Pageview> result) throws IOException, Exception {
private void close() throws IOException, Exception {
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloQuery<K,T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AccumuloResult<K,T extends PersistentBase> extends ResultBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class AccumuloStore<K,T extends PersistentBase> extends DataStoreBase<K,T> {
public static final Logger LOG = LoggerFactory.getLogger(AccumuloStore.class);
public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) {
try{
super.initialize(keyClass, persistentClass, properties);
String mock = DataStoreFactory.findProperty(properties, this, MOCK_PROPERTY, null);
String mappingFile = DataStoreFactory.getMappingFile(properties, this, DEFAULT_MAPPING_FILE);
String user = DataStoreFactory.findProperty(properties, this, USERNAME_PROPERTY, null);
String password = DataStoreFactory.findProperty(properties, this, PASSWORD_PROPERTY, null);
mapping = readMapping(mappingFile);
if (mapping.encoder == null || mapping.encoder.equals("")) {
encoder = new org.apache.gora.accumulo.encoders.BinaryEncoder();
try {
encoder = (Encoder) getClass().getClassLoader().loadClass(mapping.encoder).newInstance();
} catch (InstantiationException e) {
throw new IOException(e);
} catch (IllegalAccessException e) {
throw new IOException(e);
} catch (ClassNotFoundException e) {
throw new IOException(e);
}
try {
if (mock == null || !mock.equals("true")) {
String instance = DataStoreFactory.findProperty(properties, this, INSTANCE_NAME_PROPERTY, null);
String zookeepers = DataStoreFactory.findProperty(properties, this, ZOOKEEPERS_NAME_PROPERTY, null);
conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, password);
authInfo = new AuthInfo(user, ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());
} else {
conn = new MockInstance().getConnector(user, password);
}
if (autoCreateSchema)
createSchema();
} catch (AccumuloException e) {
throw new IOException(e);
} catch (AccumuloSecurityException e) {
throw new IOException(e);
}
}catch(IOException e){
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void createSchema() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void deleteSchema() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
public boolean schemaExists() {
public T get(K key, String[] fields) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return null;
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void put(K key, T val) {
try{
Mutation m = new Mutation(new Text(toBytes(key)));
Schema schema = val.getSchema();
StateManager stateManager = val.getStateManager();
Iterator<Field> iter = schema.getFields().iterator();
int count = 0;
Field field = iter.next();
if (!stateManager.isDirty(val, i)) {
continue;
}
Object o = val.get(i);
Pair<Text,Text> col = mapping.fieldMap.get(field.name());
switch (field.schema().getType()) {
case MAP:
if (o instanceof StatefulMap) {
StatefulMap map = (StatefulMap) o;
Set<?> es = map.states().entrySet();
for (Object entry : es) {
Object mapKey = ((Entry) entry).getKey();
State state = (State) ((Entry) entry).getValue();
switch (state) {
case NEW:
case DIRTY:
m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(map.get(mapKey))));
count;
break;
case DELETED:
m.putDelete(col.getFirst(), new Text(toBytes(mapKey)));
count;
break;
}
} else {
Map map = (Map) o;
Set<?> es = map.entrySet();
for (Object entry : es) {
Object mapKey = ((Entry) entry).getKey();
Object mapVal = ((Entry) entry).getValue();
m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(mapVal)));
count;
}
break;
case ARRAY:
GenericArray array = (GenericArray) o;
int j = 0;
for (Object item : array) {
break;
case RECORD:
SpecificDatumWriter writer = new SpecificDatumWriter(field.schema());
ByteArrayOutputStream os = new ByteArrayOutputStream();
BinaryEncoder encoder = new BinaryEncoder(os);
writer.write(o, encoder);
encoder.flush();
m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
break;
default:
m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
}
if (count > 0)
try {
getBatchWriter().addMutation(m);
} catch (MutationsRejectedException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public boolean delete(K key) {
public long deleteByQuery(Query<K,T> query) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return 0;
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return 0;
} catch (IOException e){
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return 0;
public Result<K,T> execute(Query<K,T> query) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
return null;
}
public void flush() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
public void close() {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class CassandraClient<K, T extends PersistentBase> {
public class CassandraStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
public void initialize(Class<K> keyClass, Class<T> persistent, Properties properties) {
super.initialize(keyClass, persistent, properties);
} catch (Exception e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void close() {
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
public void deleteSchema() {
public Result<K, T> execute(Query<K, T> query) {
public void flush() {
public T get(K key, String[] fields) {
boolean hasResult = false;
try {
hasResult = result.next();
} catch (Exception e) {
e.printStackTrace();
}
return this.cassandraClient.getKeyspaceName();
public void put(K key, T value) {
PersistentBase persistent = (PersistentBase) fieldValue;
PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
public boolean schemaExists() {
try{
WebPage page;
log.info("creating web page data");
page = new WebPage();
page.setUrl(new Utf8(URLS[i]));
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
for(String token : CONTENTS[i].split(" ")) {
page.addToParsedContent(new Utf8(token));
}
page.putToOutlinks(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
}
Metadata metadata = new Metadata();
metadata.setVersion(1);
metadata.putToData(new Utf8("metakey"), new Utf8("metavalue"));
page.setMetadata(metadata);
dataStore.put(URLS[i], page);
}
dataStore.flush();
log.info("finished creating web page data");
}
catch(Exception e){
log.info("error creating web page data");
}
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumReader<T extends PersistentBase>
Persistent cloned = (PersistentBase)persistent.newInstance(new StateManagerImpl());
case STRING : ((PersistentBase)cloned).put(pos, cloneObject(
field.schema(), ((PersistentBase)persistent).get(pos), ((PersistentBase)cloned).get(pos))); break;
default     : ((PersistentBase)cloned).put(pos, ((PersistentBase)persistent).get(pos)); break;
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentDatumWriter<T extends PersistentBase>
import java.io.IOException;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.Result;
public class AvroQuery<K, T extends PersistentBase> extends QueryBase<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class AvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class DataFileAvroResult<K, T extends PersistentBase> extends ResultBase<K, T> {
if(reader != null)
reader.close();
reader = null;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class AvroStore<K, T extends PersistentBase>
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
Properties properties) {
super.initialize(keyClass, persistentClass, properties);
if(properties != null) {
if(this.codecType == null) {
String codecType = DataStoreFactory.findProperty(
properties, this, CODEC_TYPE_KEY, "BINARY");
this.codecType = CodecType.valueOf(codecType);
}
public void close() {
try{
super.close();
if(encoder != null) {
encoder.flush();
}
encoder = null;
decoder = null;
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
public void flush() {
try{
super.flush();
if(encoder != null)
encoder.flush();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public T get(K key, String[] fields) {
public void put(K key, T obj) {
try{
getDatumWriter().write(obj, getEncoder());
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void write(DataOutput out) {
public void readFields(DataInput in) {
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class DataFileAvroStore<K, T extends PersistentBase> extends AvroStore<K, T> {
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
public T get(K key, String[] fields) {
public void put(K key, T obj) {
try{
getWriter().append(obj);
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
protected Result<K, T> executeQuery(Query<K, T> query) {
try{
return new DataFileAvroResult<K, T>(this, query
, createReader(createFsInput()));
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
}
protected Result<K,T> executePartial(FileSplitPartitionQuery<K,T> query) {
try{
FsInput fsInput = createFsInput();
DataFileReader<T> reader = createReader(fsInput);
return new DataFileAvroResult<K, T>(this, query, reader, fsInput
, query.getStart(), query.getLength());
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
}
public void flush() {
try{
super.flush();
if(writer != null) {
writer.flush();
}
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
public void close() {
try{
if(writer != null)
writer = null;
super.close();
} catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
import org.apache.gora.persistency.impl.PersistentBase;
public class GoraInputFormat<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.query.impl.ResultBase;
public class GoraRecordReader<K, T extends PersistentBase> extends RecordReader<K,T> {
public void executeQuery() throws IOException, Exception {
try{
return result.getProgress();
}
catch(Exception e){
return 0;
}
try{
if (counter.isModulo()) {
boolean firstBatch = (this.result == null);
if (! firstBatch) {
this.query.setStartKey(this.result.getKey());
if (this.query.getLimit() == counter.getRecordsMax()) {
}
}
if (this.result != null) {
this.result.close();
}
executeQuery();
if (! firstBatch) {
this.result.next();
}
}
counter.increment();
return this.result.next();
}
catch(Exception e){
return false;
}
try{
store.close();
}catch(Exception e){
}
try{
store.put(key, (Persistent) value);
counter.increment();
if (counter.isModulo()) {
store.flush();
}
}catch(Exception e){
}
import org.apache.gora.persistency.impl.PersistentBase;
implements Deserializer<PersistentBase> {
private Class<? extends PersistentBase> persistentClass;
private PersistentDatumReader<PersistentBase> datumReader;
public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
datumReader = new PersistentDatumReader<PersistentBase>(schema, true);
public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
implements Serialization<PersistentBase> {
public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
private PersistentDatumWriter<PersistentBase> datumWriter;
this.datumWriter = new PersistentDatumWriter<PersistentBase>();
public void serialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
public class MemStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
public static class MemQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
public static class MemResult<K, T extends PersistentBase> extends ResultBase<K, T> {
public void close() { }
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
try{
long deletedRows = 0;
Result<K,T> result = query.execute();
while(result.next()) {
if(delete(result.getKey()))
}
return 0;
}
catch(Exception e){
return 0;
}
public Result<K, T> execute(Query<K, T> query) {
public T get(K key, String[] fields) {
((PersistentBase)newObj).put(index, ((PersistentBase)obj).get(index));
public void put(K key, T obj) {
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
public void close() {
public void createSchema() { }
public void deleteSchema() {
public boolean schemaExists() {
public void flush() { }
public interface BeanFactory<K, T>{
public interface Persistent extends Cloneable{
public interface StateManager{
public abstract class PersistentBase implements Persistent, SpecificRecord {
protected static final PersistentDatumReader<PersistentBase> datumReader =
new PersistentDatumReader<PersistentBase>();
dirtyBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
readableBits = new BitSet(((PersistentBase)persistent).getSchema().getFields().size());
public interface PartitionQuery<K, T> extends Query<K, T> {
public interface Query<K, T> {
Result<K,T> execute() throws Exception, IOException;
public interface Result<K,T> {
boolean next() throws Exception, IOException;
float getProgress() throws IOException, InterruptedException, Exception;
import org.apache.gora.persistency.impl.PersistentBase;
public class FileSplitPartitionQuery<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
public class PartitionQueryImpl<K, T extends PersistentBase>
this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
this.dataStore = (DataStoreBase<K, T>) baseQuery.getDataStore();
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.gora.store.impl.DataStoreBase;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.io.Writable;
public abstract class QueryBase<K, T extends PersistentBase>
implements Query<K,T>, Writable, Configurable {
protected DataStoreBase<K,T> dataStore;
this.dataStore = (DataStoreBase<K, T>)dataStore;
public Result<K,T> execute() throws Exception, IOException {
this.dataStore = (DataStoreBase<K, T>)dataStore;
dataStore = (DataStoreBase<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(dataStoreClass), conf);
public final boolean next() throws Exception, IOException {
if(isLimitReached()) {
return false;
}
clear();
persistent = getOrCreatePersistent(persistent);
boolean ret = nextInner();
return ret;
protected T getOrCreatePersistent(T persistent) throws Exception, IOException {
if(persistent != null) {
return persistent;
}
return dataStore.newPersistent();
}
@Override
public void close() throws IOException{
public interface DataStore<K, T> {
Properties properties);
void createSchema();
void deleteSchema();
void truncateSchema();
boolean schemaExists();
K newKey();
T newPersistent();
T get(K key);
T get(K key, String[] fields);
void put(K key, T obj);
boolean delete(K key);
long deleteByQuery(Query<K, T> query);
Result<K,T> execute(Query<K, T> query);
List<PartitionQuery<K,T>> getPartitions(Query<K,T> query) throws IOException;
void flush();
void close();
public class DataStoreFactory{
import java.io.Closeable;
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.io.Writable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public abstract class DataStoreBase<K, T extends PersistentBase>
implements DataStore<K, T>, Configurable, Writable, Closeable {
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
Properties properties) {
setKeyClass(keyClass);
setPersistentClass(persistentClass);
if(this.beanFactory == null)
this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
schema = this.beanFactory.getCachedPersistent().getSchema();
fieldMap = AvroUtils.getFieldMap(schema);
autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
this.properties = properties;
datumReader = new PersistentDatumReader<T>(schema, false);
datumWriter = new PersistentDatumWriter<T>(schema, false);
public K newKey() {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
public T newPersistent() {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
public T get(K key) {
public void readFields(DataInput in) {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
public void write(DataOutput out) {
try {
Text.writeString(out, getKeyClass().getCanonicalName());
Text.writeString(out, getPersistentClass().getCanonicalName());
WritableUtils.writeProperties(out, properties);
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
}
public void truncateSchema() {
import org.apache.gora.avro.store.AvroStore;
import org.apache.gora.persistency.impl.PersistentBase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public abstract class FileBackedDataStoreBase<K, T extends PersistentBase>
public static final Logger LOG = LoggerFactory.getLogger(AvroStore.class);
Properties properties) {
protected OutputStream createOutputStream() {
OutputStream conf = null;
try{
Path path = new Path(outputPath);
FileSystem fs = path.getFileSystem(getConf());
conf = fs.create(path);
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
return conf;
try{
if(inputStream == null) {
inputStream = createInputStream();
}
return inputStream;
}catch(IOException ex){
throw new IOException(ex);
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query){
List<InputSplit> splits = null;
List<PartitionQuery<K, T>> queries = null;
try{
splits = GoraMapReduceUtils.getSplits(getConf(), inputPath);
queries = new ArrayList<PartitionQuery<K,T>>(splits.size());
for(InputSplit split : splits) {
queries.add(new FileSplitPartitionQuery<K, T>(query, (FileSplit) split));
}
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
public Result<K, T> execute(Query<K, T> query) {
Result<K, T> results = null;
try{
if(query instanceof FileSplitPartitionQuery) {
results = executePartial((FileSplitPartitionQuery<K, T>) query);
} else {
results = executeQuery(query);
}
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return results;
public void flush() {
try{
if(outputStream != null)
outputStream.flush();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void createSchema() {
public void deleteSchema() {
public boolean schemaExists() {
public void write(DataOutput out) {
try{
super.write(out);
org.apache.gora.util.IOUtils.writeNullFieldsInfo(out, inputPath, outputPath);
if(inputPath != null)
Text.writeString(out, inputPath);
if(outputPath != null)
Text.writeString(out, outputPath);
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void readFields(DataInput in) {
try{
super.readFields(in);
boolean[] nullFields = org.apache.gora.util.IOUtils.readNullFieldsInfo(in);
if(!nullFields[0])
inputPath = Text.readString(in);
if(!nullFields[1])
outputPath = Text.readString(in);
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public void close() {
IOUtils.closeStream(inputStream);
IOUtils.closeStream(outputStream);
inputStream = null;
outputStream = null;
import org.apache.gora.persistency.impl.PersistentBase;
public static<T extends PersistentBase> void serialize(OutputStream os,
public static<T extends PersistentBase> byte[] serialize(PersistentDatumWriter<T> datumWriter
public static<K, T extends PersistentBase> K deserialize(InputStream is,
public static<K, T extends PersistentBase> K deserialize(byte[] bytes,
public static<T extends PersistentBase> byte[] deserialize(PersistentDatumWriter<T> datumWriter
public void close() {
public void createSchema() {
public void deleteSchema() {
public void truncateSchema() {
public boolean schemaExists() {
public boolean delete(String key) {
public long deleteByQuery(Query<String, MockPersistent> query) {
public Result<String, MockPersistent> execute(Query<String, MockPersistent> query) {
public void flush() {
public MockPersistent get(String key, String[] fields) {
public void put(String key, MockPersistent obj) {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseGetResult<K, T extends PersistentBase> extends HBaseResult<K,T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class HBaseResult<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseScannerResult<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class HBaseStore<K, T extends PersistentBase> extends DataStoreBase<K, T>
Properties properties) {
super.initialize(keyClass, persistentClass, properties);
this.conf = HBaseConfiguration.create(getConf());
admin = new HBaseAdmin(this.conf);
LOG.error(ex1.getMessage());
LOG.error(ex1.getStackTrace().toString());
}
try{
boolean autoflush = this.conf.getBoolean("hbase.client.autoflush.default", false);
table = new HBaseTableConnection(getConf(), getSchemaName(), autoflush);
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
}
public void createSchema() {
try{
if(schemaExists()) {
return;
}
HTableDescriptor tableDesc = mapping.getTable();
admin.createTable(tableDesc);
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
public void deleteSchema() {
try{
if(!schemaExists()) {
return;
}
admin.disableTable(getSchemaName());
admin.deleteTable(getSchemaName());
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
public boolean schemaExists() {
try{
return admin.tableExists(mapping.getTableName());
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
return false;
}
public T get(K key, String[] fields) {
try{
fields = getFieldsToQuery(fields);
Get get = new Get(toBytes(key));
addFields(get, fields);
Result result = table.get(get);
return newInstance(result, fields);
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
return null;
}
public void put(K key, T persistent) {
try{
Schema schema = persistent.getSchema();
StateManager stateManager = persistent.getStateManager();
byte[] keyRaw = toBytes(key);
Put put = new Put(keyRaw);
Delete delete = new Delete(keyRaw);
boolean hasPuts = false;
boolean hasDeletes = false;
Iterator<Field> iter = schema.getFields().iterator();
Field field = iter.next();
if (!stateManager.isDirty(persistent, i)) {
continue;
}
Type type = field.schema().getType();
Object o = persistent.get(i);
HBaseColumn hcol = mapping.getColumn(field.name());
switch(type) {
case MAP:
if(o instanceof StatefulMap) {
StatefulHashMap<Utf8, ?> map = (StatefulHashMap<Utf8, ?>) o;
for (Entry<Utf8, State> e : map.states().entrySet()) {
Utf8 mapKey = e.getKey();
switch (e.getValue()) {
case DIRTY:
byte[] qual = Bytes.toBytes(mapKey.toString());
byte[] val = toBytes(map.get(mapKey), field.schema().getValueType());
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
break;
case DELETED:
qual = Bytes.toBytes(mapKey.toString());
hasDeletes = true;
delete.deleteColumn(hcol.getFamily(), qual);
break;
}
}
} else {
Set<Map.Entry> set = ((Map)o).entrySet();
for(Entry entry: set) {
byte[] qual = toBytes(entry.getKey());
byte[] val = toBytes(entry.getValue());
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
break;
case ARRAY:
if(o instanceof GenericArray) {
GenericArray arr = (GenericArray) o;
int j=0;
for(Object item : arr) {
byte[] val = toBytes(item);
hasPuts = true;
}
break;
default:
put.add(hcol.getFamily(), hcol.getQualifier(), toBytes(o, field.schema()));
hasPuts = true;
break;
}
if (hasPuts) {
table.put(put);
}
if (hasDeletes) {
table.delete(delete);
}
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
public boolean delete(K key) {
try{
table.delete(new Delete(toBytes(key)));
return true;
} catch(IOException ex2){
LOG.error(ex2.getMessage());
LOG.error(ex2.getStackTrace().toString());
return false;
public long deleteByQuery(Query<K, T> query) {
try {
String[] fields = getFieldsToQuery(query.getFields());
boolean isAllFields = Arrays.equals(fields
, getBeanFactory().getCachedPersistent().getFields());
org.apache.gora.query.Result<K, T> result = null;
result = query.execute();
ArrayList<Delete> deletes = new ArrayList<Delete>();
while(result.next()) {
Delete delete = new Delete(toBytes(result.getKey()));
deletes.add(delete);
if(!isAllFields) {
addFields(delete, query);
}
}
table.delete(deletes);
return deletes.size();
} catch (Exception e) {
e.printStackTrace();
return -1;
}
}
@Override
public void flush() {
try{
table.flushCommits();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
public org.apache.gora.query.Result<K, T> execute(Query<K, T> query){
try{
query.setFields(getFieldsToQuery(query.getFields()));
if(query.getStartKey() != null && query.getStartKey().equals(
query.getEndKey())) {
Get get = new Get(toBytes(query.getStartKey()));
addFields(get, query.getFields());
addTimeRange(get, query);
Result result = table.get(get);
return new HBaseGetResult<K,T>(this, query, result);
} else {
ResultScanner scanner = createScanner(query);
org.apache.gora.query.Result<K,T> result
= new HBaseScannerResult<K,T>(this,query, scanner);
return result;
}
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
return null;
public void close() {
try{
table.close();
}catch(IOException ex){
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
}
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlResult<K, T extends PersistentBase> extends ResultBase<K, T> {
import org.apache.gora.persistency.impl.PersistentBase;
public class HSqlInsertUpdateStatement<K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public abstract class InsertUpdateStatement<K, V extends PersistentBase> {
import org.apache.gora.persistency.impl.PersistentBase;
public static <K, T extends PersistentBase>
import org.apache.gora.persistency.impl.PersistentBase;
public class MySqlInsertUpdateStatement<K, V extends PersistentBase> extends InsertUpdateStatement<K, V> {
import org.apache.gora.persistency.impl.PersistentBase;
public class SqlStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
public void close() {
public void createSchema() {
public void deleteSchema() {
public boolean schemaExists() {
public boolean delete(K key) {
public long deleteByQuery(Query<K, T> query) {
public void flush() {
public T get(K key, String[] requestFields) {
public Result<K, T> execute(Query<K, T> query) {
public void put(K key, T persistent) {
private void parse(String input) throws IOException, ParseException, Exception {
private void storePageview(long key, Pageview pageview) throws IOException, Exception {
private void get(long key) throws IOException, Exception {
private void query(long key) throws IOException, Exception {
private void query(long startKey, long endKey) throws IOException, Exception {
private void deleteByQuery(long startKey, long endKey) throws IOException, Exception {
private void printResult(Result<Long, Pageview> result) throws IOException, Exception {
private void close() throws IOException, Exception {
Text.writeString(out, obj.getClass().getName());
conf.set(classKey, obj.getClass().getName());
}
Text.writeString(out, obj.getClass().getName());
conf.set(classKey, obj.getClass().getName());
}
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.Persistent;
public class GoraMapper<K1, V1 extends Persistent, K2, V2> extends Mapper<K1, V1, K2, V2> {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
initMapperJob(job, dataStoreClass, inKeyClass, inValueClass, outKeyClass,
outValueClass, mapperClass, null, reuseObjects);
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
import org.apache.gora.persistency.Persistent;
public interface PartitionQuery<K, T extends Persistent> extends Query<K, T> {
import org.apache.gora.persistency.Persistent;
public interface Query<K, T extends Persistent> {
void setDataStore(DataStore<K, T> dataStore);
DataStore<K, T> getDataStore();
Result<K, T> execute();
import org.apache.gora.persistency.Persistent;
public interface Result<K, T extends Persistent> {
DataStore<K, T> getDataStore();
float getProgress() throws IOException, InterruptedException;
public class PartitionQueryImpl<K, T extends PersistentBase> extends QueryBase<K, T>
implements PartitionQuery<K, T> {
public abstract class QueryBase<K, T extends PersistentBase>
implements Query<K,T>, Writable, Configurable {
public Result<K,T> execute() {
public abstract class QueryWSBase<K, T extends Persistent> implements Query<K,T> {
public Result<K,T> execute() {
public interface DataStore<K, T extends Persistent> {
void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties);
Result<K, T> execute(Query<K, T> query);
List<PartitionQuery<K, T>> getPartitions(Query<K, T> query) throws IOException;
public float getProgress() throws IOException, InterruptedException {
public class GoraMapper<K1, V1 extends Persistent, K2, V2> extends Mapper<K1, V1, K2, V2> {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
initMapperJob(job, dataStoreClass, inKeyClass, inValueClass, outKeyClass,
outValueClass, mapperClass, null, reuseObjects);
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
public static <K1, V1 extends Persistent, K2, V2> void initMapperJob(
boolean reuseObjects) throws IOException {
import org.apache.gora.persistency.Persistent;
public interface PartitionQuery<K, T extends Persistent> extends Query<K, T> {
import org.apache.gora.persistency.Persistent;
public interface Query<K, T extends Persistent> {
void setDataStore(DataStore<K, T> dataStore);
DataStore<K, T> getDataStore();
Result<K, T> execute();
import org.apache.gora.persistency.Persistent;
public interface Result<K, T extends Persistent> {
DataStore<K, T> getDataStore();
float getProgress() throws IOException, InterruptedException;
public class PartitionQueryImpl<K, T extends PersistentBase> extends QueryBase<K, T>
implements PartitionQuery<K, T> {
public abstract class QueryBase<K, T extends PersistentBase>
implements Query<K,T>, Writable, Configurable {
public Result<K,T> execute() {
public abstract class QueryWSBase<K, T extends Persistent> implements Query<K,T> {
public Result<K,T> execute() {
public interface DataStore<K, T extends Persistent> {
void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties);
Result<K, T> execute(Query<K, T> query);
List<PartitionQuery<K, T>> getPartitions(Query<K, T> query) throws IOException;
public float getProgress() throws IOException, InterruptedException {
public class HBaseScannerResult<K, T extends PersistentBase> extends HBaseResult<K, T> {
= new HBaseScannerResult<K,T>(this,query, scanner);
public ResultScanner createScanner(Query<K, T> query) throws IOException {
public class HBaseTableConnection implements HTableInterface {
public HBaseTableConnection(Configuration conf, String tableName, boolean autoflush)
throws IOException {
public static class LogAnalyticsMapper extends GoraMapper<Long, Pageview, TextLong,
LongWritable> {
protected void map(Long key, Pageview pageview, Context context)
throws IOException ,InterruptedException {
public static class LogAnalyticsReducer extends GoraReducer<TextLong, LongWritable,
String, MetricDatum> {
protected void reduce(TextLong tuple, Iterable<LongWritable> values, Context context)
public Job createJob(DataStore<Long, Pageview> inStore,
DataStore<String, MetricDatum> outStore, int numReducer) throws IOException {
GoraMapper.initMapperJob(job, inStore, TextLong.class, LongWritable.class,
LogAnalyticsMapper.class, true);
Configuration conf = new Configuration();
getDataStore(dataStoreClass, String.class, MetricDatum.class, conf);
inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, conf);
outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, conf);
public class HBaseScannerResult<K, T extends PersistentBase> extends HBaseResult<K, T> {
= new HBaseScannerResult<K,T>(this,query, scanner);
public ResultScanner createScanner(Query<K, T> query) throws IOException {
public class HBaseTableConnection implements HTableInterface {
public HBaseTableConnection(Configuration conf, String tableName, boolean autoflush)
throws IOException {
public static class LogAnalyticsMapper extends GoraMapper<Long, Pageview, TextLong,
LongWritable> {
protected void map(Long key, Pageview pageview, Context context)
throws IOException ,InterruptedException {
public static class LogAnalyticsReducer extends GoraReducer<TextLong, LongWritable,
String, MetricDatum> {
protected void reduce(TextLong tuple, Iterable<LongWritable> values, Context context)
public Job createJob(DataStore<Long, Pageview> inStore,
DataStore<String, MetricDatum> outStore, int numReducer) throws IOException {
GoraMapper.initMapperJob(job, inStore, TextLong.class, LongWritable.class,
LogAnalyticsMapper.class, true);
Configuration conf = new Configuration();
getDataStore(dataStoreClass, String.class, MetricDatum.class, conf);
inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, conf);
outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, conf);
implements DataStore<K, T>, Configurable, Writable, Closeable {
implements DataStore<K, T>, Configurable, Writable, Closeable {
setKeyClass(keyClass);
setPersistentClass(persistentClass);
if (this.beanFactory == null) {
this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
}
schema = this.beanFactory.getCachedPersistent().getSchema();
fieldMap = AvroUtils.getFieldMap(schema);
autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
this.properties = properties;
datumReader = new PersistentDatumReader<T>(schema, false);
datumWriter = new PersistentDatumWriter<T>(schema, false);
if (hcol == null) {
}
} catch (Exception ex) {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
setKeyClass(keyClass);
setPersistentClass(persistentClass);
if (this.beanFactory == null) {
this.beanFactory = new BeanFactoryImpl<K, T>(keyClass, persistentClass);
}
schema = this.beanFactory.getCachedPersistent().getSchema();
fieldMap = AvroUtils.getFieldMap(schema);
autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
this.properties = properties;
datumReader = new PersistentDatumReader<T>(schema, false);
datumWriter = new PersistentDatumWriter<T>(schema, false);
if (hcol == null) {
}
} catch (Exception ex) {
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
if (col == null) {
"Wrong gora-hbase-mapping.xml?");
}
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
LOG.error(ex.getMessage());
LOG.error(ex.getStackTrace().toString());
import java.text.SimpleDateFormat;
import org.apache.gora.util.LicenseHeaders;
import org.apache.gora.util.TimingUtil;
private static LicenseHeaders licenseHeader = new LicenseHeaders(null);
if(licenseHeader != null) {
}
if(licenseHeader != null) {
}
if (licenseHeader != null) {
line(0, licenseHeader.getLicense());
}
System.err.println("Usage: GoraCompiler <schema file> <output dir> [-license <id>]");
System.err.println("  <schema file>     - individual avsc file to be compiled");
System.err.println("  <output dir>      - output directory for generated Java files");
System.err.println("  [-license <id>]   - the preferred license header to add to the\n"
"\t\t      generated Java file. Current options include; \n"
"\t\t  ASLv2   (Apache Software License v2.0) \n"
"\t\t  AGPLv3  (GNU Affero General Public License)\n"
"\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n"
"\t\t  FDLv13  (GNU Free Documentation License v1.3)\n"
"\t\t  GPLv1   (GNU General Public License v1.0)\n"
"\t\t  GPLv2   (GNU General Public License v2.0)\n"
"\t\t  GPLv3   (GNU General Public License v3.0)\n "
"\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n"
"\t\t  LGPLv3  (GNU Lesser General Public License v2.1)\n") ;
licenseHeader.setLicenseName("ASLv2");
if ("-license".equals(args[i])) {
}
}
SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
long start = System.currentTimeMillis();
long end = System.currentTimeMillis();
return;
import java.text.SimpleDateFormat;
import org.apache.gora.util.LicenseHeaders;
import org.apache.gora.util.TimingUtil;
private static LicenseHeaders licenseHeader = new LicenseHeaders(null);
if(licenseHeader != null) {
}
if(licenseHeader != null) {
}
if (licenseHeader != null) {
line(0, licenseHeader.getLicense());
}
System.err.println("Usage: GoraCompiler <schema file> <output dir> [-license <id>]");
System.err.println("  <schema file>     - individual avsc file to be compiled");
System.err.println("  <output dir>      - output directory for generated Java files");
System.err.println("  [-license <id>]   - the preferred license header to add to the\n"
"\t\t      generated Java file. Current options include; \n"
"\t\t  ASLv2   (Apache Software License v2.0) \n"
"\t\t  AGPLv3  (GNU Affero General Public License)\n"
"\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n"
"\t\t  FDLv13  (GNU Free Documentation License v1.3)\n"
"\t\t  GPLv1   (GNU General Public License v1.0)\n"
"\t\t  GPLv2   (GNU General Public License v2.0)\n"
"\t\t  GPLv3   (GNU General Public License v3.0)\n "
"\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n"
"\t\t  LGPLv3  (GNU Lesser General Public License v2.1)\n") ;
licenseHeader.setLicenseName("ASLv2");
if ("-license".equals(args[i])) {
}
}
SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
long start = System.currentTimeMillis();
long end = System.currentTimeMillis();
return;
public String getField(int index) { return null; }
public person clone() { return null; }
public void clearNew() { }
public boolean isReadable(int fieldIndex) { return false; }
line(pIden, "public String getField(int index) {return null; }");
line(pIden, "public person clone() {return null; }");
line(pIden, "public void clearNew() {}");
line(pIden, "public boolean isReadable(int fieldIndex) {return false; }");
super(null);
super(dataStore);
KeySchemaElement rangeKeyElement = new KeySchemaElement().withAttributeName(rangeKeyName).withAttributeType(rangeKeyType);
kSchema.setRangeKeyElement(rangeKeyElement);
tablesToKeySchemas.put(tableName, kSchema);
kSchema = new KeySchema();
KeySchemaElement hashKey = new KeySchemaElement().withAttributeName(keyName).withAttributeType(keyType);
kSchema.setHashKeyElement(hashKey);
tablesToKeySchemas.put(tableName, kSchema);
itemAttribs = (HashMap<String, String>) items.get(itemNumber);
if (itemAttribs == null)
items.add(new HashMap<String, String>());
return (HashMap<String, String>) items.get(itemNumber);
if (tablesToKeySchemas.isEmpty()) return "";
if (!verifyKeySchema(tableName)) return "";
KeySchemaElement rangeKey = kSchema.getRangeKeyElement();
KeySchemaElement hashKey = kSchema.getHashKeyElement();
if (rangeKey != null){
if (hashKey != null)
return true;
else
return false;
}
if (hashKey != null)
return true;
return false;
String wrongTableName = verifyAllKeySchemas();
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
try{
public String getField(int index) { return null; }
public person clone() { return null; }
public void clearNew() { }
public boolean isReadable(int fieldIndex) { return false; }
line(pIden, "public String getField(int index) {return null; }");
line(pIden, "public person clone() {return null; }");
line(pIden, "public void clearNew() {}");
line(pIden, "public boolean isReadable(int fieldIndex) {return false; }");
super(null);
super(dataStore);
KeySchemaElement rangeKeyElement = new KeySchemaElement().withAttributeName(rangeKeyName).withAttributeType(rangeKeyType);
kSchema.setRangeKeyElement(rangeKeyElement);
tablesToKeySchemas.put(tableName, kSchema);
kSchema = new KeySchema();
KeySchemaElement hashKey = new KeySchemaElement().withAttributeName(keyName).withAttributeType(keyType);
kSchema.setHashKeyElement(hashKey);
tablesToKeySchemas.put(tableName, kSchema);
itemAttribs = (HashMap<String, String>) items.get(itemNumber);
if (itemAttribs == null)
items.add(new HashMap<String, String>());
return (HashMap<String, String>) items.get(itemNumber);
if (tablesToKeySchemas.isEmpty()) return "";
if (!verifyKeySchema(tableName)) return "";
KeySchemaElement rangeKey = kSchema.getRangeKeyElement();
KeySchemaElement hashKey = kSchema.getHashKeyElement();
if (rangeKey != null){
if (hashKey != null)
return true;
else
return false;
}
if (hashKey != null)
return true;
return false;
String wrongTableName = verifyAllKeySchemas();
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
if (mapping.getTables().isEmpty())  throw new IllegalStateException("There are not tables defined.");
try{
StatefulHashMap map = (StatefulHashMap) fieldValue;
StatefulHashMap newMap = new StatefulHashMap();
for (Object mapKey : map.keySet()) {
newMap.put(mapKey, map.get(mapKey));
newMap.putState(mapKey, map.getState(mapKey));
}
fieldValue = newMap;
StatefulHashMap map = (StatefulHashMap) fieldValue;
StatefulHashMap newMap = new StatefulHashMap();
for (Object mapKey : map.keySet()) {
newMap.put(mapKey, map.get(mapKey));
newMap.putState(mapKey, map.getState(mapKey));
}
fieldValue = newMap;
import me.prettyprint.hector.api.ddl.ComparatorType;
for (ColumnFamilyDefinition cfDef : columnFamilyDefinitions) {
cfDef.setComparatorType(ComparatorType.BYTESTYPE);
}
else {
List<ColumnFamilyDefinition> cfDefs = keyspaceDefinition.getCfDefs();
if (cfDefs == null || cfDefs.size() == 0) {
}
else {
for (ColumnFamilyDefinition cfDef : cfDefs) {
ComparatorType comparatorType = cfDef.getComparatorType();
if (! comparatorType.equals(ComparatorType.BYTESTYPE)) {
", not BytesType. It may cause a fatal error on column validation later.");
}
else {
}
}
}
}
import me.prettyprint.hector.api.ddl.ComparatorType;
for (ColumnFamilyDefinition cfDef : columnFamilyDefinitions) {
cfDef.setComparatorType(ComparatorType.BYTESTYPE);
}
else {
List<ColumnFamilyDefinition> cfDefs = keyspaceDefinition.getCfDefs();
if (cfDefs == null || cfDefs.size() == 0) {
}
else {
for (ColumnFamilyDefinition cfDef : cfDefs) {
ComparatorType comparatorType = cfDef.getComparatorType();
if (! comparatorType.equals(ComparatorType.BYTESTYPE)) {
", not BytesType. It may cause a fatal error on column validation later.");
}
else {
}
}
}
}
LOG.error("Keyspace element should not be null!");
return;
LOG.error("Error locating Cassandra Keyspace name attribute!");
LOG.error("Error locating Cassandra Keyspace cluster attribute!");
LOG.error("Error locating Cassandra Keyspace host attribute!");
LOG.error("Error locating column family name attribute!");
continue;
if (mappingElement == null) {
return null;
}
if (keyspaceElement == null) {
return null;
}
LOG.error("Error locating Cassandra Keyspace element!");
LOG.error("Error locating Cassandra Keyspace name attribute!");
continue;
else {
}
LOG.error("Error locating Cassandra Mapping class element!");
LOG.error("Error locating Cassandra Mapping class name attribute!");
else {
}
LOG.error("Keyspace element should not be null!");
return;
LOG.error("Error locating Cassandra Keyspace name attribute!");
LOG.error("Error locating Cassandra Keyspace cluster attribute!");
LOG.error("Error locating Cassandra Keyspace host attribute!");
LOG.error("Error locating column family name attribute!");
continue;
if (mappingElement == null) {
return null;
}
if (keyspaceElement == null) {
return null;
}
LOG.error("Error locating Cassandra Keyspace element!");
LOG.error("Error locating Cassandra Keyspace name attribute!");
continue;
else {
}
LOG.error("Error locating Cassandra Mapping class element!");
LOG.error("Error locating Cassandra Mapping class name attribute!");
else {
}
@SuppressWarnings("unchecked")
LOG.debug("Located Cassandra Keyspace");
LOG.debug("Located super column family");
private static final String KEYCLASS_ATTRIBUTE = "keyClass";
private static final String CLUSTER_ATTRIBUTE = "cluster";
public CassandraMapping get(Class<?> persistentClass) {
String clusterName = keyspace.getAttributeValue(CLUSTER_ATTRIBUTE);
String hostName = keyspace.getAttributeValue(HOST_ATTRIBUTE);
LOG.error("Error locating Cassandra Keyspace name attribute!");
continue;
String keyClassName = mapping.getAttributeValue(KEYCLASS_ATTRIBUTE);
String keyspaceName = mapping.getAttributeValue(KEYSPACE_ELEMENT);
LOG.error("Error locating Cassandra Mapping class name attribute!");
continue;
@SuppressWarnings("unchecked")
LOG.debug("Located Cassandra Keyspace");
LOG.debug("Located super column family");
private static final String KEYCLASS_ATTRIBUTE = "keyClass";
private static final String CLUSTER_ATTRIBUTE = "cluster";
public CassandraMapping get(Class<?> persistentClass) {
String clusterName = keyspace.getAttributeValue(CLUSTER_ATTRIBUTE);
String hostName = keyspace.getAttributeValue(HOST_ATTRIBUTE);
LOG.error("Error locating Cassandra Keyspace name attribute!");
continue;
String keyClassName = mapping.getAttributeValue(KEYCLASS_ATTRIBUTE);
String keyspaceName = mapping.getAttributeValue(KEYSPACE_ELEMENT);
LOG.error("Error locating Cassandra Mapping class name attribute!");
continue;
public class PersistentSerialization implements Serialization<PersistentBase> {
public class PersistentSerialization implements Serialization<PersistentBase> {
import java.util.Collections;
private Map<K, T> buffer = Collections.synchronizedMap(new LinkedHashMap<K, T>());
import java.util.Collections;
private Map<K, T> buffer = Collections.synchronizedMap(new LinkedHashMap<K, T>());
synchronized(mutator) {
HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);
}
synchronized(mutator) {
HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
}
synchronized(mutator) {
HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
}
synchronized(mutator) {
HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer);
}
synchronized(mutator) {
HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer);
}
synchronized(mutator) {
HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, columnName);
}
LOG.error("Keyspace element should not be null!");
return;
if (LOG.isDebugEnabled()) {
LOG.debug("Located Cassandra Keyspace");
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
LOG.debug("Located super column family");
}
if (LOG.isDebugEnabled()) {
}
if (fieldName == null) {
LOG.error("Field name is not declared.");
continue;
}
if (familyName == null) {
continue;
}
if (columnName == null) {
columnName = fieldName;
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
LOG.error("Keyspace element should not be null!");
return;
if (LOG.isDebugEnabled()) {
LOG.debug("Located Cassandra Keyspace");
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
LOG.debug("Located super column family");
}
if (LOG.isDebugEnabled()) {
}
if (fieldName == null) {
LOG.error("Field name is not declared.");
continue;
}
if (familyName == null) {
continue;
}
if (columnName == null) {
columnName = fieldName;
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
if (LOG.isDebugEnabled()) {
}
extends GoraMapper<K, T, NullWritable, NullWritable> {
assert(job.isComplete() == true);
assert(job.isComplete() == true);
extends GoraMapper<K, T, NullWritable, NullWritable> {
assert(job.isComplete() == true);
assert(job.isComplete() == true);
import java.util.ArrayList;
private final static String SCHEMA_EXTENTION = ".avsc";
public static void compileSchema(File[] srcFiles, File dest) throws IOException {
if(licenseHeader != null) {
}
for (File src : srcFiles) {
GoraCompiler compiler = new GoraCompiler(dest);
}
}
System.err.println("  <schema file>     - individual avsc file to be compiled or a directory path containing avsc files");
File inputFile = new File(args[0]);
File output = new File(args[1]);
if(!inputFile.exists() || !output.exists()){
System.err.println("input file path or output file path doesn't exists.");
System.exit(1);
}
if(inputFile.isDirectory()) {
ArrayList<File> inputSchemas = new ArrayList<File>();
File[] listOfFiles= inputFile.listFiles();
for (File file : listOfFiles) {
if (file.isFile() && file.exists() && file.getName().endsWith(SCHEMA_EXTENTION)) {
inputSchemas.add(file);
}
}
compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
}
else if (inputFile.isFile()) {
compileSchema(inputFile, output);
}
import java.util.ArrayList;
private final static String SCHEMA_EXTENTION = ".avsc";
public static void compileSchema(File[] srcFiles, File dest) throws IOException {
if(licenseHeader != null) {
}
for (File src : srcFiles) {
GoraCompiler compiler = new GoraCompiler(dest);
}
}
System.err.println("  <schema file>     - individual avsc file to be compiled or a directory path containing avsc files");
File inputFile = new File(args[0]);
File output = new File(args[1]);
if(!inputFile.exists() || !output.exists()){
System.err.println("input file path or output file path doesn't exists.");
System.exit(1);
}
if(inputFile.isDirectory()) {
ArrayList<File> inputSchemas = new ArrayList<File>();
File[] listOfFiles= inputFile.listFiles();
for (File file : listOfFiles) {
if (file.isFile() && file.exists() && file.getName().endsWith(SCHEMA_EXTENTION)) {
inputSchemas.add(file);
}
}
compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
}
else if (inputFile.isFile()) {
compileSchema(inputFile, output);
}
import org.apache.gora.util.GoraException;
if (mapping.tableName == null) {
}
if (col == null) {
}
import org.apache.gora.util.GoraException;
if (mapping.tableName == null) {
}
if (col == null) {
}
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
case UNION:
case UNION:
private int unionType;
public void setUnionType(int pUnionType){
this.unionType = pUnionType;
}
public int getUnionType(){
return unionType;
}
Serializer<?> serializer = GoraSerializerTypeInferer.getSerializer(schema);
import org.apache.avro.Schema.Type;
import org.apache.gora.cassandra.store.CassandraStore;
private CassandraColumn getUnionTypeColumn(String pFieldName, Object[] pCassandraRow){
CassandraColumn cColumn = (CassandraColumn)pCassandraRow[iCnt];
String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName());
if (pFieldName.equals(columnName))
return cColumn;
}
return null;
}
if (fieldName != null ){
int pos = this.persistent.getFieldIndex(fieldName);
Field field = fields.get(pos);
Type fieldType = field.schema().getType();
if (fieldType == Type.UNION){
cassandraColumn.setUnionType(getNonNullTypePos(field.schema().getTypes()));
}
cassandraColumn.setField(field);
Object value = cassandraColumn.getValue();
this.persistent.put(pos, value);
this.persistent.clearDirty(pos);
}
else
LOG.debug("FieldName was null while iterating CassandraRow and using Avro Union type");
private int getNonNullTypePos(List<Schema> pTypes){
int iCnt = 0;
for (Schema sch :  pTypes)
if (!sch.getName().equals("null"))
return iCnt;
else
iCnt;
return CassandraStore.DEFAULT_UNION_SCHEMA;
}
import me.prettyprint.cassandra.serializers.StringSerializer;
public CassandraColumn getCassandraColumn(String pCassandraColumnName){
for (CassandraColumn cColumn: this)
if ( pCassandraColumnName.equals(StringSerializer.get().fromByteBuffer(cColumn.getName())) )
return cColumn;
return null;
}
import org.apache.gora.cassandra.store.CassandraStore;
} else if (type == Type.UNION){
Schema unionFieldSchema = getUnionSchema(super.getUnionType(), field.schema());
value = fromByteBuffer(unionFieldSchema, byteBuffer);
private Schema getUnionSchema (int pSchemaPos, Schema pSchema){
Schema unionSchema = pSchema.getTypes().get(pSchemaPos);
if ( unionSchema == null )
pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
return unionSchema;
}
} else if (type == Type.UNION){
serializer = ByteBufferSerializer.get();
private String getMappingFamily(String pField){
String family = null;
family = this.cassandraMapping.getFamily(pField);
return family;
}
private String getMappingColumn(String pField){
String column = null;
column = this.cassandraMapping.getColumn(pField);
return column;
}
String family = this.getMappingFamily(field);
String column = this.getMappingColumn(field);
public CassandraMapping getCassandraMapping(){
return this.cassandraMapping;
}
String family = this.getMappingFamily(field);
String column = this.getMappingColumn(field);
public void addColumn(String pFamilyName, String pFieldName, String pColumnName){
this.familyMap.put(pFieldName, pFamilyName);
this.columnMap.put(pFieldName, pColumnName);
}
import java.io.InputStream;
InputStream inputStream = getClass().getClassLoader().getResourceAsStream(MAPPING_FILE);
if (inputStream == null){
}
Document document = saxBuilder.build(inputStream);
public static int DEFAULT_UNION_SCHEMA = 0;
CassandraResultSet<K> cassandraResultSet = new CassandraResultSet<K>();
case UNION:
break;
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
for (Field member: schema.getFields()) {
Object memberValue = persistentBase.get(member.pos());
if (memberValue instanceof GenericArray<?>) {
if (((GenericArray)memberValue).size() == 0) {
continue;
}
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
case UNION:
if(value != null) {
this.cassandraClient.addColumn(key, field.name(), value);
} else {
}
private int getUnionSchema(Object pValue, Schema pUnionSchema){
int unionSchemaPos = 0;
String valueType = pValue.getClass().getSimpleName();
Iterator<Schema> it = pUnionSchema.getTypes().iterator();
while ( it.hasNext() ){
String schemaName = it.next().getName();
if (valueType.equals("Utf8") && schemaName.equals(Type.STRING.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("HeapByteBuffer") && schemaName.equals(Type.STRING.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Integer") && schemaName.equals(Type.INT.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Long") && schemaName.equals(Type.LONG.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Double") && schemaName.equals(Type.DOUBLE.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Float") && schemaName.equals(Type.FLOAT.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Boolean") && schemaName.equals(Type.BOOLEAN.name().toLowerCase()))
return unionSchemaPos;
}
return 0;
}
null,
if (CONTENTS[i]!=null){
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
for(String token : CONTENTS[i].split(" ")) {
page.addToParsedContent(new Utf8(token));
}
}
import org.apache.avro.specific.FixedSize;
public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"dateOfBirth\",\"type\":\"long\"},{\"name\":\"ssn\",\"type\":\"string\"},{\"name\":\"salary\",\"type\":\"int\"},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"]},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}]}]}");
BOSS(4,"boss"),
WEBPAGE(5,"webpage"),
public static final String[] _ALL_FIELDS = {"name","dateOfBirth","ssn","salary","boss","webpage",};
private Object boss;
private WebPage webpage;
case 4: return boss;
case 5: return webpage;
case 4:boss = (Object)_value; break;
case 5:webpage = (WebPage)_value; break;
public Object getBoss() {
return (Object) get(4);
}
public void setBoss(Employee value) {
put(4, value);
}
public void setBoss(Utf8 value) {
put(4, value);
}
public WebPage getWebpage() {
return (WebPage) get(5);
}
public void setWebpage(WebPage value) {
put(5, value);
}
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.FixedSize;
public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}");
if (page.getContent() != null) {
String content = new String(page.getContent().array());
StringTokenizer itr = new StringTokenizer(content);
while (itr.hasMoreTokens()) {
word.set(itr.nextToken());
context.write(word, one);
}
break;
case UNION:
fieldType = type(fieldSchema);
line(1, "}");
for (Schema s : fieldSchema.getTypes()) {
if (s.getType().equals(Schema.Type.NULL)) continue ;
String unionFieldType = type(s);
line(1, "}");
}
break;
case NULL:
default:
import org.apache.avro.generic.GenericData;
boolean isUnionField = false ;
int unionIndex = -1 ;
if (field.schema().getType() == Type.UNION) {
isUnionField = true ;
unionIndex = GenericData.get().resolveUnion(field.schema(), o);
}
if(field.schema().getType() == Type.BYTES
|| (isUnionField
&& field.schema().getTypes().get(unionIndex).getType() == Type.BYTES)) {
return newInstance(result, fields);
delete.deleteColumn(hcol.getFamily(), qual);
hasDeletes = true;
} else {
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
}
byte[] val = toBytes(entry.getValue(), field.schema().getValueType());
delete.deleteColumn(hcol.getFamily(), qual);
hasDeletes = true;
} else {
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
}
byte[] val = toBytes(item, field.schema().getElementType());
hasDeletes = true;
} else {
hasPuts = true;
}
byte[] serializedBytes = toBytes(o, field.schema()) ;
delete.deleteColumn(hcol.getFamily(), hcol.getQualifier());
hasDeletes = true;
} else {
put.add(hcol.getFamily(), hcol.getQualifier(), serializedBytes);
hasPuts = true;
}
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
import org.apache.avro.generic.GenericData;
import org.apache.gora.avro.PersistentDatumReader;
import org.apache.gora.avro.PersistentDatumWriter;
case UNION:
if (schema.getTypes().size() == 2) {
Type type0 = schema.getTypes().get(0).getType() ;
Type type1 = schema.getTypes().get(1).getType() ;
if (!type0.equals(type1)
&& (   type0.equals(Schema.Type.NULL)
|| type1.equals(Schema.Type.NULL))) {
if (type0.equals(Schema.Type.NULL))
schema = schema.getTypes().get(1) ;
else
schema = schema.getTypes().get(0) ;
}
}
PersistentDatumReader<?> reader = null ;
if (schema.getType().equals(Schema.Type.UNION)) {
reader = (PersistentDatumReader<?>)readerMap.get(String.valueOf(schema.hashCode()));
if (reader == null) {
readerMap.put(String.valueOf(schema.hashCode()), reader);
}
} else {
reader = (PersistentDatumReader<?>)readerMap.get(schema.getFullName());
if (reader == null) {
readerMap.put(schema.getFullName(), reader);
}
return reader.read((Object)null, schema, decoder);
case UNION:
if (schema.getTypes().size() == 2) {
Type type0 = schema.getTypes().get(0).getType() ;
Type type1 = schema.getTypes().get(1).getType() ;
if (!type0.equals(type1)
&& (   type0.equals(Schema.Type.NULL)
|| type1.equals(Schema.Type.NULL))) {
if (o == null) return null ;
int index = GenericData.get().resolveUnion(schema, o);
schema = schema.getTypes().get(index) ;
}
}
PersistentDatumWriter writer = null ;
if (schema.getType().equals(Schema.Type.UNION)) {
writer = (PersistentDatumWriter<?>) writerMap.get(String.valueOf(schema.hashCode()));
if (writer == null) {
writerMap.put(String.valueOf(schema.hashCode()),writer);
}
} else {
writer = (PersistentDatumWriter<?>) writerMap.get(schema.getFullName());
if (writer == null) {
writerMap.put(schema.getFullName(),writer);
}
writer.write(schema,o, encoder);
import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
case UNION:
case UNION:
private int unionType;
public void setUnionType(int pUnionType){
this.unionType = pUnionType;
}
public int getUnionType(){
return unionType;
}
Serializer<?> serializer = GoraSerializerTypeInferer.getSerializer(schema);
import org.apache.avro.Schema.Type;
import org.apache.gora.cassandra.store.CassandraStore;
private CassandraColumn getUnionTypeColumn(String pFieldName, Object[] pCassandraRow){
CassandraColumn cColumn = (CassandraColumn)pCassandraRow[iCnt];
String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName());
if (pFieldName.equals(columnName))
return cColumn;
}
return null;
}
if (fieldName != null ){
int pos = this.persistent.getFieldIndex(fieldName);
Field field = fields.get(pos);
Type fieldType = field.schema().getType();
if (fieldType == Type.UNION){
cassandraColumn.setUnionType(getNonNullTypePos(field.schema().getTypes()));
}
cassandraColumn.setField(field);
Object value = cassandraColumn.getValue();
this.persistent.put(pos, value);
this.persistent.clearDirty(pos);
}
else
LOG.debug("FieldName was null while iterating CassandraRow and using Avro Union type");
private int getNonNullTypePos(List<Schema> pTypes){
int iCnt = 0;
for (Schema sch :  pTypes)
if (!sch.getName().equals("null"))
return iCnt;
else
iCnt;
return CassandraStore.DEFAULT_UNION_SCHEMA;
}
import me.prettyprint.cassandra.serializers.StringSerializer;
public CassandraColumn getCassandraColumn(String pCassandraColumnName){
for (CassandraColumn cColumn: this)
if ( pCassandraColumnName.equals(StringSerializer.get().fromByteBuffer(cColumn.getName())) )
return cColumn;
return null;
}
import org.apache.gora.cassandra.store.CassandraStore;
} else if (type == Type.UNION){
Schema unionFieldSchema = getUnionSchema(super.getUnionType(), field.schema());
value = fromByteBuffer(unionFieldSchema, byteBuffer);
private Schema getUnionSchema (int pSchemaPos, Schema pSchema){
Schema unionSchema = pSchema.getTypes().get(pSchemaPos);
if ( unionSchema == null )
pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
return unionSchema;
}
} else if (type == Type.UNION){
serializer = ByteBufferSerializer.get();
private String getMappingFamily(String pField){
String family = null;
family = this.cassandraMapping.getFamily(pField);
return family;
}
private String getMappingColumn(String pField){
String column = null;
column = this.cassandraMapping.getColumn(pField);
return column;
}
String family = this.getMappingFamily(field);
String column = this.getMappingColumn(field);
public CassandraMapping getCassandraMapping(){
return this.cassandraMapping;
}
String family = this.getMappingFamily(field);
String column = this.getMappingColumn(field);
public void addColumn(String pFamilyName, String pFieldName, String pColumnName){
this.familyMap.put(pFieldName, pFamilyName);
this.columnMap.put(pFieldName, pColumnName);
}
import java.io.InputStream;
InputStream inputStream = getClass().getClassLoader().getResourceAsStream(MAPPING_FILE);
if (inputStream == null){
}
Document document = saxBuilder.build(inputStream);
public static int DEFAULT_UNION_SCHEMA = 0;
CassandraResultSet<K> cassandraResultSet = new CassandraResultSet<K>();
case UNION:
break;
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
for (Field member: schema.getFields()) {
Object memberValue = persistentBase.get(member.pos());
if (memberValue instanceof GenericArray<?>) {
if (((GenericArray)memberValue).size() == 0) {
continue;
}
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
case UNION:
if(value != null) {
this.cassandraClient.addColumn(key, field.name(), value);
} else {
}
private int getUnionSchema(Object pValue, Schema pUnionSchema){
int unionSchemaPos = 0;
String valueType = pValue.getClass().getSimpleName();
Iterator<Schema> it = pUnionSchema.getTypes().iterator();
while ( it.hasNext() ){
String schemaName = it.next().getName();
if (valueType.equals("Utf8") && schemaName.equals(Type.STRING.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("HeapByteBuffer") && schemaName.equals(Type.STRING.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Integer") && schemaName.equals(Type.INT.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Long") && schemaName.equals(Type.LONG.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Double") && schemaName.equals(Type.DOUBLE.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Float") && schemaName.equals(Type.FLOAT.name().toLowerCase()))
return unionSchemaPos;
else if (valueType.equals("Boolean") && schemaName.equals(Type.BOOLEAN.name().toLowerCase()))
return unionSchemaPos;
}
return 0;
}
null,
if (CONTENTS[i]!=null){
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
for(String token : CONTENTS[i].split(" ")) {
page.addToParsedContent(new Utf8(token));
}
}
import org.apache.avro.specific.FixedSize;
public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"dateOfBirth\",\"type\":\"long\"},{\"name\":\"ssn\",\"type\":\"string\"},{\"name\":\"salary\",\"type\":\"int\"},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"]},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}]}]}");
BOSS(4,"boss"),
WEBPAGE(5,"webpage"),
public static final String[] _ALL_FIELDS = {"name","dateOfBirth","ssn","salary","boss","webpage",};
private Object boss;
private WebPage webpage;
case 4: return boss;
case 5: return webpage;
case 4:boss = (Object)_value; break;
case 5:webpage = (WebPage)_value; break;
public Object getBoss() {
return (Object) get(4);
}
public void setBoss(Employee value) {
put(4, value);
}
public void setBoss(Utf8 value) {
put(4, value);
}
public WebPage getWebpage() {
return (WebPage) get(5);
}
public void setWebpage(WebPage value) {
put(5, value);
}
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.FixedSize;
public static final Schema _SCHEMA = Schema.parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":\"string\"},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"]},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\"},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"}}]}}]}");
if (page.getContent() != null) {
String content = new String(page.getContent().array());
StringTokenizer itr = new StringTokenizer(content);
while (itr.hasMoreTokens()) {
word.set(itr.nextToken());
context.write(word, one);
}
break;
case UNION:
fieldType = type(fieldSchema);
line(1, "}");
for (Schema s : fieldSchema.getTypes()) {
if (s.getType().equals(Schema.Type.NULL)) continue ;
String unionFieldType = type(s);
line(1, "}");
}
break;
case NULL:
default:
import org.apache.avro.generic.GenericData;
boolean isUnionField = false ;
int unionIndex = -1 ;
if (field.schema().getType() == Type.UNION) {
isUnionField = true ;
unionIndex = GenericData.get().resolveUnion(field.schema(), o);
}
if(field.schema().getType() == Type.BYTES
|| (isUnionField
&& field.schema().getTypes().get(unionIndex).getType() == Type.BYTES)) {
return newInstance(result, fields);
delete.deleteColumn(hcol.getFamily(), qual);
hasDeletes = true;
} else {
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
}
byte[] val = toBytes(entry.getValue(), field.schema().getValueType());
delete.deleteColumn(hcol.getFamily(), qual);
hasDeletes = true;
} else {
put.add(hcol.getFamily(), qual, val);
hasPuts = true;
}
byte[] val = toBytes(item, field.schema().getElementType());
hasDeletes = true;
} else {
hasPuts = true;
}
byte[] serializedBytes = toBytes(o, field.schema()) ;
delete.deleteColumn(hcol.getFamily(), hcol.getQualifier());
hasDeletes = true;
} else {
put.add(hcol.getFamily(), hcol.getQualifier(), serializedBytes);
hasPuts = true;
}
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
import org.apache.avro.generic.GenericData;
import org.apache.gora.avro.PersistentDatumReader;
import org.apache.gora.avro.PersistentDatumWriter;
case UNION:
if (schema.getTypes().size() == 2) {
Type type0 = schema.getTypes().get(0).getType() ;
Type type1 = schema.getTypes().get(1).getType() ;
if (!type0.equals(type1)
&& (   type0.equals(Schema.Type.NULL)
|| type1.equals(Schema.Type.NULL))) {
if (type0.equals(Schema.Type.NULL))
schema = schema.getTypes().get(1) ;
else
schema = schema.getTypes().get(0) ;
}
}
PersistentDatumReader<?> reader = null ;
if (schema.getType().equals(Schema.Type.UNION)) {
reader = (PersistentDatumReader<?>)readerMap.get(String.valueOf(schema.hashCode()));
if (reader == null) {
readerMap.put(String.valueOf(schema.hashCode()), reader);
}
} else {
reader = (PersistentDatumReader<?>)readerMap.get(schema.getFullName());
if (reader == null) {
readerMap.put(schema.getFullName(), reader);
}
return reader.read((Object)null, schema, decoder);
case UNION:
if (schema.getTypes().size() == 2) {
Type type0 = schema.getTypes().get(0).getType() ;
Type type1 = schema.getTypes().get(1).getType() ;
if (!type0.equals(type1)
&& (   type0.equals(Schema.Type.NULL)
|| type1.equals(Schema.Type.NULL))) {
if (o == null) return null ;
int index = GenericData.get().resolveUnion(schema, o);
schema = schema.getTypes().get(index) ;
}
}
PersistentDatumWriter writer = null ;
if (schema.getType().equals(Schema.Type.UNION)) {
writer = (PersistentDatumWriter<?>) writerMap.get(String.valueOf(schema.hashCode()));
if (writer == null) {
writerMap.put(String.valueOf(schema.hashCode()),writer);
}
} else {
writer = (PersistentDatumWriter<?>) writerMap.get(schema.getFullName());
if (writer == null) {
writerMap.put(schema.getFullName(),writer);
}
writer.write(schema,o, encoder);
import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
import java.nio.ByteBuffer;
import java.util.Map;
import java.util.HashMap;
import org.apache.avro.Protocol;
import org.apache.avro.AvroRuntimeException;
import org.apache.avro.Protocol;
import org.apache.avro.ipc.AvroRemoteException;
import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.FixedSize;
import org.apache.avro.specific.SpecificExceptionBase;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificFixed;
import org.apache.gora.persistency.StatefulHashMap;
import org.apache.gora.persistency.ListGenericArray;
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
"\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;
"\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;
if (query.getFields() != null) {
}
if (schemaExists()) {
return;
}
MongoDBQuery<K, T> query = new MongoDBQuery<K, T>(this);
query.setFields(getFieldsToQuery(null));
return query;
case UNION:
if (fieldSchema.getTypes().size() == 2) {
Type type0 = fieldSchema.getTypes().get(0).getType() ;
Type type1 = fieldSchema.getTypes().get(1).getType() ;
if (!type0.equals(type1)
&& (   type0.equals(Schema.Type.NULL)
|| type1.equals(Schema.Type.NULL))) {
}
}
break;
.getValueType().getType()));
break;
import org.apache.gora.mongodb.store.MongoMapping;
public static DBObject toProjection(Query<?, ?> query, MongoMapping mapping) {
for (String k : query.getFields()) {
proj.put(mapping.getDocumentField(k), true);
}
DBObject p = MongoDBQuery.toProjection(query, mapping);
long writeCapacUnits = Long.parseLong(tableElement.getAttributeValue("writecunit"));
long writeCapacUnits = Long.parseLong(tableElement.getAttributeValue("writecunit"));
}
}
public class SolrStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
private static final Logger LOG = LoggerFactory.getLogger( SolrStore.class );
protected static final String DEFAULT_MAPPING_FILE = "gora-solr-mapping.xml";
protected static final String SOLR_URL_PROPERTY = "solr.url";
protected static final String SOLR_CONFIG_PROPERTY = "solr.config";
protected static final String SOLR_SCHEMA_PROPERTY = "solr.schema";
protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batchSize";
protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commitWithin";
protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.resultsSize";
protected static final int DEFAULT_BATCH_SIZE = 100;
protected static final int DEFAULT_COMMIT_WITHIN = 1000;
protected static final int DEFAULT_RESULTS_SIZE = 100;
private SolrMapping mapping;
private String solrServerUrl, solrConfig, solrSchema;
private SolrServer server, adminServer;
private ArrayList<SolrInputDocument> batch;
private int batchSize = DEFAULT_BATCH_SIZE;
private int commitWithin = DEFAULT_COMMIT_WITHIN;
private int resultsSize = DEFAULT_RESULTS_SIZE;
@Override
public void initialize( Class<K> keyClass, Class<T> persistentClass, Properties properties ) {
super.initialize( keyClass, persistentClass, properties );
String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
try {
mapping = readMapping( mappingFile );
}
catch ( IOException e ) {
LOG.error( e.getMessage() );
LOG.error( e.getStackTrace().toString() );
solrServerUrl = DataStoreFactory.findProperty( properties, this, SOLR_URL_PROPERTY, null );
solrConfig = DataStoreFactory.findProperty( properties, this, SOLR_CONFIG_PROPERTY, null );
solrSchema = DataStoreFactory.findProperty( properties, this, SOLR_SCHEMA_PROPERTY, null );
adminServer = new HttpSolrServer( solrServerUrl );
if ( autoCreateSchema ) {
createSchema();
}
String batchSizeString = DataStoreFactory.findProperty( properties, this, SOLR_BATCH_SIZE_PROPERTY, null );
if ( batchSizeString != null ) {
try {
batchSize = Integer.parseInt( batchSizeString );
} catch ( NumberFormatException nfe ) {
}
}
batch = new ArrayList<SolrInputDocument>( batchSize );
String commitWithinString = DataStoreFactory.findProperty( properties, this, SOLR_COMMIT_WITHIN_PROPERTY, null );
if ( commitWithinString != null ) {
try {
commitWithin = Integer.parseInt( commitWithinString );
} catch ( NumberFormatException nfe ) {
}
}
String resultsSizeString = DataStoreFactory.findProperty( properties, this, SOLR_RESULTS_SIZE_PROPERTY, null );
if ( resultsSizeString != null ) {
try {
resultsSize = Integer.parseInt( resultsSizeString );
} catch ( NumberFormatException nfe ) {
}
}
}
@SuppressWarnings("unchecked")
private SolrMapping readMapping( String filename ) throws IOException {
SolrMapping map = new SolrMapping();
try {
SAXBuilder builder = new SAXBuilder();
Document doc = builder.build( getClass().getClassLoader().getResourceAsStream( filename ) );
List<Element> classes = doc.getRootElement().getChildren( "class" );
for ( Element classElement : classes ) {
if ( classElement.getAttributeValue( "keyClass" ).equals( keyClass.getCanonicalName() )
&& classElement.getAttributeValue( "name" ).equals( persistentClass.getCanonicalName() ) ) {
String tableName = getSchemaName( classElement.getAttributeValue( "table" ), persistentClass );
map.setCoreName( tableName );
Element primaryKeyEl = classElement.getChild( "primarykey" );
map.setPrimaryKey( primaryKeyEl.getAttributeValue( "column" ) );
List<Element> fields = classElement.getChildren( "field" );
for ( Element field : fields ) {
String fieldName = field.getAttributeValue( "name" );
String columnName = field.getAttributeValue( "column" );
map.addField( fieldName, columnName );
}
break;
}
} catch ( Exception ex ) {
throw new IOException( ex );
return map;
}
public SolrMapping getMapping() {
return mapping;
}
@Override
public String getSchemaName() {
return mapping.getCoreName();
}
@Override
public void createSchema() {
try {
if ( !schemaExists() )
CoreAdminRequest.createCore( mapping.getCoreName(), mapping.getCoreName(), adminServer, solrConfig,
solrSchema );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
@Override
public void truncateSchema() {
try {
server.deleteByQuery( "*:*" );
server.commit();
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
@Override
public void deleteSchema() {
try {
server.deleteByQuery( "*:*" );
server.commit();
} catch ( Exception e ) {
try {
CoreAdminRequest.unloadCore( mapping.getCoreName(), adminServer );
} catch ( Exception e ) {
if ( e.getMessage().contains( "No such core" ) ) {
} else {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
}
@Override
public boolean schemaExists() {
boolean exists = false;
try {
CoreAdminResponse rsp = CoreAdminRequest.getStatus( mapping.getCoreName(), adminServer );
exists = rsp.getUptime( mapping.getCoreName() ) != null;
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return exists;
}
private static final String toDelimitedString( String[] arr, String sep ) {
if ( arr == null || arr.length == 0 ) {
return "";
}
StringBuilder sb = new StringBuilder();
if ( i > 0 )
sb.append( sep );
sb.append( arr[i] );
}
return sb.toString();
}
public static String escapeQueryKey( String key ) {
if ( key == null ) {
return null;
}
StringBuilder sb = new StringBuilder();
char c = key.charAt( i );
switch ( c ) {
case ':':
case '*':
break;
default:
sb.append( c );
}
}
return sb.toString();
}
@Override
public T get( K key, String[] fields ) {
ModifiableSolrParams params = new ModifiableSolrParams();
params.set( CommonParams.QT, "/get" );
params.set( CommonParams.FL, toDelimitedString( fields, "," ) );
params.set( "id",  key.toString() );
try {
QueryResponse rsp = server.query( params );
Object o = rsp.getResponse().get( "doc" );
if ( o == null ) {
}
return newInstance( (SolrDocument)o, fields );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return null;
}
public T newInstance( SolrDocument doc, String[] fields )
throws IOException {
T persistent = newPersistent();
if ( fields == null ) {
fields = fieldMap.keySet().toArray( new String[fieldMap.size()] );
String pk = mapping.getPrimaryKey();
for ( String f : fields ) {
Field field = fieldMap.get( f );
Schema fieldSchema = field.schema();
String sf = null;
if ( pk.equals( f ) ) {
sf = f;
} else {
sf = mapping.getSolrField( f );
}
Object sv = doc.get( sf );
Object v;
if ( sv == null ) {
continue;
}
switch ( fieldSchema.getType() ) {
case MAP:
case ARRAY:
case RECORD:
v = IOUtils.deserialize( (byte[]) sv, datumReader, fieldSchema, persistent.get( field.pos() ) );
persistent.put( field.pos(), v );
break;
case ENUM:
v = AvroUtils.getEnumValue( fieldSchema, (String) sv );
persistent.put( field.pos(), v );
break;
case FIXED:
throw new IOException( "???" );
case BYTES:
persistent.put( field.pos(), ByteBuffer.wrap( (byte[]) sv ) );
break;
case BOOLEAN:
case DOUBLE:
case FLOAT:
case INT:
case LONG:
persistent.put( field.pos(), sv );
break;
case STRING:
persistent.put( field.pos(), new Utf8( sv.toString() ) );
break;
case UNION:
LOG.error( "Union is not supported yet" );
break;
default:
}
persistent.setDirty( field.pos() );
persistent.clearDirty();
return persistent;
}
@Override
public void put( K key, T persistent ) {
Schema schema = persistent.getSchema();
StateManager stateManager = persistent.getStateManager();
if ( !stateManager.isDirty( persistent ) ) {
return;
}
SolrInputDocument doc = new SolrInputDocument();
doc.addField( mapping.getPrimaryKey(), key );
List<Field> fields = schema.getFields();
for ( Field field : fields ) {
String sf = mapping.getSolrField( field.name() );
if ( sf == null ) {
continue;
}
Schema fieldSchema = field.schema();
Object v = persistent.get( field.pos() );
if ( v == null ) {
continue;
}
switch ( fieldSchema.getType() ) {
case MAP:
case ARRAY:
case RECORD:
byte[] data = null;
try {
data = IOUtils.serialize( datumWriter, fieldSchema, v );
} catch ( IOException e ) {
}
doc.addField( sf, data );
break;
case BYTES:
doc.addField( sf, ( (ByteBuffer) v ).array() );
break;
case ENUM:
case STRING:
doc.addField( sf, v.toString() );
break;
case BOOLEAN:
case DOUBLE:
case FLOAT:
case INT:
case LONG:
doc.addField( sf, v );
break;
case UNION:
LOG.error( "Union is not supported yet" );
break;
default:
}
batch.add( doc );
if ( batch.size() >= batchSize ) {
try {
add( batch, commitWithin );
batch.clear();
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
}
@Override
public boolean delete( K key ) {
String keyField = mapping.getPrimaryKey();
try {
server.commit();
LOG.info( rsp.toString() );
return true;
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return false;
}
@Override
public long deleteByQuery( Query<K, T> query ) {
String q = ( (SolrQuery<K, T>) query ).toSolrQuery();
try {
UpdateResponse rsp = server.deleteByQuery( q );
server.commit();
LOG.info( rsp.toString() );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return 0;
}
@Override
public Result<K, T> execute( Query<K, T> query ) {
try {
return new SolrResult<K, T>( this, query, server, resultsSize );
} catch ( IOException e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return null;
}
@Override
public Query<K, T> newQuery() {
return new SolrQuery<K, T>( this );
}
@Override
public List<PartitionQuery<K, T>> getPartitions( Query<K, T> query )
throws IOException {
ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<PartitionQuery<K, T>>();
partitions.add( new PartitionQueryImpl<K, T>( query ) );
return partitions;
}
@Override
public void flush() {
try {
if ( batch.size() > 0 ) {
add( batch, commitWithin );
batch.clear();
}
} catch ( Exception e ) {
LOG.error(e.getMessage(), e.getStackTrace());
}
@Override
public void close() {
}
private void add(ArrayList<SolrInputDocument> batch, int commitWithin) throws SolrServerException, IOException {
if (commitWithin == 0) {
server.add( batch );
server.commit( false, true, true );
} else {
server.add( batch, commitWithin );
}
public class SolrStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
private static final Logger LOG = LoggerFactory.getLogger( SolrStore.class );
protected static final String DEFAULT_MAPPING_FILE = "gora-solr-mapping.xml";
protected static final String SOLR_URL_PROPERTY = "solr.url";
protected static final String SOLR_CONFIG_PROPERTY = "solr.config";
protected static final String SOLR_SCHEMA_PROPERTY = "solr.schema";
protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batchSize";
protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commitWithin";
protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.resultsSize";
protected static final int DEFAULT_BATCH_SIZE = 100;
protected static final int DEFAULT_COMMIT_WITHIN = 1000;
protected static final int DEFAULT_RESULTS_SIZE = 100;
private SolrMapping mapping;
private String solrServerUrl, solrConfig, solrSchema;
private SolrServer server, adminServer;
private ArrayList<SolrInputDocument> batch;
private int batchSize = DEFAULT_BATCH_SIZE;
private int commitWithin = DEFAULT_COMMIT_WITHIN;
private int resultsSize = DEFAULT_RESULTS_SIZE;
@Override
public void initialize( Class<K> keyClass, Class<T> persistentClass, Properties properties ) {
super.initialize( keyClass, persistentClass, properties );
String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
try {
mapping = readMapping( mappingFile );
}
catch ( IOException e ) {
LOG.error( e.getMessage() );
LOG.error( e.getStackTrace().toString() );
solrServerUrl = DataStoreFactory.findProperty( properties, this, SOLR_URL_PROPERTY, null );
solrConfig = DataStoreFactory.findProperty( properties, this, SOLR_CONFIG_PROPERTY, null );
solrSchema = DataStoreFactory.findProperty( properties, this, SOLR_SCHEMA_PROPERTY, null );
adminServer = new HttpSolrServer( solrServerUrl );
if ( autoCreateSchema ) {
createSchema();
}
String batchSizeString = DataStoreFactory.findProperty( properties, this, SOLR_BATCH_SIZE_PROPERTY, null );
if ( batchSizeString != null ) {
try {
batchSize = Integer.parseInt( batchSizeString );
} catch ( NumberFormatException nfe ) {
}
}
batch = new ArrayList<SolrInputDocument>( batchSize );
String commitWithinString = DataStoreFactory.findProperty( properties, this, SOLR_COMMIT_WITHIN_PROPERTY, null );
if ( commitWithinString != null ) {
try {
commitWithin = Integer.parseInt( commitWithinString );
} catch ( NumberFormatException nfe ) {
}
}
String resultsSizeString = DataStoreFactory.findProperty( properties, this, SOLR_RESULTS_SIZE_PROPERTY, null );
if ( resultsSizeString != null ) {
try {
resultsSize = Integer.parseInt( resultsSizeString );
} catch ( NumberFormatException nfe ) {
}
}
}
@SuppressWarnings("unchecked")
private SolrMapping readMapping( String filename ) throws IOException {
SolrMapping map = new SolrMapping();
try {
SAXBuilder builder = new SAXBuilder();
Document doc = builder.build( getClass().getClassLoader().getResourceAsStream( filename ) );
List<Element> classes = doc.getRootElement().getChildren( "class" );
for ( Element classElement : classes ) {
if ( classElement.getAttributeValue( "keyClass" ).equals( keyClass.getCanonicalName() )
&& classElement.getAttributeValue( "name" ).equals( persistentClass.getCanonicalName() ) ) {
String tableName = getSchemaName( classElement.getAttributeValue( "table" ), persistentClass );
map.setCoreName( tableName );
Element primaryKeyEl = classElement.getChild( "primarykey" );
map.setPrimaryKey( primaryKeyEl.getAttributeValue( "column" ) );
List<Element> fields = classElement.getChildren( "field" );
for ( Element field : fields ) {
String fieldName = field.getAttributeValue( "name" );
String columnName = field.getAttributeValue( "column" );
map.addField( fieldName, columnName );
}
break;
}
} catch ( Exception ex ) {
throw new IOException( ex );
return map;
}
public SolrMapping getMapping() {
return mapping;
}
@Override
public String getSchemaName() {
return mapping.getCoreName();
}
@Override
public void createSchema() {
try {
if ( !schemaExists() )
CoreAdminRequest.createCore( mapping.getCoreName(), mapping.getCoreName(), adminServer, solrConfig,
solrSchema );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
@Override
public void truncateSchema() {
try {
server.deleteByQuery( "*:*" );
server.commit();
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
@Override
public void deleteSchema() {
try {
server.deleteByQuery( "*:*" );
server.commit();
} catch ( Exception e ) {
try {
CoreAdminRequest.unloadCore( mapping.getCoreName(), adminServer );
} catch ( Exception e ) {
if ( e.getMessage().contains( "No such core" ) ) {
} else {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
}
@Override
public boolean schemaExists() {
boolean exists = false;
try {
CoreAdminResponse rsp = CoreAdminRequest.getStatus( mapping.getCoreName(), adminServer );
exists = rsp.getUptime( mapping.getCoreName() ) != null;
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return exists;
}
private static final String toDelimitedString( String[] arr, String sep ) {
if ( arr == null || arr.length == 0 ) {
return "";
}
StringBuilder sb = new StringBuilder();
if ( i > 0 )
sb.append( sep );
sb.append( arr[i] );
}
return sb.toString();
}
public static String escapeQueryKey( String key ) {
if ( key == null ) {
return null;
}
StringBuilder sb = new StringBuilder();
char c = key.charAt( i );
switch ( c ) {
case ':':
case '*':
break;
default:
sb.append( c );
}
}
return sb.toString();
}
@Override
public T get( K key, String[] fields ) {
ModifiableSolrParams params = new ModifiableSolrParams();
params.set( CommonParams.QT, "/get" );
params.set( CommonParams.FL, toDelimitedString( fields, "," ) );
params.set( "id",  key.toString() );
try {
QueryResponse rsp = server.query( params );
Object o = rsp.getResponse().get( "doc" );
if ( o == null ) {
}
return newInstance( (SolrDocument)o, fields );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return null;
}
public T newInstance( SolrDocument doc, String[] fields )
throws IOException {
T persistent = newPersistent();
if ( fields == null ) {
fields = fieldMap.keySet().toArray( new String[fieldMap.size()] );
String pk = mapping.getPrimaryKey();
for ( String f : fields ) {
Field field = fieldMap.get( f );
Schema fieldSchema = field.schema();
String sf = null;
if ( pk.equals( f ) ) {
sf = f;
} else {
sf = mapping.getSolrField( f );
}
Object sv = doc.get( sf );
Object v;
if ( sv == null ) {
continue;
}
switch ( fieldSchema.getType() ) {
case MAP:
case ARRAY:
case RECORD:
v = IOUtils.deserialize( (byte[]) sv, datumReader, fieldSchema, persistent.get( field.pos() ) );
persistent.put( field.pos(), v );
break;
case ENUM:
v = AvroUtils.getEnumValue( fieldSchema, (String) sv );
persistent.put( field.pos(), v );
break;
case FIXED:
throw new IOException( "???" );
case BYTES:
persistent.put( field.pos(), ByteBuffer.wrap( (byte[]) sv ) );
break;
case BOOLEAN:
case DOUBLE:
case FLOAT:
case INT:
case LONG:
persistent.put( field.pos(), sv );
break;
case STRING:
persistent.put( field.pos(), new Utf8( sv.toString() ) );
break;
case UNION:
LOG.error( "Union is not supported yet" );
break;
default:
}
persistent.setDirty( field.pos() );
persistent.clearDirty();
return persistent;
}
@Override
public void put( K key, T persistent ) {
Schema schema = persistent.getSchema();
StateManager stateManager = persistent.getStateManager();
if ( !stateManager.isDirty( persistent ) ) {
return;
}
SolrInputDocument doc = new SolrInputDocument();
doc.addField( mapping.getPrimaryKey(), key );
List<Field> fields = schema.getFields();
for ( Field field : fields ) {
String sf = mapping.getSolrField( field.name() );
if ( sf == null ) {
continue;
}
Schema fieldSchema = field.schema();
Object v = persistent.get( field.pos() );
if ( v == null ) {
continue;
}
switch ( fieldSchema.getType() ) {
case MAP:
case ARRAY:
case RECORD:
byte[] data = null;
try {
data = IOUtils.serialize( datumWriter, fieldSchema, v );
} catch ( IOException e ) {
}
doc.addField( sf, data );
break;
case BYTES:
doc.addField( sf, ( (ByteBuffer) v ).array() );
break;
case ENUM:
case STRING:
doc.addField( sf, v.toString() );
break;
case BOOLEAN:
case DOUBLE:
case FLOAT:
case INT:
case LONG:
doc.addField( sf, v );
break;
case UNION:
LOG.error( "Union is not supported yet" );
break;
default:
}
batch.add( doc );
if ( batch.size() >= batchSize ) {
try {
add( batch, commitWithin );
batch.clear();
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
}
@Override
public boolean delete( K key ) {
String keyField = mapping.getPrimaryKey();
try {
server.commit();
LOG.info( rsp.toString() );
return true;
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return false;
}
@Override
public long deleteByQuery( Query<K, T> query ) {
String q = ( (SolrQuery<K, T>) query ).toSolrQuery();
try {
UpdateResponse rsp = server.deleteByQuery( q );
server.commit();
LOG.info( rsp.toString() );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return 0;
}
@Override
public Result<K, T> execute( Query<K, T> query ) {
try {
return new SolrResult<K, T>( this, query, server, resultsSize );
} catch ( IOException e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return null;
}
@Override
public Query<K, T> newQuery() {
return new SolrQuery<K, T>( this );
}
@Override
public List<PartitionQuery<K, T>> getPartitions( Query<K, T> query )
throws IOException {
ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<PartitionQuery<K, T>>();
partitions.add( new PartitionQueryImpl<K, T>( query ) );
return partitions;
}
@Override
public void flush() {
try {
if ( batch.size() > 0 ) {
add( batch, commitWithin );
batch.clear();
}
} catch ( Exception e ) {
LOG.error(e.getMessage(), e.getStackTrace());
}
@Override
public void close() {
}
private void add(ArrayList<SolrInputDocument> batch, int commitWithin) throws SolrServerException, IOException {
if (commitWithin == 0) {
server.add( batch );
server.commit( false, true, true );
} else {
server.add( batch, commitWithin );
}
public static DBObject toProjection(String[] fields, MongoMapping mapping) {
for (String k : fields) {
import org.apache.gora.persistency.impl.BeanFactoryImpl;
import org.apache.gora.util.ClassLoadingUtils;
String[] fields = getFieldsToQuery(query.getFields());
DBObject p = MongoDBQuery.toProjection(fields, mapping);
BSONDecorator easybson = new BSONDecorator(obj);
fields = getFieldsToQuery(fields);
Object result = fromDBObject(fieldSchema, storeType, field, docf, easybson);
persistent.put(field.pos(), result);
private Object fromDBObject(final Schema fieldSchema, final DocumentFieldType storeType, final Field field, final String docf, final BSONDecorator easybson) {
Object result = null;
switch (fieldSchema.getType()) {
case MAP:
BasicDBObject map = easybson.getDBObject(docf);
StatefulHashMap<Utf8, Object> rmap = new StatefulHashMap<Utf8, Object>();
for (Entry<String, Object> e : map.entrySet()) {
String oKey = e.getKey().replace("\u00B7", ".");
switch (fieldSchema.getValueType().getType()) {
case STRING:
rmap.put(new Utf8(oKey), new Utf8((String) e.getValue()));
break;
case BYTES:
rmap.put(new Utf8(oKey), ByteBuffer.wrap((byte[]) e.getValue()));
break;
default:
rmap.put(new Utf8(oKey), e.getValue());
break;
}
}
rmap.clearStates();
result = rmap;
break;
case ARRAY:
List<Object> list = easybson.getDBList(docf);
switch (fieldSchema.getElementType().getType()) {
case STRING:
ListGenericArray<Utf8> arrS = new ListGenericArray<Utf8>(fieldSchema);
for (Object o : list)
arrS.add(new Utf8((String) o));
result = arrS;
break;
case BYTES:
ListGenericArray<ByteBuffer> arrB = new ListGenericArray<ByteBuffer>(
fieldSchema);
for (Object o : list)
arrB.add(ByteBuffer.wrap((byte[]) o));
result = arrB;
break;
default:
ListGenericArray<Object> arrT = new ListGenericArray<Object>(
fieldSchema);
for (Object o : list)
arrT.add(o);
result = arrT;
break;
}
break;
case RECORD:
DBObject rec = easybson.getDBObject(docf);
BSONDecorator innerBson = new BSONDecorator(rec);
Class<?> clazz = null;
try {
clazz = ClassLoadingUtils.loadClass(fieldSchema.getFullName());
} catch (ClassNotFoundException e) {
}
Persistent record = new BeanFactoryImpl(keyClass, clazz).newPersistent();
for (Field recField : fieldSchema.getFields()) {
Schema innerSchema = recField.schema();
DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerSchema.getName());
String recDocField = mapping.getDocumentField(recField.name()) != null ? mapping.getDocumentField(recField.name()) : recField.name();
((PersistentBase) record).put(recField.pos(), fromDBObject(innerSchema, innerStoreType, recField, recDocField, innerBson));
}
result = record;
break;
case BOOLEAN:
result = easybson.getBoolean(docf);
break;
case DOUBLE:
result = easybson.getDouble(docf);
break;
case FLOAT:
result = easybson.getDouble(docf).floatValue();
break;
case INT:
result = easybson.getInt(docf);
break;
case LONG:
result = easybson.getLong(docf);
break;
case STRING:
if (storeType == DocumentFieldType.OBJECTID) {
final Object bin = easybson.get(docf);
final ObjectId id = ObjectId.massageToObjectId(bin);
result = new Utf8(id.toString());
} else if (storeType == DocumentFieldType.DATE) {
final Object bin = easybson.get(docf);
if (bin instanceof Date) {
Calendar calendar = Calendar.getInstance(TimeZone
.getTimeZone("UTC"));
calendar.setTime((Date) bin);
result = new Utf8(DatatypeConverter.printDateTime(calendar));
} else {
result = new Utf8(bin.toString());
}
} else {
result = easybson.getUtf8String(docf);
}
break;
case ENUM:
result = AvroUtils.getEnumValue(fieldSchema,
easybson.getUtf8String(docf).toString());
break;
case BYTES:
case FIXED:
result = easybson.getBytes(docf);
break;
case NULL:
result = null;
break;
case UNION:
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Type.NULL)
|| type1.equals(Type.NULL))) {
Schema innerSchema = fieldSchema.getTypes().get(1);
DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerSchema.getName());
}
break;
default:
LOG.warn("Unable to read {}", docf);
break;
}
return result;
}
DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
result.put(docf, toDBObject(f.schema(), f.schema().getType(), storeType, value));
DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
Object o = toDBObject(f.schema(), f.schema().getType(), storeType, value);
result.put(docf, o);
private Object toDBObject(Schema fieldSchema, Type fieldType, DocumentFieldType storeType, Object value) {
Object result = null;
switch (fieldType) {
case MAP:
if (storeType != null && storeType != DocumentFieldType.DOCUMENT) {
throw new IllegalStateException(
"Field "
fieldSchema.getType()
": to store a Gora 'map', target Mongo mapping have to be of 'document' type");
}
Schema valueSchema = fieldSchema.getValueType();
result = toMongoMap((Map<Utf8, ?>) value, valueSchema.getType());
break;
case ARRAY:
if (storeType != null && storeType != DocumentFieldType.LIST) {
throw new IllegalStateException(
"Field "
fieldSchema.getType()
": To store a Gora 'array', target Mongo mapping have to be of 'list' type");
}
Schema elementSchema = fieldSchema.getElementType();
result = toMongoList((GenericArray<?>) value, elementSchema.getType());
break;
case BYTES:
if (value != null) {
result = ((ByteBuffer) value).array();
}
break;
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case BOOLEAN:
result = value;
break;
case STRING:
if (storeType == DocumentFieldType.OBJECTID) {
if (value != null) {
ObjectId id;
try {
id = new ObjectId(value.toString());
} catch (IllegalArgumentException e1) {
": Invalid string: unable to convert to ObjectId");
}
result = id;
}
} else if (storeType == DocumentFieldType.DATE) {
if (value != null) {
Calendar calendar = null;
try {
calendar = DatatypeConverter.parseDateTime(value.toString());
} catch (IllegalArgumentException e1) {
try {
calendar = DatatypeConverter.parseDate(value.toString());
} catch (IllegalArgumentException e2) {
}
}
if (calendar == null) {
}
result = calendar.getTime();
}
} else {
if (value != null) {
result = value.toString();
}
}
break;
case ENUM:
if (value != null)
result = value.toString();
break;
case RECORD:
if (value == null)
break;
BasicDBObject record = new BasicDBObject();
for (Field member : fieldSchema.getFields()) {
Object innerValue = ((PersistentBase) value).get(member.pos());
String innerDoc = mapping.getDocumentField(member.name());
Type innerType = member.schema().getType();
DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerDoc);
record.put(member.name(), toDBObject(member.schema(), innerType, innerStoreType, innerValue));
}
result = record;
break;
case UNION:
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Schema.Type.NULL)
|| type1.equals(Schema.Type.NULL))) {
Schema innerSchema = fieldSchema.getTypes().get(1);
DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerSchema.getName());
}
break;
case FIXED:
result = value;
break;
default:
break;
}
return result;
}
case LONG:
case INT:
easybson.put(key, value);
break;
case LONG:
case INT:
easybson.put(key, value);
break;
LOG.error("A record in a record! Seriously? Fuck it, it's not supported yet.");
break;
case UNION:
LOG.error("Union is not supported");
break;
case UNION:
LOG.error("Union is not supported");
break;
LOG.debug("Load from DBObject (MAIN), field:{}, schemaType:{}, docField:{}, storeType:{}", new Object[]{field.name(), fieldSchema.getType(), docf, storeType});
if (rec == null) {
return result;
}
String innerDocField = mapping.getDocumentField(recField.name()) != null ? mapping.getDocumentField(recField.name()) : recField.name();
LOG.debug("Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}", new Object[]{recField.name(), innerSchema.getType(), fieldPath, innerStoreType});
((PersistentBase) record).put(recField.pos(), fromDBObject(innerSchema, innerStoreType, recField, innerDocField, innerBson));
LOG.debug("Load from DBObject (UNION), schemaType:{}, docField:{}, storeType:{}", new Object[]{innerSchema.getType(), docf, innerStoreType});
LOG.debug("Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}", new Object[]{docf, f.schema().getType(), storeType});
LOG.debug("Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}", new Object[]{docf, f.schema().getType(), storeType});
LOG.debug("Transform value to DBObject (RECORD), docField:{}, schemaType:{}, storeType:{}", new Object[]{member.name(), member.schema().getType(), innerStoreType});
LOG.debug("Transform value to DBObject (UNION), schemaType:{}, type1:{}, storeType:{}", new Object[]{innerSchema.getType(), type1, innerStoreType});
PartitionQueryImpl<K, T> partitionQuery = new PartitionQueryImpl<K, T>(query);
partitionQuery.setConf(getConf());
partitions.add(partitionQuery);
/**
*/
private void addJavaDoc(int indent, String javadoc) throws IOException {
if (javadoc==null)
return;
if (indent<0)
return;
line(indent, "");
line(indent, "/**");
if (javadoc.contains("\n")){
String javadocLines[] = javadoc.split("\n");
for(String line : javadocLines)
}
else
line(indent, " */");
}
addJavaDoc(0,schema.getDoc());
line(0, "@SuppressWarnings(\"all\")");
addJavaDoc(1,"Variable holding the data bean schema.");
addJavaDoc(1,"Enum containing all data bean's fields.");
addJavaDoc(2,"Field's index.");
addJavaDoc(2,"Field's name.");
addJavaDoc(2,"Field's constructor\n"
"@param index field's index.\n"
"@param name field's name.");
addJavaDoc(2,"Gets field's index.\n"
"@return int field's index.");
addJavaDoc(2,"Gets field's name.\n"
"@return String field's name.");
addJavaDoc(2,"Gets field's attributes to string.\n"
"@return String field's attributes to string.");
addJavaDoc(2,"Contains all field's names.");
addJavaDoc(1,field.doc());
addJavaDoc(1,"Default Constructor");
addJavaDoc(1,"Constructor\n@param stateManager for the data bean.");
addJavaDoc(1,"Returns a new instance by using a state manager.\n"
"@param stateManager for the data bean.\n"
addJavaDoc(1,"Returns the schema of the data bean.\n"
"@return Schema for the data bean.");
addJavaDoc(1,"Gets a specific field.\n"
"@param field index of a field for the data bean.\n"
"@return Object representing a data bean's field.");
addJavaDoc(1,"Puts a value for a specific field.\n"
"@param field index of a field for the data bean.\n"
"@param value value of a field for the data bean.");
/**
*/
private void addJavaDoc(int indent, String javadoc) throws IOException {
if (javadoc==null)
return;
if (indent<0)
return;
line(indent, "");
line(indent, "/**");
if (javadoc.contains("\n")){
String javadocLines[] = javadoc.split("\n");
for(String line : javadocLines)
}
else
line(indent, " */");
}
addJavaDoc(0,schema.getDoc());
line(0, "@SuppressWarnings(\"all\")");
addJavaDoc(1,"Variable holding the data bean schema.");
addJavaDoc(1,"Enum containing all data bean's fields.");
addJavaDoc(2,"Field's index.");
addJavaDoc(2,"Field's name.");
addJavaDoc(2,"Field's constructor\n"
"@param index field's index.\n"
"@param name field's name.");
addJavaDoc(2,"Gets field's index.\n"
"@return int field's index.");
addJavaDoc(2,"Gets field's name.\n"
"@return String field's name.");
addJavaDoc(2,"Gets field's attributes to string.\n"
"@return String field's attributes to string.");
addJavaDoc(2,"Contains all field's names.");
addJavaDoc(1,field.doc());
addJavaDoc(1,"Default Constructor");
addJavaDoc(1,"Constructor\n@param stateManager for the data bean.");
addJavaDoc(1,"Returns a new instance by using a state manager.\n"
"@param stateManager for the data bean.\n"
addJavaDoc(1,"Returns the schema of the data bean.\n"
"@return Schema for the data bean.");
addJavaDoc(1,"Gets a specific field.\n"
"@param field index of a field for the data bean.\n"
"@return Object representing a data bean's field.");
addJavaDoc(1,"Puts a value for a specific field.\n"
"@param field index of a field for the data bean.\n"
"@param value value of a field for the data bean.");
import java.util.Arrays;
private final static String DEFAULT_SCHEMA_EXTENTION = ".avsc";
"\t\t      generated Java file. Current options include; \n"
"\t\t  ASLv2   (Apache Software License v2.0) \n"
"\t\t  AGPLv3  (GNU Affero General Public License)\n"
"\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n"
"\t\t  FDLv13  (GNU Free Documentation License v1.3)\n"
"\t\t  GPLv1   (GNU General Public License v1.0)\n"
"\t\t  GPLv2   (GNU General Public License v2.0)\n"
"\t\t  GPLv3   (GNU General Public License v3.0)\n "
"\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n"
"\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;
SimpleDateFormat sdf;
File inputFile;
File output;
long start;
}
inputFile = new File(args[0]);
output = new File(args[1]);
if(!inputFile.exists() || !output.exists()){
log.error("input file path or output file path doesn't exist.");
System.exit(1);
}
sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
start = System.currentTimeMillis();
if(inputFile.isDirectory()) {
ArrayList<File> inputSchemas = new ArrayList<File>();
File[] listOfFiles= inputFile.listFiles();
if ( (listOfFiles!=null) && (listOfFiles.length>0)){
for (File file : listOfFiles) {
if (file.isFile() && file.exists() && file.getName().endsWith(DEFAULT_SCHEMA_EXTENTION)) {
inputSchemas.add(file);
}
}
compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
}
else{
log.info("Path contains no files. Nothing to compile.");
}
}
else if (inputFile.isFile()) {
compileSchema(inputFile, output);
}
List<String> files = new ArrayList<String>(Arrays.asList(args));
output = new File(files.get(files.size()-1));
if(!output.exists()){
log.error("output path doesn't exist");
System.exit(1);
}
sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
start = System.currentTimeMillis();
inputFile = new File(filename);
if(!inputFile.exists()){
}
compileSchema(inputFile, output);
}
import java.util.Arrays;
private final static String DEFAULT_SCHEMA_EXTENTION = ".avsc";
"\t\t      generated Java file. Current options include; \n"
"\t\t  ASLv2   (Apache Software License v2.0) \n"
"\t\t  AGPLv3  (GNU Affero General Public License)\n"
"\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n"
"\t\t  FDLv13  (GNU Free Documentation License v1.3)\n"
"\t\t  GPLv1   (GNU General Public License v1.0)\n"
"\t\t  GPLv2   (GNU General Public License v2.0)\n"
"\t\t  GPLv3   (GNU General Public License v3.0)\n "
"\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n"
"\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;
SimpleDateFormat sdf;
File inputFile;
File output;
long start;
}
inputFile = new File(args[0]);
output = new File(args[1]);
if(!inputFile.exists() || !output.exists()){
log.error("input file path or output file path doesn't exist.");
System.exit(1);
}
sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
start = System.currentTimeMillis();
if(inputFile.isDirectory()) {
ArrayList<File> inputSchemas = new ArrayList<File>();
File[] listOfFiles= inputFile.listFiles();
if ( (listOfFiles!=null) && (listOfFiles.length>0)){
for (File file : listOfFiles) {
if (file.isFile() && file.exists() && file.getName().endsWith(DEFAULT_SCHEMA_EXTENTION)) {
inputSchemas.add(file);
}
}
compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
}
else{
log.info("Path contains no files. Nothing to compile.");
}
}
else if (inputFile.isFile()) {
compileSchema(inputFile, output);
}
List<String> files = new ArrayList<String>(Arrays.asList(args));
output = new File(files.get(files.size()-1));
if(!output.exists()){
log.error("output path doesn't exist");
System.exit(1);
}
sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
start = System.currentTimeMillis();
inputFile = new File(filename);
if(!inputFile.exists()){
}
compileSchema(inputFile, output);
}
import java.util.Arrays;
private final static String DEFAULT_SCHEMA_EXTENTION = ".avsc";
/**
*/
private void addJavaDoc(int indent, String javadoc) throws IOException {
if (javadoc==null)
return;
if (indent<0)
return;
line(indent, "");
line(indent, "/**");
if (javadoc.contains("\n")){
String javadocLines[] = javadoc.split("\n");
for(String line : javadocLines)
}
else
line(indent, " */");
}
addJavaDoc(0,schema.getDoc());
line(0, "@SuppressWarnings(\"all\")");
addJavaDoc(1,"Variable holding the data bean schema.");
addJavaDoc(1,"Enum containing all data bean's fields.");
addJavaDoc(2,"Field's index.");
addJavaDoc(2,"Field's name.");
addJavaDoc(2,"Field's constructor\n"
"@param index field's index.\n"
"@param name field's name.");
addJavaDoc(2,"Gets field's index.\n"
"@return int field's index.");
addJavaDoc(2,"Gets field's name.\n"
"@return String field's name.");
addJavaDoc(2,"Gets field's attributes to string.\n"
"@return String field's attributes to string.");
addJavaDoc(2,"Contains all field's names.");
addJavaDoc(1,field.doc());
addJavaDoc(1,"Default Constructor");
addJavaDoc(1,"Constructor\n@param stateManager for the data bean.");
addJavaDoc(1,"Returns a new instance by using a state manager.\n"
"@param stateManager for the data bean.\n"
addJavaDoc(1,"Returns the schema of the data bean.\n"
"@return Schema for the data bean.");
addJavaDoc(1,"Gets a specific field.\n"
"@param field index of a field for the data bean.\n"
"@return Object representing a data bean's field.");
addJavaDoc(1,"Puts a value for a specific field.\n"
"@param field index of a field for the data bean.\n"
"@param value value of a field for the data bean.");
"\t\t      generated Java file. Current options include; \n"
"\t\t  ASLv2   (Apache Software License v2.0) \n"
"\t\t  AGPLv3  (GNU Affero General Public License)\n"
"\t\t  CDDLv1  (Common Development and Distribution License v1.0)\n"
"\t\t  FDLv13  (GNU Free Documentation License v1.3)\n"
"\t\t  GPLv1   (GNU General Public License v1.0)\n"
"\t\t  GPLv2   (GNU General Public License v2.0)\n"
"\t\t  GPLv3   (GNU General Public License v3.0)\n "
"\t\t  LGPLv21 (GNU Lesser General Public License v2.1)\n"
"\t\t  LGPLv3  (GNU Lesser General Public License v3)\n") ;
SimpleDateFormat sdf;
File inputFile;
File output;
long start;
}
inputFile = new File(args[0]);
output = new File(args[1]);
if(!inputFile.exists() || !output.exists()){
log.error("input file path or output file path doesn't exist.");
System.exit(1);
}
sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
start = System.currentTimeMillis();
if(inputFile.isDirectory()) {
ArrayList<File> inputSchemas = new ArrayList<File>();
File[] listOfFiles= inputFile.listFiles();
if ( (listOfFiles!=null) && (listOfFiles.length>0)){
for (File file : listOfFiles) {
if (file.isFile() && file.exists() && file.getName().endsWith(DEFAULT_SCHEMA_EXTENTION)) {
inputSchemas.add(file);
}
}
compileSchema(inputSchemas.toArray(new File[inputSchemas.size()]), output);
}
else{
log.info("Path contains no files. Nothing to compile.");
}
}
else if (inputFile.isFile()) {
compileSchema(inputFile, output);
}
List<String> files = new ArrayList<String>(Arrays.asList(args));
output = new File(files.get(files.size()-1));
if(!output.exists()){
log.error("output path doesn't exist");
System.exit(1);
}
sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
start = System.currentTimeMillis();
inputFile = new File(filename);
if(!inputFile.exists()){
}
compileSchema(inputFile, output);
}
}
long writeCapacUnits = Long.parseLong(tableElement.getAttributeValue("writecunit"));
public class SolrStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
private static final Logger LOG = LoggerFactory.getLogger( SolrStore.class );
protected static final String DEFAULT_MAPPING_FILE = "gora-solr-mapping.xml";
protected static final String SOLR_URL_PROPERTY = "solr.url";
protected static final String SOLR_CONFIG_PROPERTY = "solr.config";
protected static final String SOLR_SCHEMA_PROPERTY = "solr.schema";
protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batchSize";
protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commitWithin";
protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.resultsSize";
protected static final int DEFAULT_BATCH_SIZE = 100;
protected static final int DEFAULT_COMMIT_WITHIN = 1000;
protected static final int DEFAULT_RESULTS_SIZE = 100;
private SolrMapping mapping;
private String solrServerUrl, solrConfig, solrSchema;
private SolrServer server, adminServer;
private ArrayList<SolrInputDocument> batch;
private int batchSize = DEFAULT_BATCH_SIZE;
private int commitWithin = DEFAULT_COMMIT_WITHIN;
private int resultsSize = DEFAULT_RESULTS_SIZE;
@Override
public void initialize( Class<K> keyClass, Class<T> persistentClass, Properties properties ) {
super.initialize( keyClass, persistentClass, properties );
String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
try {
mapping = readMapping( mappingFile );
}
catch ( IOException e ) {
LOG.error( e.getMessage() );
LOG.error( e.getStackTrace().toString() );
solrServerUrl = DataStoreFactory.findProperty( properties, this, SOLR_URL_PROPERTY, null );
solrConfig = DataStoreFactory.findProperty( properties, this, SOLR_CONFIG_PROPERTY, null );
solrSchema = DataStoreFactory.findProperty( properties, this, SOLR_SCHEMA_PROPERTY, null );
adminServer = new HttpSolrServer( solrServerUrl );
if ( autoCreateSchema ) {
createSchema();
}
String batchSizeString = DataStoreFactory.findProperty( properties, this, SOLR_BATCH_SIZE_PROPERTY, null );
if ( batchSizeString != null ) {
try {
batchSize = Integer.parseInt( batchSizeString );
} catch ( NumberFormatException nfe ) {
}
}
batch = new ArrayList<SolrInputDocument>( batchSize );
String commitWithinString = DataStoreFactory.findProperty( properties, this, SOLR_COMMIT_WITHIN_PROPERTY, null );
if ( commitWithinString != null ) {
try {
commitWithin = Integer.parseInt( commitWithinString );
} catch ( NumberFormatException nfe ) {
}
}
String resultsSizeString = DataStoreFactory.findProperty( properties, this, SOLR_RESULTS_SIZE_PROPERTY, null );
if ( resultsSizeString != null ) {
try {
resultsSize = Integer.parseInt( resultsSizeString );
} catch ( NumberFormatException nfe ) {
}
}
}
@SuppressWarnings("unchecked")
private SolrMapping readMapping( String filename ) throws IOException {
SolrMapping map = new SolrMapping();
try {
SAXBuilder builder = new SAXBuilder();
Document doc = builder.build( getClass().getClassLoader().getResourceAsStream( filename ) );
List<Element> classes = doc.getRootElement().getChildren( "class" );
for ( Element classElement : classes ) {
if ( classElement.getAttributeValue( "keyClass" ).equals( keyClass.getCanonicalName() )
&& classElement.getAttributeValue( "name" ).equals( persistentClass.getCanonicalName() ) ) {
String tableName = getSchemaName( classElement.getAttributeValue( "table" ), persistentClass );
map.setCoreName( tableName );
Element primaryKeyEl = classElement.getChild( "primarykey" );
map.setPrimaryKey( primaryKeyEl.getAttributeValue( "column" ) );
List<Element> fields = classElement.getChildren( "field" );
for ( Element field : fields ) {
String fieldName = field.getAttributeValue( "name" );
String columnName = field.getAttributeValue( "column" );
map.addField( fieldName, columnName );
}
break;
}
} catch ( Exception ex ) {
throw new IOException( ex );
return map;
}
public SolrMapping getMapping() {
return mapping;
}
@Override
public String getSchemaName() {
return mapping.getCoreName();
}
@Override
public void createSchema() {
try {
if ( !schemaExists() )
CoreAdminRequest.createCore( mapping.getCoreName(), mapping.getCoreName(), adminServer, solrConfig,
solrSchema );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
@Override
public void truncateSchema() {
try {
server.deleteByQuery( "*:*" );
server.commit();
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
@Override
public void deleteSchema() {
try {
server.deleteByQuery( "*:*" );
server.commit();
} catch ( Exception e ) {
try {
CoreAdminRequest.unloadCore( mapping.getCoreName(), adminServer );
} catch ( Exception e ) {
if ( e.getMessage().contains( "No such core" ) ) {
} else {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
}
@Override
public boolean schemaExists() {
boolean exists = false;
try {
CoreAdminResponse rsp = CoreAdminRequest.getStatus( mapping.getCoreName(), adminServer );
exists = rsp.getUptime( mapping.getCoreName() ) != null;
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return exists;
}
private static final String toDelimitedString( String[] arr, String sep ) {
if ( arr == null || arr.length == 0 ) {
return "";
}
StringBuilder sb = new StringBuilder();
if ( i > 0 )
sb.append( sep );
sb.append( arr[i] );
}
return sb.toString();
}
public static String escapeQueryKey( String key ) {
if ( key == null ) {
return null;
}
StringBuilder sb = new StringBuilder();
char c = key.charAt( i );
switch ( c ) {
case ':':
case '*':
break;
default:
sb.append( c );
}
}
return sb.toString();
}
@Override
public T get( K key, String[] fields ) {
ModifiableSolrParams params = new ModifiableSolrParams();
params.set( CommonParams.QT, "/get" );
params.set( CommonParams.FL, toDelimitedString( fields, "," ) );
params.set( "id",  key.toString() );
try {
QueryResponse rsp = server.query( params );
Object o = rsp.getResponse().get( "doc" );
if ( o == null ) {
}
return newInstance( (SolrDocument)o, fields );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return null;
}
public T newInstance( SolrDocument doc, String[] fields )
throws IOException {
T persistent = newPersistent();
if ( fields == null ) {
fields = fieldMap.keySet().toArray( new String[fieldMap.size()] );
String pk = mapping.getPrimaryKey();
for ( String f : fields ) {
Field field = fieldMap.get( f );
Schema fieldSchema = field.schema();
String sf = null;
if ( pk.equals( f ) ) {
sf = f;
} else {
sf = mapping.getSolrField( f );
}
Object sv = doc.get( sf );
Object v;
if ( sv == null ) {
continue;
}
switch ( fieldSchema.getType() ) {
case MAP:
case ARRAY:
case RECORD:
v = IOUtils.deserialize( (byte[]) sv, datumReader, fieldSchema, persistent.get( field.pos() ) );
persistent.put( field.pos(), v );
break;
case ENUM:
v = AvroUtils.getEnumValue( fieldSchema, (String) sv );
persistent.put( field.pos(), v );
break;
case FIXED:
throw new IOException( "???" );
case BYTES:
persistent.put( field.pos(), ByteBuffer.wrap( (byte[]) sv ) );
break;
case BOOLEAN:
case DOUBLE:
case FLOAT:
case INT:
case LONG:
persistent.put( field.pos(), sv );
break;
case STRING:
persistent.put( field.pos(), new Utf8( sv.toString() ) );
break;
case UNION:
LOG.error( "Union is not supported yet" );
break;
default:
}
persistent.setDirty( field.pos() );
persistent.clearDirty();
return persistent;
}
@Override
public void put( K key, T persistent ) {
Schema schema = persistent.getSchema();
StateManager stateManager = persistent.getStateManager();
if ( !stateManager.isDirty( persistent ) ) {
return;
}
SolrInputDocument doc = new SolrInputDocument();
doc.addField( mapping.getPrimaryKey(), key );
List<Field> fields = schema.getFields();
for ( Field field : fields ) {
String sf = mapping.getSolrField( field.name() );
if ( sf == null ) {
continue;
}
Schema fieldSchema = field.schema();
Object v = persistent.get( field.pos() );
if ( v == null ) {
continue;
}
switch ( fieldSchema.getType() ) {
case MAP:
case ARRAY:
case RECORD:
byte[] data = null;
try {
data = IOUtils.serialize( datumWriter, fieldSchema, v );
} catch ( IOException e ) {
}
doc.addField( sf, data );
break;
case BYTES:
doc.addField( sf, ( (ByteBuffer) v ).array() );
break;
case ENUM:
case STRING:
doc.addField( sf, v.toString() );
break;
case BOOLEAN:
case DOUBLE:
case FLOAT:
case INT:
case LONG:
doc.addField( sf, v );
break;
case UNION:
LOG.error( "Union is not supported yet" );
break;
default:
}
batch.add( doc );
if ( batch.size() >= batchSize ) {
try {
add( batch, commitWithin );
batch.clear();
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
}
}
@Override
public boolean delete( K key ) {
String keyField = mapping.getPrimaryKey();
try {
server.commit();
LOG.info( rsp.toString() );
return true;
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return false;
}
@Override
public long deleteByQuery( Query<K, T> query ) {
String q = ( (SolrQuery<K, T>) query ).toSolrQuery();
try {
UpdateResponse rsp = server.deleteByQuery( q );
server.commit();
LOG.info( rsp.toString() );
} catch ( Exception e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return 0;
}
@Override
public Result<K, T> execute( Query<K, T> query ) {
try {
return new SolrResult<K, T>( this, query, server, resultsSize );
} catch ( IOException e ) {
LOG.error( e.getMessage(), e.getStackTrace().toString() );
return null;
}
@Override
public Query<K, T> newQuery() {
return new SolrQuery<K, T>( this );
}
@Override
public List<PartitionQuery<K, T>> getPartitions( Query<K, T> query )
throws IOException {
ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<PartitionQuery<K, T>>();
partitions.add( new PartitionQueryImpl<K, T>( query ) );
return partitions;
}
@Override
public void flush() {
try {
if ( batch.size() > 0 ) {
add( batch, commitWithin );
batch.clear();
}
} catch ( Exception e ) {
LOG.error(e.getMessage(), e.getStackTrace());
}
@Override
public void close() {
}
private void add(ArrayList<SolrInputDocument> batch, int commitWithin) throws SolrServerException, IOException {
if (commitWithin == 0) {
server.add( batch );
server.commit( false, true, true );
} else {
server.add( batch, commitWithin );
}
pqi.setConf(getConf());
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
partitions.add(pqi);
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
list.add(pqi);
IOUtils.serialize(getConf(), out, baseQuery);
baseQuery = IOUtils.deserialize(getConf(), in, null);
startKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
endKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
return conf != null ? conf : new Configuration();
SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<K, T>(
partition.setConf(getConf());
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
partitions.add(pqi);
pqi.setConf(getConf());
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
partitions.add(pqi);
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
list.add(pqi);
IOUtils.serialize(getConf(), out, baseQuery);
baseQuery = IOUtils.deserialize(getConf(), in, null);
startKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
endKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
return conf != null ? conf : new Configuration();
SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<K, T>(
partition.setConf(getConf());
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
partitions.add(pqi);
pqi.setConf(getConf());
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
partitions.add(pqi);
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
list.add(pqi);
IOUtils.serialize(getConf(), out, baseQuery);
baseQuery = IOUtils.deserialize(getConf(), in, null);
startKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
endKey = IOUtils.deserialize(getConf(), in, null, dataStore.getKeyClass());
return conf != null ? conf : new Configuration();
SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
SerializationFactory serializationFactory = new SerializationFactory(getOrCreateConf(conf));
PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<K, T>(
partition.setConf(getConf());
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K, T>(query);
pqi.setConf(getConf());
partitions.add(pqi);
} else {
throw new IllegalStateException("MongoStore doesn't support 3 types union field yet. Please update your mapping");
} else {
throw new IllegalStateException("MongoStore doesn't support 3 types union field yet. Please update your mapping");
if (obj.isNew() || obj.isDirty()) {
} else {
obj.clearNew();
obj.clearDirty();
}
if (qUpdateSet.size() > 0) {
}
if (qUpdateUnset.size() > 0) {
}
if (!qUpdate.isEmpty()) {
mongoClientColl.update(qSel, qUpdate, true, false);
} else {
LOG.debug("No update to perform, skip {}", key);
}
import org.apache.gora.store.DataStoreFactory;
private static final String SCANNER_CACHING_PROPERTIES_KEY = "scanner.caching" ;
private static final int SCANNER_CACHING_PROPERTIES_DEFAULT = 0 ;
private int scannerCaching = SCANNER_CACHING_PROPERTIES_DEFAULT ;
try {
this.setScannerCaching(
Integer.valueOf(DataStoreFactory.findProperty(this.properties, this,
SCANNER_CACHING_PROPERTIES_KEY,
String.valueOf(SCANNER_CACHING_PROPERTIES_DEFAULT)))) ;
}catch(Exception e){
}
scan.setCaching(this.getScannerCaching()) ;
public int getScannerCaching() {
return this.scannerCaching ;
}
public HBaseStore<K, T> setScannerCaching(int numRows) {
if (numRows < 0) {
return this ;
}
this.scannerCaching = numRows ;
return this ;
}
import org.apache.gora.store.DataStoreFactory;
private static final String SCANNER_CACHING_PROPERTIES_KEY = "scanner.caching" ;
private static final int SCANNER_CACHING_PROPERTIES_DEFAULT = 0 ;
private int scannerCaching = SCANNER_CACHING_PROPERTIES_DEFAULT ;
try {
this.setScannerCaching(
Integer.valueOf(DataStoreFactory.findProperty(this.properties, this,
SCANNER_CACHING_PROPERTIES_KEY,
String.valueOf(SCANNER_CACHING_PROPERTIES_DEFAULT)))) ;
}catch(Exception e){
}
scan.setCaching(this.getScannerCaching()) ;
public int getScannerCaching() {
return this.scannerCaching ;
}
public HBaseStore<K, T> setScannerCaching(int numRows) {
if (numRows < 0) {
return this ;
}
this.scannerCaching = numRows ;
return this ;
}
try{
store.close();
}catch(Exception e){
}
try{
store.put(key, (Persistent) value);
counter.increment();
if (counter.isModulo()) {
store.flush();
}
}catch(Exception e){
}
try{
store.close();
}catch(Exception e){
}
try{
store.put(key, (Persistent) value);
counter.increment();
if (counter.isModulo()) {
store.flush();
}
}catch(Exception e){
}
"could be found. Please report this to dev@gora.apache.org");
Serializer<Object> serializer = GoraSerializerTypeInferer.getSerializer(value);
case RECORD:
PersistentBase persistent = (PersistentBase) fieldValue;
PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
for (Field member: fieldSchema.getFields()) {
newRecord.put(member.pos(), persistent.get(member.pos()));
}
fieldValue = newRecord;
break;
case MAP:
StatefulHashMap map = (StatefulHashMap) fieldValue;
StatefulHashMap newMap = new StatefulHashMap();
for (Object mapKey : map.keySet()) {
newMap.put(mapKey, map.get(mapKey));
newMap.putState(mapKey, map.getState(mapKey));
}
fieldValue = newMap;
break;
case ARRAY:
GenericArray array = (GenericArray) fieldValue;
ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
Iterator iter = array.iterator();
while (iter.hasNext()) {
newArray.add(iter.next());
}
fieldValue = newArray;
break;
case UNION:
break;
}
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
for (Field member: schema.getFields()) {
Object memberValue = persistentBase.get(member.pos());
if (memberValue instanceof GenericArray<?>) {
if (((GenericArray)memberValue).size() == 0) {
continue;
}
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
} else {
}
break;
case MAP:
if (value != null) {
if (value instanceof StatefulHashMap<?, ?>) {
this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
} else {
}
break;
case ARRAY:
if (value != null) {
if (value instanceof GenericArray<?>) {
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
} else {
}
break;
case UNION:
if(value != null) {
this.cassandraClient.addColumn(key, field.name(), value);
} else {
}
default:
@SuppressWarnings("unchecked")
@SuppressWarnings("unchecked")
@SuppressWarnings("unchecked")
"could be found. Please report this to dev@gora.apache.org");
Serializer<Object> serializer = GoraSerializerTypeInferer.getSerializer(value);
case RECORD:
PersistentBase persistent = (PersistentBase) fieldValue;
PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
for (Field member: fieldSchema.getFields()) {
newRecord.put(member.pos(), persistent.get(member.pos()));
}
fieldValue = newRecord;
break;
case MAP:
StatefulHashMap map = (StatefulHashMap) fieldValue;
StatefulHashMap newMap = new StatefulHashMap();
for (Object mapKey : map.keySet()) {
newMap.put(mapKey, map.get(mapKey));
newMap.putState(mapKey, map.getState(mapKey));
}
fieldValue = newMap;
break;
case ARRAY:
GenericArray array = (GenericArray) fieldValue;
ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
Iterator iter = array.iterator();
while (iter.hasNext()) {
newArray.add(iter.next());
}
fieldValue = newArray;
break;
case UNION:
break;
}
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
for (Field member: schema.getFields()) {
Object memberValue = persistentBase.get(member.pos());
if (memberValue instanceof GenericArray<?>) {
if (((GenericArray)memberValue).size() == 0) {
continue;
}
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
} else {
}
break;
case MAP:
if (value != null) {
if (value instanceof StatefulHashMap<?, ?>) {
this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
} else {
}
break;
case ARRAY:
if (value != null) {
if (value instanceof GenericArray<?>) {
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
} else {
}
break;
case UNION:
if(value != null) {
this.cassandraClient.addColumn(key, field.name(), value);
} else {
}
default:
@SuppressWarnings("unchecked")
@SuppressWarnings("unchecked")
@SuppressWarnings("unchecked")
, String defaultValue) throws IOException {
String mappingFilename = findProperty(properties, store, MAPPING_FILE, defaultValue);
InputStream mappingFile = store.getClass().getClassLoader().getResourceAsStream(mappingFilename);
if (mappingFile == null)
return mappingFilename;
String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
, String defaultValue) throws IOException {
String mappingFilename = findProperty(properties, store, MAPPING_FILE, defaultValue);
InputStream mappingFile = store.getClass().getClassLoader().getResourceAsStream(mappingFilename);
if (mappingFile == null)
return mappingFilename;
String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
keyStates.remove(key);
keyStates.remove(key);
import org.apache.gora.filter.Filter;
public void setFilter(Filter<K, T> filter);
public Filter<K, T> getFilter();
void setLocalFilterEnabled(boolean enable);
boolean isLocalFilterEnabled();
import org.apache.gora.filter.Filter;
@Override
public Filter<K, T> getFilter() {
return baseQuery.getFilter();
}
@Override
public void setFilter(Filter<K, T> filter) {
baseQuery.setFilter(filter);
}
import org.apache.gora.filter.Filter;
protected Filter<K, T> filter;
protected boolean localFilterEnabled=true;
@Override
public Filter<K, T> getFilter() {
return filter;
}
@Override
public void setFilter(Filter<K, T> filter) {
this.filter=filter;
}
@Override
public boolean isLocalFilterEnabled() {
return localFilterEnabled;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
this.localFilterEnabled=enable;
}
if(!nullFields[4]) {
String filterClass = Text.readString(in);
try {
filter = (Filter<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(filterClass), conf);
filter.readFields(in);
} catch (ClassNotFoundException e) {
throw new IOException(e);
}
}
localFilterEnabled = in.readBoolean();
if(filter != null) {
Text.writeString(out, filter.getClass().getCanonicalName());
filter.write(out);
}
out.writeBoolean(localFilterEnabled);
builder.append(localFilterEnabled, that.localFilterEnabled);
builder.append(localFilterEnabled);
builder.append("localFilterEnabled", localFilterEnabled);
import org.apache.gora.filter.Filter;
import java.io.IOException;
if(isLimitReached()) {
return false;
}
boolean ret;
do {
clear();
persistent = getOrCreatePersistent(persistent);
ret = nextInner();
if (ret == false) {
break;
}
} while (filter(key, persistent));
return ret;
}
protected boolean filter(K key, T persistent) {
if (!query.isLocalFilterEnabled()) {
return false;
}
Filter<K, T> filter = query.getFilter();
if (filter == null) {
return false;
}
return filter.filter(key, persistent);
import org.apache.gora.filter.Filter;
import java.util.Arrays;
@Override
public Filter<K, T> getFilter() {
return filter;
}
@Override
public void setFilter(Filter<K, T> filter) {
this.filter=filter;
}
@Override
public boolean isLocalFilterEnabled() {
return localFilterEnabled;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
this.localFilterEnabled=enable;
}
import org.apache.gora.filter.Filter;
protected Filter<K, T> filter;
protected boolean localFilterEnabled=true;
import org.apache.gora.filter.Filter;
@Override
public void setFilter(Filter<K, T> filter) {
}
@Override
public Filter<K, T> getFilter() {
return null;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
}
@Override
public boolean isLocalFilterEnabled() {
return false;
}
public class HBaseColumn {
import org.apache.gora.hbase.util.HBaseFilterUtil;
private HBaseFilterUtil<K, T> filterUtil;
filterUtil = new HBaseFilterUtil<K, T>(this.conf);
public HBaseMapping getMapping() {
return mapping;
}
default :
break;
String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getServerAddress().getHostname();
= new HBaseScannerResult<K,T>(this, query, scanner);
if (query.getFilter() != null) {
boolean succeeded = filterUtil.setFilter(scan, query.getFilter(), this);
if (succeeded) {
query.setLocalFilterEnabled(false);
}
}
} else if (clazz.isArray() && clazz.getComponentType().equals(Byte.TYPE)) {
return (byte[])o;
import org.apache.gora.filter.Filter;
public void setFilter(Filter<K, T> filter);
public Filter<K, T> getFilter();
void setLocalFilterEnabled(boolean enable);
boolean isLocalFilterEnabled();
import org.apache.gora.filter.Filter;
@Override
public Filter<K, T> getFilter() {
return baseQuery.getFilter();
}
@Override
public void setFilter(Filter<K, T> filter) {
baseQuery.setFilter(filter);
}
import org.apache.gora.filter.Filter;
protected Filter<K, T> filter;
protected boolean localFilterEnabled=true;
@Override
public Filter<K, T> getFilter() {
return filter;
}
@Override
public void setFilter(Filter<K, T> filter) {
this.filter=filter;
}
@Override
public boolean isLocalFilterEnabled() {
return localFilterEnabled;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
this.localFilterEnabled=enable;
}
if(!nullFields[4]) {
String filterClass = Text.readString(in);
try {
filter = (Filter<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(filterClass), conf);
filter.readFields(in);
} catch (ClassNotFoundException e) {
throw new IOException(e);
}
}
localFilterEnabled = in.readBoolean();
if(filter != null) {
Text.writeString(out, filter.getClass().getCanonicalName());
filter.write(out);
}
out.writeBoolean(localFilterEnabled);
builder.append(localFilterEnabled, that.localFilterEnabled);
builder.append(localFilterEnabled);
builder.append("localFilterEnabled", localFilterEnabled);
import org.apache.gora.filter.Filter;
import java.io.IOException;
if(isLimitReached()) {
return false;
}
boolean ret;
do {
clear();
persistent = getOrCreatePersistent(persistent);
ret = nextInner();
if (ret == false) {
break;
}
} while (filter(key, persistent));
return ret;
}
protected boolean filter(K key, T persistent) {
if (!query.isLocalFilterEnabled()) {
return false;
}
Filter<K, T> filter = query.getFilter();
if (filter == null) {
return false;
}
return filter.filter(key, persistent);
import org.apache.gora.filter.Filter;
import java.util.Arrays;
@Override
public Filter<K, T> getFilter() {
return filter;
}
@Override
public void setFilter(Filter<K, T> filter) {
this.filter=filter;
}
@Override
public boolean isLocalFilterEnabled() {
return localFilterEnabled;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
this.localFilterEnabled=enable;
}
import org.apache.gora.filter.Filter;
protected Filter<K, T> filter;
protected boolean localFilterEnabled=true;
import org.apache.gora.filter.Filter;
@Override
public void setFilter(Filter<K, T> filter) {
}
@Override
public Filter<K, T> getFilter() {
return null;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
}
@Override
public boolean isLocalFilterEnabled() {
return false;
}
public class HBaseColumn {
import org.apache.gora.hbase.util.HBaseFilterUtil;
private HBaseFilterUtil<K, T> filterUtil;
filterUtil = new HBaseFilterUtil<K, T>(this.conf);
public HBaseMapping getMapping() {
return mapping;
}
default :
break;
String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getServerAddress().getHostname();
= new HBaseScannerResult<K,T>(this, query, scanner);
if (query.getFilter() != null) {
boolean succeeded = filterUtil.setFilter(scan, query.getFilter(), this);
if (succeeded) {
query.setLocalFilterEnabled(false);
}
}
} else if (clazz.isArray() && clazz.getComponentType().equals(Byte.TYPE)) {
return (byte[])o;
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;
import java.util.Properties;
public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";
this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(),
new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
List<ColumnFamilyDefinition> columnFamilyDefinitions = this.cassandraMapping.getColumnFamilyDefinitions();
keyspaceDefinition = HFactory.createKeyspaceDefinition(this.cassandraMapping.getKeyspaceName(),
"org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
ccl.setReadCfConsistencyLevels(clmap);
ccl.setWriteCfConsistencyLevels(clmap);
String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
HFactory.createKeyspace("Keyspace", this.cluster, ccl);
", not BytesType. It may cause a fatal error on column validation later.");
private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
for (ColumnFamilyDefinition colFamDef : pColFams)
clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
return clMap;
}
public void addGenericArray(K key, String fieldName, GenericArray<?> array) {
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery =
HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
family = this.cassandraMapping.getFamily(pField);
column = this.cassandraMapping.getColumn(pField);
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery =
HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(),
ByteBufferSerializer.get(), ByteBufferSerializer.get());
return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
private static final String COL_FAM_CL = "cf.consistency.level";
private static final String READ_OP_CL = "read.consistency.level";
private static final String WRITE_OP_CL = "write.consistency.level";
public static String colFamConsLvl;
public static String readOpConsLvl;
public static String writeOpConsLvl;
private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
if (autoCreateSchema) {
colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
}
CassandraResultSet<K> cassandraResultSet) {
"sure to include this property in gora.properties file");
import org.apache.gora.store.DataStoreFactory;
preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;
import java.util.Properties;
public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";
this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(),
new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
List<ColumnFamilyDefinition> columnFamilyDefinitions = this.cassandraMapping.getColumnFamilyDefinitions();
keyspaceDefinition = HFactory.createKeyspaceDefinition(this.cassandraMapping.getKeyspaceName(),
"org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
ccl.setReadCfConsistencyLevels(clmap);
ccl.setWriteCfConsistencyLevels(clmap);
String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
HFactory.createKeyspace("Keyspace", this.cluster, ccl);
", not BytesType. It may cause a fatal error on column validation later.");
private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
for (ColumnFamilyDefinition colFamDef : pColFams)
clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
return clMap;
}
public void addGenericArray(K key, String fieldName, GenericArray<?> array) {
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery =
HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
family = this.cassandraMapping.getFamily(pField);
column = this.cassandraMapping.getColumn(pField);
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery =
HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(),
ByteBufferSerializer.get(), ByteBufferSerializer.get());
return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
private static final String COL_FAM_CL = "cf.consistency.level";
private static final String READ_OP_CL = "read.consistency.level";
private static final String WRITE_OP_CL = "write.consistency.level";
public static String colFamConsLvl;
public static String readOpConsLvl;
public static String writeOpConsLvl;
private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
if (autoCreateSchema) {
colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
}
CassandraResultSet<K> cassandraResultSet) {
"sure to include this property in gora.properties file");
import org.apache.gora.store.DataStoreFactory;
preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
if (ttlAttr == null) {
ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
}
HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer, ttlAttr);
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
if (ttlAttr  == null) {
ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
}
HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer, ttlAttr);
public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
synchronized(mutator) {
HectorUtils.deleteColumn(mutator, key, familyName, columnName);
}
}
public void deleteByKey(K key) {
Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
deleteColumn(key, familyMap.values().iterator().next().toString(), null);
}
private static final String GCGRACE_SECONDS_ATTRIBUTE = "gc_grace_seconds";
private static final String COLUMNS_TTL_ATTRIBUTE = "ttl";
public static final String DEFAULT_COLUMNS_TTL = "60";
public static final int DEFAULT_GCGRACE_SECONDS = 30;
private Map<String, String> columnAttrMap = new HashMap<String, String>();
LOG.error("Error locating Cassandra Keyspace name attribute!");
String gcgrace_scs = element.getAttributeValue(GCGRACE_SECONDS_ATTRIBUTE);
if (gcgrace_scs == null) {
} else {
if (LOG.isDebugEnabled()) {
}
}
cfDef.setGcGraceSeconds(gcgrace_scs!=null?Integer.parseInt(gcgrace_scs):DEFAULT_GCGRACE_SECONDS);
String ttlValue = element.getAttributeValue(COLUMNS_TTL_ATTRIBUTE);
if (ttlValue == null) {
}
this.columnAttrMap.put(columnName, ttlValue!=null?ttlValue:DEFAULT_COLUMNS_TTL);
}
public Map<String,String> getFamilyMap(){
return this.familyMap;
}
public Map<String, String> getColumnsAttribs(){
return this.columnAttrMap;
}
this.cassandraClient.deleteByKey(key);
return true;
import me.prettyprint.hector.api.mutation.MutationResult;
public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, String columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
public static<K> HColumn<ByteBuffer,ByteBuffer> createColumn(ByteBuffer name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> HColumn<String,ByteBuffer> createColumn(String name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> HColumn<Integer,ByteBuffer> createColumn(Integer name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void deleteColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName){
MutationResult mr = mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
System.out.println(mr.toString());
}
public static<K> HSuperColumn<String,ByteBuffer,ByteBuffer> createSuperColumn(String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
public static<K> HSuperColumn<String,String,ByteBuffer> createSuperColumn(String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), StringSerializer.get(), ByteBufferSerializer.get());
public static<K> HSuperColumn<String,Integer,ByteBuffer> createSuperColumn(String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), IntegerSerializer.get(), ByteBufferSerializer.get());
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
if (ttlAttr == null) {
ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
}
HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer, ttlAttr);
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
if (ttlAttr  == null) {
ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
}
HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer, ttlAttr);
public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
synchronized(mutator) {
HectorUtils.deleteColumn(mutator, key, familyName, columnName);
}
}
public void deleteByKey(K key) {
Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
deleteColumn(key, familyMap.values().iterator().next().toString(), null);
}
private static final String GCGRACE_SECONDS_ATTRIBUTE = "gc_grace_seconds";
private static final String COLUMNS_TTL_ATTRIBUTE = "ttl";
public static final String DEFAULT_COLUMNS_TTL = "60";
public static final int DEFAULT_GCGRACE_SECONDS = 30;
private Map<String, String> columnAttrMap = new HashMap<String, String>();
LOG.error("Error locating Cassandra Keyspace name attribute!");
String gcgrace_scs = element.getAttributeValue(GCGRACE_SECONDS_ATTRIBUTE);
if (gcgrace_scs == null) {
} else {
if (LOG.isDebugEnabled()) {
}
}
cfDef.setGcGraceSeconds(gcgrace_scs!=null?Integer.parseInt(gcgrace_scs):DEFAULT_GCGRACE_SECONDS);
String ttlValue = element.getAttributeValue(COLUMNS_TTL_ATTRIBUTE);
if (ttlValue == null) {
}
this.columnAttrMap.put(columnName, ttlValue!=null?ttlValue:DEFAULT_COLUMNS_TTL);
}
public Map<String,String> getFamilyMap(){
return this.familyMap;
}
public Map<String, String> getColumnsAttribs(){
return this.columnAttrMap;
}
this.cassandraClient.deleteByKey(key);
return true;
import me.prettyprint.hector.api.mutation.MutationResult;
public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, String columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
public static<K> HColumn<ByteBuffer,ByteBuffer> createColumn(ByteBuffer name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> HColumn<String,ByteBuffer> createColumn(String name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> HColumn<Integer,ByteBuffer> createColumn(Integer name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void deleteColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName){
MutationResult mr = mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
System.out.println(mr.toString());
}
public static<K> HSuperColumn<String,ByteBuffer,ByteBuffer> createSuperColumn(String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
public static<K> HSuperColumn<String,String,ByteBuffer> createSuperColumn(String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), StringSerializer.get(), ByteBufferSerializer.get());
public static<K> HSuperColumn<String,Integer,ByteBuffer> createSuperColumn(String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), IntegerSerializer.get(), ByteBufferSerializer.get());
LOG.debug("Keyclass and nameclass match.");
"Assuming they are the same.");
}
LOG.debug("Keyclass and nameclass match.");
"Assuming they are the same.");
}
} else {
LOG.error("KeyClass in gora-hbase-mapping is not the same as the one in the databean.");
} else {
LOG.error("KeyClass in gora-hbase-mapping is not the same as the one in the databean.");
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
key = (K) ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
import java.util.concurrent.TimeUnit;
import org.apache.accumulo.core.client.BatchWriterConfig;
import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;
import org.apache.accumulo.core.client.security.tokens.PasswordToken;
import org.apache.accumulo.core.security.thrift.TCredentials;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericData;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.EncoderFactory;
import org.apache.gora.accumulo.encoders.BinaryEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
private TCredentials credentials;
public Object fromBytes(Schema schema, byte data[]) throws GoraException {
Schema fromSchema = null;
if (schema.getType() == Type.UNION) {
try {
Decoder decoder = DecoderFactory.get().binaryDecoder(data, null);
int unionIndex = decoder.readIndex();
List<Schema> possibleTypes = schema.getTypes();
fromSchema = possibleTypes.get(unionIndex);
Schema effectiveSchema = possibleTypes.get(unionIndex);
if (effectiveSchema.getType() == Type.NULL) {
decoder.readNull();
return null;
} else {
data = decoder.readBytes(null).array();
}
} catch (IOException e) {
e.printStackTrace();
throw new GoraException("Error decoding union type: ", e);
}
} else {
fromSchema = schema;
}
return fromBytes(encoder, fromSchema, data);
case BOOLEAN:
return encoder.decodeBoolean(data);
case DOUBLE:
return encoder.decodeDouble(data);
case FLOAT:
return encoder.decodeFloat(data);
case INT:
return encoder.decodeInt(data);
case LONG:
return encoder.decodeLong(data);
case STRING:
return new Utf8(data);
case BYTES:
return ByteBuffer.wrap(data);
case ENUM:
return AvroUtils.getEnumValue(schema, encoder.decodeInt(data));
case ARRAY:
break;
case FIXED:
break;
case MAP:
break;
case NULL:
break;
case RECORD:
break;
case UNION:
break;
default:
break;
public byte[] toBytes(Schema toSchema, Object o) {
if (toSchema != null && toSchema.getType() == Type.UNION) {
ByteArrayOutputStream baos = new ByteArrayOutputStream();
org.apache.avro.io.BinaryEncoder avroEncoder = EncoderFactory.get().binaryEncoder(baos, null);
int unionIndex = 0;
try {
if (o == null) {
unionIndex = firstNullSchemaTypeIndex(toSchema);
avroEncoder.writeIndex(unionIndex);
avroEncoder.writeNull();
} else {
unionIndex = firstNotNullSchemaTypeIndex(toSchema);
avroEncoder.writeIndex(unionIndex);
avroEncoder.writeBytes(toBytes(o));
}
avroEncoder.flush();
return baos.toByteArray();
} catch (IOException e) {
e.printStackTrace();
return toBytes(o);
}
} else {
return toBytes(o);
}
}
private int firstNullSchemaTypeIndex(Schema toSchema) {
List<Schema> possibleTypes = toSchema.getTypes();
int unionIndex = 0;
Type pType = possibleTypes.get(i).getType();
unionIndex = i; break;
}
}
return unionIndex;
}
private int firstNotNullSchemaTypeIndex(Schema toSchema) {
List<Schema> possibleTypes = toSchema.getTypes();
int unionIndex = 0;
Type pType = possibleTypes.get(i).getType();
unionIndex = i; break;
}
}
return unionIndex;
}
return copyIfNeeded(((Utf8) o).getBytes(), 0, ((Utf8) o).getByteLength());
return encoder.encodeInt(((Enum<?>) o).ordinal());
BatchWriterConfig batchWriterConfig = new BatchWriterConfig();
batchWriterConfig.setMaxMemory(10000000);
batchWriterConfig.setMaxLatency(60000l, TimeUnit.MILLISECONDS);
batchWriterConfig.setMaxWriteThreads(4);
batchWriter = conn.createBatchWriter(mapping.tableName, batchWriterConfig);
encoder = new BinaryEncoder();
AuthenticationToken token =  new PasswordToken(password);
credentials = new TCredentials(user,
"org.apache.accumulo.core.client.security.tokens.PasswordToken",
ByteBuffer.wrap(password.getBytes()), instance);
conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, token);
conn = new MockInstance().getConnector(user, new PasswordToken(password));
credentials = new TCredentials(user,
"org.apache.accumulo.core.client.security.tokens.PasswordToken",
ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());
Map<Utf8, Object> currentMap = null;
List currentArray = null;
BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(new byte[0], null);
if (row == null) {
row = entry.getKey().getRowData();
}
byte[] val = entry.getValue().get();
Field field = fieldMap.get(getFieldName(entry));
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = field.schema().getValueType();
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
break;
case ARRAY:
currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = field.schema().getElementType();
currentField = field;
currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
break;
Schema effectiveSchema = field.schema().getTypes()
.get(firstNotNullSchemaTypeIndex(field.schema()));
if (effectiveSchema.getType() == Type.ARRAY) {
currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
}
else if (effectiveSchema.getType() == Type.MAP) {
currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = effectiveSchema.getValueType();
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
}
case RECORD:
SpecificDatumReader<?> reader = new SpecificDatumReader<Schema>(field.schema());
persistent.put(field.pos(), reader.read(null, DecoderFactory.get().binaryDecoder(val, decoder)));
break;
default:
persistent.put(field.pos(), fromBytes(field.schema(), entry.getValue().get()));
persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
private String getFieldName(Entry<Key, Value> entry) {
String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(),
entry.getKey().getColumnQualifier()));
if (fieldName == null) {
fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), null));
}
return fieldName;
}
if (col != null) {
if (col.getSecond() == null) {
scanner.fetchColumnFamily(col.getFirst());
} else {
scanner.fetchColumn(col.getFirst(), col.getSecond());
}
List<Field> fields = schema.getFields();
if (!val.isDirty(i)) {
Field field = fields.get(i);
Object o = val.get(field.pos());
case MAP:
count = putMap(m, count, field.schema().getValueType(), o, col);
break;
case ARRAY:
count = putArray(m, count, o, col);
break;
Schema effectiveSchema = field.schema().getTypes()
.get(firstNotNullSchemaTypeIndex(field.schema()));
if (effectiveSchema.getType() == Type.ARRAY) {
count = putArray(m, count, o, col);
}
else if (effectiveSchema.getType() == Type.MAP) {
count = putMap(m, count, effectiveSchema.getValueType(), o, col);
}
case RECORD:
SpecificDatumWriter<Object> writer = new SpecificDatumWriter<Object>(field.schema());
ByteArrayOutputStream os = new ByteArrayOutputStream();
org.apache.avro.io.BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
writer.write(o, encoder);
encoder.flush();
m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
count;
break;
default:
m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
count;
private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col) throws GoraException {
Text rowKey = new Text(m.getRow());
Query<K, T> query = newQuery();
query.setFields(col.getFirst().toString());
query.setStartKey((K)rowKey.toString());
query.setEndKey((K)rowKey.toString());
deleteByQuery(query);
flush();
if (o == null){
return 0;
}
Set<?> es = ((Map<?, ?>)o).entrySet();
for (Object entry : es) {
Object mapKey = ((Entry<?, ?>) entry).getKey();
Object mapVal = ((Entry<?, ?>) entry).getValue();
if ((o instanceof DirtyMapWrapper && ((DirtyMapWrapper<?, ?>)o).isDirty())
m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(valueType, mapVal)));
count;
}
}
return count;
}
private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col) {
Text rowKey = new Text(m.getRow());
Query<K, T> query = newQuery();
query.setFields(col.getFirst().toString());
query.setStartKey((K)rowKey.toString());
query.setEndKey((K)rowKey.toString());
deleteByQuery(query);
flush();
if (o == null){
return 0;
}
int j = 0;
for (Object item : array) {
count;
}
return count;
}
tl = TabletLocator.getInstance(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));
while (tl.binRanges(Collections.singletonList(createRange(query)), binnedRanges, credentials).size() > 0) {
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, new String[] {location});
@SuppressWarnings("unchecked")
import java.io.IOException;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
if (schema.getType().equals(Type.RECORD) || schema.getType().equals(Type.MAP) ){
try {
value = AvroSerializerUtil.deserializer(value, schema);
} catch (IOException e) {
}
}
String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName().duplicate());
String family = cassandraColumn.getFamily();
if (fieldName != null) {
if (fieldName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
int pos = this.persistent.getSchema().getField(fieldName).pos();
Field field = fields.get(pos);
Type fieldType = field.schema().getType();
if (fieldType.equals(Type.UNION)) {
CassandraColumn cc = getUnionTypeColumn(fieldName
CassandraStore.UNION_COL_SUFIX, cassandraRow.toArray());
Field unionField = new Field(fieldName
CassandraStore.UNION_COL_SUFIX, Schema.create(Type.INT),
null, null);
cc.setField(unionField);
Object val = cc.getValue();
cassandraColumn.setUnionType(Integer.parseInt(val.toString()));
}
cassandraColumn.setField(field);
Object value = cassandraColumn.getValue();
this.persistent.put(pos, value);
this.persistent.clearDirty(pos);
} else
@SuppressWarnings("unused")
import java.util.List;
import java.util.Map;
import org.apache.gora.cassandra.serializers.ListSerializer;
import org.apache.gora.cassandra.serializers.MapSerializer;
private Object getFieldValue(Type type, Schema fieldSchema, ByteBuffer byteBuffer){
Object value = null;
if (type.equals(Type.ARRAY)) {
ListSerializer<?> serializer = ListSerializer.get(fieldSchema.getElementType());
List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
value = genericArray;
} else if (type.equals(Type.MAP)) {
value = fromByteBuffer(fieldSchema, byteBuffer);
} else if (type.equals(Type.RECORD)){
value = fromByteBuffer(fieldSchema, byteBuffer);
} else if (type.equals(Type.UNION)){
Schema unionFieldSchema = getUnionSchema(super.getUnionType(), fieldSchema);
Type unionFieldType = unionFieldSchema.getType();
value = getFieldValue(unionFieldType, unionFieldSchema, byteBuffer);
} else {
value = fromByteBuffer(fieldSchema, byteBuffer);
}
return value;
}
Object value = getFieldValue(type, fieldSchema, byteBuffer);
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import org.apache.gora.cassandra.serializers.CharSequenceSerializer;
import org.apache.gora.cassandra.store.CassandraStore;
private Object getSuperValue(Field field, Schema fieldSchema, Type type){
List<Object> array = new ArrayList<Object>();
Map<CharSequence, Object> map = new HashMap<CharSequence, Object>();
CharSequence mapKey = CharSequenceSerializer.get().fromByteBuffer(hColumn.getName());
if (mapKey.toString().indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
Object memberValue = null;
if (fieldSchema.getValueType().getType().equals(Type.UNION)){
HColumn<ByteBuffer, ByteBuffer> cc = getUnionTypeColumn(mapKey
CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns());
Integer unionIndex = getUnionIndex(mapKey.toString(), cc);
Schema realSchema = fieldSchema.getValueType().getTypes().get(unionIndex);
memberValue = fromByteBuffer(realSchema, hColumn.getValue());
}else{
memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());
}
map.put(mapKey, memberValue);
}
if (memberName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
Schema memberSchema = memberField.schema();
Type memberType = memberSchema.getType();
if (memberType.equals(Type.UNION)){
HColumn<ByteBuffer, ByteBuffer> hc = getUnionTypeColumn(memberField.name()
CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns().toArray());
Integer unionIndex = getUnionIndex(memberField.name(),hc);
cassandraColumn.setUnionType(unionIndex);
}
record.put(record.getSchema().getField(memberName).pos(), cassandraColumn.getValue());
}
case UNION:
int schemaPos = this.getUnionType();
Schema unioSchema = fieldSchema.getTypes().get(schemaPos);
Type unionType = unioSchema.getType();
value = getSuperValue(field, unioSchema, unionType);
break;
Object memberValue = null;
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
memberValue = fromByteBuffer(fieldSchema, hColumn.getValue());
}
value = memberValue;
return value;
}
private Integer getUnionIndex(String fieldName, HColumn<ByteBuffer, ByteBuffer> uc){
Integer val = IntegerSerializer.get().fromByteBuffer(uc.getValue());
return Integer.parseInt(val.toString());
}
private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName,
List<HColumn<ByteBuffer, ByteBuffer>> columns) {
return getUnionTypeColumn(fieldName, columns.toArray());
}
private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName, Object[] hColumns) {
@SuppressWarnings("unchecked")
HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>)hColumns[iCnt];
String columnName = StringSerializer.get().fromByteBuffer(hColumn.getNameBytes().duplicate());
if (fieldName.equals(columnName))
return hColumn;
}
return null;
}
public Object getValue() {
Field field = getField();
Schema fieldSchema = field.schema();
Type type = fieldSchema.getType();
Object value = getSuperValue(field, fieldSchema, type);
import java.util.Map;
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.BytesArraySerializer;
import me.prettyprint.cassandra.serializers.ObjectSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.Persistent;
serializer = CharSequenceSerializer.get();
serializer = ListSerializer.get(schema);
} else if (value instanceof Map) {
Map map = (Map)value;
serializer = MapSerializer.get(schema);
} else if (value instanceof Persistent){
serializer = ObjectSerializer.get();
}
else {
serializer = CharSequenceSerializer.get();
if (type.equals(Type.STRING)) {
serializer = CharSequenceSerializer.get();
} else if (type.equals(Type.BOOLEAN)) {
} else if (type.equals(Type.BYTES)) {
} else if (type.equals(Type.DOUBLE)) {
} else if (type.equals(Type.FLOAT)) {
} else if (type.equals(Type.INT)) {
} else if (type.equals(Type.LONG)) {
} else if (type.equals(Type.FIXED)) {
} else if (type.equals(Type.ARRAY)) {
serializer = ListSerializer.get(schema.getElementType());
} else if (type.equals(Type.MAP)) {
serializer = MapSerializer.get(schema.getValueType());
} else if (type.equals(Type.UNION)){
} else if (type.equals(Type.RECORD)){
serializer = BytesArraySerializer.get();
serializer = CharSequenceSerializer.get();
serializer = ListSerializer.get(elementType);
serializer = MapSerializer.get(elementType);
import java.util.List;
import java.util.Map;
public static Class<? extends Object> getClass(Object value) {
return Schema.createArray( getElementSchema((GenericArray<?>)value) );
} else if (clazz.isAssignableFrom(List.class)) {
} else if (clazz.isAssignableFrom(Map.class)) {
public static Class<?> getClass(Type type) {
return List.class;
return Map.class;
public static Schema getSchema(Class<?> clazz) {
public static Class<?> getClass(Schema schema) {
public static int getFixedSize(Class<?> clazz) {
public static Schema getElementSchema(GenericArray<?> array) {
this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
"org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
clmap.put("ColumnFamily", HConsistencyLevel.ONE);
configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);
", not BytesType. It may cause a fatal error on column validation later.");
if (ttlAttr == null)
public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
synchronized(mutator) {
HectorUtils.deleteColumn(mutator, key, familyName, columnName);
}
}
public void deleteByKey(K key) {
Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
deleteColumn(key, familyMap.values().iterator().next().toString(), null);
}
if (ttlAttr == null)
public void deleteSubColumn(K key, String fieldName) {
String columnFamily = this.cassandraMapping.getFamily(fieldName);
String superColumnName = this.cassandraMapping.getColumn(fieldName);
synchronized(mutator) {
HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, null);
}
public void deleteGenericArray(K key, String fieldName) {
deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
}
if (((List<?>)itemValue).size() == 0) {
} else if (itemValue instanceof Map<?,?>) {
if (((Map<?, ?>)itemValue).size() == 0) {
public void deleteStatefulHashMap(K key, String fieldName) {
deleteSubColumn(key, fieldName);
} else {
deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
}
}
public void addStatefulHashMap(K key, String fieldName, Map<CharSequence,Object> map) {
if (isSuper( cassandraMapping.getFamily(fieldName) )) {
deleteSubColumn(key, fieldName);
if (!map.isEmpty()) {
for (CharSequence mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
if (mapValue instanceof GenericArray<?>) {
if (((List<?>)mapValue).size() == 0) {
continue;
}
} else if (mapValue instanceof Map<?,?>) {
if (((Map<?, ?>)mapValue).size() == 0) {
continue;
}
addSubColumn(key, fieldName, mapKey.toString(), mapValue);
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
family = this.cassandraMapping.getFamily(pField.substring(0,pField.indexOf(CassandraStore.UNION_COL_SUFIX)));
else
family = this.cassandraMapping.getFamily(pField);
return family;
}
if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
column = pField;
else
column = this.cassandraMapping.getColumn(pField);
return column;
}
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
return this.cassandraMapping.getKeyspaceName();
import java.util.HashMap;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.generic.GenericData.Array;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.commons.lang.ArrayUtils;
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
public static String UNION_COL_SUFIX = "_UnionIndex";
public static final ThreadLocal<BinaryEncoder> encoders =
new ThreadLocal<BinaryEncoder>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap =
new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
CassandraResultSet<K> cassandraResultSet) {
cassandraRow = new CassandraRow<K>();
@SuppressWarnings("unchecked")
addOrUpdateField(key, field, field.schema(), value.get(field.pos()));
if (fields == null){
fields = this.getFields();
}
ArrayList<String> unionFields = new ArrayList<String>();
for (String field: fields){
Field schemaField =this.fieldMap.get(field);
Type type = schemaField.schema().getType();
if (type.getName().equals("UNION".toLowerCase())){
}
}
String[] arr = unionFields.toArray(new String[unionFields.size()]);
String[] both = (String[]) ArrayUtils.addAll(fields, arr);
query.setFields(both);
@SuppressWarnings("unchecked")
T p = (T) SpecificData.get().newRecord(value, schema);
List<Field> fields = schema.getFields();
if (!value.isDirty(i)) {
continue;
Field field = fields.get(i);
Type type = field.schema().getType();
Object fieldValue = value.get(field.pos());
Schema fieldSchema = field.schema();
fieldValue = getFieldValue(fieldSchema, type, fieldValue);
p.put(field.pos(), fieldValue);
private Object getFieldValue(Schema fieldSchema, Type type, Object fieldValue ){
switch(type) {
case RECORD:
Persistent persistent = (Persistent) fieldValue;
Persistent newRecord = (Persistent) SpecificData.get().newRecord(persistent, persistent.getSchema());
for (Field member: fieldSchema.getFields()) {
if (member.pos() == 0 || !persistent.isDirty()) {
continue;
}
Schema memberSchema = member.schema();
Type memberType = memberSchema.getType();
Object memberValue = persistent.get(member.pos());
newRecord.put(member.pos(), getFieldValue(memberSchema, memberType, memberValue));
}
fieldValue = newRecord;
break;
case MAP:
Map<?, ?> map = (Map<?, ?>) fieldValue;
fieldValue = map;
break;
case ARRAY:
fieldValue = (List<?>) fieldValue;
break;
case UNION:
if (fieldValue != null){
int schemaPos = getUnionSchema(fieldValue,fieldSchema);
Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
Type unionType = unionSchema.getType();
fieldValue = getFieldValue(unionSchema, unionType, fieldValue);
}
break;
default:
break;
}
return fieldValue;
}
@SuppressWarnings({ "unchecked", "rawtypes" })
private void addOrUpdateField(K key, Field field, Schema schema, Object value) {
if (field.name().indexOf(CassandraStore.UNION_COL_SUFIX) < 0){
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
try {
byte[] byteValue = AvroSerializerUtil.serializer(persistentBase, schema);
this.cassandraClient.addColumn(key, field.name(), byteValue);
} catch (IOException e) {
} else {
String familyName =  this.cassandraClient.getCassandraMapping().getFamily(field.name());
this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
break;
case MAP:
if (value != null) {
if (value instanceof Map<?, ?>) {
Map<CharSequence,Object> map = (Map<CharSequence,Object>)value;
Schema valueSchema = schema.getValueType();
Type valueType = valueSchema.getType();
if (Type.UNION.equals(valueType)){
Map<CharSequence,Object> valueMap = new HashMap<CharSequence, Object>();
for (CharSequence mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
int valueUnionIndex = getUnionSchema(mapValue, valueSchema);
valueMap.put(mapKey, mapValue);
}
map = valueMap;
}
String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
if (!this.cassandraClient.isSuper( familyName )){
try {
byte[] byteValue = AvroSerializerUtil.serializer(map, schema);
this.cassandraClient.addColumn(key, field.name(), byteValue);
} catch (IOException e) {
}
}else{
this.cassandraClient.addStatefulHashMap(key, field.name(), map);
}
} else {
}
this.cassandraClient.deleteStatefulHashMap(key, field.name());
break;
case ARRAY:
if (value != null) {
if (value instanceof DirtyListWrapper<?>) {
DirtyListWrapper fieldValue = (DirtyListWrapper<?>)value;
GenericArray valueArray = new Array(fieldValue.size(), schema);
valueArray.add(i, fieldValue.get(i));
}
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray<?>)valueArray);
} else {
}
this.cassandraClient.deleteGenericArray(key, field.name());
break;
case UNION:
String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
if(value != null) {
int schemaPos = getUnionSchema(value, schema);
this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
if (this.cassandraClient.isSuper( familyName )){
this.cassandraClient.addSubColumn(key, columnName, columnName, schemaPos);
}else{
this.cassandraClient.addColumn(key, columnName, schemaPos);
}
Schema unionSchema = schema.getTypes().get(schemaPos);
addOrUpdateField(key, field, unionSchema, value);
} else {
if (this.cassandraClient.isSuper( familyName )){
this.cassandraClient.deleteSubColumn(key, field.name());
} else {
this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
}
}
break;
default:
Type schemaType = it.next().getType();
if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
else if (pValue instanceof Integer && schemaType.equals(Type.INT))
else if (pValue instanceof Long && schemaType.equals(Type.LONG))
else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
return unionSchemaPos;
else if (pValue instanceof Map && schemaType.equals(Type.MAP))
return unionSchemaPos;
else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
return unionSchemaPos;
else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
return DEFAULT_UNION_SCHEMA;
mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
import java.util.ArrayList;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
throws IOException {
try{
WebPage page;
log.info("creating web page data");
page = WebPage.newBuilder().build();
page.setUrl(new Utf8(URLS[i]));
page.setParsedContent(new ArrayList<CharSequence>());
if (CONTENTS[i]!=null){
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
for(String token : CONTENTS[i].split(" ")) {
page.getParsedContent().add(new Utf8(token));
}
page.getOutlinks().put(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
}
Metadata metadata = Metadata.newBuilder().build();
metadata.setVersion(1);
metadata.getData().put(new Utf8("metakey"), new Utf8("metavalue"));
page.setMetadata(metadata);
dataStore.put(URLS[i], page);
}
dataStore.flush();
log.info("finished creating web page data");
}
catch(Exception e){
log.info("error creating web page data");
}
package org.apache.gora.examples.generated;
public class Employee extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}]}");
__G__DIRTY(0, "__g__dirty"),
NAME(1, "name"),
DATE_OF_BIRTH(2, "dateOfBirth"),
SSN(3, "ssn"),
SALARY(4, "salary"),
BOSS(5, "boss"),
WEBPAGE(6, "webpage"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"name",
"dateOfBirth",
"ssn",
"salary",
"boss",
"webpage",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence name;
private java.lang.CharSequence ssn;
private java.lang.Object boss;
private org.apache.gora.examples.generated.WebPage webpage;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return name;
case 2: return dateOfBirth;
case 3: return ssn;
case 4: return salary;
case 5: return boss;
case 6: return webpage;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: name = (java.lang.CharSequence)(value); break;
case 2: dateOfBirth = (java.lang.Long)(value); break;
case 3: ssn = (java.lang.CharSequence)(value); break;
case 4: salary = (java.lang.Integer)(value); break;
case 5: boss = (java.lang.Object)(value); break;
case 6: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.CharSequence getName() {
return name;
}
public void setName(java.lang.CharSequence value) {
this.name = value;
setDirty(1);
}
public boolean isNameDirty(java.lang.CharSequence value) {
return isDirty(1);
}
public java.lang.Long getDateOfBirth() {
return dateOfBirth;
}
public void setDateOfBirth(java.lang.Long value) {
this.dateOfBirth = value;
setDirty(2);
}
public boolean isDateOfBirthDirty(java.lang.Long value) {
return isDirty(2);
}
public java.lang.CharSequence getSsn() {
return ssn;
}
public void setSsn(java.lang.CharSequence value) {
this.ssn = value;
setDirty(3);
}
public boolean isSsnDirty(java.lang.CharSequence value) {
return isDirty(3);
}
public java.lang.Integer getSalary() {
return salary;
}
public void setSalary(java.lang.Integer value) {
this.salary = value;
setDirty(4);
}
public boolean isSalaryDirty(java.lang.Integer value) {
return isDirty(4);
}
public java.lang.Object getBoss() {
return boss;
}
public void setBoss(java.lang.Object value) {
this.boss = value;
setDirty(5);
}
public boolean isBossDirty(java.lang.Object value) {
return isDirty(5);
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
return webpage;
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
this.webpage = value;
setDirty(6);
}
public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
return isDirty(6);
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder() {
return new org.apache.gora.examples.generated.Employee.Builder();
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee.Builder other) {
return new org.apache.gora.examples.generated.Employee.Builder(other);
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee other) {
return new org.apache.gora.examples.generated.Employee.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Employee>
implements org.apache.avro.data.RecordBuilder<Employee> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence name;
private long dateOfBirth;
private java.lang.CharSequence ssn;
private int salary;
private java.lang.Object boss;
private org.apache.gora.examples.generated.WebPage webpage;
private Builder() {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.Employee.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.Employee other) {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.name)) {
this.name = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.name);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.dateOfBirth)) {
this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.dateOfBirth);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.ssn)) {
this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ssn);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.salary)) {
this.salary = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.salary);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.boss)) {
this.boss = (java.lang.Object) data().deepCopy(fields()[5].schema(), other.boss);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.webpage)) {
this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[6].schema(), other.webpage);
fieldSetFlags()[6] = true;
}
}
public java.lang.CharSequence getName() {
return name;
}
public org.apache.gora.examples.generated.Employee.Builder setName(java.lang.CharSequence value) {
validate(fields()[1], value);
this.name = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasName() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.Employee.Builder clearName() {
name = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getDateOfBirth() {
return dateOfBirth;
}
public org.apache.gora.examples.generated.Employee.Builder setDateOfBirth(long value) {
validate(fields()[2], value);
this.dateOfBirth = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasDateOfBirth() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.Employee.Builder clearDateOfBirth() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.CharSequence getSsn() {
return ssn;
}
public org.apache.gora.examples.generated.Employee.Builder setSsn(java.lang.CharSequence value) {
validate(fields()[3], value);
this.ssn = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasSsn() {
return fieldSetFlags()[3];
}
public org.apache.gora.examples.generated.Employee.Builder clearSsn() {
ssn = null;
fieldSetFlags()[3] = false;
return this;
}
public java.lang.Integer getSalary() {
return salary;
}
public org.apache.gora.examples.generated.Employee.Builder setSalary(int value) {
validate(fields()[4], value);
this.salary = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasSalary() {
return fieldSetFlags()[4];
}
public org.apache.gora.examples.generated.Employee.Builder clearSalary() {
fieldSetFlags()[4] = false;
return this;
}
public java.lang.Object getBoss() {
return boss;
}
public org.apache.gora.examples.generated.Employee.Builder setBoss(java.lang.Object value) {
validate(fields()[5], value);
this.boss = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasBoss() {
return fieldSetFlags()[5];
}
public org.apache.gora.examples.generated.Employee.Builder clearBoss() {
boss = null;
fieldSetFlags()[5] = false;
return this;
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
return webpage;
}
public org.apache.gora.examples.generated.Employee.Builder setWebpage(org.apache.gora.examples.generated.WebPage value) {
validate(fields()[6], value);
this.webpage = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasWebpage() {
return fieldSetFlags()[6];
}
public org.apache.gora.examples.generated.Employee.Builder clearWebpage() {
webpage = null;
fieldSetFlags()[6] = false;
return this;
}
@Override
public Employee build() {
try {
Employee record = new Employee();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.name = fieldSetFlags()[1] ? this.name : (java.lang.CharSequence) defaultValue(fields()[1]);
record.dateOfBirth = fieldSetFlags()[2] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[2]);
record.ssn = fieldSetFlags()[3] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[3]);
record.salary = fieldSetFlags()[4] ? this.salary : (java.lang.Integer) defaultValue(fields()[4]);
record.boss = fieldSetFlags()[5] ? this.boss : (java.lang.Object) defaultValue(fields()[5]);
record.webpage = fieldSetFlags()[6] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[6]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public Employee.Tombstone getTombstone(){
return TOMBSTONE;
}
public Employee newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getName() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setName(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isNameDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getDateOfBirth() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setDateOfBirth(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDateOfBirthDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getSsn() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSsn(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSsnDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getSalary() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSalary(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSalaryDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Object getBoss() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setBoss(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isBossDirty(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class Metadata extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");
__G__DIRTY(0, "__g__dirty"),
VERSION(1, "version"),
DATA(2, "data"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"version",
"data",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return version;
case 2: return data;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: version = (java.lang.Integer)(value); break;
case 2: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.Integer getVersion() {
return version;
}
public void setVersion(java.lang.Integer value) {
this.version = value;
setDirty(1);
}
public boolean isVersionDirty(java.lang.Integer value) {
return isDirty(1);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
return data;
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.data = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(2);
}
public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(2);
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder() {
return new org.apache.gora.examples.generated.Metadata.Builder();
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata.Builder other) {
return new org.apache.gora.examples.generated.Metadata.Builder(other);
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata other) {
return new org.apache.gora.examples.generated.Metadata.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Metadata>
implements org.apache.avro.data.RecordBuilder<Metadata> {
private java.nio.ByteBuffer __g__dirty;
private int version;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
private Builder() {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.Metadata.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.Metadata other) {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.version)) {
this.version = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.version);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.data)) {
this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.data);
fieldSetFlags()[2] = true;
}
}
public java.lang.Integer getVersion() {
return version;
}
public org.apache.gora.examples.generated.Metadata.Builder setVersion(int value) {
validate(fields()[1], value);
this.version = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasVersion() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.Metadata.Builder clearVersion() {
fieldSetFlags()[1] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
return data;
}
public org.apache.gora.examples.generated.Metadata.Builder setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[2], value);
this.data = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasData() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.Metadata.Builder clearData() {
data = null;
fieldSetFlags()[2] = false;
return this;
}
@Override
public Metadata build() {
try {
Metadata record = new Metadata();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.version = fieldSetFlags()[1] ? this.version : (java.lang.Integer) defaultValue(fields()[1]);
record.data = fieldSetFlags()[2] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[2]));
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public Metadata.Tombstone getTombstone(){
return TOMBSTONE;
}
public Metadata newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getVersion() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setVersion(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isVersionDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class TokenDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");
__G__DIRTY(0, "__g__dirty"),
COUNT(1, "count"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"count",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: count = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.Integer getCount() {
return count;
}
public void setCount(java.lang.Integer value) {
this.count = value;
setDirty(1);
}
public boolean isCountDirty(java.lang.Integer value) {
return isDirty(1);
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder() {
return new org.apache.gora.examples.generated.TokenDatum.Builder();
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum other) {
return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<TokenDatum>
implements org.apache.avro.data.RecordBuilder<TokenDatum> {
private java.nio.ByteBuffer __g__dirty;
private int count;
private Builder() {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.TokenDatum other) {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.count)) {
this.count = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.count);
fieldSetFlags()[1] = true;
}
}
public java.lang.Integer getCount() {
return count;
}
public org.apache.gora.examples.generated.TokenDatum.Builder setCount(int value) {
validate(fields()[1], value);
this.count = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasCount() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.TokenDatum.Builder clearCount() {
fieldSetFlags()[1] = false;
return this;
}
@Override
public TokenDatum build() {
try {
TokenDatum record = new TokenDatum();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.count = fieldSetFlags()[1] ? this.count : (java.lang.Integer) defaultValue(fields()[1]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public TokenDatum.Tombstone getTombstone(){
return TOMBSTONE;
}
public TokenDatum newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class WebPage extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");
__G__DIRTY(0, "__g__dirty"),
URL(1, "url"),
CONTENT(2, "content"),
PARSED_CONTENT(3, "parsedContent"),
OUTLINKS(4, "outlinks"),
HEADERS(5, "headers"),
METADATA(6, "metadata"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"url",
"content",
"parsedContent",
"outlinks",
"headers",
"metadata",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence url;
private java.nio.ByteBuffer content;
private java.util.List<java.lang.CharSequence> parsedContent;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
private org.apache.gora.examples.generated.Metadata metadata;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return url;
case 2: return content;
case 3: return parsedContent;
case 4: return outlinks;
case 5: return headers;
case 6: return metadata;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: url = (java.lang.CharSequence)(value); break;
case 2: content = (java.nio.ByteBuffer)(value); break;
case 3: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
case 4: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 5: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
case 6: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public void setUrl(java.lang.CharSequence value) {
this.url = value;
setDirty(1);
}
public boolean isUrlDirty(java.lang.CharSequence value) {
return isDirty(1);
}
public java.nio.ByteBuffer getContent() {
return content;
}
public void setContent(java.nio.ByteBuffer value) {
this.content = value;
setDirty(2);
}
public boolean isContentDirty(java.nio.ByteBuffer value) {
return isDirty(2);
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
return parsedContent;
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
this.parsedContent = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper(value);
setDirty(3);
}
public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
return isDirty(3);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
return outlinks;
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.outlinks = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(4);
}
public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(4);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
return headers;
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.headers = value;
setDirty(5);
}
public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(5);
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
return metadata;
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
this.metadata = value;
setDirty(6);
}
public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
return isDirty(6);
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder() {
return new org.apache.gora.examples.generated.WebPage.Builder();
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage.Builder other) {
return new org.apache.gora.examples.generated.WebPage.Builder(other);
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage other) {
return new org.apache.gora.examples.generated.WebPage.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<WebPage>
implements org.apache.avro.data.RecordBuilder<WebPage> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence url;
private java.nio.ByteBuffer content;
private java.util.List<java.lang.CharSequence> parsedContent;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
private org.apache.gora.examples.generated.Metadata metadata;
private Builder() {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.WebPage.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.WebPage other) {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.content)) {
this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[2].schema(), other.content);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.parsedContent)) {
this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.parsedContent);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.outlinks)) {
this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.outlinks);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.headers)) {
this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[5].schema(), other.headers);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.metadata)) {
this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[6].schema(), other.metadata);
fieldSetFlags()[6] = true;
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public org.apache.gora.examples.generated.WebPage.Builder setUrl(java.lang.CharSequence value) {
validate(fields()[1], value);
this.url = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasUrl() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.WebPage.Builder clearUrl() {
url = null;
fieldSetFlags()[1] = false;
return this;
}
public java.nio.ByteBuffer getContent() {
return content;
}
public org.apache.gora.examples.generated.WebPage.Builder setContent(java.nio.ByteBuffer value) {
validate(fields()[2], value);
this.content = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasContent() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.WebPage.Builder clearContent() {
content = null;
fieldSetFlags()[2] = false;
return this;
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
return parsedContent;
}
public org.apache.gora.examples.generated.WebPage.Builder setParsedContent(java.util.List<java.lang.CharSequence> value) {
validate(fields()[3], value);
this.parsedContent = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasParsedContent() {
return fieldSetFlags()[3];
}
public org.apache.gora.examples.generated.WebPage.Builder clearParsedContent() {
parsedContent = null;
fieldSetFlags()[3] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
return outlinks;
}
public org.apache.gora.examples.generated.WebPage.Builder setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[4], value);
this.outlinks = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasOutlinks() {
return fieldSetFlags()[4];
}
public org.apache.gora.examples.generated.WebPage.Builder clearOutlinks() {
outlinks = null;
fieldSetFlags()[4] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
return headers;
}
public org.apache.gora.examples.generated.WebPage.Builder setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[5], value);
this.headers = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasHeaders() {
return fieldSetFlags()[5];
}
public org.apache.gora.examples.generated.WebPage.Builder clearHeaders() {
headers = null;
fieldSetFlags()[5] = false;
return this;
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
return metadata;
}
public org.apache.gora.examples.generated.WebPage.Builder setMetadata(org.apache.gora.examples.generated.Metadata value) {
validate(fields()[6], value);
this.metadata = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasMetadata() {
return fieldSetFlags()[6];
}
public org.apache.gora.examples.generated.WebPage.Builder clearMetadata() {
metadata = null;
fieldSetFlags()[6] = false;
return this;
}
@Override
public WebPage build() {
try {
WebPage record = new WebPage();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
record.content = fieldSetFlags()[2] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[2]);
record.parsedContent = fieldSetFlags()[3] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[3]));
record.outlinks = fieldSetFlags()[4] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[4]));
record.headers = fieldSetFlags()[5] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[5]);
record.metadata = fieldSetFlags()[6] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public WebPage.Tombstone getTombstone(){
return TOMBSTONE;
}
public WebPage newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.nio.ByteBuffer getContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setContent(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isContentDirty(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
return EncoderFactory.get().binaryEncoder(getOrCreateOutputStream(), null);
return EncoderFactory.get().jsonEncoder(schema, getOrCreateOutputStream());
return DecoderFactory.get().binaryDecoder(getOrCreateInputStream(), null);
return DecoderFactory.get().jsonDecoder(schema, getOrCreateInputStream());
int fieldIndex = persistent.getSchema().getField(fieldName).pos();
@SuppressWarnings("unchecked")
import org.apache.avro.specific.SpecificDatumReader;
implements Deserializer<Persistent> {
private Class<? extends Persistent> persistentClass;
private SpecificDatumReader<Persistent> datumReader;
public PersistentDeserializer(Class<? extends Persistent> c, boolean reuseObjects) {
datumReader = new SpecificDatumReader<Persistent>(schema);
@Override
decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);
@Override
public Persistent deserialize(Persistent persistent) throws IOException {
public class PersistentSerialization
implements Serialization<Persistent> {
public Deserializer<Persistent> getDeserializer(Class<Persistent> c) {
public Serializer<Persistent> getSerializer(Class<Persistent> c) {
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
public class PersistentSerializer implements Serializer<Persistent> {
private SpecificDatumWriter<Persistent> datumWriter;
private BinaryEncoder encoder;
this.datumWriter = new SpecificDatumWriter<Persistent>();
@Override
@Override
encoder = EncoderFactory.get().directBinaryEncoder(out, null);
public void serialize(Persistent persistent) throws IOException {
import org.apache.avro.Schema.Field;
import org.apache.gora.util.AvroUtils;
try{
long deletedRows = 0;
Result<K,T> result = query.execute();
while(result.next()) {
if(delete(result.getKey()))
}
return 0;
}
catch(Exception e){
return 0;
}
List<Field> otherFields = obj.getSchema().getFields();
String[] otherFieldStrings = new String[otherFields.size()];
otherFieldStrings[i] = otherFields.get(i).name();
}
if(Arrays.equals(fields, otherFieldStrings)) {
T newObj = (T) AvroUtils.deepClonePersistent(obj);
int index = otherFields.get(i).pos();
newObj.put(index, obj.get(index));
import java.util.List;
import org.apache.avro.Schema.Field;
import org.apache.avro.specific.SpecificRecord;
import org.apache.gora.persistency.Dirtyable;
public interface Persistent extends SpecificRecord, Dirtyable {
public static String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
public abstract Tombstone getTombstone();
public List<Field> getUnmanagedFields();
Persistent newInstance();
return keyClass.newInstance();
return (T) persistent.newInstance();
import java.util.Collection;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.gora.persistency.Dirtyable;
public abstract class PersistentBase extends SpecificRecordBase implements
Persistent {
public static class PersistentData extends SpecificData {
private static final PersistentData INSTANCE = new PersistentData();
public static PersistentData get() {
return INSTANCE;
public boolean equals(SpecificRecord obj1, SpecificRecord that) {
if (that == obj1)
if (!(that instanceof SpecificRecord))
if (obj1.getClass() != that.getClass())
return PersistentData.get().compare(obj1, that, obj1.getSchema(), true) == 0;
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
dirtyBytes.put(i, (byte) 0);
for (Field field : getSchema().getFields()) {
clearDirynessIfFieldIsDirtyable(field.pos());
}
private void clearDirynessIfFieldIsDirtyable(int fieldIndex) {
if (fieldIndex == 0)
return;
Object value = get(fieldIndex);
if (value instanceof Dirtyable) {
((Dirtyable) value).clearDirty();
public void clearDirty(int fieldIndex) {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
currentByte = (byte) ((~(1 << bitOffset)) & currentByte);
dirtyBytes.put(byteOffset, currentByte);
clearDirynessIfFieldIsDirtyable(fieldIndex);
@Override
public void clearDirty(String field) {
clearDirty(getSchema().getField(field).pos());
}
@Override
public boolean isDirty() {
boolean isSubRecordDirty = false;
for (Field field : fields) {
isSubRecordDirty = isSubRecordDirty || checkIfMutableFieldAndDirty(field);
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
boolean dirty = false;
dirty = dirty || dirtyBytes.get(i) != 0;
}
return isSubRecordDirty || dirty;
}
private boolean checkIfMutableFieldAndDirty(Field field) {
if (field.pos() == 0)
return false;
switch (field.schema().getType()) {
case RECORD:
case MAP:
case ARRAY:
Object value = get(field.pos());
return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
case UNION:
value = get(field.pos());
return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
default:
break;
}
return false;
}
@Override
public boolean isDirty(int fieldIndex) {
Field field = getSchema().getFields().get(fieldIndex);
boolean isSubRecordDirty = checkIfMutableFieldAndDirty(field);
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
return isSubRecordDirty || 0 != ((1 << bitOffset) & currentByte);
}
@Override
public boolean isDirty(String fieldName) {
Field field = getSchema().getField(fieldName);
if(field == null){
throw new IndexOutOfBoundsException
}
return isDirty(field.pos());
}
@Override
public void setDirty() {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
dirtyBytes.put(i, (byte) -128);
}
}
@Override
public void setDirty(int fieldIndex) {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
currentByte = (byte) ((1 << bitOffset) | currentByte);
dirtyBytes.put(byteOffset, currentByte);
}
@Override
public void setDirty(String field) {
setDirty(getSchema().getField(field).pos());
}
private ByteBuffer getDirtyBytes() {
return (ByteBuffer) get(0);
}
@Override
public void clear() {
Collection<Field> unmanagedFields = getUnmanagedFields();
for (Field field : getSchema().getFields()) {
if (!unmanagedFields.contains(field))
continue;
put(field.pos(), PersistentData.get().deepCopy(field.schema(), PersistentData.get().getDefaultValue(field)));
}
clearDirty();
}
@Override
public boolean equals(Object that) {
if (that == this) {
return true;
} else if (that instanceof Persistent) {
return PersistentData.get().equals(this, (SpecificRecord) that);
} else {
return false;
}
public List<Field> getUnmanagedFields(){
List<Field> fields = getSchema().getFields();
return fields.subList(1, fields.size());
return keyClass.newInstance();
try {
return (T) persistentClass.newInstance();
} catch (InstantiationException e) {
throw new RuntimeException(e);
} catch (IllegalAccessException e) {
e.printStackTrace();
throw new RuntimeException(e);
}
private void clearReadable() {
@Override
return true;
return true;
public static final String SCHEMA_NAME = "schema.name";
Properties properties = new Properties();
.getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
throws GoraException {
ReflectionUtils.newInstance(dataStoreClass);
throws GoraException {
throws GoraException {
= (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
throws GoraException {
import java.util.ArrayList;
import java.util.List;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
protected SpecificDatumReader<T> datumReader;
protected SpecificDatumWriter<T> datumWriter;
datumReader = new SpecificDatumReader<T>(schema);
datumWriter = new SpecificDatumWriter<T>(schema);
return getFields();
}
protected String[] getFields() {
List<Field> schemaFields = beanFactory.getCachedPersistent().getSchema().getFields();
List<Field> list = new ArrayList<Field>();
for (Field field : schemaFields) {
if (!Persistent.DIRTY_BYTES_FIELD_NAME.equalsIgnoreCase(field.name())) {
list.add(field);
}
}
schemaFields = list;
String[] fieldNames = new String[schemaFields.size()];
fieldNames[i] = schemaFields.get(i).name();
}
return fieldNames;
String confSchemaName = getOrCreateConf().get("preferred.schema.name");
if (confSchemaName != null) {
return confSchemaName;
}
return schemaName;
return mappingSchemaName;
return StringUtils.getClassname(persistentClass);
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
for (Field field : fields) {
public static Schema getSchema(Class<? extends Persistent> clazz)
throws SecurityException, NoSuchFieldException, IllegalArgumentException,
IllegalAccessException {
java.lang.reflect.Field field = clazz.getDeclaredField("SCHEMA$");
public static String[] getPersistentFieldNames(Persistent persistent) {
return getSchemaFieldNames(persistent.getSchema());
}
public static String[] getSchemaFieldNames(Schema schema) {
List<Field> fields = schema.getFields();
String[] fieldNames = new String[fields.size() - 1];
}
return fieldNames;
}
public static <T extends Persistent> T deepClonePersistent(T persistent) {
ByteArrayOutputStream bos = new ByteArrayOutputStream();
BinaryEncoder enc = EncoderFactory.get().binaryEncoder(bos, null);
SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<Persistent>(
persistent.getSchema());
try {
writer.write(persistent, enc);
} catch (IOException e) {
throw new RuntimeException(
"Unable to serialize avro object to byte buffer - "
"please report this issue to the Gora bugtracker "
"or your administrator.");
}
byte[] value = bos.toByteArray();
Decoder dec = DecoderFactory.get().binaryDecoder(value, null);
@SuppressWarnings("unchecked")
SpecificDatumReader<T> reader = new SpecificDatumReader<T>(
(Class<T>) persistent.getClass());
try {
return reader.read(null, dec);
} catch (IOException e) {
throw new RuntimeException(
"Unable to deserialize avro object from byte buffer - "
"please report this issue to the Gora bugtracker "
"or your administrator.");
}
}
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
public static <T> T fromBytes( byte[] val, Schema schema
, SpecificDatumReader<T> datumReader, T object)
return (T)Enum.valueOf(ReflectData.get().getClass(schema), symbol);
case STRING:  return (T)new Utf8(toString(val));
case BYTES:   return (T)ByteBuffer.wrap(val);
case INT:     return (T)Integer.valueOf(bytesToVint(val));
case LONG:    return (T)Long.valueOf(bytesToVlong(val));
case FLOAT:   return (T)Float.valueOf(toFloat(val));
case DOUBLE:  return (T)Double.valueOf(toDouble(val));
case BOOLEAN: return (T)Boolean.valueOf(val[0] != 0);
case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, schema, (SpecificRecord)object);
@SuppressWarnings("unchecked")
public static <T> byte[] toBytes(T o, Schema schema
, SpecificDatumWriter<T> datumWriter)
case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, schema, (SpecificRecord)o);
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.util.ByteBufferInputStream;
import org.apache.avro.util.ByteBufferOutputStream;
public static<T extends SpecificRecord> void serialize(OutputStream os,
SpecificDatumWriter<T> datumWriter, Schema schema, T object)
BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
datumWriter.write(object, encoder);
public static<T> void serialize(OutputStream os,
SpecificDatumWriter<T> datumWriter, Schema schema, T object)
throws IOException {
BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
datumWriter.write(object, encoder);
encoder.flush();
}
public static<T extends SpecificRecord> byte[] serialize(SpecificDatumWriter<T> datumWriter
, Schema schema, T object) throws IOException {
public static<T> byte[] serialize(SpecificDatumWriter<T> datumWriter
, Schema schema, T object) throws IOException {
ByteArrayOutputStream os = new ByteArrayOutputStream();
serialize(os, datumWriter, schema, object);
return os.toByteArray();
}
public static<K, T extends SpecificRecord> T deserialize(InputStream is,
SpecificDatumReader<T> datumReader, Schema schema, T object)
decoder = DecoderFactory.get().binaryDecoder(is, decoder);
return (T)datumReader.read(object, decoder);
public static<K, T extends SpecificRecord> T deserialize(byte[] bytes,
SpecificDatumReader<T> datumReader, Schema schema, T object)
decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
return (T)datumReader.read(object, decoder);
public static<K, T> T deserialize(byte[] bytes,
SpecificDatumReader<T> datumReader, Schema schema, T object)
throws IOException {
decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
return (T)datumReader.read(object, decoder);
import java.lang.reflect.Method;
import org.apache.avro.specific.SpecificRecordBuilderBase;
import org.apache.gora.persistency.Persistent;
public static <T extends Persistent> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) throws SecurityException
, NoSuchMethodException, IllegalArgumentException, IllegalAccessException, InvocationTargetException {
return (SpecificRecordBuilderBase<T>) clazz.getMethod("newBuilder").invoke(null);
}
import org.apache.gora.persistency.Tombstone;
public Tombstone getTombstone() {
return new Tombstone(){};
public Persistent newInstance() {
return new MockPersistent();
preferredSchema = properties.getProperty(PREF_SCH_NAME);
dynamoDBClient = getClient(properties.getProperty(CLI_TYP_PROP),(AWSCredentials)getConf());
dynamoDBClient.setEndpoint(properties.getProperty(ENDPOINT_PROP));
consistency = properties.getProperty(CONSISTENCY_READS);
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
try {
List<Field> fields = schema.getFields();
if (!persistent.isDirty(i)) {
Field field = fields.get(i);
throw new RuntimeException("HBase mapping for field ["
"] not found. Wrong gora-hbase-mapping.xml?");
addPutsAndDeletes(put, delete, o, field.schema().getType(),
field.schema(), hcol, hcol.getQualifier());
if (put.size() > 0) {
if (delete.size() > 0) {
table.delete(delete);
} catch (IOException ex2) {
private void addPutsAndDeletes(Put put, Delete delete, Object o, Type type,
Schema schema, HBaseColumn hcol, byte[] qualifier) throws IOException {
switch (type) {
case UNION:
if (isNullable(schema) && o == null) {
if (qualifier == null) {
delete.deleteFamily(hcol.getFamily());
} else {
delete.deleteColumn(hcol.getFamily(), qualifier);
}
} else {
int index = getResolvedUnionIndex(schema);
byte[] serializedBytes = toBytes(o, schema);
put.add(hcol.getFamily(), qualifier, serializedBytes);
} else {
Schema resolvedSchema = schema.getTypes().get(index);
addPutsAndDeletes(put, delete, o, resolvedSchema.getType(),
resolvedSchema, hcol, qualifier);
}
}
break;
case MAP:
if (qualifier == null) {
delete.deleteFamily(hcol.getFamily());
} else {
delete.deleteColumn(hcol.getFamily(), qualifier);
}
@SuppressWarnings({ "rawtypes", "unchecked" })
Set<Entry> set = ((Map) o).entrySet();
for (@SuppressWarnings("rawtypes") Entry entry : set) {
byte[] qual = toBytes(entry.getKey());
addPutsAndDeletes(put, delete, entry.getValue(), schema.getValueType()
.getType(), schema.getValueType(), hcol, qual);
}
break;
case ARRAY:
List<?> array = (List<?>) o;
int j = 0;
for (Object item : array) {
addPutsAndDeletes(put, delete, item, schema.getElementType().getType(),
}
break;
default:
byte[] serializedBytes = toBytes(o, schema);
put.add(hcol.getFamily(), qualifier, serializedBytes);
break;
}
}
private boolean isNullable(Schema unionSchema) {
for (Schema innerSchema : unionSchema.getTypes()) {
if (innerSchema.getType().equals(Schema.Type.NULL)) {
return true;
}
}
return false;
}
boolean isAllFields = Arrays.equals(fields, getFields());
addFamilyOrColumn(get, col, fieldSchema);
private void addFamilyOrColumn(Get get, HBaseColumn col, Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(get, col, resolvedSchema);
break;
case MAP:
case ARRAY:
get.addFamily(col.family);
break;
default:
get.addColumn(col.family, col.qualifier);
break;
}
}
private void addFields(Scan scan, Query<K, T> query) throws IOException {
addFamilyOrColumn(scan, col, fieldSchema);
private void addFamilyOrColumn(Scan scan, HBaseColumn col, Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(scan, col, resolvedSchema);
break;
case MAP:
case ARRAY:
scan.addFamily(col.family);
break;
default:
scan.addColumn(col.family, col.qualifier);
break;
}
}
private void addFields(Delete delete, Query<K, T> query)    throws IOException {
addFamilyOrColumn(delete, col, fieldSchema);
}
}
private void addFamilyOrColumn(Delete delete, HBaseColumn col,
Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(delete, col, resolvedSchema);
break;
case MAP:
case ARRAY:
delete.deleteFamily(col.family);
break;
default:
delete.deleteColumn(col.family, col.qualifier);
break;
setField(result,persistent, col, field, fieldSchema);
}
persistent.clearDirty();
return persistent;
}
private void setField(Result result, T persistent, HBaseColumn col,
Field field, Schema fieldSchema) throws IOException {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
if (val == null) {
return;
}
setField(persistent, field, val);
} else {
Schema resolvedSchema = fieldSchema.getTypes().get(index);
setField(result, persistent, col, field, resolvedSchema);
}
break;
case MAP:
NavigableMap<byte[], byte[]> qualMap = result.getNoVersionMap().get(
col.getFamily());
if (qualMap == null) {
return;
}
Schema valueSchema = fieldSchema.getValueType();
Map<Utf8, Object> map = new HashMap<Utf8, Object>();
for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
map.put(new Utf8(Bytes.toString(e.getKey())),
fromBytes(valueSchema, e.getValue()));
}
setField(persistent, field, map);
break;
case ARRAY:
qualMap = result.getFamilyMap(col.getFamily());
if (qualMap == null) {
return;
}
valueSchema = fieldSchema.getElementType();
ArrayList<Object> arrayList = new ArrayList<Object>();
DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<Object>(arrayList);
for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
dirtyListWrapper.add(fromBytes(valueSchema, e.getValue()));
}
setField(persistent, field, arrayList);
break;
default:
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
if (val == null) {
return;
}
setField(persistent, field, val);
break;
}
}
private int getResolvedUnionIndex(Schema unionScema) {
if (unionScema.getTypes().size() == 2) {
Type type0 = unionScema.getTypes().get(0).getType();
Type type1 = unionScema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {
if (type0.equals(Schema.Type.NULL))
return 1;
else
return 0;
return 2;
persistent.put(field.pos(), new DirtyMapWrapper(map));
@SuppressWarnings({ "rawtypes", "unchecked" })
private void setField(T persistent, Field field, List list) {
persistent.put(field.pos(), new DirtyListWrapper(list));
}
import java.util.Map;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.RowMutations;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
@Override
public void batch(List<? extends Row> actions, Object[] results)
throws IOException, InterruptedException {
getTable().batch(actions, results);
}
@Override
public Object[] batch(List<? extends Row> actions) throws IOException,
InterruptedException {
return getTable().batch(actions);
}
@Override
public void mutateRow(RowMutations rm) throws IOException {
}
@Override
public Result append(Append append) throws IOException {
return null;
}
@Override
public <T extends CoprocessorProtocol> T coprocessorProxy(Class<T> protocol,
byte[] row) {
return null;
}
@Override
public <T extends CoprocessorProtocol, R> Map<byte[], R> coprocessorExec(
Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable)
throws IOException, Throwable {
return null;
}
@Override
public <T extends CoprocessorProtocol, R> void coprocessorExec(
Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable,
Callback<R> callback) throws IOException, Throwable {
}
@Override
public void setAutoFlush(boolean autoFlush) {
}
@Override
public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail) {
}
@Override
public long getWriteBufferSize() {
return 0;
}
@Override
public void setWriteBufferSize(long writeBufferSize) throws IOException {
}
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.io.EncoderFactory;
private static ThreadLocal<ByteArrayOutputStream> outputStream =
new ThreadLocal<ByteArrayOutputStream>();
public static final ThreadLocal<BinaryEncoder> encoders =
new ThreadLocal<BinaryEncoder>();
public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap =
new ConcurrentHashMap<String, SpecificDatumReader<?>>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap =
new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
@SuppressWarnings({ "rawtypes" })
String schemaId = schema.getType().equals(Schema.Type.UNION) ? String.valueOf(schema.hashCode()) : schema.getFullName();
SpecificDatumReader<?> reader = (SpecificDatumReader<?>)readerMap.get(schemaId);
if (reader == null) {
SpecificDatumReader localReader=null;
if((localReader=readerMap.putIfAbsent(schemaId, reader))!=null) {
reader = localReader;
BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(val, null);
return reader.read(null, decoder);
SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap.get(schema.getFullName());
if (writer == null) {
writerMap.put(schema.getFullName(),writer);
BinaryEncoder encoderFromCache = encoders.get();
ByteArrayOutputStream bos = new ByteArrayOutputStream();
outputStream.set(bos);
BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(bos, null);
if (encoderFromCache == null) {
ByteArrayOutputStream os = outputStream.get();
writer.write(o, encoder);
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
private static final Logger LOG = LoggerFactory.getLogger(SolrStore.class);
public static int DEFAULT_UNION_SCHEMA = 0;
public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<String, SpecificDatumReader<?>>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
public void initialize(Class<K> keyClass, Class<T> persistentClass,
Properties properties) {
super.initialize(keyClass, persistentClass, properties);
String mappingFile = DataStoreFactory.getMappingFile(properties, this,
DEFAULT_MAPPING_FILE);
mapping = readMapping(mappingFile);
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
solrServerUrl = DataStoreFactory.findProperty(properties, this,
SOLR_URL_PROPERTY, null);
solrConfig = DataStoreFactory.findProperty(properties, this,
SOLR_CONFIG_PROPERTY, null);
solrSchema = DataStoreFactory.findProperty(properties, this,
SOLR_SCHEMA_PROPERTY, null);
adminServer = new HttpSolrServer(solrServerUrl);
if (autoCreateSchema) {
String batchSizeString = DataStoreFactory.findProperty(properties, this,
SOLR_BATCH_SIZE_PROPERTY, null);
if (batchSizeString != null) {
batchSize = Integer.parseInt(batchSizeString);
} catch (NumberFormatException nfe) {
DEFAULT_BATCH_SIZE);
batch = new ArrayList<SolrInputDocument>(batchSize);
String commitWithinString = DataStoreFactory.findProperty(properties, this,
SOLR_COMMIT_WITHIN_PROPERTY, null);
if (commitWithinString != null) {
commitWithin = Integer.parseInt(commitWithinString);
} catch (NumberFormatException nfe) {
String resultsSizeString = DataStoreFactory.findProperty(properties, this,
SOLR_RESULTS_SIZE_PROPERTY, null);
if (resultsSizeString != null) {
resultsSize = Integer.parseInt(resultsSizeString);
} catch (NumberFormatException nfe) {
private SolrMapping readMapping(String filename) throws IOException {
Document doc = builder.build(getClass().getClassLoader()
.getResourceAsStream(filename));
List<Element> classes = doc.getRootElement().getChildren("class");
for (Element classElement : classes) {
if (classElement.getAttributeValue("keyClass").equals(
keyClass.getCanonicalName())
&& classElement.getAttributeValue("name").equals(
persistentClass.getCanonicalName())) {
String tableName = getSchemaName(
classElement.getAttributeValue("table"), persistentClass);
map.setCoreName(tableName);
Element primaryKeyEl = classElement.getChild("primarykey");
map.setPrimaryKey(primaryKeyEl.getAttributeValue("column"));
List<Element> fields = classElement.getChildren("field");
for (Element field : fields) {
String fieldName = field.getAttributeValue("name");
String columnName = field.getAttributeValue("column");
map.addField(fieldName, columnName);
} catch (Exception ex) {
throw new IOException(ex);
if (!schemaExists())
CoreAdminRequest.createCore(mapping.getCoreName(),
mapping.getCoreName(), adminServer, solrConfig, solrSchema);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
server.deleteByQuery("*:*");
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
server.deleteByQuery("*:*");
} catch (Exception e) {
CoreAdminRequest.unloadCore(mapping.getCoreName(), adminServer);
} catch (Exception e) {
if (e.getMessage().contains("No such core")) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
CoreAdminResponse rsp = CoreAdminRequest.getStatus(mapping.getCoreName(),
adminServer);
exists = rsp.getUptime(mapping.getCoreName()) != null;
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
private static final String toDelimitedString(String[] arr, String sep) {
if (arr == null || arr.length == 0) {
if (i > 0)
sb.append(sep);
sb.append(arr[i]);
public static String escapeQueryKey(String key) {
if (key == null) {
char c = key.charAt(i);
switch (c) {
case ':':
case '*':
break;
default:
sb.append(c);
public T get(K key, String[] fields) {
params.set(CommonParams.QT, "/get");
params.set(CommonParams.FL, toDelimitedString(fields, ","));
params.set("id", key.toString());
QueryResponse rsp = server.query(params);
Object o = rsp.getResponse().get("doc");
if (o == null) {
return newInstance((SolrDocument) o, fields);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public T newInstance(SolrDocument doc, String[] fields) throws IOException {
if (fields == null) {
fields = fieldMap.keySet().toArray(new String[fieldMap.size()]);
for (String f : fields) {
Field field = fieldMap.get(f);
if (pk.equals(f)) {
sf = mapping.getSolrField(f);
Object sv = doc.get(sf);
if (sv == null) {
Object v = deserializeFieldValue(field, fieldSchema, sv, persistent);
persistent.put(field.pos(), v);
persistent.setDirty(field.pos());
@SuppressWarnings("rawtypes")
private SpecificDatumReader getDatumReader(String schemaId, Schema fieldSchema) {
SpecificDatumReader<?> reader = (SpecificDatumReader<?>) readerMap
.get(schemaId);
if (reader == null) {
SpecificDatumReader localReader = null;
if ((localReader = readerMap.putIfAbsent(schemaId, reader)) != null) {
reader = localReader;
}
}
return reader;
}
@SuppressWarnings("rawtypes")
private SpecificDatumWriter getDatumWriter(String schemaId, Schema fieldSchema) {
SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap
.get(schemaId);
if (writer == null) {
writerMap.put(schemaId, writer);
}
return writer;
}
@SuppressWarnings("unchecked")
private Object deserializeFieldValue(Field field, Schema fieldSchema,
Object solrValue, T persistent) throws IOException {
Object fieldValue = null;
switch (fieldSchema.getType()) {
case MAP:
case ARRAY:
case RECORD:
@SuppressWarnings("rawtypes")
SpecificDatumReader reader = getDatumReader(fieldSchema.getFullName(),
fieldSchema);
fieldValue = IOUtils.deserialize((byte[]) solrValue, reader, fieldSchema,
persistent.get(field.pos()));
break;
case ENUM:
fieldValue = AvroUtils.getEnumValue(fieldSchema, (String) solrValue);
break;
case FIXED:
throw new IOException("???");
case BYTES:
fieldValue = ByteBuffer.wrap((byte[]) solrValue);
break;
case STRING:
fieldValue = new Utf8(solrValue.toString());
break;
case UNION:
if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)) {
if (type0.equals(Schema.Type.NULL))
fieldSchema = fieldSchema.getTypes().get(1);
else
fieldSchema = fieldSchema.getTypes().get(0);
} else {
fieldSchema = fieldSchema.getTypes().get(0);
}
fieldValue = deserializeFieldValue(field, fieldSchema, solrValue,
persistent);
} else {
@SuppressWarnings("rawtypes")
SpecificDatumReader unionReader = getDatumReader(
String.valueOf(fieldSchema.hashCode()), fieldSchema);
fieldValue = IOUtils.deserialize((byte[]) solrValue, unionReader,
fieldSchema, persistent.get(field.pos()));
break;
}
break;
default:
fieldValue = solrValue;
}
return fieldValue;
}
public void put(K key, T persistent) {
if (!persistent.isDirty()) {
doc.addField(mapping.getPrimaryKey(), key);
for (Field field : fields) {
String sf = mapping.getSolrField(field.name());
if (sf == null) {
Object v = persistent.get(field.pos());
if (v == null) {
v = serializeFieldValue(fieldSchema, v);
doc.addField(sf, v);
batch.add(doc);
if (batch.size() >= batchSize) {
add(batch, commitWithin);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
@SuppressWarnings("unchecked")
private Object serializeFieldValue(Schema fieldSchema, Object fieldValue) {
switch (fieldSchema.getType()) {
case MAP:
case ARRAY:
case RECORD:
byte[] data = null;
try {
@SuppressWarnings("rawtypes")
SpecificDatumWriter writer = getDatumWriter(fieldSchema.getFullName(),
fieldSchema);
data = IOUtils.serialize(writer, fieldSchema, fieldValue);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
}
fieldValue = data;
break;
case BYTES:
fieldValue = ((ByteBuffer) fieldValue).array();
break;
case ENUM:
case STRING:
fieldValue = fieldValue.toString();
break;
case UNION:
if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
int schemaPos = getUnionSchema(fieldValue, fieldSchema);
Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
fieldValue = serializeFieldValue(unionSchema, fieldValue);
} else {
byte[] serilazeData = null;
try {
@SuppressWarnings("rawtypes")
SpecificDatumWriter writer = getDatumWriter(
String.valueOf(fieldSchema.hashCode()), fieldSchema);
serilazeData = IOUtils.serialize(writer, fieldSchema, fieldValue);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
}
fieldValue = serilazeData;
}
break;
default:
break;
}
return fieldValue;
}
private boolean isNullable(Schema unionSchema) {
for (Schema innerSchema : unionSchema.getTypes()) {
if (innerSchema.getType().equals(Schema.Type.NULL)) {
return true;
}
}
return false;
}
private int getUnionSchema(Object pValue, Schema pUnionSchema) {
int unionSchemaPos = 0;
Iterator<Schema> it = pUnionSchema.getTypes().iterator();
while (it.hasNext()) {
Type schemaType = it.next().getType();
if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
return unionSchemaPos;
else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
return unionSchemaPos;
else if (pValue instanceof Integer && schemaType.equals(Type.INT))
return unionSchemaPos;
else if (pValue instanceof Long && schemaType.equals(Type.LONG))
return unionSchemaPos;
else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
return unionSchemaPos;
else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
return unionSchemaPos;
else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
return unionSchemaPos;
else if (pValue instanceof Map && schemaType.equals(Type.MAP))
return unionSchemaPos;
else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
return unionSchemaPos;
else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
return unionSchemaPos;
unionSchemaPos;
}
return DEFAULT_UNION_SCHEMA;
}
public boolean delete(K key) {
escapeQueryKey(key.toString()));
LOG.info(rsp.toString());
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public long deleteByQuery(Query<K, T> query) {
String q = ((SolrQuery<K, T>) query).toSolrQuery();
UpdateResponse rsp = server.deleteByQuery(q);
LOG.info(rsp.toString());
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public Result<K, T> execute(Query<K, T> query) {
return new SolrResult<K, T>(this, query, server, resultsSize);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
return new SolrQuery<K, T>(this);
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
if (batch.size() > 0) {
add(batch, commitWithin);
} catch (Exception e) {
private void add(ArrayList<SolrInputDocument> batch, int commitWithin)
throws SolrServerException, IOException {
server.add(batch);
server.commit(false, true, true);
server.add(batch, commitWithin);
}
CharSequence url = pageview.getUrl();
package org.apache.gora.tutorial.log.generated;
public class MetricDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}]}");
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence metricDimension;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return metricDimension;
case 2: return timestamp;
case 3: return metric;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: metricDimension = (java.lang.CharSequence)(value); break;
case 2: timestamp = (java.lang.Long)(value); break;
case 3: metric = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public java.lang.CharSequence getMetricDimension() {
return metricDimension;
public void setMetricDimension(java.lang.CharSequence value) {
this.metricDimension = value;
setDirty(1);
public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
return isDirty(1);
public java.lang.Long getTimestamp() {
return timestamp;
public void setTimestamp(java.lang.Long value) {
this.timestamp = value;
setDirty(2);
public boolean isTimestampDirty(java.lang.Long value) {
return isDirty(2);
public java.lang.Long getMetric() {
return metric;
}
public void setMetric(java.lang.Long value) {
this.metric = value;
setDirty(3);
}
public boolean isMetricDirty(java.lang.Long value) {
return isDirty(3);
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder() {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder();
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MetricDatum>
implements org.apache.avro.data.RecordBuilder<MetricDatum> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence metricDimension;
private long timestamp;
private long metric;
private Builder() {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
}
private Builder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
super(other);
}
private Builder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.metricDimension)) {
this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.metricDimension);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.metric)) {
this.metric = (java.lang.Long) data().deepCopy(fields()[3].schema(), other.metric);
fieldSetFlags()[3] = true;
}
}
public java.lang.CharSequence getMetricDimension() {
return metricDimension;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetricDimension(java.lang.CharSequence value) {
validate(fields()[1], value);
this.metricDimension = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasMetricDimension() {
return fieldSetFlags()[1];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetricDimension() {
metricDimension = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getTimestamp() {
return timestamp;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setTimestamp(long value) {
validate(fields()[2], value);
this.timestamp = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasTimestamp() {
return fieldSetFlags()[2];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearTimestamp() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.Long getMetric() {
return metric;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetric(long value) {
validate(fields()[3], value);
this.metric = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasMetric() {
return fieldSetFlags()[3];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetric() {
fieldSetFlags()[3] = false;
return this;
}
@Override
public MetricDatum build() {
try {
MetricDatum record = new MetricDatum();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.metricDimension = fieldSetFlags()[1] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[1]);
record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
record.metric = fieldSetFlags()[3] ? this.metric : (java.lang.Long) defaultValue(fields()[3]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
}
}
public MetricDatum.Tombstone getTombstone(){
return TOMBSTONE;
}
public MetricDatum newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getMetricDimension() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetricDimension(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getMetric() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetric(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
}
package org.apache.gora.tutorial.log.generated;
public class Pageview extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AAA=\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}]}");
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[2]);
private java.lang.CharSequence url;
private java.lang.CharSequence ip;
private java.lang.CharSequence httpMethod;
private java.lang.CharSequence referrer;
private java.lang.CharSequence userAgent;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return url;
case 2: return timestamp;
case 3: return ip;
case 4: return httpMethod;
case 5: return httpStatusCode;
case 6: return responseSize;
case 7: return referrer;
case 8: return userAgent;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: url = (java.lang.CharSequence)(value); break;
case 2: timestamp = (java.lang.Long)(value); break;
case 3: ip = (java.lang.CharSequence)(value); break;
case 4: httpMethod = (java.lang.CharSequence)(value); break;
case 5: httpStatusCode = (java.lang.Integer)(value); break;
case 6: responseSize = (java.lang.Integer)(value); break;
case 7: referrer = (java.lang.CharSequence)(value); break;
case 8: userAgent = (java.lang.CharSequence)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public java.lang.CharSequence getUrl() {
return url;
public void setUrl(java.lang.CharSequence value) {
this.url = value;
setDirty(1);
public boolean isUrlDirty(java.lang.CharSequence value) {
return isDirty(1);
public java.lang.Long getTimestamp() {
return timestamp;
public void setTimestamp(java.lang.Long value) {
this.timestamp = value;
setDirty(2);
public boolean isTimestampDirty(java.lang.Long value) {
return isDirty(2);
public java.lang.CharSequence getIp() {
return ip;
public void setIp(java.lang.CharSequence value) {
this.ip = value;
setDirty(3);
public boolean isIpDirty(java.lang.CharSequence value) {
return isDirty(3);
public java.lang.CharSequence getHttpMethod() {
return httpMethod;
public void setHttpMethod(java.lang.CharSequence value) {
this.httpMethod = value;
setDirty(4);
public boolean isHttpMethodDirty(java.lang.CharSequence value) {
return isDirty(4);
public java.lang.Integer getHttpStatusCode() {
return httpStatusCode;
public void setHttpStatusCode(java.lang.Integer value) {
this.httpStatusCode = value;
setDirty(5);
public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
return isDirty(5);
public java.lang.Integer getResponseSize() {
return responseSize;
public void setResponseSize(java.lang.Integer value) {
this.responseSize = value;
setDirty(6);
}
public boolean isResponseSizeDirty(java.lang.Integer value) {
return isDirty(6);
}
public java.lang.CharSequence getReferrer() {
return referrer;
}
public void setReferrer(java.lang.CharSequence value) {
this.referrer = value;
setDirty(7);
}
public boolean isReferrerDirty(java.lang.CharSequence value) {
return isDirty(7);
}
public java.lang.CharSequence getUserAgent() {
return userAgent;
}
public void setUserAgent(java.lang.CharSequence value) {
this.userAgent = value;
setDirty(8);
}
public boolean isUserAgentDirty(java.lang.CharSequence value) {
return isDirty(8);
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder() {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder();
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview other) {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Pageview>
implements org.apache.avro.data.RecordBuilder<Pageview> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence url;
private long timestamp;
private java.lang.CharSequence ip;
private java.lang.CharSequence httpMethod;
private int httpStatusCode;
private int responseSize;
private java.lang.CharSequence referrer;
private java.lang.CharSequence userAgent;
private Builder() {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
}
private Builder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
super(other);
}
private Builder(org.apache.gora.tutorial.log.generated.Pageview other) {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.ip)) {
this.ip = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ip);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.httpMethod)) {
this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[4].schema(), other.httpMethod);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.httpStatusCode)) {
this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.httpStatusCode);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.responseSize)) {
this.responseSize = (java.lang.Integer) data().deepCopy(fields()[6].schema(), other.responseSize);
fieldSetFlags()[6] = true;
}
if (isValidValue(fields()[7], other.referrer)) {
this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.referrer);
fieldSetFlags()[7] = true;
}
if (isValidValue(fields()[8], other.userAgent)) {
this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[8].schema(), other.userAgent);
fieldSetFlags()[8] = true;
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setUrl(java.lang.CharSequence value) {
validate(fields()[1], value);
this.url = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasUrl() {
return fieldSetFlags()[1];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUrl() {
url = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getTimestamp() {
return timestamp;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setTimestamp(long value) {
validate(fields()[2], value);
this.timestamp = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasTimestamp() {
return fieldSetFlags()[2];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearTimestamp() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.CharSequence getIp() {
return ip;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setIp(java.lang.CharSequence value) {
validate(fields()[3], value);
this.ip = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasIp() {
return fieldSetFlags()[3];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearIp() {
ip = null;
fieldSetFlags()[3] = false;
return this;
}
public java.lang.CharSequence getHttpMethod() {
return httpMethod;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpMethod(java.lang.CharSequence value) {
validate(fields()[4], value);
this.httpMethod = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasHttpMethod() {
return fieldSetFlags()[4];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpMethod() {
httpMethod = null;
fieldSetFlags()[4] = false;
return this;
}
public java.lang.Integer getHttpStatusCode() {
return httpStatusCode;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpStatusCode(int value) {
validate(fields()[5], value);
this.httpStatusCode = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasHttpStatusCode() {
return fieldSetFlags()[5];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpStatusCode() {
fieldSetFlags()[5] = false;
return this;
}
public java.lang.Integer getResponseSize() {
return responseSize;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setResponseSize(int value) {
validate(fields()[6], value);
this.responseSize = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasResponseSize() {
return fieldSetFlags()[6];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearResponseSize() {
fieldSetFlags()[6] = false;
return this;
}
public java.lang.CharSequence getReferrer() {
return referrer;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setReferrer(java.lang.CharSequence value) {
validate(fields()[7], value);
this.referrer = value;
fieldSetFlags()[7] = true;
return this;
}
public boolean hasReferrer() {
return fieldSetFlags()[7];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearReferrer() {
referrer = null;
fieldSetFlags()[7] = false;
return this;
}
public java.lang.CharSequence getUserAgent() {
return userAgent;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setUserAgent(java.lang.CharSequence value) {
validate(fields()[8], value);
this.userAgent = value;
fieldSetFlags()[8] = true;
return this;
}
public boolean hasUserAgent() {
return fieldSetFlags()[8];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUserAgent() {
userAgent = null;
fieldSetFlags()[8] = false;
return this;
}
@Override
public Pageview build() {
try {
Pageview record = new Pageview();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[2]);
record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
record.ip = fieldSetFlags()[3] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[3]);
record.httpMethod = fieldSetFlags()[4] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[4]);
record.httpStatusCode = fieldSetFlags()[5] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[5]);
record.responseSize = fieldSetFlags()[6] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[6]);
record.referrer = fieldSetFlags()[7] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[7]);
record.userAgent = fieldSetFlags()[8] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[8]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
}
}
public Pageview.Tombstone getTombstone(){
return TOMBSTONE;
}
public Pageview newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Pageview implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isReferrerDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
}
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
key = (K) ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
import java.util.concurrent.TimeUnit;
import org.apache.accumulo.core.client.BatchWriterConfig;
import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;
import org.apache.accumulo.core.client.security.tokens.PasswordToken;
import org.apache.accumulo.core.security.thrift.TCredentials;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericData;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.EncoderFactory;
import org.apache.gora.accumulo.encoders.BinaryEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
private TCredentials credentials;
public Object fromBytes(Schema schema, byte data[]) throws GoraException {
Schema fromSchema = null;
if (schema.getType() == Type.UNION) {
try {
Decoder decoder = DecoderFactory.get().binaryDecoder(data, null);
int unionIndex = decoder.readIndex();
List<Schema> possibleTypes = schema.getTypes();
fromSchema = possibleTypes.get(unionIndex);
Schema effectiveSchema = possibleTypes.get(unionIndex);
if (effectiveSchema.getType() == Type.NULL) {
decoder.readNull();
return null;
} else {
data = decoder.readBytes(null).array();
}
} catch (IOException e) {
e.printStackTrace();
throw new GoraException("Error decoding union type: ", e);
}
} else {
fromSchema = schema;
}
return fromBytes(encoder, fromSchema, data);
case BOOLEAN:
return encoder.decodeBoolean(data);
case DOUBLE:
return encoder.decodeDouble(data);
case FLOAT:
return encoder.decodeFloat(data);
case INT:
return encoder.decodeInt(data);
case LONG:
return encoder.decodeLong(data);
case STRING:
return new Utf8(data);
case BYTES:
return ByteBuffer.wrap(data);
case ENUM:
return AvroUtils.getEnumValue(schema, encoder.decodeInt(data));
case ARRAY:
break;
case FIXED:
break;
case MAP:
break;
case NULL:
break;
case RECORD:
break;
case UNION:
break;
default:
break;
public byte[] toBytes(Schema toSchema, Object o) {
if (toSchema != null && toSchema.getType() == Type.UNION) {
ByteArrayOutputStream baos = new ByteArrayOutputStream();
org.apache.avro.io.BinaryEncoder avroEncoder = EncoderFactory.get().binaryEncoder(baos, null);
int unionIndex = 0;
try {
if (o == null) {
unionIndex = firstNullSchemaTypeIndex(toSchema);
avroEncoder.writeIndex(unionIndex);
avroEncoder.writeNull();
} else {
unionIndex = firstNotNullSchemaTypeIndex(toSchema);
avroEncoder.writeIndex(unionIndex);
avroEncoder.writeBytes(toBytes(o));
}
avroEncoder.flush();
return baos.toByteArray();
} catch (IOException e) {
e.printStackTrace();
return toBytes(o);
}
} else {
return toBytes(o);
}
}
private int firstNullSchemaTypeIndex(Schema toSchema) {
List<Schema> possibleTypes = toSchema.getTypes();
int unionIndex = 0;
Type pType = possibleTypes.get(i).getType();
unionIndex = i; break;
}
}
return unionIndex;
}
private int firstNotNullSchemaTypeIndex(Schema toSchema) {
List<Schema> possibleTypes = toSchema.getTypes();
int unionIndex = 0;
Type pType = possibleTypes.get(i).getType();
unionIndex = i; break;
}
}
return unionIndex;
}
return copyIfNeeded(((Utf8) o).getBytes(), 0, ((Utf8) o).getByteLength());
return encoder.encodeInt(((Enum<?>) o).ordinal());
BatchWriterConfig batchWriterConfig = new BatchWriterConfig();
batchWriterConfig.setMaxMemory(10000000);
batchWriterConfig.setMaxLatency(60000l, TimeUnit.MILLISECONDS);
batchWriterConfig.setMaxWriteThreads(4);
batchWriter = conn.createBatchWriter(mapping.tableName, batchWriterConfig);
encoder = new BinaryEncoder();
AuthenticationToken token =  new PasswordToken(password);
credentials = new TCredentials(user,
"org.apache.accumulo.core.client.security.tokens.PasswordToken",
ByteBuffer.wrap(password.getBytes()), instance);
conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, token);
conn = new MockInstance().getConnector(user, new PasswordToken(password));
credentials = new TCredentials(user,
"org.apache.accumulo.core.client.security.tokens.PasswordToken",
ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());
Map<Utf8, Object> currentMap = null;
List currentArray = null;
BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(new byte[0], null);
if (row == null) {
row = entry.getKey().getRowData();
}
byte[] val = entry.getValue().get();
Field field = fieldMap.get(getFieldName(entry));
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = field.schema().getValueType();
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
break;
case ARRAY:
currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = field.schema().getElementType();
currentField = field;
currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
break;
Schema effectiveSchema = field.schema().getTypes()
.get(firstNotNullSchemaTypeIndex(field.schema()));
if (effectiveSchema.getType() == Type.ARRAY) {
currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
}
else if (effectiveSchema.getType() == Type.MAP) {
currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = effectiveSchema.getValueType();
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
}
case RECORD:
SpecificDatumReader<?> reader = new SpecificDatumReader<Schema>(field.schema());
persistent.put(field.pos(), reader.read(null, DecoderFactory.get().binaryDecoder(val, decoder)));
break;
default:
persistent.put(field.pos(), fromBytes(field.schema(), entry.getValue().get()));
persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
private String getFieldName(Entry<Key, Value> entry) {
String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(),
entry.getKey().getColumnQualifier()));
if (fieldName == null) {
fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), null));
}
return fieldName;
}
if (col != null) {
if (col.getSecond() == null) {
scanner.fetchColumnFamily(col.getFirst());
} else {
scanner.fetchColumn(col.getFirst(), col.getSecond());
}
List<Field> fields = schema.getFields();
if (!val.isDirty(i)) {
Field field = fields.get(i);
Object o = val.get(field.pos());
case MAP:
count = putMap(m, count, field.schema().getValueType(), o, col);
break;
case ARRAY:
count = putArray(m, count, o, col);
break;
Schema effectiveSchema = field.schema().getTypes()
.get(firstNotNullSchemaTypeIndex(field.schema()));
if (effectiveSchema.getType() == Type.ARRAY) {
count = putArray(m, count, o, col);
}
else if (effectiveSchema.getType() == Type.MAP) {
count = putMap(m, count, effectiveSchema.getValueType(), o, col);
}
case RECORD:
SpecificDatumWriter<Object> writer = new SpecificDatumWriter<Object>(field.schema());
ByteArrayOutputStream os = new ByteArrayOutputStream();
org.apache.avro.io.BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
writer.write(o, encoder);
encoder.flush();
m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
count;
break;
default:
m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
count;
private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col) throws GoraException {
Text rowKey = new Text(m.getRow());
Query<K, T> query = newQuery();
query.setFields(col.getFirst().toString());
query.setStartKey((K)rowKey.toString());
query.setEndKey((K)rowKey.toString());
deleteByQuery(query);
flush();
if (o == null){
return 0;
}
Set<?> es = ((Map<?, ?>)o).entrySet();
for (Object entry : es) {
Object mapKey = ((Entry<?, ?>) entry).getKey();
Object mapVal = ((Entry<?, ?>) entry).getValue();
if ((o instanceof DirtyMapWrapper && ((DirtyMapWrapper<?, ?>)o).isDirty())
m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(valueType, mapVal)));
count;
}
}
return count;
}
private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col) {
Text rowKey = new Text(m.getRow());
Query<K, T> query = newQuery();
query.setFields(col.getFirst().toString());
query.setStartKey((K)rowKey.toString());
query.setEndKey((K)rowKey.toString());
deleteByQuery(query);
flush();
if (o == null){
return 0;
}
int j = 0;
for (Object item : array) {
count;
}
return count;
}
tl = TabletLocator.getInstance(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));
while (tl.binRanges(Collections.singletonList(createRange(query)), binnedRanges, credentials).size() > 0) {
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, new String[] {location});
@SuppressWarnings("unchecked")
import java.io.IOException;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
if (schema.getType().equals(Type.RECORD) || schema.getType().equals(Type.MAP) ){
try {
value = AvroSerializerUtil.deserializer(value, schema);
} catch (IOException e) {
}
}
String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName().duplicate());
String family = cassandraColumn.getFamily();
if (fieldName != null) {
if (fieldName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
int pos = this.persistent.getSchema().getField(fieldName).pos();
Field field = fields.get(pos);
Type fieldType = field.schema().getType();
if (fieldType.equals(Type.UNION)) {
CassandraColumn cc = getUnionTypeColumn(fieldName
CassandraStore.UNION_COL_SUFIX, cassandraRow.toArray());
Field unionField = new Field(fieldName
CassandraStore.UNION_COL_SUFIX, Schema.create(Type.INT),
null, null);
cc.setField(unionField);
Object val = cc.getValue();
cassandraColumn.setUnionType(Integer.parseInt(val.toString()));
}
cassandraColumn.setField(field);
Object value = cassandraColumn.getValue();
this.persistent.put(pos, value);
this.persistent.clearDirty(pos);
} else
@SuppressWarnings("unused")
import java.util.List;
import java.util.Map;
import org.apache.gora.cassandra.serializers.ListSerializer;
import org.apache.gora.cassandra.serializers.MapSerializer;
private Object getFieldValue(Type type, Schema fieldSchema, ByteBuffer byteBuffer){
Object value = null;
if (type.equals(Type.ARRAY)) {
ListSerializer<?> serializer = ListSerializer.get(fieldSchema.getElementType());
List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
value = genericArray;
} else if (type.equals(Type.MAP)) {
value = fromByteBuffer(fieldSchema, byteBuffer);
} else if (type.equals(Type.RECORD)){
value = fromByteBuffer(fieldSchema, byteBuffer);
} else if (type.equals(Type.UNION)){
Schema unionFieldSchema = getUnionSchema(super.getUnionType(), fieldSchema);
Type unionFieldType = unionFieldSchema.getType();
value = getFieldValue(unionFieldType, unionFieldSchema, byteBuffer);
} else {
value = fromByteBuffer(fieldSchema, byteBuffer);
}
return value;
}
Object value = getFieldValue(type, fieldSchema, byteBuffer);
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import org.apache.gora.cassandra.serializers.CharSequenceSerializer;
import org.apache.gora.cassandra.store.CassandraStore;
private Object getSuperValue(Field field, Schema fieldSchema, Type type){
List<Object> array = new ArrayList<Object>();
Map<CharSequence, Object> map = new HashMap<CharSequence, Object>();
CharSequence mapKey = CharSequenceSerializer.get().fromByteBuffer(hColumn.getName());
if (mapKey.toString().indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
Object memberValue = null;
if (fieldSchema.getValueType().getType().equals(Type.UNION)){
HColumn<ByteBuffer, ByteBuffer> cc = getUnionTypeColumn(mapKey
CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns());
Integer unionIndex = getUnionIndex(mapKey.toString(), cc);
Schema realSchema = fieldSchema.getValueType().getTypes().get(unionIndex);
memberValue = fromByteBuffer(realSchema, hColumn.getValue());
}else{
memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());
}
map.put(mapKey, memberValue);
}
if (memberName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
Schema memberSchema = memberField.schema();
Type memberType = memberSchema.getType();
if (memberType.equals(Type.UNION)){
HColumn<ByteBuffer, ByteBuffer> hc = getUnionTypeColumn(memberField.name()
CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns().toArray());
Integer unionIndex = getUnionIndex(memberField.name(),hc);
cassandraColumn.setUnionType(unionIndex);
}
record.put(record.getSchema().getField(memberName).pos(), cassandraColumn.getValue());
}
case UNION:
int schemaPos = this.getUnionType();
Schema unioSchema = fieldSchema.getTypes().get(schemaPos);
Type unionType = unioSchema.getType();
value = getSuperValue(field, unioSchema, unionType);
break;
Object memberValue = null;
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
memberValue = fromByteBuffer(fieldSchema, hColumn.getValue());
}
value = memberValue;
return value;
}
private Integer getUnionIndex(String fieldName, HColumn<ByteBuffer, ByteBuffer> uc){
Integer val = IntegerSerializer.get().fromByteBuffer(uc.getValue());
return Integer.parseInt(val.toString());
}
private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName,
List<HColumn<ByteBuffer, ByteBuffer>> columns) {
return getUnionTypeColumn(fieldName, columns.toArray());
}
private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName, Object[] hColumns) {
@SuppressWarnings("unchecked")
HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>)hColumns[iCnt];
String columnName = StringSerializer.get().fromByteBuffer(hColumn.getNameBytes().duplicate());
if (fieldName.equals(columnName))
return hColumn;
}
return null;
}
public Object getValue() {
Field field = getField();
Schema fieldSchema = field.schema();
Type type = fieldSchema.getType();
Object value = getSuperValue(field, fieldSchema, type);
import java.util.Map;
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.BytesArraySerializer;
import me.prettyprint.cassandra.serializers.ObjectSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.Persistent;
serializer = CharSequenceSerializer.get();
serializer = ListSerializer.get(schema);
} else if (value instanceof Map) {
Map map = (Map)value;
serializer = MapSerializer.get(schema);
} else if (value instanceof Persistent){
serializer = ObjectSerializer.get();
}
else {
serializer = CharSequenceSerializer.get();
if (type.equals(Type.STRING)) {
serializer = CharSequenceSerializer.get();
} else if (type.equals(Type.BOOLEAN)) {
} else if (type.equals(Type.BYTES)) {
} else if (type.equals(Type.DOUBLE)) {
} else if (type.equals(Type.FLOAT)) {
} else if (type.equals(Type.INT)) {
} else if (type.equals(Type.LONG)) {
} else if (type.equals(Type.FIXED)) {
} else if (type.equals(Type.ARRAY)) {
serializer = ListSerializer.get(schema.getElementType());
} else if (type.equals(Type.MAP)) {
serializer = MapSerializer.get(schema.getValueType());
} else if (type.equals(Type.UNION)){
} else if (type.equals(Type.RECORD)){
serializer = BytesArraySerializer.get();
serializer = CharSequenceSerializer.get();
serializer = ListSerializer.get(elementType);
serializer = MapSerializer.get(elementType);
import java.util.List;
import java.util.Map;
public static Class<? extends Object> getClass(Object value) {
return Schema.createArray( getElementSchema((GenericArray<?>)value) );
} else if (clazz.isAssignableFrom(List.class)) {
} else if (clazz.isAssignableFrom(Map.class)) {
public static Class<?> getClass(Type type) {
return List.class;
return Map.class;
public static Schema getSchema(Class<?> clazz) {
public static Class<?> getClass(Schema schema) {
public static int getFixedSize(Class<?> clazz) {
public static Schema getElementSchema(GenericArray<?> array) {
this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
"org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
clmap.put("ColumnFamily", HConsistencyLevel.ONE);
configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);
", not BytesType. It may cause a fatal error on column validation later.");
if (ttlAttr == null)
public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
synchronized(mutator) {
HectorUtils.deleteColumn(mutator, key, familyName, columnName);
}
}
public void deleteByKey(K key) {
Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
deleteColumn(key, familyMap.values().iterator().next().toString(), null);
}
if (ttlAttr == null)
public void deleteSubColumn(K key, String fieldName) {
String columnFamily = this.cassandraMapping.getFamily(fieldName);
String superColumnName = this.cassandraMapping.getColumn(fieldName);
synchronized(mutator) {
HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, null);
}
public void deleteGenericArray(K key, String fieldName) {
deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
}
if (((List<?>)itemValue).size() == 0) {
} else if (itemValue instanceof Map<?,?>) {
if (((Map<?, ?>)itemValue).size() == 0) {
public void deleteStatefulHashMap(K key, String fieldName) {
deleteSubColumn(key, fieldName);
} else {
deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
}
}
public void addStatefulHashMap(K key, String fieldName, Map<CharSequence,Object> map) {
if (isSuper( cassandraMapping.getFamily(fieldName) )) {
deleteSubColumn(key, fieldName);
if (!map.isEmpty()) {
for (CharSequence mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
if (mapValue instanceof GenericArray<?>) {
if (((List<?>)mapValue).size() == 0) {
continue;
}
} else if (mapValue instanceof Map<?,?>) {
if (((Map<?, ?>)mapValue).size() == 0) {
continue;
}
addSubColumn(key, fieldName, mapKey.toString(), mapValue);
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
family = this.cassandraMapping.getFamily(pField.substring(0,pField.indexOf(CassandraStore.UNION_COL_SUFIX)));
else
family = this.cassandraMapping.getFamily(pField);
return family;
}
if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
column = pField;
else
column = this.cassandraMapping.getColumn(pField);
return column;
}
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
return this.cassandraMapping.getKeyspaceName();
import java.util.HashMap;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.generic.GenericData.Array;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.commons.lang.ArrayUtils;
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
public static String UNION_COL_SUFIX = "_UnionIndex";
public static final ThreadLocal<BinaryEncoder> encoders =
new ThreadLocal<BinaryEncoder>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap =
new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
CassandraResultSet<K> cassandraResultSet) {
cassandraRow = new CassandraRow<K>();
@SuppressWarnings("unchecked")
addOrUpdateField(key, field, field.schema(), value.get(field.pos()));
if (fields == null){
fields = this.getFields();
}
ArrayList<String> unionFields = new ArrayList<String>();
for (String field: fields){
Field schemaField =this.fieldMap.get(field);
Type type = schemaField.schema().getType();
if (type.getName().equals("UNION".toLowerCase())){
}
}
String[] arr = unionFields.toArray(new String[unionFields.size()]);
String[] both = (String[]) ArrayUtils.addAll(fields, arr);
query.setFields(both);
@SuppressWarnings("unchecked")
T p = (T) SpecificData.get().newRecord(value, schema);
List<Field> fields = schema.getFields();
if (!value.isDirty(i)) {
continue;
Field field = fields.get(i);
Type type = field.schema().getType();
Object fieldValue = value.get(field.pos());
Schema fieldSchema = field.schema();
fieldValue = getFieldValue(fieldSchema, type, fieldValue);
p.put(field.pos(), fieldValue);
private Object getFieldValue(Schema fieldSchema, Type type, Object fieldValue ){
switch(type) {
case RECORD:
Persistent persistent = (Persistent) fieldValue;
Persistent newRecord = (Persistent) SpecificData.get().newRecord(persistent, persistent.getSchema());
for (Field member: fieldSchema.getFields()) {
if (member.pos() == 0 || !persistent.isDirty()) {
continue;
}
Schema memberSchema = member.schema();
Type memberType = memberSchema.getType();
Object memberValue = persistent.get(member.pos());
newRecord.put(member.pos(), getFieldValue(memberSchema, memberType, memberValue));
}
fieldValue = newRecord;
break;
case MAP:
Map<?, ?> map = (Map<?, ?>) fieldValue;
fieldValue = map;
break;
case ARRAY:
fieldValue = (List<?>) fieldValue;
break;
case UNION:
if (fieldValue != null){
int schemaPos = getUnionSchema(fieldValue,fieldSchema);
Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
Type unionType = unionSchema.getType();
fieldValue = getFieldValue(unionSchema, unionType, fieldValue);
}
break;
default:
break;
}
return fieldValue;
}
@SuppressWarnings({ "unchecked", "rawtypes" })
private void addOrUpdateField(K key, Field field, Schema schema, Object value) {
if (field.name().indexOf(CassandraStore.UNION_COL_SUFIX) < 0){
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
try {
byte[] byteValue = AvroSerializerUtil.serializer(persistentBase, schema);
this.cassandraClient.addColumn(key, field.name(), byteValue);
} catch (IOException e) {
} else {
String familyName =  this.cassandraClient.getCassandraMapping().getFamily(field.name());
this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
break;
case MAP:
if (value != null) {
if (value instanceof Map<?, ?>) {
Map<CharSequence,Object> map = (Map<CharSequence,Object>)value;
Schema valueSchema = schema.getValueType();
Type valueType = valueSchema.getType();
if (Type.UNION.equals(valueType)){
Map<CharSequence,Object> valueMap = new HashMap<CharSequence, Object>();
for (CharSequence mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
int valueUnionIndex = getUnionSchema(mapValue, valueSchema);
valueMap.put(mapKey, mapValue);
}
map = valueMap;
}
String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
if (!this.cassandraClient.isSuper( familyName )){
try {
byte[] byteValue = AvroSerializerUtil.serializer(map, schema);
this.cassandraClient.addColumn(key, field.name(), byteValue);
} catch (IOException e) {
}
}else{
this.cassandraClient.addStatefulHashMap(key, field.name(), map);
}
} else {
}
this.cassandraClient.deleteStatefulHashMap(key, field.name());
break;
case ARRAY:
if (value != null) {
if (value instanceof DirtyListWrapper<?>) {
DirtyListWrapper fieldValue = (DirtyListWrapper<?>)value;
GenericArray valueArray = new Array(fieldValue.size(), schema);
valueArray.add(i, fieldValue.get(i));
}
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray<?>)valueArray);
} else {
}
this.cassandraClient.deleteGenericArray(key, field.name());
break;
case UNION:
String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
if(value != null) {
int schemaPos = getUnionSchema(value, schema);
this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
if (this.cassandraClient.isSuper( familyName )){
this.cassandraClient.addSubColumn(key, columnName, columnName, schemaPos);
}else{
this.cassandraClient.addColumn(key, columnName, schemaPos);
}
Schema unionSchema = schema.getTypes().get(schemaPos);
addOrUpdateField(key, field, unionSchema, value);
} else {
if (this.cassandraClient.isSuper( familyName )){
this.cassandraClient.deleteSubColumn(key, field.name());
} else {
this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
}
}
break;
default:
Type schemaType = it.next().getType();
if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
else if (pValue instanceof Integer && schemaType.equals(Type.INT))
else if (pValue instanceof Long && schemaType.equals(Type.LONG))
else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
return unionSchemaPos;
else if (pValue instanceof Map && schemaType.equals(Type.MAP))
return unionSchemaPos;
else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
return unionSchemaPos;
else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
return DEFAULT_UNION_SCHEMA;
mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
import java.util.ArrayList;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
throws IOException {
try{
WebPage page;
log.info("creating web page data");
page = WebPage.newBuilder().build();
page.setUrl(new Utf8(URLS[i]));
page.setParsedContent(new ArrayList<CharSequence>());
if (CONTENTS[i]!=null){
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
for(String token : CONTENTS[i].split(" ")) {
page.getParsedContent().add(new Utf8(token));
}
page.getOutlinks().put(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
}
Metadata metadata = Metadata.newBuilder().build();
metadata.setVersion(1);
metadata.getData().put(new Utf8("metakey"), new Utf8("metavalue"));
page.setMetadata(metadata);
dataStore.put(URLS[i], page);
}
dataStore.flush();
log.info("finished creating web page data");
}
catch(Exception e){
log.info("error creating web page data");
}
package org.apache.gora.examples.generated;
public class Employee extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}]}");
__G__DIRTY(0, "__g__dirty"),
NAME(1, "name"),
DATE_OF_BIRTH(2, "dateOfBirth"),
SSN(3, "ssn"),
SALARY(4, "salary"),
BOSS(5, "boss"),
WEBPAGE(6, "webpage"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"name",
"dateOfBirth",
"ssn",
"salary",
"boss",
"webpage",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence name;
private java.lang.CharSequence ssn;
private java.lang.Object boss;
private org.apache.gora.examples.generated.WebPage webpage;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return name;
case 2: return dateOfBirth;
case 3: return ssn;
case 4: return salary;
case 5: return boss;
case 6: return webpage;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: name = (java.lang.CharSequence)(value); break;
case 2: dateOfBirth = (java.lang.Long)(value); break;
case 3: ssn = (java.lang.CharSequence)(value); break;
case 4: salary = (java.lang.Integer)(value); break;
case 5: boss = (java.lang.Object)(value); break;
case 6: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.CharSequence getName() {
return name;
}
public void setName(java.lang.CharSequence value) {
this.name = value;
setDirty(1);
}
public boolean isNameDirty(java.lang.CharSequence value) {
return isDirty(1);
}
public java.lang.Long getDateOfBirth() {
return dateOfBirth;
}
public void setDateOfBirth(java.lang.Long value) {
this.dateOfBirth = value;
setDirty(2);
}
public boolean isDateOfBirthDirty(java.lang.Long value) {
return isDirty(2);
}
public java.lang.CharSequence getSsn() {
return ssn;
}
public void setSsn(java.lang.CharSequence value) {
this.ssn = value;
setDirty(3);
}
public boolean isSsnDirty(java.lang.CharSequence value) {
return isDirty(3);
}
public java.lang.Integer getSalary() {
return salary;
}
public void setSalary(java.lang.Integer value) {
this.salary = value;
setDirty(4);
}
public boolean isSalaryDirty(java.lang.Integer value) {
return isDirty(4);
}
public java.lang.Object getBoss() {
return boss;
}
public void setBoss(java.lang.Object value) {
this.boss = value;
setDirty(5);
}
public boolean isBossDirty(java.lang.Object value) {
return isDirty(5);
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
return webpage;
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
this.webpage = value;
setDirty(6);
}
public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
return isDirty(6);
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder() {
return new org.apache.gora.examples.generated.Employee.Builder();
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee.Builder other) {
return new org.apache.gora.examples.generated.Employee.Builder(other);
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee other) {
return new org.apache.gora.examples.generated.Employee.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Employee>
implements org.apache.avro.data.RecordBuilder<Employee> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence name;
private long dateOfBirth;
private java.lang.CharSequence ssn;
private int salary;
private java.lang.Object boss;
private org.apache.gora.examples.generated.WebPage webpage;
private Builder() {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.Employee.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.Employee other) {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.name)) {
this.name = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.name);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.dateOfBirth)) {
this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.dateOfBirth);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.ssn)) {
this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ssn);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.salary)) {
this.salary = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.salary);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.boss)) {
this.boss = (java.lang.Object) data().deepCopy(fields()[5].schema(), other.boss);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.webpage)) {
this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[6].schema(), other.webpage);
fieldSetFlags()[6] = true;
}
}
public java.lang.CharSequence getName() {
return name;
}
public org.apache.gora.examples.generated.Employee.Builder setName(java.lang.CharSequence value) {
validate(fields()[1], value);
this.name = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasName() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.Employee.Builder clearName() {
name = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getDateOfBirth() {
return dateOfBirth;
}
public org.apache.gora.examples.generated.Employee.Builder setDateOfBirth(long value) {
validate(fields()[2], value);
this.dateOfBirth = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasDateOfBirth() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.Employee.Builder clearDateOfBirth() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.CharSequence getSsn() {
return ssn;
}
public org.apache.gora.examples.generated.Employee.Builder setSsn(java.lang.CharSequence value) {
validate(fields()[3], value);
this.ssn = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasSsn() {
return fieldSetFlags()[3];
}
public org.apache.gora.examples.generated.Employee.Builder clearSsn() {
ssn = null;
fieldSetFlags()[3] = false;
return this;
}
public java.lang.Integer getSalary() {
return salary;
}
public org.apache.gora.examples.generated.Employee.Builder setSalary(int value) {
validate(fields()[4], value);
this.salary = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasSalary() {
return fieldSetFlags()[4];
}
public org.apache.gora.examples.generated.Employee.Builder clearSalary() {
fieldSetFlags()[4] = false;
return this;
}
public java.lang.Object getBoss() {
return boss;
}
public org.apache.gora.examples.generated.Employee.Builder setBoss(java.lang.Object value) {
validate(fields()[5], value);
this.boss = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasBoss() {
return fieldSetFlags()[5];
}
public org.apache.gora.examples.generated.Employee.Builder clearBoss() {
boss = null;
fieldSetFlags()[5] = false;
return this;
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
return webpage;
}
public org.apache.gora.examples.generated.Employee.Builder setWebpage(org.apache.gora.examples.generated.WebPage value) {
validate(fields()[6], value);
this.webpage = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasWebpage() {
return fieldSetFlags()[6];
}
public org.apache.gora.examples.generated.Employee.Builder clearWebpage() {
webpage = null;
fieldSetFlags()[6] = false;
return this;
}
@Override
public Employee build() {
try {
Employee record = new Employee();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.name = fieldSetFlags()[1] ? this.name : (java.lang.CharSequence) defaultValue(fields()[1]);
record.dateOfBirth = fieldSetFlags()[2] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[2]);
record.ssn = fieldSetFlags()[3] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[3]);
record.salary = fieldSetFlags()[4] ? this.salary : (java.lang.Integer) defaultValue(fields()[4]);
record.boss = fieldSetFlags()[5] ? this.boss : (java.lang.Object) defaultValue(fields()[5]);
record.webpage = fieldSetFlags()[6] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[6]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public Employee.Tombstone getTombstone(){
return TOMBSTONE;
}
public Employee newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getName() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setName(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isNameDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getDateOfBirth() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setDateOfBirth(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDateOfBirthDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getSsn() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSsn(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSsnDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getSalary() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSalary(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSalaryDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Object getBoss() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setBoss(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isBossDirty(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class Metadata extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");
__G__DIRTY(0, "__g__dirty"),
VERSION(1, "version"),
DATA(2, "data"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"version",
"data",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return version;
case 2: return data;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: version = (java.lang.Integer)(value); break;
case 2: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.Integer getVersion() {
return version;
}
public void setVersion(java.lang.Integer value) {
this.version = value;
setDirty(1);
}
public boolean isVersionDirty(java.lang.Integer value) {
return isDirty(1);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
return data;
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.data = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(2);
}
public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(2);
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder() {
return new org.apache.gora.examples.generated.Metadata.Builder();
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata.Builder other) {
return new org.apache.gora.examples.generated.Metadata.Builder(other);
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata other) {
return new org.apache.gora.examples.generated.Metadata.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Metadata>
implements org.apache.avro.data.RecordBuilder<Metadata> {
private java.nio.ByteBuffer __g__dirty;
private int version;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
private Builder() {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.Metadata.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.Metadata other) {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.version)) {
this.version = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.version);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.data)) {
this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.data);
fieldSetFlags()[2] = true;
}
}
public java.lang.Integer getVersion() {
return version;
}
public org.apache.gora.examples.generated.Metadata.Builder setVersion(int value) {
validate(fields()[1], value);
this.version = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasVersion() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.Metadata.Builder clearVersion() {
fieldSetFlags()[1] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
return data;
}
public org.apache.gora.examples.generated.Metadata.Builder setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[2], value);
this.data = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasData() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.Metadata.Builder clearData() {
data = null;
fieldSetFlags()[2] = false;
return this;
}
@Override
public Metadata build() {
try {
Metadata record = new Metadata();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.version = fieldSetFlags()[1] ? this.version : (java.lang.Integer) defaultValue(fields()[1]);
record.data = fieldSetFlags()[2] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[2]));
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public Metadata.Tombstone getTombstone(){
return TOMBSTONE;
}
public Metadata newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getVersion() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setVersion(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isVersionDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class TokenDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");
__G__DIRTY(0, "__g__dirty"),
COUNT(1, "count"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"count",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: count = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.Integer getCount() {
return count;
}
public void setCount(java.lang.Integer value) {
this.count = value;
setDirty(1);
}
public boolean isCountDirty(java.lang.Integer value) {
return isDirty(1);
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder() {
return new org.apache.gora.examples.generated.TokenDatum.Builder();
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum other) {
return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<TokenDatum>
implements org.apache.avro.data.RecordBuilder<TokenDatum> {
private java.nio.ByteBuffer __g__dirty;
private int count;
private Builder() {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.TokenDatum other) {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.count)) {
this.count = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.count);
fieldSetFlags()[1] = true;
}
}
public java.lang.Integer getCount() {
return count;
}
public org.apache.gora.examples.generated.TokenDatum.Builder setCount(int value) {
validate(fields()[1], value);
this.count = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasCount() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.TokenDatum.Builder clearCount() {
fieldSetFlags()[1] = false;
return this;
}
@Override
public TokenDatum build() {
try {
TokenDatum record = new TokenDatum();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.count = fieldSetFlags()[1] ? this.count : (java.lang.Integer) defaultValue(fields()[1]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public TokenDatum.Tombstone getTombstone(){
return TOMBSTONE;
}
public TokenDatum newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class WebPage extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");
__G__DIRTY(0, "__g__dirty"),
URL(1, "url"),
CONTENT(2, "content"),
PARSED_CONTENT(3, "parsedContent"),
OUTLINKS(4, "outlinks"),
HEADERS(5, "headers"),
METADATA(6, "metadata"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"url",
"content",
"parsedContent",
"outlinks",
"headers",
"metadata",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence url;
private java.nio.ByteBuffer content;
private java.util.List<java.lang.CharSequence> parsedContent;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
private org.apache.gora.examples.generated.Metadata metadata;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return url;
case 2: return content;
case 3: return parsedContent;
case 4: return outlinks;
case 5: return headers;
case 6: return metadata;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: url = (java.lang.CharSequence)(value); break;
case 2: content = (java.nio.ByteBuffer)(value); break;
case 3: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
case 4: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 5: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
case 6: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public void setUrl(java.lang.CharSequence value) {
this.url = value;
setDirty(1);
}
public boolean isUrlDirty(java.lang.CharSequence value) {
return isDirty(1);
}
public java.nio.ByteBuffer getContent() {
return content;
}
public void setContent(java.nio.ByteBuffer value) {
this.content = value;
setDirty(2);
}
public boolean isContentDirty(java.nio.ByteBuffer value) {
return isDirty(2);
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
return parsedContent;
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
this.parsedContent = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper(value);
setDirty(3);
}
public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
return isDirty(3);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
return outlinks;
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.outlinks = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(4);
}
public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(4);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
return headers;
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.headers = value;
setDirty(5);
}
public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(5);
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
return metadata;
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
this.metadata = value;
setDirty(6);
}
public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
return isDirty(6);
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder() {
return new org.apache.gora.examples.generated.WebPage.Builder();
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage.Builder other) {
return new org.apache.gora.examples.generated.WebPage.Builder(other);
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage other) {
return new org.apache.gora.examples.generated.WebPage.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<WebPage>
implements org.apache.avro.data.RecordBuilder<WebPage> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence url;
private java.nio.ByteBuffer content;
private java.util.List<java.lang.CharSequence> parsedContent;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
private org.apache.gora.examples.generated.Metadata metadata;
private Builder() {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.WebPage.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.WebPage other) {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.content)) {
this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[2].schema(), other.content);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.parsedContent)) {
this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.parsedContent);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.outlinks)) {
this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.outlinks);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.headers)) {
this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[5].schema(), other.headers);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.metadata)) {
this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[6].schema(), other.metadata);
fieldSetFlags()[6] = true;
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public org.apache.gora.examples.generated.WebPage.Builder setUrl(java.lang.CharSequence value) {
validate(fields()[1], value);
this.url = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasUrl() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.WebPage.Builder clearUrl() {
url = null;
fieldSetFlags()[1] = false;
return this;
}
public java.nio.ByteBuffer getContent() {
return content;
}
public org.apache.gora.examples.generated.WebPage.Builder setContent(java.nio.ByteBuffer value) {
validate(fields()[2], value);
this.content = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasContent() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.WebPage.Builder clearContent() {
content = null;
fieldSetFlags()[2] = false;
return this;
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
return parsedContent;
}
public org.apache.gora.examples.generated.WebPage.Builder setParsedContent(java.util.List<java.lang.CharSequence> value) {
validate(fields()[3], value);
this.parsedContent = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasParsedContent() {
return fieldSetFlags()[3];
}
public org.apache.gora.examples.generated.WebPage.Builder clearParsedContent() {
parsedContent = null;
fieldSetFlags()[3] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
return outlinks;
}
public org.apache.gora.examples.generated.WebPage.Builder setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[4], value);
this.outlinks = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasOutlinks() {
return fieldSetFlags()[4];
}
public org.apache.gora.examples.generated.WebPage.Builder clearOutlinks() {
outlinks = null;
fieldSetFlags()[4] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
return headers;
}
public org.apache.gora.examples.generated.WebPage.Builder setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[5], value);
this.headers = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasHeaders() {
return fieldSetFlags()[5];
}
public org.apache.gora.examples.generated.WebPage.Builder clearHeaders() {
headers = null;
fieldSetFlags()[5] = false;
return this;
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
return metadata;
}
public org.apache.gora.examples.generated.WebPage.Builder setMetadata(org.apache.gora.examples.generated.Metadata value) {
validate(fields()[6], value);
this.metadata = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasMetadata() {
return fieldSetFlags()[6];
}
public org.apache.gora.examples.generated.WebPage.Builder clearMetadata() {
metadata = null;
fieldSetFlags()[6] = false;
return this;
}
@Override
public WebPage build() {
try {
WebPage record = new WebPage();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
record.content = fieldSetFlags()[2] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[2]);
record.parsedContent = fieldSetFlags()[3] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[3]));
record.outlinks = fieldSetFlags()[4] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[4]));
record.headers = fieldSetFlags()[5] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[5]);
record.metadata = fieldSetFlags()[6] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public WebPage.Tombstone getTombstone(){
return TOMBSTONE;
}
public WebPage newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.nio.ByteBuffer getContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setContent(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isContentDirty(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
return EncoderFactory.get().binaryEncoder(getOrCreateOutputStream(), null);
return EncoderFactory.get().jsonEncoder(schema, getOrCreateOutputStream());
return DecoderFactory.get().binaryDecoder(getOrCreateInputStream(), null);
return DecoderFactory.get().jsonDecoder(schema, getOrCreateInputStream());
int fieldIndex = persistent.getSchema().getField(fieldName).pos();
@SuppressWarnings("unchecked")
import org.apache.avro.specific.SpecificDatumReader;
implements Deserializer<Persistent> {
private Class<? extends Persistent> persistentClass;
private SpecificDatumReader<Persistent> datumReader;
public PersistentDeserializer(Class<? extends Persistent> c, boolean reuseObjects) {
datumReader = new SpecificDatumReader<Persistent>(schema);
@Override
decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);
@Override
public Persistent deserialize(Persistent persistent) throws IOException {
public class PersistentSerialization
implements Serialization<Persistent> {
public Deserializer<Persistent> getDeserializer(Class<Persistent> c) {
public Serializer<Persistent> getSerializer(Class<Persistent> c) {
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
public class PersistentSerializer implements Serializer<Persistent> {
private SpecificDatumWriter<Persistent> datumWriter;
private BinaryEncoder encoder;
this.datumWriter = new SpecificDatumWriter<Persistent>();
@Override
@Override
encoder = EncoderFactory.get().directBinaryEncoder(out, null);
public void serialize(Persistent persistent) throws IOException {
import org.apache.avro.Schema.Field;
import org.apache.gora.util.AvroUtils;
try{
long deletedRows = 0;
Result<K,T> result = query.execute();
while(result.next()) {
if(delete(result.getKey()))
}
return 0;
}
catch(Exception e){
return 0;
}
List<Field> otherFields = obj.getSchema().getFields();
String[] otherFieldStrings = new String[otherFields.size()];
otherFieldStrings[i] = otherFields.get(i).name();
}
if(Arrays.equals(fields, otherFieldStrings)) {
T newObj = (T) AvroUtils.deepClonePersistent(obj);
int index = otherFields.get(i).pos();
newObj.put(index, obj.get(index));
import java.util.List;
import org.apache.avro.Schema.Field;
import org.apache.avro.specific.SpecificRecord;
import org.apache.gora.persistency.Dirtyable;
public interface Persistent extends SpecificRecord, Dirtyable {
public static String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
public abstract Tombstone getTombstone();
public List<Field> getUnmanagedFields();
Persistent newInstance();
return keyClass.newInstance();
return (T) persistent.newInstance();
import java.util.Collection;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.gora.persistency.Dirtyable;
public abstract class PersistentBase extends SpecificRecordBase implements
Persistent {
public static class PersistentData extends SpecificData {
private static final PersistentData INSTANCE = new PersistentData();
public static PersistentData get() {
return INSTANCE;
public boolean equals(SpecificRecord obj1, SpecificRecord that) {
if (that == obj1)
if (!(that instanceof SpecificRecord))
if (obj1.getClass() != that.getClass())
return PersistentData.get().compare(obj1, that, obj1.getSchema(), true) == 0;
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
dirtyBytes.put(i, (byte) 0);
for (Field field : getSchema().getFields()) {
clearDirynessIfFieldIsDirtyable(field.pos());
}
private void clearDirynessIfFieldIsDirtyable(int fieldIndex) {
if (fieldIndex == 0)
return;
Object value = get(fieldIndex);
if (value instanceof Dirtyable) {
((Dirtyable) value).clearDirty();
public void clearDirty(int fieldIndex) {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
currentByte = (byte) ((~(1 << bitOffset)) & currentByte);
dirtyBytes.put(byteOffset, currentByte);
clearDirynessIfFieldIsDirtyable(fieldIndex);
@Override
public void clearDirty(String field) {
clearDirty(getSchema().getField(field).pos());
}
@Override
public boolean isDirty() {
boolean isSubRecordDirty = false;
for (Field field : fields) {
isSubRecordDirty = isSubRecordDirty || checkIfMutableFieldAndDirty(field);
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
boolean dirty = false;
dirty = dirty || dirtyBytes.get(i) != 0;
}
return isSubRecordDirty || dirty;
}
private boolean checkIfMutableFieldAndDirty(Field field) {
if (field.pos() == 0)
return false;
switch (field.schema().getType()) {
case RECORD:
case MAP:
case ARRAY:
Object value = get(field.pos());
return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
case UNION:
value = get(field.pos());
return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
default:
break;
}
return false;
}
@Override
public boolean isDirty(int fieldIndex) {
Field field = getSchema().getFields().get(fieldIndex);
boolean isSubRecordDirty = checkIfMutableFieldAndDirty(field);
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
return isSubRecordDirty || 0 != ((1 << bitOffset) & currentByte);
}
@Override
public boolean isDirty(String fieldName) {
Field field = getSchema().getField(fieldName);
if(field == null){
throw new IndexOutOfBoundsException
}
return isDirty(field.pos());
}
@Override
public void setDirty() {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
dirtyBytes.put(i, (byte) -128);
}
}
@Override
public void setDirty(int fieldIndex) {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
currentByte = (byte) ((1 << bitOffset) | currentByte);
dirtyBytes.put(byteOffset, currentByte);
}
@Override
public void setDirty(String field) {
setDirty(getSchema().getField(field).pos());
}
private ByteBuffer getDirtyBytes() {
return (ByteBuffer) get(0);
}
@Override
public void clear() {
Collection<Field> unmanagedFields = getUnmanagedFields();
for (Field field : getSchema().getFields()) {
if (!unmanagedFields.contains(field))
continue;
put(field.pos(), PersistentData.get().deepCopy(field.schema(), PersistentData.get().getDefaultValue(field)));
}
clearDirty();
}
@Override
public boolean equals(Object that) {
if (that == this) {
return true;
} else if (that instanceof Persistent) {
return PersistentData.get().equals(this, (SpecificRecord) that);
} else {
return false;
}
public List<Field> getUnmanagedFields(){
List<Field> fields = getSchema().getFields();
return fields.subList(1, fields.size());
return keyClass.newInstance();
try {
return (T) persistentClass.newInstance();
} catch (InstantiationException e) {
throw new RuntimeException(e);
} catch (IllegalAccessException e) {
e.printStackTrace();
throw new RuntimeException(e);
}
private void clearReadable() {
@Override
return true;
return true;
public static final String SCHEMA_NAME = "schema.name";
Properties properties = new Properties();
.getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
throws GoraException {
ReflectionUtils.newInstance(dataStoreClass);
throws GoraException {
throws GoraException {
= (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
throws GoraException {
import java.util.ArrayList;
import java.util.List;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
protected SpecificDatumReader<T> datumReader;
protected SpecificDatumWriter<T> datumWriter;
datumReader = new SpecificDatumReader<T>(schema);
datumWriter = new SpecificDatumWriter<T>(schema);
return getFields();
}
protected String[] getFields() {
List<Field> schemaFields = beanFactory.getCachedPersistent().getSchema().getFields();
List<Field> list = new ArrayList<Field>();
for (Field field : schemaFields) {
if (!Persistent.DIRTY_BYTES_FIELD_NAME.equalsIgnoreCase(field.name())) {
list.add(field);
}
}
schemaFields = list;
String[] fieldNames = new String[schemaFields.size()];
fieldNames[i] = schemaFields.get(i).name();
}
return fieldNames;
String confSchemaName = getOrCreateConf().get("preferred.schema.name");
if (confSchemaName != null) {
return confSchemaName;
}
return schemaName;
return mappingSchemaName;
return StringUtils.getClassname(persistentClass);
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
for (Field field : fields) {
public static Schema getSchema(Class<? extends Persistent> clazz)
throws SecurityException, NoSuchFieldException, IllegalArgumentException,
IllegalAccessException {
java.lang.reflect.Field field = clazz.getDeclaredField("SCHEMA$");
public static String[] getPersistentFieldNames(Persistent persistent) {
return getSchemaFieldNames(persistent.getSchema());
}
public static String[] getSchemaFieldNames(Schema schema) {
List<Field> fields = schema.getFields();
String[] fieldNames = new String[fields.size() - 1];
}
return fieldNames;
}
public static <T extends Persistent> T deepClonePersistent(T persistent) {
ByteArrayOutputStream bos = new ByteArrayOutputStream();
BinaryEncoder enc = EncoderFactory.get().binaryEncoder(bos, null);
SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<Persistent>(
persistent.getSchema());
try {
writer.write(persistent, enc);
} catch (IOException e) {
throw new RuntimeException(
"Unable to serialize avro object to byte buffer - "
"please report this issue to the Gora bugtracker "
"or your administrator.");
}
byte[] value = bos.toByteArray();
Decoder dec = DecoderFactory.get().binaryDecoder(value, null);
@SuppressWarnings("unchecked")
SpecificDatumReader<T> reader = new SpecificDatumReader<T>(
(Class<T>) persistent.getClass());
try {
return reader.read(null, dec);
} catch (IOException e) {
throw new RuntimeException(
"Unable to deserialize avro object from byte buffer - "
"please report this issue to the Gora bugtracker "
"or your administrator.");
}
}
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
public static <T> T fromBytes( byte[] val, Schema schema
, SpecificDatumReader<T> datumReader, T object)
return (T)Enum.valueOf(ReflectData.get().getClass(schema), symbol);
case STRING:  return (T)new Utf8(toString(val));
case BYTES:   return (T)ByteBuffer.wrap(val);
case INT:     return (T)Integer.valueOf(bytesToVint(val));
case LONG:    return (T)Long.valueOf(bytesToVlong(val));
case FLOAT:   return (T)Float.valueOf(toFloat(val));
case DOUBLE:  return (T)Double.valueOf(toDouble(val));
case BOOLEAN: return (T)Boolean.valueOf(val[0] != 0);
case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, schema, (SpecificRecord)object);
@SuppressWarnings("unchecked")
public static <T> byte[] toBytes(T o, Schema schema
, SpecificDatumWriter<T> datumWriter)
case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, schema, (SpecificRecord)o);
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.util.ByteBufferInputStream;
import org.apache.avro.util.ByteBufferOutputStream;
public static<T extends SpecificRecord> void serialize(OutputStream os,
SpecificDatumWriter<T> datumWriter, Schema schema, T object)
BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
datumWriter.write(object, encoder);
public static<T> void serialize(OutputStream os,
SpecificDatumWriter<T> datumWriter, Schema schema, T object)
throws IOException {
BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
datumWriter.write(object, encoder);
encoder.flush();
}
public static<T extends SpecificRecord> byte[] serialize(SpecificDatumWriter<T> datumWriter
, Schema schema, T object) throws IOException {
public static<T> byte[] serialize(SpecificDatumWriter<T> datumWriter
, Schema schema, T object) throws IOException {
ByteArrayOutputStream os = new ByteArrayOutputStream();
serialize(os, datumWriter, schema, object);
return os.toByteArray();
}
public static<K, T extends SpecificRecord> T deserialize(InputStream is,
SpecificDatumReader<T> datumReader, Schema schema, T object)
decoder = DecoderFactory.get().binaryDecoder(is, decoder);
return (T)datumReader.read(object, decoder);
public static<K, T extends SpecificRecord> T deserialize(byte[] bytes,
SpecificDatumReader<T> datumReader, Schema schema, T object)
decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
return (T)datumReader.read(object, decoder);
public static<K, T> T deserialize(byte[] bytes,
SpecificDatumReader<T> datumReader, Schema schema, T object)
throws IOException {
decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
return (T)datumReader.read(object, decoder);
import java.lang.reflect.Method;
import org.apache.avro.specific.SpecificRecordBuilderBase;
import org.apache.gora.persistency.Persistent;
public static <T extends Persistent> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) throws SecurityException
, NoSuchMethodException, IllegalArgumentException, IllegalAccessException, InvocationTargetException {
return (SpecificRecordBuilderBase<T>) clazz.getMethod("newBuilder").invoke(null);
}
import org.apache.gora.persistency.Tombstone;
public Tombstone getTombstone() {
return new Tombstone(){};
public Persistent newInstance() {
return new MockPersistent();
preferredSchema = properties.getProperty(PREF_SCH_NAME);
dynamoDBClient = getClient(properties.getProperty(CLI_TYP_PROP),(AWSCredentials)getConf());
dynamoDBClient.setEndpoint(properties.getProperty(ENDPOINT_PROP));
consistency = properties.getProperty(CONSISTENCY_READS);
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
try {
List<Field> fields = schema.getFields();
if (!persistent.isDirty(i)) {
Field field = fields.get(i);
throw new RuntimeException("HBase mapping for field ["
"] not found. Wrong gora-hbase-mapping.xml?");
addPutsAndDeletes(put, delete, o, field.schema().getType(),
field.schema(), hcol, hcol.getQualifier());
if (put.size() > 0) {
if (delete.size() > 0) {
table.delete(delete);
} catch (IOException ex2) {
private void addPutsAndDeletes(Put put, Delete delete, Object o, Type type,
Schema schema, HBaseColumn hcol, byte[] qualifier) throws IOException {
switch (type) {
case UNION:
if (isNullable(schema) && o == null) {
if (qualifier == null) {
delete.deleteFamily(hcol.getFamily());
} else {
delete.deleteColumn(hcol.getFamily(), qualifier);
}
} else {
int index = getResolvedUnionIndex(schema);
byte[] serializedBytes = toBytes(o, schema);
put.add(hcol.getFamily(), qualifier, serializedBytes);
} else {
Schema resolvedSchema = schema.getTypes().get(index);
addPutsAndDeletes(put, delete, o, resolvedSchema.getType(),
resolvedSchema, hcol, qualifier);
}
}
break;
case MAP:
if (qualifier == null) {
delete.deleteFamily(hcol.getFamily());
} else {
delete.deleteColumn(hcol.getFamily(), qualifier);
}
@SuppressWarnings({ "rawtypes", "unchecked" })
Set<Entry> set = ((Map) o).entrySet();
for (@SuppressWarnings("rawtypes") Entry entry : set) {
byte[] qual = toBytes(entry.getKey());
addPutsAndDeletes(put, delete, entry.getValue(), schema.getValueType()
.getType(), schema.getValueType(), hcol, qual);
}
break;
case ARRAY:
List<?> array = (List<?>) o;
int j = 0;
for (Object item : array) {
addPutsAndDeletes(put, delete, item, schema.getElementType().getType(),
}
break;
default:
byte[] serializedBytes = toBytes(o, schema);
put.add(hcol.getFamily(), qualifier, serializedBytes);
break;
}
}
private boolean isNullable(Schema unionSchema) {
for (Schema innerSchema : unionSchema.getTypes()) {
if (innerSchema.getType().equals(Schema.Type.NULL)) {
return true;
}
}
return false;
}
boolean isAllFields = Arrays.equals(fields, getFields());
addFamilyOrColumn(get, col, fieldSchema);
private void addFamilyOrColumn(Get get, HBaseColumn col, Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(get, col, resolvedSchema);
break;
case MAP:
case ARRAY:
get.addFamily(col.family);
break;
default:
get.addColumn(col.family, col.qualifier);
break;
}
}
private void addFields(Scan scan, Query<K, T> query) throws IOException {
addFamilyOrColumn(scan, col, fieldSchema);
private void addFamilyOrColumn(Scan scan, HBaseColumn col, Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(scan, col, resolvedSchema);
break;
case MAP:
case ARRAY:
scan.addFamily(col.family);
break;
default:
scan.addColumn(col.family, col.qualifier);
break;
}
}
private void addFields(Delete delete, Query<K, T> query)    throws IOException {
addFamilyOrColumn(delete, col, fieldSchema);
}
}
private void addFamilyOrColumn(Delete delete, HBaseColumn col,
Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(delete, col, resolvedSchema);
break;
case MAP:
case ARRAY:
delete.deleteFamily(col.family);
break;
default:
delete.deleteColumn(col.family, col.qualifier);
break;
setField(result,persistent, col, field, fieldSchema);
}
persistent.clearDirty();
return persistent;
}
private void setField(Result result, T persistent, HBaseColumn col,
Field field, Schema fieldSchema) throws IOException {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
if (val == null) {
return;
}
setField(persistent, field, val);
} else {
Schema resolvedSchema = fieldSchema.getTypes().get(index);
setField(result, persistent, col, field, resolvedSchema);
}
break;
case MAP:
NavigableMap<byte[], byte[]> qualMap = result.getNoVersionMap().get(
col.getFamily());
if (qualMap == null) {
return;
}
Schema valueSchema = fieldSchema.getValueType();
Map<Utf8, Object> map = new HashMap<Utf8, Object>();
for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
map.put(new Utf8(Bytes.toString(e.getKey())),
fromBytes(valueSchema, e.getValue()));
}
setField(persistent, field, map);
break;
case ARRAY:
qualMap = result.getFamilyMap(col.getFamily());
if (qualMap == null) {
return;
}
valueSchema = fieldSchema.getElementType();
ArrayList<Object> arrayList = new ArrayList<Object>();
DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<Object>(arrayList);
for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
dirtyListWrapper.add(fromBytes(valueSchema, e.getValue()));
}
setField(persistent, field, arrayList);
break;
default:
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
if (val == null) {
return;
}
setField(persistent, field, val);
break;
}
}
private int getResolvedUnionIndex(Schema unionScema) {
if (unionScema.getTypes().size() == 2) {
Type type0 = unionScema.getTypes().get(0).getType();
Type type1 = unionScema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {
if (type0.equals(Schema.Type.NULL))
return 1;
else
return 0;
return 2;
persistent.put(field.pos(), new DirtyMapWrapper(map));
@SuppressWarnings({ "rawtypes", "unchecked" })
private void setField(T persistent, Field field, List list) {
persistent.put(field.pos(), new DirtyListWrapper(list));
}
import java.util.Map;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.RowMutations;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
@Override
public void batch(List<? extends Row> actions, Object[] results)
throws IOException, InterruptedException {
getTable().batch(actions, results);
}
@Override
public Object[] batch(List<? extends Row> actions) throws IOException,
InterruptedException {
return getTable().batch(actions);
}
@Override
public void mutateRow(RowMutations rm) throws IOException {
}
@Override
public Result append(Append append) throws IOException {
return null;
}
@Override
public <T extends CoprocessorProtocol> T coprocessorProxy(Class<T> protocol,
byte[] row) {
return null;
}
@Override
public <T extends CoprocessorProtocol, R> Map<byte[], R> coprocessorExec(
Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable)
throws IOException, Throwable {
return null;
}
@Override
public <T extends CoprocessorProtocol, R> void coprocessorExec(
Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable,
Callback<R> callback) throws IOException, Throwable {
}
@Override
public void setAutoFlush(boolean autoFlush) {
}
@Override
public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail) {
}
@Override
public long getWriteBufferSize() {
return 0;
}
@Override
public void setWriteBufferSize(long writeBufferSize) throws IOException {
}
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.io.EncoderFactory;
private static ThreadLocal<ByteArrayOutputStream> outputStream =
new ThreadLocal<ByteArrayOutputStream>();
public static final ThreadLocal<BinaryEncoder> encoders =
new ThreadLocal<BinaryEncoder>();
public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap =
new ConcurrentHashMap<String, SpecificDatumReader<?>>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap =
new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
@SuppressWarnings({ "rawtypes" })
String schemaId = schema.getType().equals(Schema.Type.UNION) ? String.valueOf(schema.hashCode()) : schema.getFullName();
SpecificDatumReader<?> reader = (SpecificDatumReader<?>)readerMap.get(schemaId);
if (reader == null) {
SpecificDatumReader localReader=null;
if((localReader=readerMap.putIfAbsent(schemaId, reader))!=null) {
reader = localReader;
BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(val, null);
return reader.read(null, decoder);
SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap.get(schema.getFullName());
if (writer == null) {
writerMap.put(schema.getFullName(),writer);
BinaryEncoder encoderFromCache = encoders.get();
ByteArrayOutputStream bos = new ByteArrayOutputStream();
outputStream.set(bos);
BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(bos, null);
if (encoderFromCache == null) {
ByteArrayOutputStream os = outputStream.get();
writer.write(o, encoder);
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
private static final Logger LOG = LoggerFactory.getLogger(SolrStore.class);
public static int DEFAULT_UNION_SCHEMA = 0;
public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<String, SpecificDatumReader<?>>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
public void initialize(Class<K> keyClass, Class<T> persistentClass,
Properties properties) {
super.initialize(keyClass, persistentClass, properties);
String mappingFile = DataStoreFactory.getMappingFile(properties, this,
DEFAULT_MAPPING_FILE);
mapping = readMapping(mappingFile);
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
solrServerUrl = DataStoreFactory.findProperty(properties, this,
SOLR_URL_PROPERTY, null);
solrConfig = DataStoreFactory.findProperty(properties, this,
SOLR_CONFIG_PROPERTY, null);
solrSchema = DataStoreFactory.findProperty(properties, this,
SOLR_SCHEMA_PROPERTY, null);
adminServer = new HttpSolrServer(solrServerUrl);
if (autoCreateSchema) {
String batchSizeString = DataStoreFactory.findProperty(properties, this,
SOLR_BATCH_SIZE_PROPERTY, null);
if (batchSizeString != null) {
batchSize = Integer.parseInt(batchSizeString);
} catch (NumberFormatException nfe) {
DEFAULT_BATCH_SIZE);
batch = new ArrayList<SolrInputDocument>(batchSize);
String commitWithinString = DataStoreFactory.findProperty(properties, this,
SOLR_COMMIT_WITHIN_PROPERTY, null);
if (commitWithinString != null) {
commitWithin = Integer.parseInt(commitWithinString);
} catch (NumberFormatException nfe) {
String resultsSizeString = DataStoreFactory.findProperty(properties, this,
SOLR_RESULTS_SIZE_PROPERTY, null);
if (resultsSizeString != null) {
resultsSize = Integer.parseInt(resultsSizeString);
} catch (NumberFormatException nfe) {
private SolrMapping readMapping(String filename) throws IOException {
Document doc = builder.build(getClass().getClassLoader()
.getResourceAsStream(filename));
List<Element> classes = doc.getRootElement().getChildren("class");
for (Element classElement : classes) {
if (classElement.getAttributeValue("keyClass").equals(
keyClass.getCanonicalName())
&& classElement.getAttributeValue("name").equals(
persistentClass.getCanonicalName())) {
String tableName = getSchemaName(
classElement.getAttributeValue("table"), persistentClass);
map.setCoreName(tableName);
Element primaryKeyEl = classElement.getChild("primarykey");
map.setPrimaryKey(primaryKeyEl.getAttributeValue("column"));
List<Element> fields = classElement.getChildren("field");
for (Element field : fields) {
String fieldName = field.getAttributeValue("name");
String columnName = field.getAttributeValue("column");
map.addField(fieldName, columnName);
} catch (Exception ex) {
throw new IOException(ex);
if (!schemaExists())
CoreAdminRequest.createCore(mapping.getCoreName(),
mapping.getCoreName(), adminServer, solrConfig, solrSchema);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
server.deleteByQuery("*:*");
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
server.deleteByQuery("*:*");
} catch (Exception e) {
CoreAdminRequest.unloadCore(mapping.getCoreName(), adminServer);
} catch (Exception e) {
if (e.getMessage().contains("No such core")) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
CoreAdminResponse rsp = CoreAdminRequest.getStatus(mapping.getCoreName(),
adminServer);
exists = rsp.getUptime(mapping.getCoreName()) != null;
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
private static final String toDelimitedString(String[] arr, String sep) {
if (arr == null || arr.length == 0) {
if (i > 0)
sb.append(sep);
sb.append(arr[i]);
public static String escapeQueryKey(String key) {
if (key == null) {
char c = key.charAt(i);
switch (c) {
case ':':
case '*':
break;
default:
sb.append(c);
public T get(K key, String[] fields) {
params.set(CommonParams.QT, "/get");
params.set(CommonParams.FL, toDelimitedString(fields, ","));
params.set("id", key.toString());
QueryResponse rsp = server.query(params);
Object o = rsp.getResponse().get("doc");
if (o == null) {
return newInstance((SolrDocument) o, fields);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public T newInstance(SolrDocument doc, String[] fields) throws IOException {
if (fields == null) {
fields = fieldMap.keySet().toArray(new String[fieldMap.size()]);
for (String f : fields) {
Field field = fieldMap.get(f);
if (pk.equals(f)) {
sf = mapping.getSolrField(f);
Object sv = doc.get(sf);
if (sv == null) {
Object v = deserializeFieldValue(field, fieldSchema, sv, persistent);
persistent.put(field.pos(), v);
persistent.setDirty(field.pos());
@SuppressWarnings("rawtypes")
private SpecificDatumReader getDatumReader(String schemaId, Schema fieldSchema) {
SpecificDatumReader<?> reader = (SpecificDatumReader<?>) readerMap
.get(schemaId);
if (reader == null) {
SpecificDatumReader localReader = null;
if ((localReader = readerMap.putIfAbsent(schemaId, reader)) != null) {
reader = localReader;
}
}
return reader;
}
@SuppressWarnings("rawtypes")
private SpecificDatumWriter getDatumWriter(String schemaId, Schema fieldSchema) {
SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap
.get(schemaId);
if (writer == null) {
writerMap.put(schemaId, writer);
}
return writer;
}
@SuppressWarnings("unchecked")
private Object deserializeFieldValue(Field field, Schema fieldSchema,
Object solrValue, T persistent) throws IOException {
Object fieldValue = null;
switch (fieldSchema.getType()) {
case MAP:
case ARRAY:
case RECORD:
@SuppressWarnings("rawtypes")
SpecificDatumReader reader = getDatumReader(fieldSchema.getFullName(),
fieldSchema);
fieldValue = IOUtils.deserialize((byte[]) solrValue, reader, fieldSchema,
persistent.get(field.pos()));
break;
case ENUM:
fieldValue = AvroUtils.getEnumValue(fieldSchema, (String) solrValue);
break;
case FIXED:
throw new IOException("???");
case BYTES:
fieldValue = ByteBuffer.wrap((byte[]) solrValue);
break;
case STRING:
fieldValue = new Utf8(solrValue.toString());
break;
case UNION:
if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)) {
if (type0.equals(Schema.Type.NULL))
fieldSchema = fieldSchema.getTypes().get(1);
else
fieldSchema = fieldSchema.getTypes().get(0);
} else {
fieldSchema = fieldSchema.getTypes().get(0);
}
fieldValue = deserializeFieldValue(field, fieldSchema, solrValue,
persistent);
} else {
@SuppressWarnings("rawtypes")
SpecificDatumReader unionReader = getDatumReader(
String.valueOf(fieldSchema.hashCode()), fieldSchema);
fieldValue = IOUtils.deserialize((byte[]) solrValue, unionReader,
fieldSchema, persistent.get(field.pos()));
break;
}
break;
default:
fieldValue = solrValue;
}
return fieldValue;
}
public void put(K key, T persistent) {
if (!persistent.isDirty()) {
doc.addField(mapping.getPrimaryKey(), key);
for (Field field : fields) {
String sf = mapping.getSolrField(field.name());
if (sf == null) {
Object v = persistent.get(field.pos());
if (v == null) {
v = serializeFieldValue(fieldSchema, v);
doc.addField(sf, v);
batch.add(doc);
if (batch.size() >= batchSize) {
add(batch, commitWithin);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
@SuppressWarnings("unchecked")
private Object serializeFieldValue(Schema fieldSchema, Object fieldValue) {
switch (fieldSchema.getType()) {
case MAP:
case ARRAY:
case RECORD:
byte[] data = null;
try {
@SuppressWarnings("rawtypes")
SpecificDatumWriter writer = getDatumWriter(fieldSchema.getFullName(),
fieldSchema);
data = IOUtils.serialize(writer, fieldSchema, fieldValue);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
}
fieldValue = data;
break;
case BYTES:
fieldValue = ((ByteBuffer) fieldValue).array();
break;
case ENUM:
case STRING:
fieldValue = fieldValue.toString();
break;
case UNION:
if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
int schemaPos = getUnionSchema(fieldValue, fieldSchema);
Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
fieldValue = serializeFieldValue(unionSchema, fieldValue);
} else {
byte[] serilazeData = null;
try {
@SuppressWarnings("rawtypes")
SpecificDatumWriter writer = getDatumWriter(
String.valueOf(fieldSchema.hashCode()), fieldSchema);
serilazeData = IOUtils.serialize(writer, fieldSchema, fieldValue);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
}
fieldValue = serilazeData;
}
break;
default:
break;
}
return fieldValue;
}
private boolean isNullable(Schema unionSchema) {
for (Schema innerSchema : unionSchema.getTypes()) {
if (innerSchema.getType().equals(Schema.Type.NULL)) {
return true;
}
}
return false;
}
private int getUnionSchema(Object pValue, Schema pUnionSchema) {
int unionSchemaPos = 0;
Iterator<Schema> it = pUnionSchema.getTypes().iterator();
while (it.hasNext()) {
Type schemaType = it.next().getType();
if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
return unionSchemaPos;
else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
return unionSchemaPos;
else if (pValue instanceof Integer && schemaType.equals(Type.INT))
return unionSchemaPos;
else if (pValue instanceof Long && schemaType.equals(Type.LONG))
return unionSchemaPos;
else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
return unionSchemaPos;
else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
return unionSchemaPos;
else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
return unionSchemaPos;
else if (pValue instanceof Map && schemaType.equals(Type.MAP))
return unionSchemaPos;
else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
return unionSchemaPos;
else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
return unionSchemaPos;
unionSchemaPos;
}
return DEFAULT_UNION_SCHEMA;
}
public boolean delete(K key) {
escapeQueryKey(key.toString()));
LOG.info(rsp.toString());
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public long deleteByQuery(Query<K, T> query) {
String q = ((SolrQuery<K, T>) query).toSolrQuery();
UpdateResponse rsp = server.deleteByQuery(q);
LOG.info(rsp.toString());
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public Result<K, T> execute(Query<K, T> query) {
return new SolrResult<K, T>(this, query, server, resultsSize);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
return new SolrQuery<K, T>(this);
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
if (batch.size() > 0) {
add(batch, commitWithin);
} catch (Exception e) {
private void add(ArrayList<SolrInputDocument> batch, int commitWithin)
throws SolrServerException, IOException {
server.add(batch);
server.commit(false, true, true);
server.add(batch, commitWithin);
}
CharSequence url = pageview.getUrl();
package org.apache.gora.tutorial.log.generated;
public class MetricDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}]}");
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence metricDimension;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return metricDimension;
case 2: return timestamp;
case 3: return metric;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: metricDimension = (java.lang.CharSequence)(value); break;
case 2: timestamp = (java.lang.Long)(value); break;
case 3: metric = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public java.lang.CharSequence getMetricDimension() {
return metricDimension;
public void setMetricDimension(java.lang.CharSequence value) {
this.metricDimension = value;
setDirty(1);
public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
return isDirty(1);
public java.lang.Long getTimestamp() {
return timestamp;
public void setTimestamp(java.lang.Long value) {
this.timestamp = value;
setDirty(2);
public boolean isTimestampDirty(java.lang.Long value) {
return isDirty(2);
public java.lang.Long getMetric() {
return metric;
}
public void setMetric(java.lang.Long value) {
this.metric = value;
setDirty(3);
}
public boolean isMetricDirty(java.lang.Long value) {
return isDirty(3);
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder() {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder();
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MetricDatum>
implements org.apache.avro.data.RecordBuilder<MetricDatum> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence metricDimension;
private long timestamp;
private long metric;
private Builder() {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
}
private Builder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
super(other);
}
private Builder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.metricDimension)) {
this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.metricDimension);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.metric)) {
this.metric = (java.lang.Long) data().deepCopy(fields()[3].schema(), other.metric);
fieldSetFlags()[3] = true;
}
}
public java.lang.CharSequence getMetricDimension() {
return metricDimension;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetricDimension(java.lang.CharSequence value) {
validate(fields()[1], value);
this.metricDimension = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasMetricDimension() {
return fieldSetFlags()[1];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetricDimension() {
metricDimension = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getTimestamp() {
return timestamp;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setTimestamp(long value) {
validate(fields()[2], value);
this.timestamp = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasTimestamp() {
return fieldSetFlags()[2];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearTimestamp() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.Long getMetric() {
return metric;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetric(long value) {
validate(fields()[3], value);
this.metric = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasMetric() {
return fieldSetFlags()[3];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetric() {
fieldSetFlags()[3] = false;
return this;
}
@Override
public MetricDatum build() {
try {
MetricDatum record = new MetricDatum();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.metricDimension = fieldSetFlags()[1] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[1]);
record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
record.metric = fieldSetFlags()[3] ? this.metric : (java.lang.Long) defaultValue(fields()[3]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
}
}
public MetricDatum.Tombstone getTombstone(){
return TOMBSTONE;
}
public MetricDatum newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getMetricDimension() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetricDimension(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getMetric() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetric(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
}
package org.apache.gora.tutorial.log.generated;
public class Pageview extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AAA=\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}]}");
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[2]);
private java.lang.CharSequence url;
private java.lang.CharSequence ip;
private java.lang.CharSequence httpMethod;
private java.lang.CharSequence referrer;
private java.lang.CharSequence userAgent;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return url;
case 2: return timestamp;
case 3: return ip;
case 4: return httpMethod;
case 5: return httpStatusCode;
case 6: return responseSize;
case 7: return referrer;
case 8: return userAgent;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: url = (java.lang.CharSequence)(value); break;
case 2: timestamp = (java.lang.Long)(value); break;
case 3: ip = (java.lang.CharSequence)(value); break;
case 4: httpMethod = (java.lang.CharSequence)(value); break;
case 5: httpStatusCode = (java.lang.Integer)(value); break;
case 6: responseSize = (java.lang.Integer)(value); break;
case 7: referrer = (java.lang.CharSequence)(value); break;
case 8: userAgent = (java.lang.CharSequence)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public java.lang.CharSequence getUrl() {
return url;
public void setUrl(java.lang.CharSequence value) {
this.url = value;
setDirty(1);
public boolean isUrlDirty(java.lang.CharSequence value) {
return isDirty(1);
public java.lang.Long getTimestamp() {
return timestamp;
public void setTimestamp(java.lang.Long value) {
this.timestamp = value;
setDirty(2);
public boolean isTimestampDirty(java.lang.Long value) {
return isDirty(2);
public java.lang.CharSequence getIp() {
return ip;
public void setIp(java.lang.CharSequence value) {
this.ip = value;
setDirty(3);
public boolean isIpDirty(java.lang.CharSequence value) {
return isDirty(3);
public java.lang.CharSequence getHttpMethod() {
return httpMethod;
public void setHttpMethod(java.lang.CharSequence value) {
this.httpMethod = value;
setDirty(4);
public boolean isHttpMethodDirty(java.lang.CharSequence value) {
return isDirty(4);
public java.lang.Integer getHttpStatusCode() {
return httpStatusCode;
public void setHttpStatusCode(java.lang.Integer value) {
this.httpStatusCode = value;
setDirty(5);
public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
return isDirty(5);
public java.lang.Integer getResponseSize() {
return responseSize;
public void setResponseSize(java.lang.Integer value) {
this.responseSize = value;
setDirty(6);
}
public boolean isResponseSizeDirty(java.lang.Integer value) {
return isDirty(6);
}
public java.lang.CharSequence getReferrer() {
return referrer;
}
public void setReferrer(java.lang.CharSequence value) {
this.referrer = value;
setDirty(7);
}
public boolean isReferrerDirty(java.lang.CharSequence value) {
return isDirty(7);
}
public java.lang.CharSequence getUserAgent() {
return userAgent;
}
public void setUserAgent(java.lang.CharSequence value) {
this.userAgent = value;
setDirty(8);
}
public boolean isUserAgentDirty(java.lang.CharSequence value) {
return isDirty(8);
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder() {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder();
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview other) {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Pageview>
implements org.apache.avro.data.RecordBuilder<Pageview> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence url;
private long timestamp;
private java.lang.CharSequence ip;
private java.lang.CharSequence httpMethod;
private int httpStatusCode;
private int responseSize;
private java.lang.CharSequence referrer;
private java.lang.CharSequence userAgent;
private Builder() {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
}
private Builder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
super(other);
}
private Builder(org.apache.gora.tutorial.log.generated.Pageview other) {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.ip)) {
this.ip = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ip);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.httpMethod)) {
this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[4].schema(), other.httpMethod);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.httpStatusCode)) {
this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.httpStatusCode);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.responseSize)) {
this.responseSize = (java.lang.Integer) data().deepCopy(fields()[6].schema(), other.responseSize);
fieldSetFlags()[6] = true;
}
if (isValidValue(fields()[7], other.referrer)) {
this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.referrer);
fieldSetFlags()[7] = true;
}
if (isValidValue(fields()[8], other.userAgent)) {
this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[8].schema(), other.userAgent);
fieldSetFlags()[8] = true;
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setUrl(java.lang.CharSequence value) {
validate(fields()[1], value);
this.url = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasUrl() {
return fieldSetFlags()[1];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUrl() {
url = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getTimestamp() {
return timestamp;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setTimestamp(long value) {
validate(fields()[2], value);
this.timestamp = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasTimestamp() {
return fieldSetFlags()[2];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearTimestamp() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.CharSequence getIp() {
return ip;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setIp(java.lang.CharSequence value) {
validate(fields()[3], value);
this.ip = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasIp() {
return fieldSetFlags()[3];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearIp() {
ip = null;
fieldSetFlags()[3] = false;
return this;
}
public java.lang.CharSequence getHttpMethod() {
return httpMethod;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpMethod(java.lang.CharSequence value) {
validate(fields()[4], value);
this.httpMethod = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasHttpMethod() {
return fieldSetFlags()[4];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpMethod() {
httpMethod = null;
fieldSetFlags()[4] = false;
return this;
}
public java.lang.Integer getHttpStatusCode() {
return httpStatusCode;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpStatusCode(int value) {
validate(fields()[5], value);
this.httpStatusCode = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasHttpStatusCode() {
return fieldSetFlags()[5];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpStatusCode() {
fieldSetFlags()[5] = false;
return this;
}
public java.lang.Integer getResponseSize() {
return responseSize;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setResponseSize(int value) {
validate(fields()[6], value);
this.responseSize = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasResponseSize() {
return fieldSetFlags()[6];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearResponseSize() {
fieldSetFlags()[6] = false;
return this;
}
public java.lang.CharSequence getReferrer() {
return referrer;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setReferrer(java.lang.CharSequence value) {
validate(fields()[7], value);
this.referrer = value;
fieldSetFlags()[7] = true;
return this;
}
public boolean hasReferrer() {
return fieldSetFlags()[7];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearReferrer() {
referrer = null;
fieldSetFlags()[7] = false;
return this;
}
public java.lang.CharSequence getUserAgent() {
return userAgent;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setUserAgent(java.lang.CharSequence value) {
validate(fields()[8], value);
this.userAgent = value;
fieldSetFlags()[8] = true;
return this;
}
public boolean hasUserAgent() {
return fieldSetFlags()[8];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUserAgent() {
userAgent = null;
fieldSetFlags()[8] = false;
return this;
}
@Override
public Pageview build() {
try {
Pageview record = new Pageview();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[2]);
record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
record.ip = fieldSetFlags()[3] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[3]);
record.httpMethod = fieldSetFlags()[4] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[4]);
record.httpStatusCode = fieldSetFlags()[5] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[5]);
record.responseSize = fieldSetFlags()[6] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[6]);
record.referrer = fieldSetFlags()[7] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[7]);
record.userAgent = fieldSetFlags()[8] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[8]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
}
}
public Pageview.Tombstone getTombstone(){
return TOMBSTONE;
}
public Pageview newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Pageview implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isReferrerDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
}
package org.apache.gora.compiler.utils;
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
Schema.Parser parser = new Schema.Parser();
return parser.parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
package org.apache.gora.compiler.utils;
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
Schema.Parser parser = new Schema.Parser();
return parser.parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
import org.apache.gora.store.DataStoreFactory;
private static final String SCANNER_CACHING_PROPERTIES_KEY = "scanner.caching" ;
private static final int SCANNER_CACHING_PROPERTIES_DEFAULT = 0 ;
private int scannerCaching = SCANNER_CACHING_PROPERTIES_DEFAULT ;
try {
this.setScannerCaching(
Integer.valueOf(DataStoreFactory.findProperty(this.properties, this,
SCANNER_CACHING_PROPERTIES_KEY,
String.valueOf(SCANNER_CACHING_PROPERTIES_DEFAULT)))) ;
}catch(Exception e){
}
scan.setCaching(this.getScannerCaching()) ;
public int getScannerCaching() {
return this.scannerCaching ;
}
public HBaseStore<K, T> setScannerCaching(int numRows) {
if (numRows < 0) {
return this ;
}
this.scannerCaching = numRows ;
return this ;
}
try{
store.close();
}catch(Exception e){
}
try{
store.put(key, (Persistent) value);
counter.increment();
if (counter.isModulo()) {
store.flush();
}
}catch(Exception e){
}
"could be found. Please report this to dev@gora.apache.org");
Serializer<Object> serializer = GoraSerializerTypeInferer.getSerializer(value);
case RECORD:
PersistentBase persistent = (PersistentBase) fieldValue;
PersistentBase newRecord = (PersistentBase) persistent.newInstance(new StateManagerImpl());
for (Field member: fieldSchema.getFields()) {
newRecord.put(member.pos(), persistent.get(member.pos()));
}
fieldValue = newRecord;
break;
case MAP:
StatefulHashMap map = (StatefulHashMap) fieldValue;
StatefulHashMap newMap = new StatefulHashMap();
for (Object mapKey : map.keySet()) {
newMap.put(mapKey, map.get(mapKey));
newMap.putState(mapKey, map.getState(mapKey));
}
fieldValue = newMap;
break;
case ARRAY:
GenericArray array = (GenericArray) fieldValue;
ListGenericArray newArray = new ListGenericArray(fieldSchema.getElementType());
Iterator iter = array.iterator();
while (iter.hasNext()) {
newArray.add(iter.next());
}
fieldValue = newArray;
break;
case UNION:
break;
}
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
for (Field member: schema.getFields()) {
Object memberValue = persistentBase.get(member.pos());
if (memberValue instanceof GenericArray<?>) {
if (((GenericArray)memberValue).size() == 0) {
continue;
}
this.cassandraClient.addSubColumn(key, field.name(), member.name(), memberValue);
} else {
}
break;
case MAP:
if (value != null) {
if (value instanceof StatefulHashMap<?, ?>) {
this.cassandraClient.addStatefulHashMap(key, field.name(), (StatefulHashMap<Utf8,Object>)value);
} else {
}
break;
case ARRAY:
if (value != null) {
if (value instanceof GenericArray<?>) {
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray)value);
} else {
}
break;
case UNION:
if(value != null) {
this.cassandraClient.addColumn(key, field.name(), value);
} else {
}
default:
@SuppressWarnings("unchecked")
@SuppressWarnings("unchecked")
@SuppressWarnings("unchecked")
, String defaultValue) throws IOException {
String mappingFilename = findProperty(properties, store, MAPPING_FILE, defaultValue);
InputStream mappingFile = store.getClass().getClassLoader().getResourceAsStream(mappingFilename);
if (mappingFile == null)
return mappingFilename;
String mappingFile = DataStoreFactory.getMappingFile( properties, this, DEFAULT_MAPPING_FILE );
keyStates.remove(key);
import org.apache.gora.filter.Filter;
public void setFilter(Filter<K, T> filter);
public Filter<K, T> getFilter();
void setLocalFilterEnabled(boolean enable);
boolean isLocalFilterEnabled();
import org.apache.gora.filter.Filter;
@Override
public Filter<K, T> getFilter() {
return baseQuery.getFilter();
}
@Override
public void setFilter(Filter<K, T> filter) {
baseQuery.setFilter(filter);
}
import org.apache.gora.filter.Filter;
protected Filter<K, T> filter;
protected boolean localFilterEnabled=true;
@Override
public Filter<K, T> getFilter() {
return filter;
}
@Override
public void setFilter(Filter<K, T> filter) {
this.filter=filter;
}
@Override
public boolean isLocalFilterEnabled() {
return localFilterEnabled;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
this.localFilterEnabled=enable;
}
if(!nullFields[4]) {
String filterClass = Text.readString(in);
try {
filter = (Filter<K, T>) ReflectionUtils.newInstance(ClassLoadingUtils.loadClass(filterClass), conf);
filter.readFields(in);
} catch (ClassNotFoundException e) {
throw new IOException(e);
}
}
localFilterEnabled = in.readBoolean();
if(filter != null) {
Text.writeString(out, filter.getClass().getCanonicalName());
filter.write(out);
}
out.writeBoolean(localFilterEnabled);
builder.append(localFilterEnabled, that.localFilterEnabled);
builder.append(localFilterEnabled);
builder.append("localFilterEnabled", localFilterEnabled);
import org.apache.gora.filter.Filter;
import java.io.IOException;
if(isLimitReached()) {
return false;
}
boolean ret;
do {
clear();
persistent = getOrCreatePersistent(persistent);
ret = nextInner();
if (ret == false) {
break;
}
} while (filter(key, persistent));
return ret;
}
protected boolean filter(K key, T persistent) {
if (!query.isLocalFilterEnabled()) {
return false;
}
Filter<K, T> filter = query.getFilter();
if (filter == null) {
return false;
}
return filter.filter(key, persistent);
import org.apache.gora.filter.Filter;
import java.util.Arrays;
@Override
public Filter<K, T> getFilter() {
return filter;
}
@Override
public void setFilter(Filter<K, T> filter) {
this.filter=filter;
}
@Override
public boolean isLocalFilterEnabled() {
return localFilterEnabled;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
this.localFilterEnabled=enable;
}
import org.apache.gora.filter.Filter;
protected Filter<K, T> filter;
protected boolean localFilterEnabled=true;
import org.apache.gora.filter.Filter;
@Override
public void setFilter(Filter<K, T> filter) {
}
@Override
public Filter<K, T> getFilter() {
return null;
}
@Override
public void setLocalFilterEnabled(boolean enable) {
}
@Override
public boolean isLocalFilterEnabled() {
return false;
}
public class HBaseColumn {
import org.apache.gora.hbase.util.HBaseFilterUtil;
private HBaseFilterUtil<K, T> filterUtil;
filterUtil = new HBaseFilterUtil<K, T>(this.conf);
public HBaseMapping getMapping() {
return mapping;
}
default :
break;
String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getServerAddress().getHostname();
= new HBaseScannerResult<K,T>(this, query, scanner);
if (query.getFilter() != null) {
boolean succeeded = filterUtil.setFilter(scan, query.getFilter(), this);
if (succeeded) {
query.setLocalFilterEnabled(false);
}
}
} else if (clazz.isArray() && clazz.getComponentType().equals(Byte.TYPE)) {
return (byte[])o;
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;
import java.util.Properties;
public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";
this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(),
new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
List<ColumnFamilyDefinition> columnFamilyDefinitions = this.cassandraMapping.getColumnFamilyDefinitions();
keyspaceDefinition = HFactory.createKeyspaceDefinition(this.cassandraMapping.getKeyspaceName(),
"org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
ccl.setReadCfConsistencyLevels(clmap);
ccl.setWriteCfConsistencyLevels(clmap);
String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
HFactory.createKeyspace("Keyspace", this.cluster, ccl);
", not BytesType. It may cause a fatal error on column validation later.");
private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
for (ColumnFamilyDefinition colFamDef : pColFams)
clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
return clMap;
}
public void addGenericArray(K key, String fieldName, GenericArray<?> array) {
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery =
HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
family = this.cassandraMapping.getFamily(pField);
column = this.cassandraMapping.getColumn(pField);
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery =
HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(),
ByteBufferSerializer.get(), ByteBufferSerializer.get());
return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
private static final String COL_FAM_CL = "cf.consistency.level";
private static final String READ_OP_CL = "read.consistency.level";
private static final String WRITE_OP_CL = "write.consistency.level";
public static String colFamConsLvl;
public static String readOpConsLvl;
public static String writeOpConsLvl;
private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
if (autoCreateSchema) {
colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
}
CassandraResultSet<K> cassandraResultSet) {
"sure to include this property in gora.properties file");
import org.apache.gora.store.DataStoreFactory;
preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
if (ttlAttr == null) {
ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
}
HectorUtils.insertColumn(mutator, key, columnFamily, columnName, byteBuffer, ttlAttr);
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(fieldName);
if (ttlAttr  == null) {
ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
}
HectorUtils.insertSubColumn(mutator, key, columnFamily, superColumnName, columnName, byteBuffer, ttlAttr);
public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
synchronized(mutator) {
HectorUtils.deleteColumn(mutator, key, familyName, columnName);
}
}
public void deleteByKey(K key) {
Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
deleteColumn(key, familyMap.values().iterator().next().toString(), null);
}
private static final String GCGRACE_SECONDS_ATTRIBUTE = "gc_grace_seconds";
private static final String COLUMNS_TTL_ATTRIBUTE = "ttl";
public static final String DEFAULT_COLUMNS_TTL = "60";
public static final int DEFAULT_GCGRACE_SECONDS = 30;
private Map<String, String> columnAttrMap = new HashMap<String, String>();
LOG.error("Error locating Cassandra Keyspace name attribute!");
String gcgrace_scs = element.getAttributeValue(GCGRACE_SECONDS_ATTRIBUTE);
if (gcgrace_scs == null) {
} else {
if (LOG.isDebugEnabled()) {
}
}
cfDef.setGcGraceSeconds(gcgrace_scs!=null?Integer.parseInt(gcgrace_scs):DEFAULT_GCGRACE_SECONDS);
String ttlValue = element.getAttributeValue(COLUMNS_TTL_ATTRIBUTE);
if (ttlValue == null) {
}
this.columnAttrMap.put(columnName, ttlValue!=null?ttlValue:DEFAULT_COLUMNS_TTL);
}
public Map<String,String> getFamilyMap(){
return this.familyMap;
}
public Map<String, String> getColumnsAttribs(){
return this.columnAttrMap;
}
this.cassandraClient.deleteByKey(key);
return true;
import me.prettyprint.hector.api.mutation.MutationResult;
public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
public static<K> void insertColumn(Mutator<K> mutator, K key, String columnFamily, String columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createColumn(columnName, columnValue, ttlAttr));
public static<K> HColumn<ByteBuffer,ByteBuffer> createColumn(ByteBuffer name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> HColumn<String,ByteBuffer> createColumn(String name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> HColumn<Integer,ByteBuffer> createColumn(Integer name, ByteBuffer value, String ttlAttr) {
return HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get()).setTtl(Integer.parseInt(ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void insertSubColumn(Mutator<K> mutator, K key, String columnFamily, String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
mutator.insert(key, columnFamily, createSuperColumn(superColumnName, columnName, columnValue, ttlAttr));
public static<K> void deleteColumn(Mutator<K> mutator, K key, String columnFamily, ByteBuffer columnName){
MutationResult mr = mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
System.out.println(mr.toString());
}
public static<K> HSuperColumn<String,ByteBuffer,ByteBuffer> createSuperColumn(String superColumnName, ByteBuffer columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
public static<K> HSuperColumn<String,String,ByteBuffer> createSuperColumn(String superColumnName, String columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), StringSerializer.get(), ByteBufferSerializer.get());
public static<K> HSuperColumn<String,Integer,ByteBuffer> createSuperColumn(String superColumnName, Integer columnName, ByteBuffer columnValue, String ttlAttr) {
return HFactory.createSuperColumn(superColumnName, Arrays.asList(createColumn(columnName, columnValue, ttlAttr)), StringSerializer.get(), IntegerSerializer.get(), ByteBufferSerializer.get());
LOG.debug("Keyclass and nameclass match.");
"Assuming they are the same.");
}
} else {
LOG.error("KeyClass in gora-hbase-mapping is not the same as the one in the databean.");
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
@SuppressWarnings("resource")
key = (K) ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
import java.util.concurrent.TimeUnit;
import org.apache.accumulo.core.client.BatchWriterConfig;
import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;
import org.apache.accumulo.core.client.security.tokens.PasswordToken;
import org.apache.accumulo.core.security.thrift.TCredentials;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericData;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.EncoderFactory;
import org.apache.gora.accumulo.encoders.BinaryEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
private TCredentials credentials;
public Object fromBytes(Schema schema, byte data[]) throws GoraException {
Schema fromSchema = null;
if (schema.getType() == Type.UNION) {
try {
Decoder decoder = DecoderFactory.get().binaryDecoder(data, null);
int unionIndex = decoder.readIndex();
List<Schema> possibleTypes = schema.getTypes();
fromSchema = possibleTypes.get(unionIndex);
Schema effectiveSchema = possibleTypes.get(unionIndex);
if (effectiveSchema.getType() == Type.NULL) {
decoder.readNull();
return null;
} else {
data = decoder.readBytes(null).array();
}
} catch (IOException e) {
e.printStackTrace();
throw new GoraException("Error decoding union type: ", e);
}
} else {
fromSchema = schema;
}
return fromBytes(encoder, fromSchema, data);
case BOOLEAN:
return encoder.decodeBoolean(data);
case DOUBLE:
return encoder.decodeDouble(data);
case FLOAT:
return encoder.decodeFloat(data);
case INT:
return encoder.decodeInt(data);
case LONG:
return encoder.decodeLong(data);
case STRING:
return new Utf8(data);
case BYTES:
return ByteBuffer.wrap(data);
case ENUM:
return AvroUtils.getEnumValue(schema, encoder.decodeInt(data));
case ARRAY:
break;
case FIXED:
break;
case MAP:
break;
case NULL:
break;
case RECORD:
break;
case UNION:
break;
default:
break;
public byte[] toBytes(Schema toSchema, Object o) {
if (toSchema != null && toSchema.getType() == Type.UNION) {
ByteArrayOutputStream baos = new ByteArrayOutputStream();
org.apache.avro.io.BinaryEncoder avroEncoder = EncoderFactory.get().binaryEncoder(baos, null);
int unionIndex = 0;
try {
if (o == null) {
unionIndex = firstNullSchemaTypeIndex(toSchema);
avroEncoder.writeIndex(unionIndex);
avroEncoder.writeNull();
} else {
unionIndex = firstNotNullSchemaTypeIndex(toSchema);
avroEncoder.writeIndex(unionIndex);
avroEncoder.writeBytes(toBytes(o));
}
avroEncoder.flush();
return baos.toByteArray();
} catch (IOException e) {
e.printStackTrace();
return toBytes(o);
}
} else {
return toBytes(o);
}
}
private int firstNullSchemaTypeIndex(Schema toSchema) {
List<Schema> possibleTypes = toSchema.getTypes();
int unionIndex = 0;
Type pType = possibleTypes.get(i).getType();
unionIndex = i; break;
}
}
return unionIndex;
}
private int firstNotNullSchemaTypeIndex(Schema toSchema) {
List<Schema> possibleTypes = toSchema.getTypes();
int unionIndex = 0;
Type pType = possibleTypes.get(i).getType();
unionIndex = i; break;
}
}
return unionIndex;
}
return copyIfNeeded(((Utf8) o).getBytes(), 0, ((Utf8) o).getByteLength());
return encoder.encodeInt(((Enum<?>) o).ordinal());
BatchWriterConfig batchWriterConfig = new BatchWriterConfig();
batchWriterConfig.setMaxMemory(10000000);
batchWriterConfig.setMaxLatency(60000l, TimeUnit.MILLISECONDS);
batchWriterConfig.setMaxWriteThreads(4);
batchWriter = conn.createBatchWriter(mapping.tableName, batchWriterConfig);
encoder = new BinaryEncoder();
AuthenticationToken token =  new PasswordToken(password);
credentials = new TCredentials(user,
"org.apache.accumulo.core.client.security.tokens.PasswordToken",
ByteBuffer.wrap(password.getBytes()), instance);
conn = new ZooKeeperInstance(instance, zookeepers).getConnector(user, token);
conn = new MockInstance().getConnector(user, new PasswordToken(password));
credentials = new TCredentials(user,
"org.apache.accumulo.core.client.security.tokens.PasswordToken",
ByteBuffer.wrap(password.getBytes()), conn.getInstance().getInstanceID());
Map<Utf8, Object> currentMap = null;
List currentArray = null;
BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(new byte[0], null);
if (row == null) {
row = entry.getKey().getRowData();
}
byte[] val = entry.getValue().get();
Field field = fieldMap.get(getFieldName(entry));
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = field.schema().getValueType();
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
break;
case ARRAY:
currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = field.schema().getElementType();
currentField = field;
currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
break;
Schema effectiveSchema = field.schema().getTypes()
.get(firstNotNullSchemaTypeIndex(field.schema()));
if (effectiveSchema.getType() == Type.ARRAY) {
currentArray = new DirtyListWrapper<Object>(new ArrayList<Object>());
currentArray.add(fromBytes(currentSchema, entry.getValue().get()));
}
else if (effectiveSchema.getType() == Type.MAP) {
currentMap = new DirtyMapWrapper<Utf8, Object>(new HashMap<Utf8, Object>());
currentPos = field.pos();
currentFam = entry.getKey().getColumnFamily();
currentSchema = effectiveSchema.getValueType();
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
fromBytes(currentSchema, entry.getValue().get()));
}
case RECORD:
SpecificDatumReader<?> reader = new SpecificDatumReader<Schema>(field.schema());
persistent.put(field.pos(), reader.read(null, DecoderFactory.get().binaryDecoder(val, decoder)));
break;
default:
persistent.put(field.pos(), fromBytes(field.schema(), entry.getValue().get()));
persistent.put(currentPos, new GenericData.Array<T>(currentField.schema(), currentArray));
private String getFieldName(Entry<Key, Value> entry) {
String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(),
entry.getKey().getColumnQualifier()));
if (fieldName == null) {
fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(), null));
}
return fieldName;
}
if (col != null) {
if (col.getSecond() == null) {
scanner.fetchColumnFamily(col.getFirst());
} else {
scanner.fetchColumn(col.getFirst(), col.getSecond());
}
List<Field> fields = schema.getFields();
if (!val.isDirty(i)) {
Field field = fields.get(i);
Object o = val.get(field.pos());
case MAP:
count = putMap(m, count, field.schema().getValueType(), o, col);
break;
case ARRAY:
count = putArray(m, count, o, col);
break;
Schema effectiveSchema = field.schema().getTypes()
.get(firstNotNullSchemaTypeIndex(field.schema()));
if (effectiveSchema.getType() == Type.ARRAY) {
count = putArray(m, count, o, col);
}
else if (effectiveSchema.getType() == Type.MAP) {
count = putMap(m, count, effectiveSchema.getValueType(), o, col);
}
case RECORD:
SpecificDatumWriter<Object> writer = new SpecificDatumWriter<Object>(field.schema());
ByteArrayOutputStream os = new ByteArrayOutputStream();
org.apache.avro.io.BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
writer.write(o, encoder);
encoder.flush();
m.put(col.getFirst(), col.getSecond(), new Value(os.toByteArray()));
count;
break;
default:
m.put(col.getFirst(), col.getSecond(), new Value(toBytes(o)));
count;
private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col) throws GoraException {
Text rowKey = new Text(m.getRow());
Query<K, T> query = newQuery();
query.setFields(col.getFirst().toString());
query.setStartKey((K)rowKey.toString());
query.setEndKey((K)rowKey.toString());
deleteByQuery(query);
flush();
if (o == null){
return 0;
}
Set<?> es = ((Map<?, ?>)o).entrySet();
for (Object entry : es) {
Object mapKey = ((Entry<?, ?>) entry).getKey();
Object mapVal = ((Entry<?, ?>) entry).getValue();
if ((o instanceof DirtyMapWrapper && ((DirtyMapWrapper<?, ?>)o).isDirty())
m.put(col.getFirst(), new Text(toBytes(mapKey)), new Value(toBytes(valueType, mapVal)));
count;
}
}
return count;
}
private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col) {
Text rowKey = new Text(m.getRow());
Query<K, T> query = newQuery();
query.setFields(col.getFirst().toString());
query.setStartKey((K)rowKey.toString());
query.setEndKey((K)rowKey.toString());
deleteByQuery(query);
flush();
if (o == null){
return 0;
}
int j = 0;
for (Object item : array) {
count;
}
return count;
}
tl = TabletLocator.getInstance(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));
while (tl.binRanges(Collections.singletonList(createRange(query)), binnedRanges, credentials).size() > 0) {
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, new String[] {location});
@SuppressWarnings("unchecked")
import java.io.IOException;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
if (schema.getType().equals(Type.RECORD) || schema.getType().equals(Type.MAP) ){
try {
value = AvroSerializerUtil.deserializer(value, schema);
} catch (IOException e) {
}
}
String columnName = StringSerializer.get().fromByteBuffer(cColumn.getName().duplicate());
String family = cassandraColumn.getFamily();
if (fieldName != null) {
if (fieldName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
int pos = this.persistent.getSchema().getField(fieldName).pos();
Field field = fields.get(pos);
Type fieldType = field.schema().getType();
if (fieldType.equals(Type.UNION)) {
CassandraColumn cc = getUnionTypeColumn(fieldName
CassandraStore.UNION_COL_SUFIX, cassandraRow.toArray());
Field unionField = new Field(fieldName
CassandraStore.UNION_COL_SUFIX, Schema.create(Type.INT),
null, null);
cc.setField(unionField);
Object val = cc.getValue();
cassandraColumn.setUnionType(Integer.parseInt(val.toString()));
}
cassandraColumn.setField(field);
Object value = cassandraColumn.getValue();
this.persistent.put(pos, value);
this.persistent.clearDirty(pos);
} else
@SuppressWarnings("unused")
import java.util.List;
import java.util.Map;
import org.apache.gora.cassandra.serializers.ListSerializer;
import org.apache.gora.cassandra.serializers.MapSerializer;
private Object getFieldValue(Type type, Schema fieldSchema, ByteBuffer byteBuffer){
Object value = null;
if (type.equals(Type.ARRAY)) {
ListSerializer<?> serializer = ListSerializer.get(fieldSchema.getElementType());
List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
value = genericArray;
} else if (type.equals(Type.MAP)) {
value = fromByteBuffer(fieldSchema, byteBuffer);
} else if (type.equals(Type.RECORD)){
value = fromByteBuffer(fieldSchema, byteBuffer);
} else if (type.equals(Type.UNION)){
Schema unionFieldSchema = getUnionSchema(super.getUnionType(), fieldSchema);
Type unionFieldType = unionFieldSchema.getType();
value = getFieldValue(unionFieldType, unionFieldSchema, byteBuffer);
} else {
value = fromByteBuffer(fieldSchema, byteBuffer);
}
return value;
}
Object value = getFieldValue(type, fieldSchema, byteBuffer);
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import org.apache.gora.cassandra.serializers.CharSequenceSerializer;
import org.apache.gora.cassandra.store.CassandraStore;
private Object getSuperValue(Field field, Schema fieldSchema, Type type){
List<Object> array = new ArrayList<Object>();
Map<CharSequence, Object> map = new HashMap<CharSequence, Object>();
CharSequence mapKey = CharSequenceSerializer.get().fromByteBuffer(hColumn.getName());
if (mapKey.toString().indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
Object memberValue = null;
if (fieldSchema.getValueType().getType().equals(Type.UNION)){
HColumn<ByteBuffer, ByteBuffer> cc = getUnionTypeColumn(mapKey
CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns());
Integer unionIndex = getUnionIndex(mapKey.toString(), cc);
Schema realSchema = fieldSchema.getValueType().getTypes().get(unionIndex);
memberValue = fromByteBuffer(realSchema, hColumn.getValue());
}else{
memberValue = fromByteBuffer(fieldSchema.getValueType(), hColumn.getValue());
}
map.put(mapKey, memberValue);
}
if (memberName.indexOf(CassandraStore.UNION_COL_SUFIX) < 0) {
Schema memberSchema = memberField.schema();
Type memberType = memberSchema.getType();
if (memberType.equals(Type.UNION)){
HColumn<ByteBuffer, ByteBuffer> hc = getUnionTypeColumn(memberField.name()
CassandraStore.UNION_COL_SUFIX, this.hSuperColumn.getColumns().toArray());
Integer unionIndex = getUnionIndex(memberField.name(),hc);
cassandraColumn.setUnionType(unionIndex);
}
record.put(record.getSchema().getField(memberName).pos(), cassandraColumn.getValue());
}
case UNION:
int schemaPos = this.getUnionType();
Schema unioSchema = fieldSchema.getTypes().get(schemaPos);
Type unionType = unioSchema.getType();
value = getSuperValue(field, unioSchema, unionType);
break;
Object memberValue = null;
for (HColumn<ByteBuffer, ByteBuffer> hColumn : this.hSuperColumn.getColumns()) {
memberValue = fromByteBuffer(fieldSchema, hColumn.getValue());
}
value = memberValue;
return value;
}
private Integer getUnionIndex(String fieldName, HColumn<ByteBuffer, ByteBuffer> uc){
Integer val = IntegerSerializer.get().fromByteBuffer(uc.getValue());
return Integer.parseInt(val.toString());
}
private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName,
List<HColumn<ByteBuffer, ByteBuffer>> columns) {
return getUnionTypeColumn(fieldName, columns.toArray());
}
private HColumn<ByteBuffer, ByteBuffer> getUnionTypeColumn(String fieldName, Object[] hColumns) {
@SuppressWarnings("unchecked")
HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>)hColumns[iCnt];
String columnName = StringSerializer.get().fromByteBuffer(hColumn.getNameBytes().duplicate());
if (fieldName.equals(columnName))
return hColumn;
}
return null;
}
public Object getValue() {
Field field = getField();
Schema fieldSchema = field.schema();
Type type = fieldSchema.getType();
Object value = getSuperValue(field, fieldSchema, type);
import java.util.Map;
import me.prettyprint.cassandra.serializers.ByteBufferSerializer;
import me.prettyprint.cassandra.serializers.BytesArraySerializer;
import me.prettyprint.cassandra.serializers.ObjectSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import org.apache.gora.persistency.Persistent;
serializer = CharSequenceSerializer.get();
serializer = ListSerializer.get(schema);
} else if (value instanceof Map) {
Map map = (Map)value;
serializer = MapSerializer.get(schema);
} else if (value instanceof Persistent){
serializer = ObjectSerializer.get();
}
else {
serializer = CharSequenceSerializer.get();
if (type.equals(Type.STRING)) {
serializer = CharSequenceSerializer.get();
} else if (type.equals(Type.BOOLEAN)) {
} else if (type.equals(Type.BYTES)) {
} else if (type.equals(Type.DOUBLE)) {
} else if (type.equals(Type.FLOAT)) {
} else if (type.equals(Type.INT)) {
} else if (type.equals(Type.LONG)) {
} else if (type.equals(Type.FIXED)) {
} else if (type.equals(Type.ARRAY)) {
serializer = ListSerializer.get(schema.getElementType());
} else if (type.equals(Type.MAP)) {
serializer = MapSerializer.get(schema.getValueType());
} else if (type.equals(Type.UNION)){
} else if (type.equals(Type.RECORD)){
serializer = BytesArraySerializer.get();
serializer = CharSequenceSerializer.get();
serializer = ListSerializer.get(elementType);
serializer = MapSerializer.get(elementType);
import java.util.List;
import java.util.Map;
public static Class<? extends Object> getClass(Object value) {
return Schema.createArray( getElementSchema((GenericArray<?>)value) );
} else if (clazz.isAssignableFrom(List.class)) {
} else if (clazz.isAssignableFrom(Map.class)) {
public static Class<?> getClass(Type type) {
return List.class;
return Map.class;
public static Schema getSchema(Class<?> clazz) {
public static Class<?> getClass(Schema schema) {
public static int getFixedSize(Class<?> clazz) {
public static Schema getElementSchema(GenericArray<?> array) {
this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(), new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
"org.apache.cassandra.locator.SimpleStrategy", 1, columnFamilyDefinitions);
ConfigurableConsistencyLevel configurableConsistencyLevel = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = new HashMap<String, HConsistencyLevel>();
clmap.put("ColumnFamily", HConsistencyLevel.ONE);
configurableConsistencyLevel.setReadCfConsistencyLevels(clmap);
configurableConsistencyLevel.setWriteCfConsistencyLevels(clmap);
HFactory.createKeyspace("Keyspace", this.cluster, configurableConsistencyLevel);
", not BytesType. It may cause a fatal error on column validation later.");
if (ttlAttr == null)
public void deleteColumn(K key, String familyName, ByteBuffer columnName) {
synchronized(mutator) {
HectorUtils.deleteColumn(mutator, key, familyName, columnName);
}
}
public void deleteByKey(K key) {
Map<String, String> familyMap = this.cassandraMapping.getFamilyMap();
deleteColumn(key, familyMap.values().iterator().next().toString(), null);
}
if (ttlAttr == null)
public void deleteSubColumn(K key, String fieldName) {
String columnFamily = this.cassandraMapping.getFamily(fieldName);
String superColumnName = this.cassandraMapping.getColumn(fieldName);
synchronized(mutator) {
HectorUtils.deleteSubColumn(mutator, key, columnFamily, superColumnName, null);
}
public void deleteGenericArray(K key, String fieldName) {
deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
}
if (((List<?>)itemValue).size() == 0) {
} else if (itemValue instanceof Map<?,?>) {
if (((Map<?, ?>)itemValue).size() == 0) {
public void deleteStatefulHashMap(K key, String fieldName) {
deleteSubColumn(key, fieldName);
} else {
deleteColumn(key, cassandraMapping.getFamily(fieldName), toByteBuffer(fieldName));
}
}
public void addStatefulHashMap(K key, String fieldName, Map<CharSequence,Object> map) {
if (isSuper( cassandraMapping.getFamily(fieldName) )) {
deleteSubColumn(key, fieldName);
if (!map.isEmpty()) {
for (CharSequence mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
if (mapValue instanceof GenericArray<?>) {
if (((List<?>)mapValue).size() == 0) {
continue;
}
} else if (mapValue instanceof Map<?,?>) {
if (((Map<?, ?>)mapValue).size() == 0) {
continue;
}
addSubColumn(key, fieldName, mapKey.toString(), mapValue);
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
family = this.cassandraMapping.getFamily(pField.substring(0,pField.indexOf(CassandraStore.UNION_COL_SUFIX)));
else
family = this.cassandraMapping.getFamily(pField);
return family;
}
if (pField.indexOf(CassandraStore.UNION_COL_SUFIX) > 0)
column = pField;
else
column = this.cassandraMapping.getColumn(pField);
return column;
}
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
return this.cassandraMapping.getKeyspaceName();
import java.util.HashMap;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.generic.GenericData.Array;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.commons.lang.ArrayUtils;
import org.apache.gora.persistency.Persistent;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.cassandra.serializers.AvroSerializerUtil;
private CassandraClient<K, T>  cassandraClient = new CassandraClient<K, T>();
public static String UNION_COL_SUFIX = "_UnionIndex";
public static final ThreadLocal<BinaryEncoder> encoders =
new ThreadLocal<BinaryEncoder>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap =
new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
CassandraResultSet<K> cassandraResultSet) {
cassandraRow = new CassandraRow<K>();
@SuppressWarnings("unchecked")
addOrUpdateField(key, field, field.schema(), value.get(field.pos()));
if (fields == null){
fields = this.getFields();
}
ArrayList<String> unionFields = new ArrayList<String>();
for (String field: fields){
Field schemaField =this.fieldMap.get(field);
Type type = schemaField.schema().getType();
if (type.getName().equals("UNION".toLowerCase())){
}
}
String[] arr = unionFields.toArray(new String[unionFields.size()]);
String[] both = (String[]) ArrayUtils.addAll(fields, arr);
query.setFields(both);
@SuppressWarnings("unchecked")
T p = (T) SpecificData.get().newRecord(value, schema);
List<Field> fields = schema.getFields();
if (!value.isDirty(i)) {
continue;
Field field = fields.get(i);
Type type = field.schema().getType();
Object fieldValue = value.get(field.pos());
Schema fieldSchema = field.schema();
fieldValue = getFieldValue(fieldSchema, type, fieldValue);
p.put(field.pos(), fieldValue);
private Object getFieldValue(Schema fieldSchema, Type type, Object fieldValue ){
switch(type) {
case RECORD:
Persistent persistent = (Persistent) fieldValue;
Persistent newRecord = (Persistent) SpecificData.get().newRecord(persistent, persistent.getSchema());
for (Field member: fieldSchema.getFields()) {
if (member.pos() == 0 || !persistent.isDirty()) {
continue;
}
Schema memberSchema = member.schema();
Type memberType = memberSchema.getType();
Object memberValue = persistent.get(member.pos());
newRecord.put(member.pos(), getFieldValue(memberSchema, memberType, memberValue));
}
fieldValue = newRecord;
break;
case MAP:
Map<?, ?> map = (Map<?, ?>) fieldValue;
fieldValue = map;
break;
case ARRAY:
fieldValue = (List<?>) fieldValue;
break;
case UNION:
if (fieldValue != null){
int schemaPos = getUnionSchema(fieldValue,fieldSchema);
Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
Type unionType = unionSchema.getType();
fieldValue = getFieldValue(unionSchema, unionType, fieldValue);
}
break;
default:
break;
}
return fieldValue;
}
@SuppressWarnings({ "unchecked", "rawtypes" })
private void addOrUpdateField(K key, Field field, Schema schema, Object value) {
if (field.name().indexOf(CassandraStore.UNION_COL_SUFIX) < 0){
switch (type) {
case STRING:
case BOOLEAN:
case INT:
case LONG:
case BYTES:
case FLOAT:
case DOUBLE:
case FIXED:
this.cassandraClient.addColumn(key, field.name(), value);
break;
case RECORD:
if (value != null) {
if (value instanceof PersistentBase) {
PersistentBase persistentBase = (PersistentBase) value;
try {
byte[] byteValue = AvroSerializerUtil.serializer(persistentBase, schema);
this.cassandraClient.addColumn(key, field.name(), byteValue);
} catch (IOException e) {
} else {
String familyName =  this.cassandraClient.getCassandraMapping().getFamily(field.name());
this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
break;
case MAP:
if (value != null) {
if (value instanceof Map<?, ?>) {
Map<CharSequence,Object> map = (Map<CharSequence,Object>)value;
Schema valueSchema = schema.getValueType();
Type valueType = valueSchema.getType();
if (Type.UNION.equals(valueType)){
Map<CharSequence,Object> valueMap = new HashMap<CharSequence, Object>();
for (CharSequence mapKey: map.keySet()) {
Object mapValue = map.get(mapKey);
int valueUnionIndex = getUnionSchema(mapValue, valueSchema);
valueMap.put(mapKey, mapValue);
}
map = valueMap;
}
String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
if (!this.cassandraClient.isSuper( familyName )){
try {
byte[] byteValue = AvroSerializerUtil.serializer(map, schema);
this.cassandraClient.addColumn(key, field.name(), byteValue);
} catch (IOException e) {
}
}else{
this.cassandraClient.addStatefulHashMap(key, field.name(), map);
}
} else {
}
this.cassandraClient.deleteStatefulHashMap(key, field.name());
break;
case ARRAY:
if (value != null) {
if (value instanceof DirtyListWrapper<?>) {
DirtyListWrapper fieldValue = (DirtyListWrapper<?>)value;
GenericArray valueArray = new Array(fieldValue.size(), schema);
valueArray.add(i, fieldValue.get(i));
}
this.cassandraClient.addGenericArray(key, field.name(), (GenericArray<?>)valueArray);
} else {
}
this.cassandraClient.deleteGenericArray(key, field.name());
break;
case UNION:
String familyName = this.cassandraClient.getCassandraMapping().getFamily(field.name());
if(value != null) {
int schemaPos = getUnionSchema(value, schema);
this.cassandraClient.getCassandraMapping().addColumn(familyName, columnName, columnName);
if (this.cassandraClient.isSuper( familyName )){
this.cassandraClient.addSubColumn(key, columnName, columnName, schemaPos);
}else{
this.cassandraClient.addColumn(key, columnName, schemaPos);
}
Schema unionSchema = schema.getTypes().get(schemaPos);
addOrUpdateField(key, field, unionSchema, value);
} else {
if (this.cassandraClient.isSuper( familyName )){
this.cassandraClient.deleteSubColumn(key, field.name());
} else {
this.cassandraClient.deleteColumn(key, familyName, this.cassandraClient.toByteBuffer(field.name()));
}
}
break;
default:
Type schemaType = it.next().getType();
if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
else if (pValue instanceof Integer && schemaType.equals(Type.INT))
else if (pValue instanceof Long && schemaType.equals(Type.LONG))
else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
return unionSchemaPos;
else if (pValue instanceof Map && schemaType.equals(Type.MAP))
return unionSchemaPos;
else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
return unionSchemaPos;
else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
return DEFAULT_UNION_SCHEMA;
mutator.delete(key, columnFamily, columnName, ByteBufferSerializer.get());
import java.util.ArrayList;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
throws IOException {
try{
WebPage page;
log.info("creating web page data");
page = WebPage.newBuilder().build();
page.setUrl(new Utf8(URLS[i]));
page.setParsedContent(new ArrayList<CharSequence>());
if (CONTENTS[i]!=null){
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes()));
for(String token : CONTENTS[i].split(" ")) {
page.getParsedContent().add(new Utf8(token));
}
page.getOutlinks().put(new Utf8(URLS[LINKS[i][j]]), new Utf8(ANCHORS[i][j]));
}
Metadata metadata = Metadata.newBuilder().build();
metadata.setVersion(1);
metadata.getData().put(new Utf8("metakey"), new Utf8("metavalue"));
page.setMetadata(metadata);
dataStore.put(URLS[i], page);
}
dataStore.flush();
log.info("finished creating web page data");
}
catch(Exception e){
log.info("error creating web page data");
}
package org.apache.gora.examples.generated;
public class Employee extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}]}");
__G__DIRTY(0, "__g__dirty"),
NAME(1, "name"),
DATE_OF_BIRTH(2, "dateOfBirth"),
SSN(3, "ssn"),
SALARY(4, "salary"),
BOSS(5, "boss"),
WEBPAGE(6, "webpage"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"name",
"dateOfBirth",
"ssn",
"salary",
"boss",
"webpage",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence name;
private java.lang.CharSequence ssn;
private java.lang.Object boss;
private org.apache.gora.examples.generated.WebPage webpage;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return name;
case 2: return dateOfBirth;
case 3: return ssn;
case 4: return salary;
case 5: return boss;
case 6: return webpage;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: name = (java.lang.CharSequence)(value); break;
case 2: dateOfBirth = (java.lang.Long)(value); break;
case 3: ssn = (java.lang.CharSequence)(value); break;
case 4: salary = (java.lang.Integer)(value); break;
case 5: boss = (java.lang.Object)(value); break;
case 6: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.CharSequence getName() {
return name;
}
public void setName(java.lang.CharSequence value) {
this.name = value;
setDirty(1);
}
public boolean isNameDirty(java.lang.CharSequence value) {
return isDirty(1);
}
public java.lang.Long getDateOfBirth() {
return dateOfBirth;
}
public void setDateOfBirth(java.lang.Long value) {
this.dateOfBirth = value;
setDirty(2);
}
public boolean isDateOfBirthDirty(java.lang.Long value) {
return isDirty(2);
}
public java.lang.CharSequence getSsn() {
return ssn;
}
public void setSsn(java.lang.CharSequence value) {
this.ssn = value;
setDirty(3);
}
public boolean isSsnDirty(java.lang.CharSequence value) {
return isDirty(3);
}
public java.lang.Integer getSalary() {
return salary;
}
public void setSalary(java.lang.Integer value) {
this.salary = value;
setDirty(4);
}
public boolean isSalaryDirty(java.lang.Integer value) {
return isDirty(4);
}
public java.lang.Object getBoss() {
return boss;
}
public void setBoss(java.lang.Object value) {
this.boss = value;
setDirty(5);
}
public boolean isBossDirty(java.lang.Object value) {
return isDirty(5);
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
return webpage;
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
this.webpage = value;
setDirty(6);
}
public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
return isDirty(6);
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder() {
return new org.apache.gora.examples.generated.Employee.Builder();
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee.Builder other) {
return new org.apache.gora.examples.generated.Employee.Builder(other);
}
public static org.apache.gora.examples.generated.Employee.Builder newBuilder(org.apache.gora.examples.generated.Employee other) {
return new org.apache.gora.examples.generated.Employee.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Employee>
implements org.apache.avro.data.RecordBuilder<Employee> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence name;
private long dateOfBirth;
private java.lang.CharSequence ssn;
private int salary;
private java.lang.Object boss;
private org.apache.gora.examples.generated.WebPage webpage;
private Builder() {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.Employee.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.Employee other) {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.name)) {
this.name = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.name);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.dateOfBirth)) {
this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.dateOfBirth);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.ssn)) {
this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ssn);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.salary)) {
this.salary = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.salary);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.boss)) {
this.boss = (java.lang.Object) data().deepCopy(fields()[5].schema(), other.boss);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.webpage)) {
this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[6].schema(), other.webpage);
fieldSetFlags()[6] = true;
}
}
public java.lang.CharSequence getName() {
return name;
}
public org.apache.gora.examples.generated.Employee.Builder setName(java.lang.CharSequence value) {
validate(fields()[1], value);
this.name = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasName() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.Employee.Builder clearName() {
name = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getDateOfBirth() {
return dateOfBirth;
}
public org.apache.gora.examples.generated.Employee.Builder setDateOfBirth(long value) {
validate(fields()[2], value);
this.dateOfBirth = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasDateOfBirth() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.Employee.Builder clearDateOfBirth() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.CharSequence getSsn() {
return ssn;
}
public org.apache.gora.examples.generated.Employee.Builder setSsn(java.lang.CharSequence value) {
validate(fields()[3], value);
this.ssn = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasSsn() {
return fieldSetFlags()[3];
}
public org.apache.gora.examples.generated.Employee.Builder clearSsn() {
ssn = null;
fieldSetFlags()[3] = false;
return this;
}
public java.lang.Integer getSalary() {
return salary;
}
public org.apache.gora.examples.generated.Employee.Builder setSalary(int value) {
validate(fields()[4], value);
this.salary = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasSalary() {
return fieldSetFlags()[4];
}
public org.apache.gora.examples.generated.Employee.Builder clearSalary() {
fieldSetFlags()[4] = false;
return this;
}
public java.lang.Object getBoss() {
return boss;
}
public org.apache.gora.examples.generated.Employee.Builder setBoss(java.lang.Object value) {
validate(fields()[5], value);
this.boss = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasBoss() {
return fieldSetFlags()[5];
}
public org.apache.gora.examples.generated.Employee.Builder clearBoss() {
boss = null;
fieldSetFlags()[5] = false;
return this;
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
return webpage;
}
public org.apache.gora.examples.generated.Employee.Builder setWebpage(org.apache.gora.examples.generated.WebPage value) {
validate(fields()[6], value);
this.webpage = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasWebpage() {
return fieldSetFlags()[6];
}
public org.apache.gora.examples.generated.Employee.Builder clearWebpage() {
webpage = null;
fieldSetFlags()[6] = false;
return this;
}
@Override
public Employee build() {
try {
Employee record = new Employee();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.name = fieldSetFlags()[1] ? this.name : (java.lang.CharSequence) defaultValue(fields()[1]);
record.dateOfBirth = fieldSetFlags()[2] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[2]);
record.ssn = fieldSetFlags()[3] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[3]);
record.salary = fieldSetFlags()[4] ? this.salary : (java.lang.Integer) defaultValue(fields()[4]);
record.boss = fieldSetFlags()[5] ? this.boss : (java.lang.Object) defaultValue(fields()[5]);
record.webpage = fieldSetFlags()[6] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[6]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public Employee.Tombstone getTombstone(){
return TOMBSTONE;
}
public Employee newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getName() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setName(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isNameDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getDateOfBirth() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setDateOfBirth(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDateOfBirthDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getSsn() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSsn(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSsnDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getSalary() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSalary(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSalaryDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Object getBoss() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setBoss(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isBossDirty(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isWebpageDirty(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class Metadata extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");
__G__DIRTY(0, "__g__dirty"),
VERSION(1, "version"),
DATA(2, "data"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"version",
"data",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return version;
case 2: return data;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: version = (java.lang.Integer)(value); break;
case 2: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.Integer getVersion() {
return version;
}
public void setVersion(java.lang.Integer value) {
this.version = value;
setDirty(1);
}
public boolean isVersionDirty(java.lang.Integer value) {
return isDirty(1);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
return data;
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.data = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(2);
}
public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(2);
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder() {
return new org.apache.gora.examples.generated.Metadata.Builder();
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata.Builder other) {
return new org.apache.gora.examples.generated.Metadata.Builder(other);
}
public static org.apache.gora.examples.generated.Metadata.Builder newBuilder(org.apache.gora.examples.generated.Metadata other) {
return new org.apache.gora.examples.generated.Metadata.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Metadata>
implements org.apache.avro.data.RecordBuilder<Metadata> {
private java.nio.ByteBuffer __g__dirty;
private int version;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> data;
private Builder() {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.Metadata.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.Metadata other) {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.version)) {
this.version = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.version);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.data)) {
this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.data);
fieldSetFlags()[2] = true;
}
}
public java.lang.Integer getVersion() {
return version;
}
public org.apache.gora.examples.generated.Metadata.Builder setVersion(int value) {
validate(fields()[1], value);
this.version = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasVersion() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.Metadata.Builder clearVersion() {
fieldSetFlags()[1] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
return data;
}
public org.apache.gora.examples.generated.Metadata.Builder setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[2], value);
this.data = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasData() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.Metadata.Builder clearData() {
data = null;
fieldSetFlags()[2] = false;
return this;
}
@Override
public Metadata build() {
try {
Metadata record = new Metadata();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.version = fieldSetFlags()[1] ? this.version : (java.lang.Integer) defaultValue(fields()[1]);
record.data = fieldSetFlags()[2] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[2]));
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public Metadata.Tombstone getTombstone(){
return TOMBSTONE;
}
public Metadata newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getVersion() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setVersion(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isVersionDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDataDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class TokenDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");
__G__DIRTY(0, "__g__dirty"),
COUNT(1, "count"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"count",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: count = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.Integer getCount() {
return count;
}
public void setCount(java.lang.Integer value) {
this.count = value;
setDirty(1);
}
public boolean isCountDirty(java.lang.Integer value) {
return isDirty(1);
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder() {
return new org.apache.gora.examples.generated.TokenDatum.Builder();
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
}
public static org.apache.gora.examples.generated.TokenDatum.Builder newBuilder(org.apache.gora.examples.generated.TokenDatum other) {
return new org.apache.gora.examples.generated.TokenDatum.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<TokenDatum>
implements org.apache.avro.data.RecordBuilder<TokenDatum> {
private java.nio.ByteBuffer __g__dirty;
private int count;
private Builder() {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.TokenDatum.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.TokenDatum other) {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.count)) {
this.count = (java.lang.Integer) data().deepCopy(fields()[1].schema(), other.count);
fieldSetFlags()[1] = true;
}
}
public java.lang.Integer getCount() {
return count;
}
public org.apache.gora.examples.generated.TokenDatum.Builder setCount(int value) {
validate(fields()[1], value);
this.count = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasCount() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.TokenDatum.Builder clearCount() {
fieldSetFlags()[1] = false;
return this;
}
@Override
public TokenDatum build() {
try {
TokenDatum record = new TokenDatum();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.count = fieldSetFlags()[1] ? this.count : (java.lang.Integer) defaultValue(fields()[1]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public TokenDatum.Tombstone getTombstone(){
return TOMBSTONE;
}
public TokenDatum newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
public class WebPage extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");
__G__DIRTY(0, "__g__dirty"),
URL(1, "url"),
CONTENT(2, "content"),
PARSED_CONTENT(3, "parsedContent"),
OUTLINKS(4, "outlinks"),
HEADERS(5, "headers"),
METADATA(6, "metadata"),
public static final String[] _ALL_FIELDS = {
"__g__dirty",
"url",
"content",
"parsedContent",
"outlinks",
"headers",
"metadata",
};
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence url;
private java.nio.ByteBuffer content;
private java.util.List<java.lang.CharSequence> parsedContent;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
private org.apache.gora.examples.generated.Metadata metadata;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return url;
case 2: return content;
case 3: return parsedContent;
case 4: return outlinks;
case 5: return headers;
case 6: return metadata;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: url = (java.lang.CharSequence)(value); break;
case 2: content = (java.nio.ByteBuffer)(value); break;
case 3: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
case 4: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 5: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
case 6: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public void setUrl(java.lang.CharSequence value) {
this.url = value;
setDirty(1);
}
public boolean isUrlDirty(java.lang.CharSequence value) {
return isDirty(1);
}
public java.nio.ByteBuffer getContent() {
return content;
}
public void setContent(java.nio.ByteBuffer value) {
this.content = value;
setDirty(2);
}
public boolean isContentDirty(java.nio.ByteBuffer value) {
return isDirty(2);
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
return parsedContent;
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
this.parsedContent = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper(value);
setDirty(3);
}
public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
return isDirty(3);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
return outlinks;
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.outlinks = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(4);
}
public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(4);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
return headers;
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.headers = value;
setDirty(5);
}
public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
return isDirty(5);
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
return metadata;
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
this.metadata = value;
setDirty(6);
}
public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
return isDirty(6);
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder() {
return new org.apache.gora.examples.generated.WebPage.Builder();
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage.Builder other) {
return new org.apache.gora.examples.generated.WebPage.Builder(other);
}
public static org.apache.gora.examples.generated.WebPage.Builder newBuilder(org.apache.gora.examples.generated.WebPage other) {
return new org.apache.gora.examples.generated.WebPage.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<WebPage>
implements org.apache.avro.data.RecordBuilder<WebPage> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence url;
private java.nio.ByteBuffer content;
private java.util.List<java.lang.CharSequence> parsedContent;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> outlinks;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> headers;
private org.apache.gora.examples.generated.Metadata metadata;
private Builder() {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
}
private Builder(org.apache.gora.examples.generated.WebPage.Builder other) {
super(other);
}
private Builder(org.apache.gora.examples.generated.WebPage other) {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.content)) {
this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[2].schema(), other.content);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.parsedContent)) {
this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.parsedContent);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.outlinks)) {
this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.outlinks);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.headers)) {
this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[5].schema(), other.headers);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.metadata)) {
this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[6].schema(), other.metadata);
fieldSetFlags()[6] = true;
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public org.apache.gora.examples.generated.WebPage.Builder setUrl(java.lang.CharSequence value) {
validate(fields()[1], value);
this.url = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasUrl() {
return fieldSetFlags()[1];
}
public org.apache.gora.examples.generated.WebPage.Builder clearUrl() {
url = null;
fieldSetFlags()[1] = false;
return this;
}
public java.nio.ByteBuffer getContent() {
return content;
}
public org.apache.gora.examples.generated.WebPage.Builder setContent(java.nio.ByteBuffer value) {
validate(fields()[2], value);
this.content = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasContent() {
return fieldSetFlags()[2];
}
public org.apache.gora.examples.generated.WebPage.Builder clearContent() {
content = null;
fieldSetFlags()[2] = false;
return this;
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
return parsedContent;
}
public org.apache.gora.examples.generated.WebPage.Builder setParsedContent(java.util.List<java.lang.CharSequence> value) {
validate(fields()[3], value);
this.parsedContent = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasParsedContent() {
return fieldSetFlags()[3];
}
public org.apache.gora.examples.generated.WebPage.Builder clearParsedContent() {
parsedContent = null;
fieldSetFlags()[3] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
return outlinks;
}
public org.apache.gora.examples.generated.WebPage.Builder setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[4], value);
this.outlinks = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasOutlinks() {
return fieldSetFlags()[4];
}
public org.apache.gora.examples.generated.WebPage.Builder clearOutlinks() {
outlinks = null;
fieldSetFlags()[4] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
return headers;
}
public org.apache.gora.examples.generated.WebPage.Builder setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[5], value);
this.headers = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasHeaders() {
return fieldSetFlags()[5];
}
public org.apache.gora.examples.generated.WebPage.Builder clearHeaders() {
headers = null;
fieldSetFlags()[5] = false;
return this;
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
return metadata;
}
public org.apache.gora.examples.generated.WebPage.Builder setMetadata(org.apache.gora.examples.generated.Metadata value) {
validate(fields()[6], value);
this.metadata = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasMetadata() {
return fieldSetFlags()[6];
}
public org.apache.gora.examples.generated.WebPage.Builder clearMetadata() {
metadata = null;
fieldSetFlags()[6] = false;
return this;
}
@Override
public WebPage build() {
try {
WebPage record = new WebPage();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
record.content = fieldSetFlags()[2] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[2]);
record.parsedContent = fieldSetFlags()[3] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[3]));
record.outlinks = fieldSetFlags()[4] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[4]));
record.headers = fieldSetFlags()[5] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[5]);
record.metadata = fieldSetFlags()[6] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
public WebPage.Tombstone getTombstone(){
return TOMBSTONE;
}
public WebPage newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.nio.ByteBuffer getContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setContent(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isContentDirty(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isParsedContentDirty(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isOutlinksDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHeadersDirty(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetadataDirty(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
return EncoderFactory.get().binaryEncoder(getOrCreateOutputStream(), null);
return EncoderFactory.get().jsonEncoder(schema, getOrCreateOutputStream());
return DecoderFactory.get().binaryDecoder(getOrCreateInputStream(), null);
return DecoderFactory.get().jsonDecoder(schema, getOrCreateInputStream());
int fieldIndex = persistent.getSchema().getField(fieldName).pos();
@SuppressWarnings("unchecked")
import org.apache.avro.specific.SpecificDatumReader;
implements Deserializer<Persistent> {
private Class<? extends Persistent> persistentClass;
private SpecificDatumReader<Persistent> datumReader;
public PersistentDeserializer(Class<? extends Persistent> c, boolean reuseObjects) {
datumReader = new SpecificDatumReader<Persistent>(schema);
@Override
decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);
@Override
public Persistent deserialize(Persistent persistent) throws IOException {
public class PersistentSerialization
implements Serialization<Persistent> {
public Deserializer<Persistent> getDeserializer(Class<Persistent> c) {
public Serializer<Persistent> getSerializer(Class<Persistent> c) {
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
public class PersistentSerializer implements Serializer<Persistent> {
private SpecificDatumWriter<Persistent> datumWriter;
private BinaryEncoder encoder;
this.datumWriter = new SpecificDatumWriter<Persistent>();
@Override
@Override
encoder = EncoderFactory.get().directBinaryEncoder(out, null);
public void serialize(Persistent persistent) throws IOException {
import org.apache.avro.Schema.Field;
import org.apache.gora.util.AvroUtils;
try{
long deletedRows = 0;
Result<K,T> result = query.execute();
while(result.next()) {
if(delete(result.getKey()))
}
return 0;
}
catch(Exception e){
return 0;
}
List<Field> otherFields = obj.getSchema().getFields();
String[] otherFieldStrings = new String[otherFields.size()];
otherFieldStrings[i] = otherFields.get(i).name();
}
if(Arrays.equals(fields, otherFieldStrings)) {
T newObj = (T) AvroUtils.deepClonePersistent(obj);
int index = otherFields.get(i).pos();
newObj.put(index, obj.get(index));
import java.util.List;
import org.apache.avro.Schema.Field;
import org.apache.avro.specific.SpecificRecord;
import org.apache.gora.persistency.Dirtyable;
public interface Persistent extends SpecificRecord, Dirtyable {
public static String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
public abstract Tombstone getTombstone();
public List<Field> getUnmanagedFields();
Persistent newInstance();
return keyClass.newInstance();
return (T) persistent.newInstance();
import java.util.Collection;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.gora.persistency.Dirtyable;
public abstract class PersistentBase extends SpecificRecordBase implements
Persistent {
public static class PersistentData extends SpecificData {
private static final PersistentData INSTANCE = new PersistentData();
public static PersistentData get() {
return INSTANCE;
public boolean equals(SpecificRecord obj1, SpecificRecord that) {
if (that == obj1)
if (!(that instanceof SpecificRecord))
if (obj1.getClass() != that.getClass())
return PersistentData.get().compare(obj1, that, obj1.getSchema(), true) == 0;
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
dirtyBytes.put(i, (byte) 0);
for (Field field : getSchema().getFields()) {
clearDirynessIfFieldIsDirtyable(field.pos());
}
private void clearDirynessIfFieldIsDirtyable(int fieldIndex) {
if (fieldIndex == 0)
return;
Object value = get(fieldIndex);
if (value instanceof Dirtyable) {
((Dirtyable) value).clearDirty();
public void clearDirty(int fieldIndex) {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
currentByte = (byte) ((~(1 << bitOffset)) & currentByte);
dirtyBytes.put(byteOffset, currentByte);
clearDirynessIfFieldIsDirtyable(fieldIndex);
@Override
public void clearDirty(String field) {
clearDirty(getSchema().getField(field).pos());
}
@Override
public boolean isDirty() {
boolean isSubRecordDirty = false;
for (Field field : fields) {
isSubRecordDirty = isSubRecordDirty || checkIfMutableFieldAndDirty(field);
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
boolean dirty = false;
dirty = dirty || dirtyBytes.get(i) != 0;
}
return isSubRecordDirty || dirty;
}
private boolean checkIfMutableFieldAndDirty(Field field) {
if (field.pos() == 0)
return false;
switch (field.schema().getType()) {
case RECORD:
case MAP:
case ARRAY:
Object value = get(field.pos());
return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
case UNION:
value = get(field.pos());
return !(value instanceof Dirtyable) || value==null ? false : ((Dirtyable) value).isDirty();
default:
break;
}
return false;
}
@Override
public boolean isDirty(int fieldIndex) {
Field field = getSchema().getFields().get(fieldIndex);
boolean isSubRecordDirty = checkIfMutableFieldAndDirty(field);
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
return isSubRecordDirty || 0 != ((1 << bitOffset) & currentByte);
}
@Override
public boolean isDirty(String fieldName) {
Field field = getSchema().getField(fieldName);
if(field == null){
throw new IndexOutOfBoundsException
}
return isDirty(field.pos());
}
@Override
public void setDirty() {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
dirtyBytes.put(i, (byte) -128);
}
}
@Override
public void setDirty(int fieldIndex) {
ByteBuffer dirtyBytes = getDirtyBytes();
assert (dirtyBytes.position() == 0);
int byteOffset = fieldIndex / 8;
int bitOffset = fieldIndex % 8;
byte currentByte = dirtyBytes.get(byteOffset);
currentByte = (byte) ((1 << bitOffset) | currentByte);
dirtyBytes.put(byteOffset, currentByte);
}
@Override
public void setDirty(String field) {
setDirty(getSchema().getField(field).pos());
}
private ByteBuffer getDirtyBytes() {
return (ByteBuffer) get(0);
}
@Override
public void clear() {
Collection<Field> unmanagedFields = getUnmanagedFields();
for (Field field : getSchema().getFields()) {
if (!unmanagedFields.contains(field))
continue;
put(field.pos(), PersistentData.get().deepCopy(field.schema(), PersistentData.get().getDefaultValue(field)));
}
clearDirty();
}
@Override
public boolean equals(Object that) {
if (that == this) {
return true;
} else if (that instanceof Persistent) {
return PersistentData.get().equals(this, (SpecificRecord) that);
} else {
return false;
}
public List<Field> getUnmanagedFields(){
List<Field> fields = getSchema().getFields();
return fields.subList(1, fields.size());
return keyClass.newInstance();
try {
return (T) persistentClass.newInstance();
} catch (InstantiationException e) {
throw new RuntimeException(e);
} catch (IllegalAccessException e) {
e.printStackTrace();
throw new RuntimeException(e);
}
private void clearReadable() {
@Override
return true;
return true;
public static final String SCHEMA_NAME = "schema.name";
Properties properties = new Properties();
.getResourceAsStream(GORA_DEFAULT_PROPERTIES_FILE);
throws GoraException {
ReflectionUtils.newInstance(dataStoreClass);
throws GoraException {
throws GoraException {
= (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
throws GoraException {
import java.util.ArrayList;
import java.util.List;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
protected SpecificDatumReader<T> datumReader;
protected SpecificDatumWriter<T> datumWriter;
datumReader = new SpecificDatumReader<T>(schema);
datumWriter = new SpecificDatumWriter<T>(schema);
return getFields();
}
protected String[] getFields() {
List<Field> schemaFields = beanFactory.getCachedPersistent().getSchema().getFields();
List<Field> list = new ArrayList<Field>();
for (Field field : schemaFields) {
if (!Persistent.DIRTY_BYTES_FIELD_NAME.equalsIgnoreCase(field.name())) {
list.add(field);
}
}
schemaFields = list;
String[] fieldNames = new String[schemaFields.size()];
fieldNames[i] = schemaFields.get(i).name();
}
return fieldNames;
String confSchemaName = getOrCreateConf().get("preferred.schema.name");
if (confSchemaName != null) {
return confSchemaName;
}
return schemaName;
return mappingSchemaName;
return StringUtils.getClassname(persistentClass);
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
for (Field field : fields) {
public static Schema getSchema(Class<? extends Persistent> clazz)
throws SecurityException, NoSuchFieldException, IllegalArgumentException,
IllegalAccessException {
java.lang.reflect.Field field = clazz.getDeclaredField("SCHEMA$");
public static String[] getPersistentFieldNames(Persistent persistent) {
return getSchemaFieldNames(persistent.getSchema());
}
public static String[] getSchemaFieldNames(Schema schema) {
List<Field> fields = schema.getFields();
String[] fieldNames = new String[fields.size() - 1];
}
return fieldNames;
}
public static <T extends Persistent> T deepClonePersistent(T persistent) {
ByteArrayOutputStream bos = new ByteArrayOutputStream();
BinaryEncoder enc = EncoderFactory.get().binaryEncoder(bos, null);
SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<Persistent>(
persistent.getSchema());
try {
writer.write(persistent, enc);
} catch (IOException e) {
throw new RuntimeException(
"Unable to serialize avro object to byte buffer - "
"please report this issue to the Gora bugtracker "
"or your administrator.");
}
byte[] value = bos.toByteArray();
Decoder dec = DecoderFactory.get().binaryDecoder(value, null);
@SuppressWarnings("unchecked")
SpecificDatumReader<T> reader = new SpecificDatumReader<T>(
(Class<T>) persistent.getClass());
try {
return reader.read(null, dec);
} catch (IOException e) {
throw new RuntimeException(
"Unable to deserialize avro object from byte buffer - "
"please report this issue to the Gora bugtracker "
"or your administrator.");
}
}
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
public static <T> T fromBytes( byte[] val, Schema schema
, SpecificDatumReader<T> datumReader, T object)
return (T)Enum.valueOf(ReflectData.get().getClass(schema), symbol);
case STRING:  return (T)new Utf8(toString(val));
case BYTES:   return (T)ByteBuffer.wrap(val);
case INT:     return (T)Integer.valueOf(bytesToVint(val));
case LONG:    return (T)Long.valueOf(bytesToVlong(val));
case FLOAT:   return (T)Float.valueOf(toFloat(val));
case DOUBLE:  return (T)Double.valueOf(toDouble(val));
case BOOLEAN: return (T)Boolean.valueOf(val[0] != 0);
case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, schema, (SpecificRecord)object);
@SuppressWarnings("unchecked")
public static <T> byte[] toBytes(T o, Schema schema
, SpecificDatumWriter<T> datumWriter)
case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, schema, (SpecificRecord)o);
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.util.ByteBufferInputStream;
import org.apache.avro.util.ByteBufferOutputStream;
public static<T extends SpecificRecord> void serialize(OutputStream os,
SpecificDatumWriter<T> datumWriter, Schema schema, T object)
BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
datumWriter.write(object, encoder);
public static<T> void serialize(OutputStream os,
SpecificDatumWriter<T> datumWriter, Schema schema, T object)
throws IOException {
BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(os, null);
datumWriter.write(object, encoder);
encoder.flush();
}
public static<T extends SpecificRecord> byte[] serialize(SpecificDatumWriter<T> datumWriter
, Schema schema, T object) throws IOException {
public static<T> byte[] serialize(SpecificDatumWriter<T> datumWriter
, Schema schema, T object) throws IOException {
ByteArrayOutputStream os = new ByteArrayOutputStream();
serialize(os, datumWriter, schema, object);
return os.toByteArray();
}
public static<K, T extends SpecificRecord> T deserialize(InputStream is,
SpecificDatumReader<T> datumReader, Schema schema, T object)
decoder = DecoderFactory.get().binaryDecoder(is, decoder);
return (T)datumReader.read(object, decoder);
public static<K, T extends SpecificRecord> T deserialize(byte[] bytes,
SpecificDatumReader<T> datumReader, Schema schema, T object)
decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
return (T)datumReader.read(object, decoder);
public static<K, T> T deserialize(byte[] bytes,
SpecificDatumReader<T> datumReader, Schema schema, T object)
throws IOException {
decoder = DecoderFactory.get().binaryDecoder(bytes, decoder);
return (T)datumReader.read(object, decoder);
import java.lang.reflect.Method;
import org.apache.avro.specific.SpecificRecordBuilderBase;
import org.apache.gora.persistency.Persistent;
public static <T extends Persistent> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz) throws SecurityException
, NoSuchMethodException, IllegalArgumentException, IllegalAccessException, InvocationTargetException {
return (SpecificRecordBuilderBase<T>) clazz.getMethod("newBuilder").invoke(null);
}
import org.apache.gora.persistency.Tombstone;
public Tombstone getTombstone() {
return new Tombstone(){};
public Persistent newInstance() {
return new MockPersistent();
preferredSchema = properties.getProperty(PREF_SCH_NAME);
dynamoDBClient = getClient(properties.getProperty(CLI_TYP_PROP),(AWSCredentials)getConf());
dynamoDBClient.setEndpoint(properties.getProperty(ENDPOINT_PROP));
consistency = properties.getProperty(CONSISTENCY_READS);
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
try {
List<Field> fields = schema.getFields();
if (!persistent.isDirty(i)) {
Field field = fields.get(i);
throw new RuntimeException("HBase mapping for field ["
"] not found. Wrong gora-hbase-mapping.xml?");
addPutsAndDeletes(put, delete, o, field.schema().getType(),
field.schema(), hcol, hcol.getQualifier());
if (put.size() > 0) {
if (delete.size() > 0) {
table.delete(delete);
} catch (IOException ex2) {
private void addPutsAndDeletes(Put put, Delete delete, Object o, Type type,
Schema schema, HBaseColumn hcol, byte[] qualifier) throws IOException {
switch (type) {
case UNION:
if (isNullable(schema) && o == null) {
if (qualifier == null) {
delete.deleteFamily(hcol.getFamily());
} else {
delete.deleteColumn(hcol.getFamily(), qualifier);
}
} else {
int index = getResolvedUnionIndex(schema);
byte[] serializedBytes = toBytes(o, schema);
put.add(hcol.getFamily(), qualifier, serializedBytes);
} else {
Schema resolvedSchema = schema.getTypes().get(index);
addPutsAndDeletes(put, delete, o, resolvedSchema.getType(),
resolvedSchema, hcol, qualifier);
}
}
break;
case MAP:
if (qualifier == null) {
delete.deleteFamily(hcol.getFamily());
} else {
delete.deleteColumn(hcol.getFamily(), qualifier);
}
@SuppressWarnings({ "rawtypes", "unchecked" })
Set<Entry> set = ((Map) o).entrySet();
for (@SuppressWarnings("rawtypes") Entry entry : set) {
byte[] qual = toBytes(entry.getKey());
addPutsAndDeletes(put, delete, entry.getValue(), schema.getValueType()
.getType(), schema.getValueType(), hcol, qual);
}
break;
case ARRAY:
List<?> array = (List<?>) o;
int j = 0;
for (Object item : array) {
addPutsAndDeletes(put, delete, item, schema.getElementType().getType(),
}
break;
default:
byte[] serializedBytes = toBytes(o, schema);
put.add(hcol.getFamily(), qualifier, serializedBytes);
break;
}
}
private boolean isNullable(Schema unionSchema) {
for (Schema innerSchema : unionSchema.getTypes()) {
if (innerSchema.getType().equals(Schema.Type.NULL)) {
return true;
}
}
return false;
}
boolean isAllFields = Arrays.equals(fields, getFields());
addFamilyOrColumn(get, col, fieldSchema);
private void addFamilyOrColumn(Get get, HBaseColumn col, Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(get, col, resolvedSchema);
break;
case MAP:
case ARRAY:
get.addFamily(col.family);
break;
default:
get.addColumn(col.family, col.qualifier);
break;
}
}
private void addFields(Scan scan, Query<K, T> query) throws IOException {
addFamilyOrColumn(scan, col, fieldSchema);
private void addFamilyOrColumn(Scan scan, HBaseColumn col, Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(scan, col, resolvedSchema);
break;
case MAP:
case ARRAY:
scan.addFamily(col.family);
break;
default:
scan.addColumn(col.family, col.qualifier);
break;
}
}
private void addFields(Delete delete, Query<K, T> query)    throws IOException {
addFamilyOrColumn(delete, col, fieldSchema);
}
}
private void addFamilyOrColumn(Delete delete, HBaseColumn col,
Schema fieldSchema) {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
Schema resolvedSchema = fieldSchema.getTypes().get(index);
addFamilyOrColumn(delete, col, resolvedSchema);
break;
case MAP:
case ARRAY:
delete.deleteFamily(col.family);
break;
default:
delete.deleteColumn(col.family, col.qualifier);
break;
setField(result,persistent, col, field, fieldSchema);
}
persistent.clearDirty();
return persistent;
}
private void setField(Result result, T persistent, HBaseColumn col,
Field field, Schema fieldSchema) throws IOException {
switch (fieldSchema.getType()) {
case UNION:
int index = getResolvedUnionIndex(fieldSchema);
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
if (val == null) {
return;
}
setField(persistent, field, val);
} else {
Schema resolvedSchema = fieldSchema.getTypes().get(index);
setField(result, persistent, col, field, resolvedSchema);
}
break;
case MAP:
NavigableMap<byte[], byte[]> qualMap = result.getNoVersionMap().get(
col.getFamily());
if (qualMap == null) {
return;
}
Schema valueSchema = fieldSchema.getValueType();
Map<Utf8, Object> map = new HashMap<Utf8, Object>();
for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
map.put(new Utf8(Bytes.toString(e.getKey())),
fromBytes(valueSchema, e.getValue()));
}
setField(persistent, field, map);
break;
case ARRAY:
qualMap = result.getFamilyMap(col.getFamily());
if (qualMap == null) {
return;
}
valueSchema = fieldSchema.getElementType();
ArrayList<Object> arrayList = new ArrayList<Object>();
DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<Object>(arrayList);
for (Entry<byte[], byte[]> e : qualMap.entrySet()) {
dirtyListWrapper.add(fromBytes(valueSchema, e.getValue()));
}
setField(persistent, field, arrayList);
break;
default:
byte[] val = result.getValue(col.getFamily(), col.getQualifier());
if (val == null) {
return;
}
setField(persistent, field, val);
break;
}
}
private int getResolvedUnionIndex(Schema unionScema) {
if (unionScema.getTypes().size() == 2) {
Type type0 = unionScema.getTypes().get(0).getType();
Type type1 = unionScema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {
if (type0.equals(Schema.Type.NULL))
return 1;
else
return 0;
return 2;
persistent.put(field.pos(), new DirtyMapWrapper(map));
@SuppressWarnings({ "rawtypes", "unchecked" })
private void setField(T persistent, Field field, List list) {
persistent.put(field.pos(), new DirtyListWrapper(list));
}
import java.util.Map;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.RowMutations;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Call;
import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
@Override
public void batch(List<? extends Row> actions, Object[] results)
throws IOException, InterruptedException {
getTable().batch(actions, results);
}
@Override
public Object[] batch(List<? extends Row> actions) throws IOException,
InterruptedException {
return getTable().batch(actions);
}
@Override
public void mutateRow(RowMutations rm) throws IOException {
}
@Override
public Result append(Append append) throws IOException {
return null;
}
@Override
public <T extends CoprocessorProtocol> T coprocessorProxy(Class<T> protocol,
byte[] row) {
return null;
}
@Override
public <T extends CoprocessorProtocol, R> Map<byte[], R> coprocessorExec(
Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable)
throws IOException, Throwable {
return null;
}
@Override
public <T extends CoprocessorProtocol, R> void coprocessorExec(
Class<T> protocol, byte[] startKey, byte[] endKey, Call<T, R> callable,
Callback<R> callback) throws IOException, Throwable {
}
@Override
public void setAutoFlush(boolean autoFlush) {
}
@Override
public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail) {
}
@Override
public long getWriteBufferSize() {
return 0;
}
@Override
public void setWriteBufferSize(long writeBufferSize) throws IOException {
}
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.io.EncoderFactory;
private static ThreadLocal<ByteArrayOutputStream> outputStream =
new ThreadLocal<ByteArrayOutputStream>();
public static final ThreadLocal<BinaryEncoder> encoders =
new ThreadLocal<BinaryEncoder>();
public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap =
new ConcurrentHashMap<String, SpecificDatumReader<?>>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap =
new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
@SuppressWarnings({ "rawtypes" })
String schemaId = schema.getType().equals(Schema.Type.UNION) ? String.valueOf(schema.hashCode()) : schema.getFullName();
SpecificDatumReader<?> reader = (SpecificDatumReader<?>)readerMap.get(schemaId);
if (reader == null) {
SpecificDatumReader localReader=null;
if((localReader=readerMap.putIfAbsent(schemaId, reader))!=null) {
reader = localReader;
BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(val, null);
return reader.read(null, decoder);
SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap.get(schema.getFullName());
if (writer == null) {
writerMap.put(schema.getFullName(),writer);
BinaryEncoder encoderFromCache = encoders.get();
ByteArrayOutputStream bos = new ByteArrayOutputStream();
outputStream.set(bos);
BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(bos, null);
if (encoderFromCache == null) {
ByteArrayOutputStream os = outputStream.get();
writer.write(o, encoder);
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import org.apache.avro.Schema.Type;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.gora.persistency.Persistent;
private static final Logger LOG = LoggerFactory.getLogger(SolrStore.class);
public static int DEFAULT_UNION_SCHEMA = 0;
public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<String, SpecificDatumReader<?>>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<String, SpecificDatumWriter<?>>();
public void initialize(Class<K> keyClass, Class<T> persistentClass,
Properties properties) {
super.initialize(keyClass, persistentClass, properties);
String mappingFile = DataStoreFactory.getMappingFile(properties, this,
DEFAULT_MAPPING_FILE);
mapping = readMapping(mappingFile);
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(e.getStackTrace().toString());
solrServerUrl = DataStoreFactory.findProperty(properties, this,
SOLR_URL_PROPERTY, null);
solrConfig = DataStoreFactory.findProperty(properties, this,
SOLR_CONFIG_PROPERTY, null);
solrSchema = DataStoreFactory.findProperty(properties, this,
SOLR_SCHEMA_PROPERTY, null);
adminServer = new HttpSolrServer(solrServerUrl);
if (autoCreateSchema) {
String batchSizeString = DataStoreFactory.findProperty(properties, this,
SOLR_BATCH_SIZE_PROPERTY, null);
if (batchSizeString != null) {
batchSize = Integer.parseInt(batchSizeString);
} catch (NumberFormatException nfe) {
DEFAULT_BATCH_SIZE);
batch = new ArrayList<SolrInputDocument>(batchSize);
String commitWithinString = DataStoreFactory.findProperty(properties, this,
SOLR_COMMIT_WITHIN_PROPERTY, null);
if (commitWithinString != null) {
commitWithin = Integer.parseInt(commitWithinString);
} catch (NumberFormatException nfe) {
String resultsSizeString = DataStoreFactory.findProperty(properties, this,
SOLR_RESULTS_SIZE_PROPERTY, null);
if (resultsSizeString != null) {
resultsSize = Integer.parseInt(resultsSizeString);
} catch (NumberFormatException nfe) {
private SolrMapping readMapping(String filename) throws IOException {
Document doc = builder.build(getClass().getClassLoader()
.getResourceAsStream(filename));
List<Element> classes = doc.getRootElement().getChildren("class");
for (Element classElement : classes) {
if (classElement.getAttributeValue("keyClass").equals(
keyClass.getCanonicalName())
&& classElement.getAttributeValue("name").equals(
persistentClass.getCanonicalName())) {
String tableName = getSchemaName(
classElement.getAttributeValue("table"), persistentClass);
map.setCoreName(tableName);
Element primaryKeyEl = classElement.getChild("primarykey");
map.setPrimaryKey(primaryKeyEl.getAttributeValue("column"));
List<Element> fields = classElement.getChildren("field");
for (Element field : fields) {
String fieldName = field.getAttributeValue("name");
String columnName = field.getAttributeValue("column");
map.addField(fieldName, columnName);
} catch (Exception ex) {
throw new IOException(ex);
if (!schemaExists())
CoreAdminRequest.createCore(mapping.getCoreName(),
mapping.getCoreName(), adminServer, solrConfig, solrSchema);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
server.deleteByQuery("*:*");
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
server.deleteByQuery("*:*");
} catch (Exception e) {
CoreAdminRequest.unloadCore(mapping.getCoreName(), adminServer);
} catch (Exception e) {
if (e.getMessage().contains("No such core")) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
CoreAdminResponse rsp = CoreAdminRequest.getStatus(mapping.getCoreName(),
adminServer);
exists = rsp.getUptime(mapping.getCoreName()) != null;
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
private static final String toDelimitedString(String[] arr, String sep) {
if (arr == null || arr.length == 0) {
if (i > 0)
sb.append(sep);
sb.append(arr[i]);
public static String escapeQueryKey(String key) {
if (key == null) {
char c = key.charAt(i);
switch (c) {
case ':':
case '*':
break;
default:
sb.append(c);
public T get(K key, String[] fields) {
params.set(CommonParams.QT, "/get");
params.set(CommonParams.FL, toDelimitedString(fields, ","));
params.set("id", key.toString());
QueryResponse rsp = server.query(params);
Object o = rsp.getResponse().get("doc");
if (o == null) {
return newInstance((SolrDocument) o, fields);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public T newInstance(SolrDocument doc, String[] fields) throws IOException {
if (fields == null) {
fields = fieldMap.keySet().toArray(new String[fieldMap.size()]);
for (String f : fields) {
Field field = fieldMap.get(f);
if (pk.equals(f)) {
sf = mapping.getSolrField(f);
Object sv = doc.get(sf);
if (sv == null) {
Object v = deserializeFieldValue(field, fieldSchema, sv, persistent);
persistent.put(field.pos(), v);
persistent.setDirty(field.pos());
@SuppressWarnings("rawtypes")
private SpecificDatumReader getDatumReader(String schemaId, Schema fieldSchema) {
SpecificDatumReader<?> reader = (SpecificDatumReader<?>) readerMap
.get(schemaId);
if (reader == null) {
SpecificDatumReader localReader = null;
if ((localReader = readerMap.putIfAbsent(schemaId, reader)) != null) {
reader = localReader;
}
}
return reader;
}
@SuppressWarnings("rawtypes")
private SpecificDatumWriter getDatumWriter(String schemaId, Schema fieldSchema) {
SpecificDatumWriter writer = (SpecificDatumWriter<?>) writerMap
.get(schemaId);
if (writer == null) {
writerMap.put(schemaId, writer);
}
return writer;
}
@SuppressWarnings("unchecked")
private Object deserializeFieldValue(Field field, Schema fieldSchema,
Object solrValue, T persistent) throws IOException {
Object fieldValue = null;
switch (fieldSchema.getType()) {
case MAP:
case ARRAY:
case RECORD:
@SuppressWarnings("rawtypes")
SpecificDatumReader reader = getDatumReader(fieldSchema.getFullName(),
fieldSchema);
fieldValue = IOUtils.deserialize((byte[]) solrValue, reader, fieldSchema,
persistent.get(field.pos()));
break;
case ENUM:
fieldValue = AvroUtils.getEnumValue(fieldSchema, (String) solrValue);
break;
case FIXED:
throw new IOException("???");
case BYTES:
fieldValue = ByteBuffer.wrap((byte[]) solrValue);
break;
case STRING:
fieldValue = new Utf8(solrValue.toString());
break;
case UNION:
if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)) {
if (type0.equals(Schema.Type.NULL))
fieldSchema = fieldSchema.getTypes().get(1);
else
fieldSchema = fieldSchema.getTypes().get(0);
} else {
fieldSchema = fieldSchema.getTypes().get(0);
}
fieldValue = deserializeFieldValue(field, fieldSchema, solrValue,
persistent);
} else {
@SuppressWarnings("rawtypes")
SpecificDatumReader unionReader = getDatumReader(
String.valueOf(fieldSchema.hashCode()), fieldSchema);
fieldValue = IOUtils.deserialize((byte[]) solrValue, unionReader,
fieldSchema, persistent.get(field.pos()));
break;
}
break;
default:
fieldValue = solrValue;
}
return fieldValue;
}
public void put(K key, T persistent) {
if (!persistent.isDirty()) {
doc.addField(mapping.getPrimaryKey(), key);
for (Field field : fields) {
String sf = mapping.getSolrField(field.name());
if (sf == null) {
Object v = persistent.get(field.pos());
if (v == null) {
v = serializeFieldValue(fieldSchema, v);
doc.addField(sf, v);
batch.add(doc);
if (batch.size() >= batchSize) {
add(batch, commitWithin);
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
@SuppressWarnings("unchecked")
private Object serializeFieldValue(Schema fieldSchema, Object fieldValue) {
switch (fieldSchema.getType()) {
case MAP:
case ARRAY:
case RECORD:
byte[] data = null;
try {
@SuppressWarnings("rawtypes")
SpecificDatumWriter writer = getDatumWriter(fieldSchema.getFullName(),
fieldSchema);
data = IOUtils.serialize(writer, fieldSchema, fieldValue);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
}
fieldValue = data;
break;
case BYTES:
fieldValue = ((ByteBuffer) fieldValue).array();
break;
case ENUM:
case STRING:
fieldValue = fieldValue.toString();
break;
case UNION:
if (fieldSchema.getTypes().size() == 2 && isNullable(fieldSchema)) {
int schemaPos = getUnionSchema(fieldValue, fieldSchema);
Schema unionSchema = fieldSchema.getTypes().get(schemaPos);
fieldValue = serializeFieldValue(unionSchema, fieldValue);
} else {
byte[] serilazeData = null;
try {
@SuppressWarnings("rawtypes")
SpecificDatumWriter writer = getDatumWriter(
String.valueOf(fieldSchema.hashCode()), fieldSchema);
serilazeData = IOUtils.serialize(writer, fieldSchema, fieldValue);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
}
fieldValue = serilazeData;
}
break;
default:
break;
}
return fieldValue;
}
private boolean isNullable(Schema unionSchema) {
for (Schema innerSchema : unionSchema.getTypes()) {
if (innerSchema.getType().equals(Schema.Type.NULL)) {
return true;
}
}
return false;
}
private int getUnionSchema(Object pValue, Schema pUnionSchema) {
int unionSchemaPos = 0;
Iterator<Schema> it = pUnionSchema.getTypes().iterator();
while (it.hasNext()) {
Type schemaType = it.next().getType();
if (pValue instanceof Utf8 && schemaType.equals(Type.STRING))
return unionSchemaPos;
else if (pValue instanceof ByteBuffer && schemaType.equals(Type.BYTES))
return unionSchemaPos;
else if (pValue instanceof Integer && schemaType.equals(Type.INT))
return unionSchemaPos;
else if (pValue instanceof Long && schemaType.equals(Type.LONG))
return unionSchemaPos;
else if (pValue instanceof Double && schemaType.equals(Type.DOUBLE))
return unionSchemaPos;
else if (pValue instanceof Float && schemaType.equals(Type.FLOAT))
return unionSchemaPos;
else if (pValue instanceof Boolean && schemaType.equals(Type.BOOLEAN))
return unionSchemaPos;
else if (pValue instanceof Map && schemaType.equals(Type.MAP))
return unionSchemaPos;
else if (pValue instanceof List && schemaType.equals(Type.ARRAY))
return unionSchemaPos;
else if (pValue instanceof Persistent && schemaType.equals(Type.RECORD))
return unionSchemaPos;
unionSchemaPos;
}
return DEFAULT_UNION_SCHEMA;
}
public boolean delete(K key) {
escapeQueryKey(key.toString()));
LOG.info(rsp.toString());
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public long deleteByQuery(Query<K, T> query) {
String q = ((SolrQuery<K, T>) query).toSolrQuery();
UpdateResponse rsp = server.deleteByQuery(q);
LOG.info(rsp.toString());
} catch (Exception e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
public Result<K, T> execute(Query<K, T> query) {
return new SolrResult<K, T>(this, query, server, resultsSize);
} catch (IOException e) {
LOG.error(e.getMessage(), e.getStackTrace().toString());
return new SolrQuery<K, T>(this);
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
if (batch.size() > 0) {
add(batch, commitWithin);
} catch (Exception e) {
private void add(ArrayList<SolrInputDocument> batch, int commitWithin)
throws SolrServerException, IOException {
server.add(batch);
server.commit(false, true, true);
server.add(batch, commitWithin);
}
CharSequence url = pageview.getUrl();
package org.apache.gora.tutorial.log.generated;
public class MetricDatum extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AA==\"},{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}]}");
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[1]);
private java.lang.CharSequence metricDimension;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return metricDimension;
case 2: return timestamp;
case 3: return metric;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: metricDimension = (java.lang.CharSequence)(value); break;
case 2: timestamp = (java.lang.Long)(value); break;
case 3: metric = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public java.lang.CharSequence getMetricDimension() {
return metricDimension;
public void setMetricDimension(java.lang.CharSequence value) {
this.metricDimension = value;
setDirty(1);
public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
return isDirty(1);
public java.lang.Long getTimestamp() {
return timestamp;
public void setTimestamp(java.lang.Long value) {
this.timestamp = value;
setDirty(2);
public boolean isTimestampDirty(java.lang.Long value) {
return isDirty(2);
public java.lang.Long getMetric() {
return metric;
}
public void setMetric(java.lang.Long value) {
this.metric = value;
setDirty(3);
}
public boolean isMetricDirty(java.lang.Long value) {
return isDirty(3);
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder() {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder();
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
}
public static org.apache.gora.tutorial.log.generated.MetricDatum.Builder newBuilder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
return new org.apache.gora.tutorial.log.generated.MetricDatum.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MetricDatum>
implements org.apache.avro.data.RecordBuilder<MetricDatum> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence metricDimension;
private long timestamp;
private long metric;
private Builder() {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
}
private Builder(org.apache.gora.tutorial.log.generated.MetricDatum.Builder other) {
super(other);
}
private Builder(org.apache.gora.tutorial.log.generated.MetricDatum other) {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.metricDimension)) {
this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.metricDimension);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.metric)) {
this.metric = (java.lang.Long) data().deepCopy(fields()[3].schema(), other.metric);
fieldSetFlags()[3] = true;
}
}
public java.lang.CharSequence getMetricDimension() {
return metricDimension;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetricDimension(java.lang.CharSequence value) {
validate(fields()[1], value);
this.metricDimension = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasMetricDimension() {
return fieldSetFlags()[1];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetricDimension() {
metricDimension = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getTimestamp() {
return timestamp;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setTimestamp(long value) {
validate(fields()[2], value);
this.timestamp = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasTimestamp() {
return fieldSetFlags()[2];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearTimestamp() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.Long getMetric() {
return metric;
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder setMetric(long value) {
validate(fields()[3], value);
this.metric = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasMetric() {
return fieldSetFlags()[3];
}
public org.apache.gora.tutorial.log.generated.MetricDatum.Builder clearMetric() {
fieldSetFlags()[3] = false;
return this;
}
@Override
public MetricDatum build() {
try {
MetricDatum record = new MetricDatum();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[1]);
record.metricDimension = fieldSetFlags()[1] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[1]);
record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
record.metric = fieldSetFlags()[3] ? this.metric : (java.lang.Long) defaultValue(fields()[3]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
}
}
public MetricDatum.Tombstone getTombstone(){
return TOMBSTONE;
}
public MetricDatum newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getMetricDimension() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetricDimension(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDimensionDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getMetric() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetric(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
}
package org.apache.gora.tutorial.log.generated;
public class Pageview extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"__g__dirty\",\"type\":\"bytes\",\"doc\":\"Bytes used to represent weather or not a field is dirty.\",\"default\":\"AAA=\"},{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}]}");
private java.nio.ByteBuffer __g__dirty = java.nio.ByteBuffer.wrap(new byte[2]);
private java.lang.CharSequence url;
private java.lang.CharSequence ip;
private java.lang.CharSequence httpMethod;
private java.lang.CharSequence referrer;
private java.lang.CharSequence userAgent;
public org.apache.avro.Schema getSchema() { return SCHEMA$; }
public java.lang.Object get(int field$) {
switch (field$) {
case 0: return __g__dirty;
case 1: return url;
case 2: return timestamp;
case 3: return ip;
case 4: return httpMethod;
case 5: return httpStatusCode;
case 6: return responseSize;
case 7: return referrer;
case 8: return userAgent;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public void put(int field$, java.lang.Object value) {
switch (field$) {
case 0: __g__dirty = (java.nio.ByteBuffer)(value); break;
case 1: url = (java.lang.CharSequence)(value); break;
case 2: timestamp = (java.lang.Long)(value); break;
case 3: ip = (java.lang.CharSequence)(value); break;
case 4: httpMethod = (java.lang.CharSequence)(value); break;
case 5: httpStatusCode = (java.lang.Integer)(value); break;
case 6: responseSize = (java.lang.Integer)(value); break;
case 7: referrer = (java.lang.CharSequence)(value); break;
case 8: userAgent = (java.lang.CharSequence)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
public java.lang.CharSequence getUrl() {
return url;
public void setUrl(java.lang.CharSequence value) {
this.url = value;
setDirty(1);
public boolean isUrlDirty(java.lang.CharSequence value) {
return isDirty(1);
public java.lang.Long getTimestamp() {
return timestamp;
public void setTimestamp(java.lang.Long value) {
this.timestamp = value;
setDirty(2);
public boolean isTimestampDirty(java.lang.Long value) {
return isDirty(2);
public java.lang.CharSequence getIp() {
return ip;
public void setIp(java.lang.CharSequence value) {
this.ip = value;
setDirty(3);
public boolean isIpDirty(java.lang.CharSequence value) {
return isDirty(3);
public java.lang.CharSequence getHttpMethod() {
return httpMethod;
public void setHttpMethod(java.lang.CharSequence value) {
this.httpMethod = value;
setDirty(4);
public boolean isHttpMethodDirty(java.lang.CharSequence value) {
return isDirty(4);
public java.lang.Integer getHttpStatusCode() {
return httpStatusCode;
public void setHttpStatusCode(java.lang.Integer value) {
this.httpStatusCode = value;
setDirty(5);
public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
return isDirty(5);
public java.lang.Integer getResponseSize() {
return responseSize;
public void setResponseSize(java.lang.Integer value) {
this.responseSize = value;
setDirty(6);
}
public boolean isResponseSizeDirty(java.lang.Integer value) {
return isDirty(6);
}
public java.lang.CharSequence getReferrer() {
return referrer;
}
public void setReferrer(java.lang.CharSequence value) {
this.referrer = value;
setDirty(7);
}
public boolean isReferrerDirty(java.lang.CharSequence value) {
return isDirty(7);
}
public java.lang.CharSequence getUserAgent() {
return userAgent;
}
public void setUserAgent(java.lang.CharSequence value) {
this.userAgent = value;
setDirty(8);
}
public boolean isUserAgentDirty(java.lang.CharSequence value) {
return isDirty(8);
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder() {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder();
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
}
public static org.apache.gora.tutorial.log.generated.Pageview.Builder newBuilder(org.apache.gora.tutorial.log.generated.Pageview other) {
return new org.apache.gora.tutorial.log.generated.Pageview.Builder(other);
}
private static java.nio.ByteBuffer deepCopyToWriteOnlyBuffer(
java.nio.ByteBuffer input) {
java.nio.ByteBuffer copy = java.nio.ByteBuffer.allocate(input.capacity());
int position = input.position();
input.reset();
int mark = input.position();
int limit = input.limit();
input.rewind();
input.limit(input.capacity());
copy.put(input);
input.rewind();
copy.rewind();
input.position(mark);
input.mark();
copy.position(mark);
copy.mark();
input.position(position);
copy.position(position);
input.limit(limit);
copy.limit(limit);
return copy.asReadOnlyBuffer();
}
public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Pageview>
implements org.apache.avro.data.RecordBuilder<Pageview> {
private java.nio.ByteBuffer __g__dirty;
private java.lang.CharSequence url;
private long timestamp;
private java.lang.CharSequence ip;
private java.lang.CharSequence httpMethod;
private int httpStatusCode;
private int responseSize;
private java.lang.CharSequence referrer;
private java.lang.CharSequence userAgent;
private Builder() {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
}
private Builder(org.apache.gora.tutorial.log.generated.Pageview.Builder other) {
super(other);
}
private Builder(org.apache.gora.tutorial.log.generated.Pageview other) {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
if (isValidValue(fields()[0], other.__g__dirty)) {
this.__g__dirty = (java.nio.ByteBuffer) data().deepCopy(fields()[0].schema(), other.__g__dirty);
fieldSetFlags()[0] = true;
}
if (isValidValue(fields()[1], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[1].schema(), other.url);
fieldSetFlags()[1] = true;
}
if (isValidValue(fields()[2], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.timestamp);
fieldSetFlags()[2] = true;
}
if (isValidValue(fields()[3], other.ip)) {
this.ip = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.ip);
fieldSetFlags()[3] = true;
}
if (isValidValue(fields()[4], other.httpMethod)) {
this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[4].schema(), other.httpMethod);
fieldSetFlags()[4] = true;
}
if (isValidValue(fields()[5], other.httpStatusCode)) {
this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.httpStatusCode);
fieldSetFlags()[5] = true;
}
if (isValidValue(fields()[6], other.responseSize)) {
this.responseSize = (java.lang.Integer) data().deepCopy(fields()[6].schema(), other.responseSize);
fieldSetFlags()[6] = true;
}
if (isValidValue(fields()[7], other.referrer)) {
this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.referrer);
fieldSetFlags()[7] = true;
}
if (isValidValue(fields()[8], other.userAgent)) {
this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[8].schema(), other.userAgent);
fieldSetFlags()[8] = true;
}
}
public java.lang.CharSequence getUrl() {
return url;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setUrl(java.lang.CharSequence value) {
validate(fields()[1], value);
this.url = value;
fieldSetFlags()[1] = true;
return this;
}
public boolean hasUrl() {
return fieldSetFlags()[1];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUrl() {
url = null;
fieldSetFlags()[1] = false;
return this;
}
public java.lang.Long getTimestamp() {
return timestamp;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setTimestamp(long value) {
validate(fields()[2], value);
this.timestamp = value;
fieldSetFlags()[2] = true;
return this;
}
public boolean hasTimestamp() {
return fieldSetFlags()[2];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearTimestamp() {
fieldSetFlags()[2] = false;
return this;
}
public java.lang.CharSequence getIp() {
return ip;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setIp(java.lang.CharSequence value) {
validate(fields()[3], value);
this.ip = value;
fieldSetFlags()[3] = true;
return this;
}
public boolean hasIp() {
return fieldSetFlags()[3];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearIp() {
ip = null;
fieldSetFlags()[3] = false;
return this;
}
public java.lang.CharSequence getHttpMethod() {
return httpMethod;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpMethod(java.lang.CharSequence value) {
validate(fields()[4], value);
this.httpMethod = value;
fieldSetFlags()[4] = true;
return this;
}
public boolean hasHttpMethod() {
return fieldSetFlags()[4];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpMethod() {
httpMethod = null;
fieldSetFlags()[4] = false;
return this;
}
public java.lang.Integer getHttpStatusCode() {
return httpStatusCode;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setHttpStatusCode(int value) {
validate(fields()[5], value);
this.httpStatusCode = value;
fieldSetFlags()[5] = true;
return this;
}
public boolean hasHttpStatusCode() {
return fieldSetFlags()[5];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearHttpStatusCode() {
fieldSetFlags()[5] = false;
return this;
}
public java.lang.Integer getResponseSize() {
return responseSize;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setResponseSize(int value) {
validate(fields()[6], value);
this.responseSize = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasResponseSize() {
return fieldSetFlags()[6];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearResponseSize() {
fieldSetFlags()[6] = false;
return this;
}
public java.lang.CharSequence getReferrer() {
return referrer;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setReferrer(java.lang.CharSequence value) {
validate(fields()[7], value);
this.referrer = value;
fieldSetFlags()[7] = true;
return this;
}
public boolean hasReferrer() {
return fieldSetFlags()[7];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearReferrer() {
referrer = null;
fieldSetFlags()[7] = false;
return this;
}
public java.lang.CharSequence getUserAgent() {
return userAgent;
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder setUserAgent(java.lang.CharSequence value) {
validate(fields()[8], value);
this.userAgent = value;
fieldSetFlags()[8] = true;
return this;
}
public boolean hasUserAgent() {
return fieldSetFlags()[8];
}
public org.apache.gora.tutorial.log.generated.Pageview.Builder clearUserAgent() {
userAgent = null;
fieldSetFlags()[8] = false;
return this;
}
@Override
public Pageview build() {
try {
Pageview record = new Pageview();
record.__g__dirty = fieldSetFlags()[0] ? this.__g__dirty : (java.nio.ByteBuffer) java.nio.ByteBuffer.wrap(new byte[2]);
record.url = fieldSetFlags()[1] ? this.url : (java.lang.CharSequence) defaultValue(fields()[1]);
record.timestamp = fieldSetFlags()[2] ? this.timestamp : (java.lang.Long) defaultValue(fields()[2]);
record.ip = fieldSetFlags()[3] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[3]);
record.httpMethod = fieldSetFlags()[4] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[4]);
record.httpStatusCode = fieldSetFlags()[5] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[5]);
record.responseSize = fieldSetFlags()[6] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[6]);
record.referrer = fieldSetFlags()[7] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[7]);
record.userAgent = fieldSetFlags()[8] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[8]);
return record;
} catch (Exception e) {
throw new org.apache.avro.AvroRuntimeException(e);
}
}
}
public Pageview.Tombstone getTombstone(){
return TOMBSTONE;
}
public Pageview newInstance(){
return newBuilder().build();
}
private static final Tombstone TOMBSTONE = new Tombstone();
public static final class Tombstone extends Pageview implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isReferrerDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
}
package org.apache.gora.compiler.utils;
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
@SuppressWarnings("unused")
Schema.Parser parser = new Schema.Parser();
return parser.parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
String dbFieldName = mapping.getDocumentField(k);
if (dbFieldName != null && dbFieldName.length() > 0) {
proj.put(dbFieldName, true);
}
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
if (obj.isDirty()) {
persistent.clearDirty();
Map<Utf8, Object> rmap = new HashMap<Utf8, Object>();
result = new DirtyMapWrapper(rmap);
List<Utf8> arrS = new ArrayList<Utf8>();
result = new DirtyListWrapper<Utf8>(arrS);
List<ByteBuffer> arrB = new ArrayList<ByteBuffer>();
result = new DirtyListWrapper<ByteBuffer>(arrB);
List<Object> arrT = new ArrayList<Object>();
result = new DirtyListWrapper<Object>(arrT);
if (persistent.isDirty(f.pos()) && (persistent.get(f.pos()) != null)) {
if (persistent.isDirty(f.pos()) && (persistent.get(f.pos()) == null)) {
result = toMongoList((List<?>) value, elementSchema.getType());
toMongoList((List<?>) value, field.schema().getElementType()
toMongoList((List<?>) recValue, member.schema()
private BasicDBList toMongoList(Collection<?> array, Type type) {
import com.mongodb.BasicDBObject;
import com.mongodb.DBObject;
for (String k : fields) {
String dbFieldName = mapping.getDocumentField(k);
if (dbFieldName != null && dbFieldName.length() > 0) {
proj.put(dbFieldName, true);
}
public class MongoDBResult<K, T extends PersistentBase> extends
ResultBase<K, T> {
getQuery().getFields());
import org.apache.gora.persistency.impl.PersistentBase;
import org.jdom.Document;
import org.jdom.Element;
import org.jdom.input.SAXBuilder;
"' , assuming they are the same.");
field.getAttributeValue(ATT_FIELD),
field.getAttributeValue(ATT_TYPE));
import org.apache.gora.mongodb.utils.BSONDecorator;
import org.apache.gora.mongodb.utils.GoraDBEncoder;
import org.apache.gora.persistency.impl.DirtyListWrapper;
import org.apache.gora.persistency.impl.DirtyMapWrapper;
return;
MongoDBQuery<K, T> query = new MongoDBQuery<K, T>(this);
query.setFields(getFieldsToQuery(null));
return query;
PartitionQueryImpl<K, T> partitionQuery = new PartitionQueryImpl<K, T>(
query);
LOG.debug(
"Load from DBObject (MAIN), field:{}, schemaType:{}, docField:{}, storeType:{}",
new Object[] { field.name(), fieldSchema.getType(), docf, storeType });
Object result = fromDBObject(fieldSchema, storeType, field, docf,
easybson);
persistent.clearDirty();
private Object fromDBObject(final Schema fieldSchema,
final DocumentFieldType storeType, final Field field, final String docf,
final BSONDecorator easybson) {
Object result = null;
switch (fieldSchema.getType()) {
case MAP:
BasicDBObject map = easybson.getDBObject(docf);
if (map != null) {
Map<Utf8, Object> rmap = new HashMap<Utf8, Object>();
for (Entry<String, Object> e : map.entrySet()) {
String oKey = e.getKey().replace("\u00B7", ".");
switch (fieldSchema.getValueType().getType()) {
case STRING:
rmap.put(new Utf8(oKey), new Utf8((String) e.getValue()));
break;
case BYTES:
rmap.put(new Utf8(oKey), ByteBuffer.wrap((byte[]) e.getValue()));
break;
default:
rmap.put(new Utf8(oKey), e.getValue());
break;
}
result = new DirtyMapWrapper(rmap);
}
break;
case ARRAY:
List<Object> list = easybson.getDBList(docf);
switch (fieldSchema.getElementType().getType()) {
case STRING:
List<Utf8> arrS = new ArrayList<Utf8>();
for (Object o : list)
arrS.add(new Utf8((String) o));
result = new DirtyListWrapper<Utf8>(arrS);
break;
case BYTES:
List<ByteBuffer> arrB = new ArrayList<ByteBuffer>();
for (Object o : list)
arrB.add(ByteBuffer.wrap((byte[]) o));
result = new DirtyListWrapper<ByteBuffer>(arrB);
break;
default:
List<Object> arrT = new ArrayList<Object>();
for (Object o : list)
arrT.add(o);
result = new DirtyListWrapper<Object>(arrT);
break;
}
break;
case RECORD:
DBObject rec = easybson.getDBObject(docf);
if (rec == null) {
}
BSONDecorator innerBson = new BSONDecorator(rec);
Class<?> clazz = null;
try {
clazz = ClassLoadingUtils.loadClass(fieldSchema.getFullName());
} catch (ClassNotFoundException e) {
}
Persistent record = new BeanFactoryImpl(keyClass, clazz).newPersistent();
for (Field recField : fieldSchema.getFields()) {
Schema innerSchema = recField.schema();
DocumentFieldType innerStoreType = mapping
.getDocumentFieldType(innerSchema.getName());
String innerDocField = mapping.getDocumentField(recField.name()) != null ? mapping
.getDocumentField(recField.name()) : recField.name();
LOG.debug(
"Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}",
new Object[] { recField.name(), innerSchema.getType(), fieldPath,
innerStoreType });
((PersistentBase) record).put(
recField.pos(),
fromDBObject(innerSchema, innerStoreType, recField, innerDocField,
innerBson));
}
result = record;
break;
case BOOLEAN:
result = easybson.getBoolean(docf);
break;
case DOUBLE:
result = easybson.getDouble(docf);
break;
case FLOAT:
result = easybson.getDouble(docf).floatValue();
break;
case INT:
result = easybson.getInt(docf);
break;
case LONG:
result = easybson.getLong(docf);
break;
case STRING:
if (storeType == DocumentFieldType.OBJECTID) {
final Object bin = easybson.get(docf);
final ObjectId id = ObjectId.massageToObjectId(bin);
result = new Utf8(id.toString());
} else if (storeType == DocumentFieldType.DATE) {
final Object bin = easybson.get(docf);
if (bin instanceof Date) {
Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
calendar.setTime((Date) bin);
result = new Utf8(DatatypeConverter.printDateTime(calendar));
} else {
result = new Utf8(bin.toString());
}
} else {
result = easybson.getUtf8String(docf);
}
break;
case ENUM:
result = AvroUtils.getEnumValue(fieldSchema, easybson.getUtf8String(docf)
.toString());
break;
case BYTES:
case FIXED:
result = easybson.getBytes(docf);
break;
case NULL:
result = null;
break;
case UNION:
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Type.NULL) || type1.equals(Type.NULL))) {
Schema innerSchema = fieldSchema.getTypes().get(1);
DocumentFieldType innerStoreType = mapping
.getDocumentFieldType(innerSchema.getName());
LOG.debug(
"Load from DBObject (UNION), schemaType:{}, docField:{}, storeType:{}",
new Object[] { innerSchema.getType(), docf, innerStoreType });
result = fromDBObject(innerSchema, innerStoreType, field, docf,
} else {
throw new IllegalStateException(
"MongoStore doesn't support 3 types union field yet. Please update your mapping");
}
break;
default:
LOG.warn("Unable to read {}", docf);
break;
}
return result;
}
LOG.debug(
"Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}",
new Object[] { docf, f.schema().getType(), storeType });
result.put(docf,
toDBObject(f.schema(), f.schema().getType(), storeType, value));
LOG.debug(
"Transform value to DBObject (MAIN), docField:{}, schemaType:{}, storeType:{}",
new Object[] { docf, f.schema().getType(), storeType });
Object o = toDBObject(f.schema(), f.schema().getType(), storeType,
value);
private Object toDBObject(Schema fieldSchema, Type fieldType,
DocumentFieldType storeType, Object value) {
Object result = null;
switch (fieldType) {
case MAP:
if (storeType != null && storeType != DocumentFieldType.DOCUMENT) {
throw new IllegalStateException(
"Field "
fieldSchema.getType()
": to store a Gora 'map', target Mongo mapping have to be of 'document' type");
}
Schema valueSchema = fieldSchema.getValueType();
result = toMongoMap((Map<Utf8, ?>) value, valueSchema.getType());
break;
case ARRAY:
if (storeType != null && storeType != DocumentFieldType.LIST) {
throw new IllegalStateException(
"Field "
fieldSchema.getType()
": To store a Gora 'array', target Mongo mapping have to be of 'list' type");
}
Schema elementSchema = fieldSchema.getElementType();
result = toMongoList((List<?>) value, elementSchema.getType());
break;
case BYTES:
if (value != null) {
result = ((ByteBuffer) value).array();
}
break;
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case BOOLEAN:
result = value;
break;
case STRING:
if (storeType == DocumentFieldType.OBJECTID) {
if (value != null) {
ObjectId id;
try {
id = new ObjectId(value.toString());
} catch (IllegalArgumentException e1) {
": Invalid string: unable to convert to ObjectId");
}
result = id;
} else if (storeType == DocumentFieldType.DATE) {
if (value != null) {
Calendar calendar = null;
try {
calendar = DatatypeConverter.parseDateTime(value.toString());
} catch (IllegalArgumentException e1) {
try {
calendar = DatatypeConverter.parseDate(value.toString());
} catch (IllegalArgumentException e2) {
}
}
if (calendar == null) {
}
result = calendar.getTime();
}
} else {
if (value != null) {
result = value.toString();
}
}
break;
case ENUM:
if (value != null)
result = value.toString();
break;
case RECORD:
if (value == null)
break;
BasicDBObject record = new BasicDBObject();
for (Field member : fieldSchema.getFields()) {
Object innerValue = ((PersistentBase) value).get(member.pos());
String innerDoc = mapping.getDocumentField(member.name());
Type innerType = member.schema().getType();
DocumentFieldType innerStoreType = mapping
.getDocumentFieldType(innerDoc);
LOG.debug(
"Transform value to DBObject (RECORD), docField:{}, schemaType:{}, storeType:{}",
new Object[] { member.name(), member.schema().getType(),
innerStoreType });
record.put(member.name(),
toDBObject(member.schema(), innerType, innerStoreType, innerValue));
}
result = record;
break;
case UNION:
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {
Schema innerSchema = fieldSchema.getTypes().get(1);
DocumentFieldType innerStoreType = mapping
.getDocumentFieldType(innerSchema.getName());
LOG.debug(
"Transform value to DBObject (UNION), schemaType:{}, type1:{}, storeType:{}",
new Object[] { innerSchema.getType(), type1, innerStoreType });
} else {
throw new IllegalStateException(
"MongoStore doesn't support 3 types union field yet. Please update your mapping");
}
break;
case FIXED:
result = value;
break;
default:
break;
return result;
}
easybson.put(key, value);
break;
toMongoList((List<?>) recValue, member.schema().getElementType()
.getType()));
easybson.put(key, value);
break;
result = toMongoMap((Map<CharSequence, ?>) value, valueSchema.getType());
toMongoMap((Map<CharSequence, ?>) value, field.schema()
.getValueType().getType()));
toMongoMap((Map<CharSequence, ?>) recValue, member.schema()
private BasicDBObject toMongoMap(Map<CharSequence, ?> jmap, Type type) {
for (Entry<CharSequence, ?> e : jmap.entrySet()) {
for (String field : fields) {
final String docf = mapping.getDocumentField(field);
if (docf != null) {
proj.put(docf, true);
}
}
if (docf == null || !easybson.containsField(docf))
obj.clearDirty();
new Object[] { innerSchema.getType(), docf, storeType });
result = fromDBObject(innerSchema, storeType, field, docf, easybson);
new Object[] { innerSchema.getType(), type1, storeType });
result = toDBObject(innerSchema, type1, storeType, value);
performPut(key, obj);
private void performPut(K key, T obj) {
String host = paramsIterator.next();
String port = paramsIterator.next();
DB db = mapsOfClients.get(servers).getDB(dbname);
String docf = mapping.getDocumentField(field);
T persistent = newInstance(res, fields);
WriteResult writeResult = mongoClientColl.remove(removeKey);
WriteResult writeResult = mongoClientColl.remove(q);
Object bin = easybson.get(docf);
ObjectId id = ObjectId.massageToObjectId(bin);
Object bin = easybson.get(docf);
String mKey = decodeFieldKey(e.getKey());
Object mValue = e.getValue();
rmap.put(new Utf8(mKey), new Utf8((String) mValue));
rmap.put(new Utf8(mKey), ByteBuffer.wrap((byte[]) mValue));
rmap.put(new Utf8(mKey), mValue);
record.put(
String mKey = encodeFieldKey(e.getKey().toString());
Object mValue = e.getValue();
map.put(mKey, mValue.toString());
map.put(mKey, ((ByteBuffer) mValue).array());
break;
protected String encodeFieldKey(final String key) {
if (key == null) {
return null;
}
return key.replace(".", "\u00B7");
}
protected String decodeFieldKey(final String key) {
if (key == null) {
return null;
}
return key.replace("\u00B7", ".");
}
import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.DOCUMENT;
import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.LIST;
import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.valueOf;
import com.google.common.collect.ImmutableList;
if (!ImmutableList.of(DOCUMENT, LIST).contains(
documentFields.get(intermediateFieldName)))
newDocumentField(docFieldName, valueOf(fieldType.toUpperCase()));
result = fromMongoMap(fieldSchema, map);
if (list != null) {
result = fromMongoList(fieldSchema, list);
result = fromMongoString(storeType, docf, easybson);
private Object fromMongoList(Schema fieldSchema, List<Object> list) {
Object result;
switch (fieldSchema.getElementType().getType()) {
case STRING:
List<Utf8> arrS = new ArrayList<Utf8>();
for (Object o : list)
arrS.add(new Utf8((String) o));
result = new DirtyListWrapper<Utf8>(arrS);
break;
case BYTES:
List<ByteBuffer> arrB = new ArrayList<ByteBuffer>();
for (Object o : list)
arrB.add(ByteBuffer.wrap((byte[]) o));
result = new DirtyListWrapper<ByteBuffer>(arrB);
break;
default:
List<Object> arrT = new ArrayList<Object>();
for (Object o : list)
arrT.add(o);
result = new DirtyListWrapper<Object>(arrT);
break;
}
return result;
}
private Object fromMongoMap(Schema fieldSchema, BasicDBObject map) {
Object result;
Map<Utf8, Object> rmap = new HashMap<Utf8, Object>();
for (Entry<String, Object> e : map.entrySet()) {
String mKey = decodeFieldKey(e.getKey());
Object mValue = e.getValue();
switch (fieldSchema.getValueType().getType()) {
case STRING:
rmap.put(new Utf8(mKey), new Utf8((String) mValue));
break;
case BYTES:
rmap.put(new Utf8(mKey), ByteBuffer.wrap((byte[]) mValue));
break;
default:
rmap.put(new Utf8(mKey), mValue);
break;
}
}
result = new DirtyMapWrapper(rmap);
return result;
}
private Object fromMongoString(final DocumentFieldType storeType,
final String docf, final BSONDecorator easybson) {
Object result;
if (storeType == DocumentFieldType.OBJECTID) {
Object bin = easybson.get(docf);
ObjectId id = ObjectId.massageToObjectId(bin);
result = new Utf8(id.toString());
} else if (storeType == DocumentFieldType.DATE) {
Object bin = easybson.get(docf);
if (bin instanceof Date) {
Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
calendar.setTime((Date) bin);
result = new Utf8(DatatypeConverter.printDateTime(calendar));
} else {
result = new Utf8(bin.toString());
}
} else {
result = easybson.getUtf8String(docf);
}
return result;
}
result = mapToMongo((Map<CharSequence, ?>) value, valueSchema.getType());
result = listToMongo((List<?>) value, elementSchema.getType());
result = stringToMongo(fieldSchema, storeType, value);
private Object stringToMongo(final Schema fieldSchema,
final DocumentFieldType storeType, final Object value) {
Object result = null;
if (storeType == DocumentFieldType.OBJECTID) {
if (value != null) {
ObjectId id;
try {
id = new ObjectId(value.toString());
} catch (IllegalArgumentException e1) {
": Invalid string: unable to convert to ObjectId");
}
result = id;
}
} else if (storeType == DocumentFieldType.DATE) {
if (value != null) {
Calendar calendar = null;
try {
calendar = DatatypeConverter.parseDateTime(value.toString());
} catch (IllegalArgumentException e1) {
try {
calendar = DatatypeConverter.parseDate(value.toString());
} catch (IllegalArgumentException e2) {
}
}
if (calendar == null) {
}
result = calendar.getTime();
}
} else {
if (value != null) {
result = value.toString();
}
}
return result;
}
private BasicDBObject mapToMongo(Map<CharSequence, ?> jmap, Type type) {
map.put(mKey, e.getValue());
private BasicDBList listToMongo(Collection<?> array, Type type) {
public void initialize(final Class<K> keyClass,
final Class<T> pPersistentClass, final Properties properties) {
private MongoClient getClient(final String servers)
throws UnknownHostException {
private DB getDB(final String servers, final String dbname,
final String login, final String secret) throws UnknownHostException {
public String getSchemaName(final String mappingSchemaName,
final Class<?> persistentClass) {
public T get(final K key, final String[] fields) {
String[] dbFields = getFieldsToQuery(fields);
for (String field : dbFields) {
T persistent = newInstance(res, dbFields);
public void put(final K key, final T obj) {
private void performPut(final K key, final T obj) {
public boolean delete(final K key) {
public long deleteByQuery(final Query<K, T> query) {
public Result<K, T> execute(final Query<K, T> query) {
public List<PartitionQuery<K, T>> getPartitions(final Query<K, T> query)
public T newInstance(final DBObject obj, final String[] fields) {
String[] dbFields = getFieldsToQuery(fields);
for (String f : dbFields) {
result = null;
break;
result = fromMongoRecord(fieldSchema, docf, rec);
result = fromMongoUnion(fieldSchema, storeType, field, docf, easybson);
private Object fromMongoUnion(final Schema fieldSchema,
final DocumentFieldType storeType, final Field field, final String docf,
final BSONDecorator easybson) {
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Type.NULL) || type1.equals(Type.NULL))) {
Schema innerSchema = fieldSchema.getTypes().get(1);
LOG.debug(
"Load from DBObject (UNION), schemaType:{}, docField:{}, storeType:{}",
new Object[] { innerSchema.getType(), docf, storeType });
result = fromDBObject(innerSchema, storeType, field, docf, easybson);
} else {
throw new IllegalStateException(
"MongoStore doesn't support 3 types union field yet. Please update your mapping");
}
return result;
}
private Object fromMongoRecord(final Schema fieldSchema, final String docf,
final DBObject rec) {
Object result;
BSONDecorator innerBson = new BSONDecorator(rec);
Class<?> clazz = null;
try {
clazz = ClassLoadingUtils.loadClass(fieldSchema.getFullName());
} catch (ClassNotFoundException e) {
}
Persistent record = new BeanFactoryImpl(keyClass, clazz).newPersistent();
for (Field recField : fieldSchema.getFields()) {
Schema innerSchema = recField.schema();
DocumentFieldType innerStoreType = mapping
.getDocumentFieldType(innerSchema.getName());
String innerDocField = mapping.getDocumentField(recField.name()) != null ? mapping
.getDocumentField(recField.name()) : recField.name();
LOG.debug(
"Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}",
new Object[] { recField.name(), innerSchema.getType(), fieldPath,
innerStoreType });
record.put(
recField.pos(),
fromDBObject(innerSchema, innerStoreType, recField, innerDocField,
innerBson));
}
result = record;
return result;
}
private Object fromMongoList(final Schema fieldSchema, final List<Object> list) {
private Object fromMongoMap(final Schema fieldSchema, final BasicDBObject map) {
private BasicDBObject newUpdateSetInstance(final T persistent) {
private BasicDBObject newUpdateUnsetInstance(final T persistent) {
private Object toDBObject(final Schema fieldSchema, final Type fieldType,
final DocumentFieldType storeType, final Object value) {
result = recordToMongo(fieldSchema, value);
result = unionToMongo(fieldSchema, storeType, value);
private Object unionToMongo(final Schema fieldSchema,
final DocumentFieldType storeType, final Object value) {
Type type0 = fieldSchema.getTypes().get(0).getType();
Type type1 = fieldSchema.getTypes().get(1).getType();
if (!type0.equals(type1)
&& (type0.equals(Type.NULL) || type1.equals(Type.NULL))) {
Schema innerSchema = fieldSchema.getTypes().get(1);
LOG.debug(
"Transform value to DBObject (UNION), schemaType:{}, type1:{}, storeType:{}",
new Object[] { innerSchema.getType(), type1, storeType });
result = toDBObject(innerSchema, type1, storeType, value);
} else {
throw new IllegalStateException(
"MongoStore doesn't support 3 types union field yet. Please update your mapping");
}
return result;
}
private BasicDBObject recordToMongo(final Schema fieldSchema,
final Object value) {
BasicDBObject record = new BasicDBObject();
for (Field member : fieldSchema.getFields()) {
Object innerValue = ((PersistentBase) value).get(member.pos());
String innerDoc = mapping.getDocumentField(member.name());
Type innerType = member.schema().getType();
DocumentFieldType innerStoreType = mapping.getDocumentFieldType(innerDoc);
LOG.debug(
"Transform value to DBObject (RECORD), docField:{}, schemaType:{}, storeType:{}",
new Object[] { member.name(), member.schema().getType(),
innerStoreType });
record.put(member.name(),
toDBObject(member.schema(), innerType, innerStoreType, innerValue));
}
return record;
}
private BasicDBObject mapToMongo(final Map<CharSequence, ?> jmap,
final Type type) {
private BasicDBList listToMongo(final Collection<?> array, final Type type) {
result = fromMongoMap(docf, fieldSchema, easybson, field);
result = fromMongoList(docf, fieldSchema, easybson, field);
private Object fromMongoList(final String docf, final Schema fieldSchema,
final BSONDecorator easybson, final Field f) {
List<Object> list = easybson.getDBList(docf);
if (list == null) {
return null;
List<Object> rlist = new ArrayList<Object>();
for (Object item : list) {
DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
Object o = fromDBObject(fieldSchema.getElementType(), storeType, f,
"item", new BSONDecorator(new BasicDBObject("item", item)));
rlist.add(o);
}
return new DirtyListWrapper(rlist);
private Object fromMongoMap(final String docf, final Schema fieldSchema,
final BSONDecorator easybson, final Field f) {
BasicDBObject map = easybson.getDBObject(docf);
if (map == null) {
return null;
}
String mapKey = e.getKey();
String decodedMapKey = decodeFieldKey(mapKey);
DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
Object o = fromDBObject(fieldSchema.getValueType(), storeType, f, mapKey,
new BSONDecorator(map));
rmap.put(new Utf8(decodedMapKey), o);
return new DirtyMapWrapper(rmap);
Object o = toDBObject(docf, f.schema(), f.schema().getType(),
storeType, value);
result.put(docf, o);
Object o = toDBObject(docf, f.schema(), f.schema().getType(),
storeType, value);
private Object toDBObject(final String docf, final Schema fieldSchema,
final Type fieldType, final DocumentFieldType storeType,
final Object value) {
result = mapToMongo(docf, (Map<CharSequence, ?>) value, valueSchema,
valueSchema.getType());
result = listToMongo(docf, (List<?>) value, elementSchema,
elementSchema.getType());
result = recordToMongo(docf, fieldSchema, value);
result = unionToMongo(docf, fieldSchema, storeType, value);
LOG.error("Unknown field type: {}", fieldSchema.getType());
private Object unionToMongo(final String docf, final Schema fieldSchema,
result = toDBObject(docf, innerSchema, type1, storeType, value);
private BasicDBObject recordToMongo(final String docf,
final Schema fieldSchema, final Object value) {
record.put(
member.name(),
toDBObject(docf, member.schema(), innerType, innerStoreType,
innerValue));
private BasicDBObject mapToMongo(final String docf,
final Map<CharSequence, ?> value, final Schema fieldSchema,
final Type fieldType) {
if (value == null)
for (Entry<CharSequence, ?> e : value.entrySet()) {
String mapKey = e.getKey().toString();
String encodedMapKey = encodeFieldKey(mapKey);
Object mapValue = e.getValue();
DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
Object result = toDBObject(docf, fieldSchema, fieldType, storeType,
mapValue);
map.put(encodedMapKey, result);
private BasicDBList listToMongo(final String docf, final Collection<?> array,
final Schema fieldSchema, final Type fieldType) {
DocumentFieldType storeType = mapping.getDocumentFieldType(docf);
Object result = toDBObject(docf, fieldSchema, fieldType, storeType, item);
list.add(result);
import static org.apache.gora.mongodb.store.MongoMapping.DocumentFieldType.*;
import org.apache.gora.mongodb.filters.MongoFilterUtil;
private MongoFilterUtil<K, T> filterUtil;
public MongoStore() {
this.mapping = new MongoMapping();
}
filterUtil = new MongoFilterUtil<K, T>(getConf());
public MongoMapping getMapping() {
return mapping;
}
if (query.getFilter() != null) {
boolean succeeded = filterUtil.setFilter(q, query.getFilter(), this);
if (succeeded) {
query.setLocalFilterEnabled(false);
}
}
public String encodeFieldKey(final String key) {
public String decodeFieldKey(final String key) {
import java.net.MalformedURLException;
import org.apache.hadoop.util.StringUtils;
import org.apache.solr.client.solrj.impl.CloudSolrServer;
import org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer;
import org.apache.solr.client.solrj.impl.LBHttpSolrServer;
protected static final String SOLR_SOLRJSERVER_IMPL = "solr.solrjserver";
private String solrServerUrl, solrConfig, solrSchema, solrJServerImpl;
solrJServerImpl = DataStoreFactory.findProperty(properties, this,
SOLR_SOLRJSERVER_IMPL, "http");
String solrJServerType = ((solrJServerImpl == null || solrJServerImpl.equals(""))?"http":solrJServerImpl);
if (solrJServerType.toString().toLowerCase().equals("http")) {
LOG.info("Using HttpSolrServer Solrj implementation.");
this.adminServer = new HttpSolrServer(solrServerUrl);
} else if (solrJServerType.toString().toLowerCase().equals("cloud")) {
LOG.info("Using CloudSolrServer Solrj implementation.");
try {
this.adminServer = new CloudSolrServer(solrServerUrl);
} catch (MalformedURLException e) {
e.printStackTrace();
}
try {
} catch (MalformedURLException e) {
e.printStackTrace();
}
} else if (solrJServerType.toString().toLowerCase().equals("concurrent")) {
LOG.info("Using ConcurrentUpdateSolrServer Solrj implementation.");
this.adminServer = new ConcurrentUpdateSolrServer(solrServerUrl, 1000, 10);
} else if (solrJServerType.toString().toLowerCase().equals("loadbalance")) {
LOG.info("Using LBHttpSolrServer Solrj implementation.");
String[] solrUrlElements = StringUtils.split(solrServerUrl);
try {
this.adminServer = new LBHttpSolrServer(solrUrlElements);
} catch (MalformedURLException e) {
e.printStackTrace();
}
try {
} catch (MalformedURLException e) {
e.printStackTrace();
}
}
factory.setFilterUtil(this);
this.adminServer = new CloudSolrServer(solrServerUrl);
import org.apache.avro.Schema;
import org.apache.avro.Schema.Type;
import org.apache.gora.store.DataStore;
Schema persistentSchema = query.getDataStore().newPersistent().getSchema();
if (persistentSchema.getField(field).schema().getType() == Type.UNION)
Schema persistentSchema = query.getDataStore().newPersistent().getSchema();
if (persistentSchema.getField(field).schema().getType() == Type.UNION)
LOG.error("Error reading Gora records");
e.printStackTrace();
return false;
keyspaceDefinition = HFactory.createKeyspaceDefinition(
this.cassandraMapping.getKeyspaceName(),
this.cassandraMapping.getKeyspaceReplicationStrategy(),
this.cassandraMapping.getKeyspaceReplicationFactor(),
columnFamilyDefinitions
);
return;
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(columnName);
if ( null == ttlAttr ){
ttlAttr = CassandraMapping.DEFAULT_COLUMNS_TTL;
} else {
}
String ttlAttr = this.cassandraMapping.getColumnsAttribs().get(superColumnName);
if ( null == ttlAttr ) {
} else {
}
private static final String REPLICATION_FACTOR_ATTRIBUTE = "replication_factor";
private static final String REPLICATION_STRATEGY_ATTRIBUTE = "placement_strategy";
public static final String DEFAULT_REPLICATION_FACTOR = "1";
public static final String DEFAULT_REPLICATION_STRATEGY = "org.apache.cassandra.locator.SimpleStrategy";
public static final String DEFAULT_COLUMNS_TTL = "0";
public static final String DEFAULT_GCGRACE_SECONDS = "0";
private String keyspaceStrategy;
private int 	 keyspaceRF;
public String getKeyspaceReplicationStrategy() {
return this.keyspaceStrategy;
}
public int getKeyspaceReplicationFactor() {
return this.keyspaceRF;
}
this.keyspaceStrategy = keyspace.getAttributeValue( REPLICATION_STRATEGY_ATTRIBUTE );
if( null == this.keyspaceStrategy ) {
this.keyspaceStrategy = DEFAULT_REPLICATION_STRATEGY;
}
if( LOG.isDebugEnabled() ) {
}
String tmp = keyspace.getAttributeValue( REPLICATION_FACTOR_ATTRIBUTE );
if( null == tmp ) {
tmp = DEFAULT_REPLICATION_FACTOR;
}
this.keyspaceRF = Integer.parseInt( tmp );
if( LOG.isDebugEnabled() ) {
}
" and is viable ONLY FOR A SINGLE NODE CLUSTER");
LOG.warn("please update the gora-cassandra-mapping.xml file to avoid seeing this warning");
cfDef.setGcGraceSeconds(Integer.parseInt( gcgrace_scs!=null?gcgrace_scs:DEFAULT_GCGRACE_SECONDS));
int ttl = Integer.parseInt(ttlAttr);
HColumn<ByteBuffer,ByteBuffer> col = HFactory.createColumn(name, value, ByteBufferSerializer.get(), ByteBufferSerializer.get());
if( 0 < ttl ) {
col.setTtl( ttl );
}
return col;
int ttl = Integer.parseInt(ttlAttr);
HColumn<String,ByteBuffer> col = HFactory.createColumn(name, value, StringSerializer.get(), ByteBufferSerializer.get());
if( 0 < ttl ) {
col.setTtl( ttl );
}
return col;
int ttl = Integer.parseInt(ttlAttr);
HColumn<Integer,ByteBuffer> col = HFactory.createColumn(name, value, IntegerSerializer.get(), ByteBufferSerializer.get());
if( 0 < ttl ) {
col.setTtl( ttl );
}
return col;
"file backed data stores");
@SuppressWarnings({ "unchecked", "rawtypes" })
return new DirtyListWrapper<Object>(rlist);
return new DirtyMapWrapper<Utf8, Object>(rmap);
@SuppressWarnings("unchecked")
LOG.warn("Check that 'keyClass' and 'name' parameters in gora-solr-mapping.xml "
"match with intended values. A mapping mismatch has been found therefore "
"no mapping has been initialized for class mapping at position "
query.setFields(fields);
enc.flush();
return ByteBuffer.wrap(get(0).toString().getBytes());
if (obj == null) {
return null;
}
return (ByteBuffer) get(0);
import static org.apache.gora.cassandra.store.CassandraStore.colFamConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.readOpConsLvl;
import static org.apache.gora.cassandra.store.CassandraStore.writeOpConsLvl;
import java.util.Properties;
public static final String DEFAULT_HECTOR_CONSIS_LEVEL = "QUORUM";
this.cluster = HFactory.getOrCreateCluster(this.cassandraMapping.getClusterName(),
new CassandraHostConfigurator(this.cassandraMapping.getHostName()));
this.cassandraMapping.getKeyspaceName(),
ConfigurableConsistencyLevel ccl = new ConfigurableConsistencyLevel();
Map<String, HConsistencyLevel> clmap = getConsisLevelForColFams(columnFamilyDefinitions);
ccl.setReadCfConsistencyLevels(clmap);
ccl.setWriteCfConsistencyLevels(clmap);
String opConsisLvl = (readOpConsLvl!=null || !readOpConsLvl.isEmpty())?readOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultReadConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
opConsisLvl = (writeOpConsLvl!=null || !writeOpConsLvl.isEmpty())?writeOpConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
ccl.setDefaultWriteConsistencyLevel(HConsistencyLevel.valueOf(opConsisLvl));
HFactory.createKeyspace("Keyspace", this.cluster, ccl);
private Map<String, HConsistencyLevel> getConsisLevelForColFams(List<ColumnFamilyDefinition> pColFams) {
Map<String, HConsistencyLevel> clMap = new HashMap<String, HConsistencyLevel>();
String colFamConsisLvl = (colFamConsLvl != null && !colFamConsLvl.isEmpty())?colFamConsLvl:DEFAULT_HECTOR_CONSIS_LEVEL;
for (ColumnFamilyDefinition colFamDef : pColFams)
clMap.put(colFamDef.getName(), HConsistencyLevel.valueOf(colFamConsisLvl));
return clMap;
}
RangeSlicesQuery<K, ByteBuffer, ByteBuffer> rangeSlicesQuery = HFactory.createRangeSlicesQuery
(this.keyspace, this.keySerializer, ByteBufferSerializer.get(), ByteBufferSerializer.get());
RangeSuperSlicesQuery<K, String, ByteBuffer, ByteBuffer> rangeSuperSlicesQuery = HFactory.createRangeSuperSlicesQuery
(this.keyspace, this.keySerializer, StringSerializer.get(), ByteBufferSerializer.get(), ByteBufferSerializer.get());
return this.cassandraMapping.getKeyspaceName();
import org.apache.gora.store.DataStoreFactory;
private static final String COL_FAM_CL = "cf.consistency.level";
private static final String READ_OP_CL = "read.consistency.level";
private static final String WRITE_OP_CL = "write.consistency.level";
public static String colFamConsLvl;
public static String readOpConsLvl;
public static String writeOpConsLvl;
private CassandraClient<K, T> cassandraClient = new CassandraClient<K, T>();
if (autoCreateSchema) {
colFamConsLvl = DataStoreFactory.findProperty(properties, this, COL_FAM_CL, null);
readOpConsLvl = DataStoreFactory.findProperty(properties, this, READ_OP_CL, null);
writeOpConsLvl = DataStoreFactory.findProperty(properties, this, WRITE_OP_CL, null);
}
preferredSchema = DataStoreFactory.findProperty(properties, this, PREF_SCH_NAME, null);
dynamoDBClient = getClient(DataStoreFactory.findProperty(properties, this, CLI_TYP_PROP, null),(AWSCredentials)getConf());
dynamoDBClient.setEndpoint(DataStoreFactory.findProperty(properties, this, ENDPOINT_PROP, null));
consistency = DataStoreFactory.findProperty(properties, this, CONSISTENCY_READS, null);
import org.apache.accumulo.core.security.CredentialHelper;
} else {
AuthenticationToken token = new PasswordToken(password);
conn = new MockInstance().getConnector(user, token);
credentials = CredentialHelper.create(user, token, conn.getInstance().getInstanceID());
} catch(IOException e){
}
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
currentMap.put(new Utf8(entry.getKey().getColumnQualifierData().toArray()),
String fieldName = mapping.columnMap.get(new Pair<Text,Text>(entry.getKey().getColumnFamily(),
Object o = val.get(field.pos());
Object mapVal = ((Entry<?, ?>) entry).getValue();
}
}
}
import java.util.regex.Matcher;
import java.util.regex.Pattern;
String regex = "[a-z_\\.]*";
if (!key.matches(regex)) {
log.warn("Keys should be LOWERCASE. Please change that!");
key = key.toLowerCase();
}
protected static final String SOLR_BATCH_SIZE_PROPERTY = "solr.batch_size";
protected static final String SOLR_COMMIT_WITHIN_PROPERTY = "solr.commit_within";
protected static final String SOLR_RESULTS_SIZE_PROPERTY = "solr.results_size";
Schema newSchema = originalSchema;
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Employee\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"dateOfBirth\",\"type\":\"long\",\"default\":0},{\"name\":\"ssn\",\"type\":\"string\",\"default\":\"\"},{\"name\":\"salary\",\"type\":\"int\",\"default\":0},{\"name\":\"boss\",\"type\":[\"null\",\"Employee\",\"string\"],\"default\":null},{\"name\":\"webpage\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"WebPage\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}],\"default\":null}],\"default\":null}");
NAME(0, "name"),
DATE_OF_BIRTH(1, "dateOfBirth"),
SSN(2, "ssn"),
SALARY(3, "salary"),
BOSS(4, "boss"),
WEBPAGE(5, "webpage"),
public int getFieldsCount() {
return Employee._ALL_FIELDS.length;
}
case 0: return name;
case 1: return dateOfBirth;
case 2: return ssn;
case 3: return salary;
case 4: return boss;
case 5: return webpage;
@SuppressWarnings(value="unchecked")
case 0: name = (java.lang.CharSequence)(value); break;
case 1: dateOfBirth = (java.lang.Long)(value); break;
case 2: ssn = (java.lang.CharSequence)(value); break;
case 3: salary = (java.lang.Integer)(value); break;
case 4: boss = (java.lang.Object)(value); break;
case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
setDirty(0);
return isDirty(0);
setDirty(1);
return isDirty(1);
setDirty(2);
return isDirty(2);
setDirty(3);
return isDirty(3);
setDirty(4);
return isDirty(4);
setDirty(5);
return isDirty(5);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.name)) {
this.name = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.name);
if (isValidValue(fields()[1], other.dateOfBirth)) {
this.dateOfBirth = (java.lang.Long) data().deepCopy(fields()[1].schema(), other.dateOfBirth);
if (isValidValue(fields()[2], other.ssn)) {
this.ssn = (java.lang.CharSequence) data().deepCopy(fields()[2].schema(), other.ssn);
if (isValidValue(fields()[3], other.salary)) {
this.salary = (java.lang.Integer) data().deepCopy(fields()[3].schema(), other.salary);
if (isValidValue(fields()[4], other.boss)) {
this.boss = (java.lang.Object) data().deepCopy(fields()[4].schema(), other.boss);
if (isValidValue(fields()[5], other.webpage)) {
this.webpage = (org.apache.gora.examples.generated.WebPage) data().deepCopy(fields()[5].schema(), other.webpage);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
validate(fields()[1], value);
fieldSetFlags()[1] = true;
return fieldSetFlags()[1];
fieldSetFlags()[1] = false;
validate(fields()[2], value);
fieldSetFlags()[2] = true;
return fieldSetFlags()[2];
fieldSetFlags()[2] = false;
validate(fields()[3], value);
fieldSetFlags()[3] = true;
return fieldSetFlags()[3];
fieldSetFlags()[3] = false;
validate(fields()[4], value);
fieldSetFlags()[4] = true;
return fieldSetFlags()[4];
fieldSetFlags()[4] = false;
validate(fields()[5], value);
fieldSetFlags()[5] = true;
return fieldSetFlags()[5];
fieldSetFlags()[5] = false;
record.name = fieldSetFlags()[0] ? this.name : (java.lang.CharSequence) defaultValue(fields()[0]);
record.dateOfBirth = fieldSetFlags()[1] ? this.dateOfBirth : (java.lang.Long) defaultValue(fields()[1]);
record.ssn = fieldSetFlags()[2] ? this.ssn : (java.lang.CharSequence) defaultValue(fields()[2]);
record.salary = fieldSetFlags()[3] ? this.salary : (java.lang.Integer) defaultValue(fields()[3]);
record.boss = fieldSetFlags()[4] ? this.boss : (java.lang.Object) defaultValue(fields()[4]);
record.webpage = fieldSetFlags()[5] ? this.webpage : (org.apache.gora.examples.generated.WebPage) defaultValue(fields()[5]);
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"ImmutableFields\",\"namespace\":\"org.apache.gora.examples.generated\",\"doc\":\"Record with only immutable or dirtyable fields, used for testing\",\"fields\":[{\"name\":\"v1\",\"type\":\"int\",\"default\":0},{\"name\":\"v2\",\"type\":[{\"type\":\"record\",\"name\":\"V2\",\"fields\":[{\"name\":\"v3\",\"type\":\"int\",\"default\":0}]},\"null\"],\"default\":null}]}");
V1(0, "v1"),
V2(1, "v2"),
public int getFieldsCount() {
return ImmutableFields._ALL_FIELDS.length;
}
case 0: return v1;
case 1: return v2;
case 0: v1 = (java.lang.Integer)(value); break;
case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
setDirty(0);
return isDirty(0);
setDirty(1);
return isDirty(1);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.v1)) {
this.v1 = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.v1);
if (isValidValue(fields()[1], other.v2)) {
this.v2 = (org.apache.gora.examples.generated.V2) data().deepCopy(fields()[1].schema(), other.v2);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
validate(fields()[1], value);
fieldSetFlags()[1] = true;
return fieldSetFlags()[1];
fieldSetFlags()[1] = false;
record.v1 = fieldSetFlags()[0] ? this.v1 : (java.lang.Integer) defaultValue(fields()[0]);
record.v2 = fieldSetFlags()[1] ? this.v2 : (org.apache.gora.examples.generated.V2) defaultValue(fields()[1]);
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");
VERSION(0, "version"),
DATA(1, "data"),
public int getFieldsCount() {
return Metadata._ALL_FIELDS.length;
}
case 0: return version;
case 1: return data;
case 0: version = (java.lang.Integer)(value); break;
case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
setDirty(0);
return isDirty(0);
setDirty(1);
return isDirty(1);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.version)) {
this.version = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.version);
if (isValidValue(fields()[1], other.data)) {
this.data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[1].schema(), other.data);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
validate(fields()[1], value);
fieldSetFlags()[1] = true;
return fieldSetFlags()[1];
fieldSetFlags()[1] = false;
record.version = fieldSetFlags()[0] ? this.version : (java.lang.Integer) defaultValue(fields()[0]);
record.data = fieldSetFlags()[1] ? this.data : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[1]));
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TokenDatum\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"count\",\"type\":\"int\",\"default\":0}]}");
COUNT(0, "count"),
public int getFieldsCount() {
return TokenDatum._ALL_FIELDS.length;
}
case 0: return count;
case 0: count = (java.lang.Integer)(value); break;
setDirty(0);
return isDirty(0);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.count)) {
this.count = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.count);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
record.count = fieldSetFlags()[0] ? this.count : (java.lang.Integer) defaultValue(fields()[0]);
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"V2\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"v3\",\"type\":\"int\",\"default\":0}]}");
V3(0, "v3"),
public int getFieldsCount() {
return V2._ALL_FIELDS.length;
}
case 0: return v3;
case 0: v3 = (java.lang.Integer)(value); break;
setDirty(0);
return isDirty(0);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.v3)) {
this.v3 = (java.lang.Integer) data().deepCopy(fields()[0].schema(), other.v3);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
record.v3 = fieldSetFlags()[0] ? this.v3 : (java.lang.Integer) defaultValue(fields()[0]);
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":null},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]},\"default\":null}],\"default\":null}");
URL(0, "url"),
CONTENT(1, "content"),
PARSED_CONTENT(2, "parsedContent"),
OUTLINKS(3, "outlinks"),
HEADERS(4, "headers"),
METADATA(5, "metadata"),
public int getFieldsCount() {
return WebPage._ALL_FIELDS.length;
}
case 0: return url;
case 1: return content;
case 2: return parsedContent;
case 3: return outlinks;
case 4: return headers;
case 5: return metadata;
case 0: url = (java.lang.CharSequence)(value); break;
case 1: content = (java.nio.ByteBuffer)(value); break;
case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
setDirty(0);
return isDirty(0);
setDirty(1);
return isDirty(1);
setDirty(2);
return isDirty(2);
setDirty(3);
return isDirty(3);
setDirty(4);
return isDirty(4);
setDirty(5);
return isDirty(5);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.url);
if (isValidValue(fields()[1], other.content)) {
this.content = (java.nio.ByteBuffer) data().deepCopy(fields()[1].schema(), other.content);
if (isValidValue(fields()[2], other.parsedContent)) {
this.parsedContent = (java.util.List<java.lang.CharSequence>) data().deepCopy(fields()[2].schema(), other.parsedContent);
if (isValidValue(fields()[3], other.outlinks)) {
this.outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[3].schema(), other.outlinks);
if (isValidValue(fields()[4], other.headers)) {
this.headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[4].schema(), other.headers);
if (isValidValue(fields()[5], other.metadata)) {
this.metadata = (org.apache.gora.examples.generated.Metadata) data().deepCopy(fields()[5].schema(), other.metadata);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
validate(fields()[1], value);
fieldSetFlags()[1] = true;
return fieldSetFlags()[1];
fieldSetFlags()[1] = false;
validate(fields()[2], value);
fieldSetFlags()[2] = true;
return fieldSetFlags()[2];
fieldSetFlags()[2] = false;
validate(fields()[3], value);
fieldSetFlags()[3] = true;
return fieldSetFlags()[3];
fieldSetFlags()[3] = false;
validate(fields()[4], value);
fieldSetFlags()[4] = true;
return fieldSetFlags()[4];
fieldSetFlags()[4] = false;
validate(fields()[5], value);
fieldSetFlags()[5] = true;
return fieldSetFlags()[5];
fieldSetFlags()[5] = false;
record.url = fieldSetFlags()[0] ? this.url : (java.lang.CharSequence) defaultValue(fields()[0]);
record.content = fieldSetFlags()[1] ? this.content : (java.nio.ByteBuffer) defaultValue(fields()[1]);
record.parsedContent = fieldSetFlags()[2] ? this.parsedContent : (java.util.List<java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)defaultValue(fields()[2]));
record.outlinks = fieldSetFlags()[3] ? this.outlinks : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[3]));
record.headers = fieldSetFlags()[4] ? this.headers : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[4]);
record.metadata = fieldSetFlags()[5] ? this.metadata : (org.apache.gora.examples.generated.Metadata) Metadata.newBuilder().build();
}
private java.nio.ByteBuffer __g__dirty;
public PersistentBase() {
__g__dirty = java.nio.ByteBuffer.wrap(new byte[getFieldsCount()]);
}
public abstract int getFieldsCount();
return __g__dirty;
return fields;
String[] fieldNames = new String[fields.size()];
fieldNames[i] = fields.get(i).name();
public int getFieldsCount() {
return MockPersistent._ALL_FIELDS.length;
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MetricDatum\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"metricDimension\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"metric\",\"type\":\"long\",\"default\":0}],\"default\":null}");
public static enum Field {
METRIC_DIMENSION(0, "metricDimension"),
TIMESTAMP(1, "timestamp"),
METRIC(2, "metric"),
;
private int index;
private String name;
Field(int index, String name) {this.index=index;this.name=name;}
public int getIndex() {return index;}
public String getName() {return name;}
public String toString() {return name;}
};
public static final String[] _ALL_FIELDS = {
"metricDimension",
"timestamp",
"metric",
};
public int getFieldsCount() {
return MetricDatum._ALL_FIELDS.length;
}
case 0: return metricDimension;
case 1: return timestamp;
case 2: return metric;
case 0: metricDimension = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: metric = (java.lang.Long)(value); break;
setDirty(0);
return isDirty(0);
setDirty(1);
return isDirty(1);
setDirty(2);
return isDirty(2);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.metricDimension)) {
this.metricDimension = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.metricDimension);
if (isValidValue(fields()[1], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[1].schema(), other.timestamp);
if (isValidValue(fields()[2], other.metric)) {
this.metric = (java.lang.Long) data().deepCopy(fields()[2].schema(), other.metric);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
validate(fields()[1], value);
fieldSetFlags()[1] = true;
return fieldSetFlags()[1];
fieldSetFlags()[1] = false;
validate(fields()[2], value);
fieldSetFlags()[2] = true;
return fieldSetFlags()[2];
fieldSetFlags()[2] = false;
record.metricDimension = fieldSetFlags()[0] ? this.metricDimension : (java.lang.CharSequence) defaultValue(fields()[0]);
record.timestamp = fieldSetFlags()[1] ? this.timestamp : (java.lang.Long) defaultValue(fields()[1]);
record.metric = fieldSetFlags()[2] ? this.metric : (java.lang.Long) defaultValue(fields()[2]);
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Pageview\",\"namespace\":\"org.apache.gora.tutorial.log.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"timestamp\",\"type\":\"long\",\"default\":0},{\"name\":\"ip\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpMethod\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"httpStatusCode\",\"type\":\"int\",\"default\":0},{\"name\":\"responseSize\",\"type\":\"int\",\"default\":0},{\"name\":\"referrer\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"userAgent\",\"type\":[\"null\",\"string\"],\"default\":null}],\"default\":null}");
public static enum Field {
URL(0, "url"),
TIMESTAMP(1, "timestamp"),
IP(2, "ip"),
HTTP_METHOD(3, "httpMethod"),
HTTP_STATUS_CODE(4, "httpStatusCode"),
RESPONSE_SIZE(5, "responseSize"),
REFERRER(6, "referrer"),
USER_AGENT(7, "userAgent"),
;
private int index;
private String name;
Field(int index, String name) {this.index=index;this.name=name;}
public int getIndex() {return index;}
public String getName() {return name;}
public String toString() {return name;}
};
public static final String[] _ALL_FIELDS = {
"url",
"timestamp",
"ip",
"httpMethod",
"httpStatusCode",
"responseSize",
"referrer",
"userAgent",
};
public int getFieldsCount() {
return Pageview._ALL_FIELDS.length;
}
case 0: return url;
case 1: return timestamp;
case 2: return ip;
case 3: return httpMethod;
case 4: return httpStatusCode;
case 5: return responseSize;
case 6: return referrer;
case 7: return userAgent;
case 0: url = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: ip = (java.lang.CharSequence)(value); break;
case 3: httpMethod = (java.lang.CharSequence)(value); break;
case 4: httpStatusCode = (java.lang.Integer)(value); break;
case 5: responseSize = (java.lang.Integer)(value); break;
case 6: referrer = (java.lang.CharSequence)(value); break;
case 7: userAgent = (java.lang.CharSequence)(value); break;
setDirty(0);
return isDirty(0);
setDirty(1);
return isDirty(1);
setDirty(2);
return isDirty(2);
setDirty(3);
return isDirty(3);
setDirty(4);
return isDirty(4);
setDirty(5);
return isDirty(5);
setDirty(6);
return isDirty(6);
setDirty(7);
return isDirty(7);
private static java.nio.ByteBuffer deepCopyToReadOnlyBuffer(
if (isValidValue(fields()[0], other.url)) {
this.url = (java.lang.CharSequence) data().deepCopy(fields()[0].schema(), other.url);
if (isValidValue(fields()[1], other.timestamp)) {
this.timestamp = (java.lang.Long) data().deepCopy(fields()[1].schema(), other.timestamp);
if (isValidValue(fields()[2], other.ip)) {
this.ip = (java.lang.CharSequence) data().deepCopy(fields()[2].schema(), other.ip);
if (isValidValue(fields()[3], other.httpMethod)) {
this.httpMethod = (java.lang.CharSequence) data().deepCopy(fields()[3].schema(), other.httpMethod);
if (isValidValue(fields()[4], other.httpStatusCode)) {
this.httpStatusCode = (java.lang.Integer) data().deepCopy(fields()[4].schema(), other.httpStatusCode);
if (isValidValue(fields()[5], other.responseSize)) {
this.responseSize = (java.lang.Integer) data().deepCopy(fields()[5].schema(), other.responseSize);
if (isValidValue(fields()[6], other.referrer)) {
this.referrer = (java.lang.CharSequence) data().deepCopy(fields()[6].schema(), other.referrer);
if (isValidValue(fields()[7], other.userAgent)) {
this.userAgent = (java.lang.CharSequence) data().deepCopy(fields()[7].schema(), other.userAgent);
validate(fields()[0], value);
fieldSetFlags()[0] = true;
return fieldSetFlags()[0];
fieldSetFlags()[0] = false;
validate(fields()[1], value);
fieldSetFlags()[1] = true;
return fieldSetFlags()[1];
fieldSetFlags()[1] = false;
validate(fields()[2], value);
fieldSetFlags()[2] = true;
return fieldSetFlags()[2];
fieldSetFlags()[2] = false;
validate(fields()[3], value);
fieldSetFlags()[3] = true;
return fieldSetFlags()[3];
fieldSetFlags()[3] = false;
validate(fields()[4], value);
fieldSetFlags()[4] = true;
return fieldSetFlags()[4];
fieldSetFlags()[4] = false;
validate(fields()[5], value);
fieldSetFlags()[5] = true;
return fieldSetFlags()[5];
fieldSetFlags()[5] = false;
validate(fields()[6], value);
fieldSetFlags()[6] = true;
return fieldSetFlags()[6];
fieldSetFlags()[6] = false;
validate(fields()[7], value);
fieldSetFlags()[7] = true;
return fieldSetFlags()[7];
fieldSetFlags()[7] = false;
record.url = fieldSetFlags()[0] ? this.url : (java.lang.CharSequence) defaultValue(fields()[0]);
record.timestamp = fieldSetFlags()[1] ? this.timestamp : (java.lang.Long) defaultValue(fields()[1]);
record.ip = fieldSetFlags()[2] ? this.ip : (java.lang.CharSequence) defaultValue(fields()[2]);
record.httpMethod = fieldSetFlags()[3] ? this.httpMethod : (java.lang.CharSequence) defaultValue(fields()[3]);
record.httpStatusCode = fieldSetFlags()[4] ? this.httpStatusCode : (java.lang.Integer) defaultValue(fields()[4]);
record.responseSize = fieldSetFlags()[5] ? this.responseSize : (java.lang.Integer) defaultValue(fields()[5]);
record.referrer = fieldSetFlags()[6] ? this.referrer : (java.lang.CharSequence) defaultValue(fields()[6]);
record.userAgent = fieldSetFlags()[7] ? this.userAgent : (java.lang.CharSequence) defaultValue(fields()[7]);
return TOMBSTONE;
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isReferrerDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
import java.net.MalformedURLException;
import java.net.URLClassLoader;
} catch (MalformedURLException ex) {
LOG.error("Error while trying to read the mapping file {}. "
"Expected to be in the classpath "
"(ClassLoader#getResource(java.lang.String)).",
filename) ;
LOG.error("Actual classpath = {}", Arrays.asList(
((URLClassLoader) getClass().getClassLoader()).getURLs()));
throw ex ;
import java.net.MalformedURLException;
import java.net.URLClassLoader;
} catch (MalformedURLException ex) {
LOG.error("Error while trying to read the mapping file {}. "
"Expected to be in the classpath "
"(ClassLoader#getResource(java.lang.String)).",
filename) ;
LOG.error("Actual classpath = {}", Arrays.asList(
((URLClassLoader) getClass().getClassLoader()).getURLs()));
throw ex ;
import org.apache.gora.shims.hadoop.HadoopShim;
import org.apache.gora.shims.hadoop.HadoopShimFactory;
private static HadoopShim hadoopShim = HadoopShimFactory.INSTANCE().getHadoopShim();
Job job = hadoopShim.createJob(conf);
return hadoopShim.createJobContext(job.getConfiguration());
return hadoopShim.createJobContext(conf);
@SuppressWarnings("unchecked")
private static final HadoopShim hadoopShim = HadoopShimFactory.INSTANCE().getHadoopShim();
public void initialize(Class<K> keyClass, Class<T> persistentClass)
throws Exception {
initialize(keyClass, persistentClass, null);
}
public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) throws Exception {
Map<String, String> accessMap = null;
if (properties != null) {
String username = properties
.getProperty("gora.cassandrastore.username");
if (username != null) {
accessMap = new HashMap<String, String>();
accessMap.put("username", username);
String password = properties
.getProperty("gora.cassandrastore.password");
if (password != null) {
accessMap.put("password", password);
}
}
}
new CassandraHostConfigurator(this.cassandraMapping.getHostName()), accessMap);
this.cassandraClient.initialize(keyClass, persistent, properties);
e.printStackTrace();
e.printStackTrace();
public void initialize(Class<K> keyClass, Class<T> persistentClass)
throws Exception {
initialize(keyClass, persistentClass, null);
}
public void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) throws Exception {
Map<String, String> accessMap = null;
if (properties != null) {
String username = properties
.getProperty("gora.cassandrastore.username");
if (username != null) {
accessMap = new HashMap<String, String>();
accessMap.put("username", username);
String password = properties
.getProperty("gora.cassandrastore.password");
if (password != null) {
accessMap.put("password", password);
}
}
}
new CassandraHostConfigurator(this.cassandraMapping.getHostName()), accessMap);
this.cassandraClient.initialize(keyClass, persistent, properties);
e.printStackTrace();
e.printStackTrace();
Map<String, String> accessMap = null;
if (properties != null) {
String username = properties
.getProperty("gora.cassandrastore.username");
if (username != null) {
accessMap = new HashMap<String, String>();
accessMap.put("username", username);
String password = properties
.getProperty("gora.cassandrastore.password");
if (password != null) {
accessMap.put("password", password);
}
}
}
public boolean isNameDirty() {
public boolean isDateOfBirthDirty() {
public boolean isSsnDirty() {
public boolean isSalaryDirty() {
public boolean isBossDirty() {
public boolean isWebpageDirty() {
public boolean isNameDirty() {
public boolean isDateOfBirthDirty() {
public boolean isSsnDirty() {
public boolean isSalaryDirty() {
public boolean isBossDirty() {
public boolean isWebpageDirty() {
public boolean isV1Dirty() {
public boolean isV2Dirty() {
public boolean isV1Dirty() {
public boolean isV2Dirty() {
public boolean isVersionDirty() {
public boolean isDataDirty() {
public boolean isVersionDirty() {
public boolean isDataDirty() {
public boolean isCountDirty() {
public boolean isCountDirty() {
public boolean isV3Dirty() {
public boolean isV3Dirty() {
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":{}},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":[\"null\",\"string\"]},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]},\"default\":null}]}");
public boolean isUrlDirty() {
public boolean isContentDirty() {
public boolean isParsedContentDirty() {
public boolean isOutlinksDirty() {
public boolean isHeadersDirty() {
public boolean isMetadataDirty() {
public boolean isUrlDirty() {
public boolean isContentDirty() {
public boolean isParsedContentDirty() {
public boolean isOutlinksDirty() {
public boolean isHeadersDirty() {
public boolean isMetadataDirty() {
public boolean isPrevDirty() {
public boolean isClientDirty() {
public boolean isCountDirty() {
public boolean isPrevDirty() {
public boolean isClientDirty() {
public boolean isCountDirty() {
public boolean isCountDirty() {
public boolean isCountDirty() {
public boolean isMetricDimensionDirty() {
public boolean isTimestampDirty() {
public boolean isMetricDirty() {
public boolean isMetricDimensionDirty() {
public boolean isTimestampDirty() {
public boolean isMetricDirty() {
public boolean isUrlDirty() {
public boolean isTimestampDirty() {
public boolean isIpDirty() {
public boolean isHttpMethodDirty() {
public boolean isHttpStatusCodeDirty() {
public boolean isResponseSizeDirty() {
public boolean isReferrerDirty() {
public boolean isUserAgentDirty() {
return TOMBSTONE;
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isReferrerDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
return newInstance(res, dbFields);
final BSONDecorator easybson, final Field f) {
List<Object> rlist = new ArrayList<Object>();
return new DirtyListWrapper(rlist);
final BSONDecorator easybson, final Field f) {
if (map == null) {
return new DirtyMapWrapper(rmap);
}
BasicDBObject map = new BasicDBObject();
return map;
BasicDBList list = new BasicDBList();
return list;
} else if (value instanceof CharSequence) {
serializer = CharSequenceSerializer.get();
if (valueClass.equals(Utf8.class) || valueClass.equals(CharSequence.class)) {
if (this.keySerializer == null)
if (pValue instanceof CharSequence && schemaType.equals(Type.STRING))
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.io.compress.Compression.Algorithm;
import org.apache.hadoop.hbase.regionserver.BloomType;
private TableName tableName;
return tableName.getNameAsString();
this.tableName = TableName.valueOf(tableName);
Map<String, HColumnDescriptor> families = tableToFamilies.get(tableName.getNameAsString());
String regionLocation = table.getRegionLocation(keys.getFirst()[i]).getHostname();
import java.io.InterruptedIOException;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Durability;
import org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException;
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
import com.google.protobuf.Descriptors.MethodDescriptor;
import com.google.protobuf.Message;
import com.google.protobuf.Service;
import com.google.protobuf.ServiceException;
private final boolean autoFlush;
private final TableName tableName;
this.tableName = TableName.valueOf(tableName);
this.autoFlush = autoflush;
public synchronized void flushCommits() throws RetriesExhaustedWithDetailsException, InterruptedIOException {
table.setAutoFlushTo(autoFlush);
return tableName.getName();
return autoFlush;
@Deprecated
getTable().mutateRow(rm);
return getTable().append(append);
public void setAutoFlush(boolean autoFlush, boolean clearBufferOnFail){
}
@Override
public TableName getName() {
return tableName;
}
@Override
public Boolean[] exists(List<Get> gets) throws IOException {
return getTable().exists(gets);
}
@Override
public <R> void
batchCallback(List<? extends Row> actions, Object[] results, Callback<R> callback)
throws IOException, InterruptedException {
getTable().batchCallback(actions, results, callback);
@Deprecated
@Override
public <R> Object[] batchCallback(List<? extends Row> actions, Callback<R> callback)
throws IOException, InterruptedException {
return getTable().batchCallback(actions, callback);
}
@Override
public long incrementColumnValue(byte[] row, byte[] family, byte[] qualifier, long amount,
Durability durability) throws IOException {
return getTable().incrementColumnValue(row, family, qualifier, amount,durability);
}
@Override
public CoprocessorRpcChannel coprocessorService(byte[] row) {
return null;
}
@Override
public <T extends Service, R> Map<byte[], R> coprocessorService(Class<T> service,
byte[] startKey, byte[] endKey, Call<T, R> callable) throws ServiceException, Throwable {
return getTable().coprocessorService(service, startKey, endKey, callable);
}
@Override
public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
byte[] endKey, Call<T, R> callable, Callback<R> callback) throws ServiceException, Throwable {
getTable().coprocessorService(service, startKey, endKey, callable, callback);;
}
@Override
public void setAutoFlushTo(boolean autoFlush) {
}
@Override
public <R extends Message> Map<byte[], R> batchCoprocessorService(
MethodDescriptor methodDescriptor, Message request, byte[] startKey, byte[] endKey,
R responsePrototype) throws ServiceException, Throwable {
return getTable().batchCoprocessorService(methodDescriptor, request, startKey, endKey, responsePrototype);
}
@Override
public <R extends Message> void batchCoprocessorService(MethodDescriptor methodDescriptor,
Message request, byte[] startKey, byte[] endKey, R responsePrototype, Callback<R> callback)
throws ServiceException, Throwable {
getTable().batchCoprocessorService(methodDescriptor, request, startKey, endKey, responsePrototype, callback);
}
@Deprecated
@Override
public Object[] batch(List<? extends Row> actions) throws IOException, InterruptedException {
return getTable().batch(actions);
}
@Override
public boolean checkAndMutate(byte[] arg0, byte[] arg1, byte[] arg2, CompareOp arg3, byte[] arg4,
RowMutations arg5) throws IOException {
return false;
}
final FilterOp filterOp, final List<Object> rawOperands) {
List<String> operands = convertOperandsToString(rawOperands);
private List<String> convertOperandsToString(List<Object> rawOperands) {
List<String> operands = new ArrayList<String>(rawOperands.size());
for (Object rawOperand : rawOperands) {
if (rawOperand != null) {
operands.add(rawOperand.toString());
}
}
return operands;
}
LOG.error(e.getMessage(), e);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(e.getMessage(), e);
LOG.error(ge.getMessage(), ge);
LOG.error(ex1.getMessage(), ex1);
LOG.error(ex2.getMessage(), ex2);
LOG.error(ex2.getMessage(), ex2);
LOG.error(ex2.getMessage(), ex2);
LOG.error(ex2.getMessage(), ex2);
LOG.error(ex2.getMessage(), ex2);
LOG.error(ex2.getMessage(), ex2);
LOG.error(ex2.getMessage(), ex2);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(ex.getMessage(), ex);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
LOG.error(e.getMessage(), e);
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.solr.client.solrj.impl.*;
protected static final String SOLR_SERVER_USER_AUTH = "solr.solrjserver.user_auth";
protected static final String SOLR_SERVER_USERNAME = "solr.solrjserver.username";
protected static final String SOLR_SERVER_PASSWORD = "solr.solrjserver.password";
private boolean serverUserAuth;
private String serverUsername;
private String serverPassword;
serverUserAuth = DataStoreFactory.findBooleanProperty(properties, this,
SOLR_SERVER_USER_AUTH, "false");
if (serverUserAuth) {
serverUsername = DataStoreFactory.findProperty(properties, this,
SOLR_SERVER_USERNAME, null);
serverPassword = DataStoreFactory.findProperty(properties, this,
SOLR_SERVER_PASSWORD, null);
}
if (serverUserAuth) {
HttpClientUtil.setBasicAuth(
(DefaultHttpClient) ((HttpSolrServer) adminServer).getHttpClient(),
serverUsername, serverPassword);
HttpClientUtil.setBasicAuth(
(DefaultHttpClient) ((HttpSolrServer) server).getHttpClient(),
serverUsername, serverPassword);
}
if (serverUserAuth) {
HttpClientUtil.setBasicAuth(
(DefaultHttpClient) ((CloudSolrServer) adminServer).getLbServer().getHttpClient(),
serverUsername, serverPassword);
HttpClientUtil.setBasicAuth(
(DefaultHttpClient) ((CloudSolrServer) server).getLbServer().getHttpClient(),
serverUsername, serverPassword);
}
if (serverUserAuth) {
HttpClientUtil.setBasicAuth(
(DefaultHttpClient) ((LBHttpSolrServer) adminServer).getHttpClient(),
serverUsername, serverPassword);
HttpClientUtil.setBasicAuth(
(DefaultHttpClient) ((LBHttpSolrServer) server).getHttpClient(),
serverUsername, serverPassword);
}
return (value != null) ? new Utf8(value) : null;
ResultBase<K, T> {
private int size;
if (cursor == null) {
} else if (size == 0) {
} else {
return offset / (float) size;
}
getQuery().getFields());
this.size = cursor.size();
import org.apache.hadoop.conf.Configurable;
this.clazzV = clazzV;
public JavaPairRDD<K, V> initializeInput(JavaSparkContext sparkContext,
GoraMapReduceUtils.setIOSerializations(conf, true);
try {
IOUtils
.storeToConf(dataStore.newQuery(), conf, GoraInputFormat.QUERY_KEY);
} catch (IOException ioex) {
throw new RuntimeException(ioex.getMessage());
}
return sparkContext.newAPIHadoopRDD(conf, GoraInputFormat.class, clazzK,
clazzV);
}
public JavaPairRDD<K, V> initializeInput(JavaSparkContext sparkContext,
DataStore<K, V> dataStore) {
Configuration hadoopConf;
if ((dataStore instanceof Configurable) && ((Configurable) dataStore).getConf() != null) {
hadoopConf = ((Configurable) dataStore).getConf();
} else {
hadoopConf = new Configuration();
}
GoraMapReduceUtils.setIOSerializations(hadoopConf, true);
try {
IOUtils.storeToConf(dataStore.newQuery(), hadoopConf,
GoraInputFormat.QUERY_KEY);
} catch (IOException ioex) {
throw new RuntimeException(ioex.getMessage());
}
return sparkContext.newAPIHadoopRDD(hadoopConf, GoraInputFormat.class,
clazzK, clazzV);
}
import org.apache.hadoop.conf.Configuration;
public class LogAnalyticsSpark {
if (args.length < 2) {
System.err.println(USAGE);
System.exit(1);
String inStoreClass = args[0];
String outStoreClass = args[1];
LogAnalyticsSpark logAnalyticsSpark = new LogAnalyticsSpark();
int ret = logAnalyticsSpark.run(inStoreClass, outStoreClass);
System.exit(ret);
public int run(String inStoreClass, String outStoreClass) throws Exception {
GoraSpark<Long, Pageview> goraSpark = new GoraSpark<>(Long.class,
Pageview.class);
SparkConf sparkConf = new SparkConf().setAppName(
JavaSparkContext sc = new JavaSparkContext(sparkConf);
Configuration hadoopConf = new Configuration();
inStoreClass, Long.class, Pageview.class, hadoopConf);
JavaPairRDD<Long, Pageview> goraRDD = goraSpark.initializeInput(sc, dataStore);
Class[] c = new Class[1];
c[0] = Pageview.class;
sparkConf.registerKryoClasses(c);
String firstOneURL = goraRDD.first()._2().getUrl().toString();
System.out.println(firstOneURL);
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import scala.Tuple2;
import java.util.concurrent.TimeUnit;
private static final long DAY_MILIS = 1000 * 60 * 60 * 24;
private static Function<Pageview, Tuple2<String, Long>> s = new Function<Pageview, Tuple2<String, Long>>() {
@Override
public Tuple2<String, Long> call(Pageview pageview) throws Exception {
String key = pageview.getUrl().toString();
Long value = getDay(pageview.getTimestamp());
return new Tuple2<>(key, value);
}
};
private static long getDay(long timeStamp) {
return (timeStamp / DAY_MILIS) * DAY_MILIS;
}
JavaPairRDD<Long, Pageview> goraRDD = goraSpark.initializeInput(sc,
dataStore);
JavaRDD<Tuple2<String, Long>> mappedGoraRdd = goraRDD.values().map(s);
private static Function<Pageview, Tuple2<Tuple2<String, Long>, Long>> s = new Function<Pageview, Tuple2<Tuple2<String, Long>, Long>> () {
public Tuple2<Tuple2<String, Long>, Long> call(Pageview pageview) throws Exception {
String url = pageview.getUrl().toString();
Long day = getDay(pageview.getTimestamp());
Tuple2<String, Long> keyTuple =new Tuple2<>(url, day);
return new Tuple2<>(keyTuple, 1L);
JavaRDD<Tuple2<Tuple2<String, Long>, Long>> mappedGoraRdd = goraRDD.values().map(s);
public JavaPairRDD<K, V> initialize(JavaSparkContext sparkContext,
public JavaPairRDD<K, V> initialize(JavaSparkContext sparkContext,
import org.apache.gora.tutorial.log.generated.MetricDatum;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
private static Function<Pageview, Tuple2<Tuple2<String, Long>, Long>> mapFunc = new Function<Pageview, Tuple2<Tuple2<String, Long>, Long>>() {
public Tuple2<Tuple2<String, Long>, Long> call(Pageview pageview)
throws Exception {
Tuple2<String, Long> keyTuple = new Tuple2<>(url, day);
private static Function2<Long, Long, Long> redFunc = new Function2<Long, Long, Long>() {
@Override
public Long call(Long aLong, Long aLong2) throws Exception {
}
};
private static PairFunction<Tuple2<Tuple2<String, Long>, Long>, String, MetricDatum> metricFunc = new PairFunction<Tuple2<Tuple2<String, Long>, Long>, String, MetricDatum>() {
@Override
public Tuple2<String, MetricDatum> call(
Tuple2<Tuple2<String, Long>, Long> tuple2LongTuple2) throws Exception {
String dimension = tuple2LongTuple2._1()._1();
long timestamp = tuple2LongTuple2._1()._2();
MetricDatum metricDatum = new MetricDatum();
metricDatum.setMetricDimension(dimension);
metricDatum.setTimestamp(timestamp);
String key = metricDatum.getMetricDimension().toString();
metricDatum.setMetric(tuple2LongTuple2._2());
return new Tuple2<>(key, metricDatum);
}
};
JavaPairRDD<Long, Pageview> goraRDD = goraSpark.initialize(sc,
JavaRDD<Tuple2<Tuple2<String, Long>, Long>> mappedGoraRdd = goraRDD
.values().map(mapFunc);
JavaPairRDD<String, MetricDatum> reducedGoraRdd = JavaPairRDD
.fromJavaRDD(mappedGoraRdd).reduceByKey(redFunc).mapToPair(metricFunc);
public class GoraSparkEngine<K, V extends Persistent> {
public GoraSparkEngine(Class<K> clazzK, Class<V> clazzV) {
if ((dataStore instanceof Configurable)
&& ((Configurable) dataStore).getConf() != null) {
hadoopConf = ((Configurable) dataStore).getConf();
} else {
hadoopConf = new Configuration();
return initialize(sparkContext, hadoopConf, dataStore);
import java.util.Map;
import org.apache.gora.spark.GoraSparkEngine;
GoraSparkEngine<Long, Pageview> goraSparkEngine = new GoraSparkEngine<>(Long.class,
JavaPairRDD<Long, Pageview> goraRDD = goraSparkEngine.initialize(sc, dataStore);
Map<String, MetricDatum> metricDatumMap = reducedGoraRdd.collectAsMap();
for (String key : metricDatumMap.keySet()) {
System.out.println(key);
}
import org.apache.gora.mapreduce.GoraMapReduceUtils;
import org.apache.gora.mapreduce.GoraOutputFormat;
import org.apache.gora.persistency.Persistent;
import org.apache.hadoop.mapreduce.Job;
DataStore<Long, Pageview> inStore = DataStoreFactory.getDataStore(
JavaPairRDD<Long, Pageview> goraRDD = goraSparkEngine.initialize(sc, inStore);
DataStore<String, MetricDatum> outStore = DataStoreFactory.getDataStore(
outStoreClass, String.class, MetricDatum.class, hadoopConf);
GoraMapReduceUtils.setIOSerializations(hadoopConf, true);
Job job = Job.getInstance(hadoopConf);
job.setOutputFormatClass(GoraOutputFormat.class);
job.setOutputKeyClass(outStore.getKeyClass());
job.setOutputValueClass(outStore.getPersistentClass());
job.getConfiguration().setClass(GoraOutputFormat.DATA_STORE_CLASS, outStore.getClass(),
DataStore.class);
job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_KEY_CLASS, outStore.getKeyClass(), Object.class);
job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_VALUE_CLASS,
outStore.getPersistentClass(), Persistent.class);
reducedGoraRdd.saveAsNewAPIHadoopDataset(job.getConfiguration());
inStore.close();
outStore.close();
cursor.addOption(Bytes.QUERYOPTION_NOTIMEOUT);
MongoStoreParameters parameters = MongoStoreParameters.load(properties, getConf());
new Object[] { parameters.getMappingFile() });
builder.fromFile(parameters.getMappingFile());
mongoClientDB = getDB(parameters);
parameters.getDbname(), parameters.getServers() });
private MongoClient getClient(MongoStoreParameters params)
MongoClientOptions.Builder optBuilder = new MongoClientOptions.Builder()
if (params.getReadPreference() != null) {
optBuilder.readPreference(ReadPreference.valueOf(params.getReadPreference()));
}
if (params.getWriteConcern() != null) {
optBuilder.writeConcern(WriteConcern.valueOf(params.getWriteConcern()));
}
List<MongoCredential> credentials = null;
if (params.getLogin() != null && params.getSecret() != null) {
credentials = new ArrayList<MongoCredential>();
credentials.add(MongoCredential.createCredential(params.getLogin(), params.getDbname(), params.getSecret().toCharArray()));
}
Iterable<String> serversArray = Splitter.on(",").split(params.getServers());
Iterator<String> paramsIterator = Splitter.on(":").trimResults().split(server).iterator();
return new MongoClient(addrs, credentials, optBuilder.build());
private DB getDB(MongoStoreParameters parameters) throws UnknownHostException {
if (!mapsOfClients.containsKey(parameters.getServers()))
mapsOfClients.put(parameters.getServers(), getClient(parameters));
DB db = mapsOfClients.get(parameters.getServers()).getDB(parameters.getDbname());
return db;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger log = LoggerFactory.getLogger(LogAnalyticsSpark.class);
try {
int ret = logAnalyticsSpark.run(args);
System.exit(ret);
} catch (Exception ex){
log.error("Error occurred!");
}
public int run(String[] args) throws Exception {
DataStore<Long, Pageview> inStore;
DataStore<String, MetricDatum> outStore;
Configuration hadoopConf = new Configuration();
if (args.length > 0) {
String dataStoreClass = args[0];
inStore = DataStoreFactory.getDataStore(
dataStoreClass, Long.class, Pageview.class, hadoopConf);
if (args.length > 1) {
dataStoreClass = args[1];
}
outStore = DataStoreFactory.getDataStore(
dataStoreClass, String.class, MetricDatum.class, hadoopConf);
} else {
inStore = DataStoreFactory.getDataStore(Long.class, Pageview.class, hadoopConf);
outStore = DataStoreFactory.getDataStore(String.class, MetricDatum.class, hadoopConf);
}
log.info("Log completed with success");
import org.apache.gora.mapreduce.GoraOutputFormat;
import org.apache.hadoop.mapreduce.Job;
public <K, V extends Persistent> Configuration setOutput(Job job,
DataStore<K, V> dataStore, boolean reuseObjects) {
return setOutput(job, dataStore.getClass(), dataStore.getKeyClass(),
dataStore.getPersistentClass(), reuseObjects);
}
@SuppressWarnings("rawtypes")
public <K, V extends Persistent> Configuration setOutput(Job job,
Class<? extends DataStore> dataStoreClass,
Class<K> keyClass, Class<V> persistentClass,
boolean reuseObjects) {
job.setOutputFormatClass(GoraOutputFormat.class);
job.setOutputKeyClass(keyClass);
job.setOutputValueClass(persistentClass);
job.getConfiguration().setClass(GoraOutputFormat.DATA_STORE_CLASS, dataStoreClass,
DataStore.class);
job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_KEY_CLASS, keyClass, Object.class);
job.getConfiguration().setClass(GoraOutputFormat.OUTPUT_VALUE_CLASS,
persistentClass, Persistent.class);
return job.getConfiguration();
}
Configuration sparkHadoopConf = goraSparkEngine.setOutput(job, outStore, true);
reducedGoraRdd.saveAsNewAPIHadoopDataset(sparkHadoopConf);
public <K, V extends Persistent> Configuration generateOutputConf(DataStore<K, V> dataStore,
boolean reuseObjects) throws IOException {
Configuration hadoopConf = new Configuration();
GoraMapReduceUtils.setIOSerializations(hadoopConf, true);
Job job = Job.getInstance(hadoopConf);
return generateOutputConf(job, dataStore.getClass(), dataStore.getKeyClass(),
public <K, V extends Persistent> Configuration generateOutputConf(Job job,
DataStore<K, V> dataStore, boolean reuseObjects) {
return generateOutputConf(job, dataStore.getClass(), dataStore.getKeyClass(),
dataStore.getPersistentClass(), reuseObjects);
}
public <K, V extends Persistent> Configuration generateOutputConf(Job job,
Configuration sparkHadoopConf = goraSparkEngine.generateOutputConf(outStore, true);
public <K, V extends Persistent> Configuration generateOutputConf(DataStore<K, V> dataStore)
throws IOException {
dataStore.getPersistentClass());
dataStore.getPersistentClass());
Class<K> keyClass, Class<V> persistentClass) {
Configuration sparkHadoopConf = goraSparkEngine.generateOutputConf(outStore);
"Gora Spark Integration Application").setMaster("local");
c[0] = inStore.getPersistentClass();
if (autoCreateSchema && !schemaExists())
count = putMap(m, count, field.schema().getValueType(), o, col, field.name());
count = putArray(m, count, o, col, field.name());
count = putArray(m, count, o, col, field.name());
count = putMap(m, count, effectiveSchema.getValueType(), o, col, field.name());
private int putMap(Mutation m, int count, Schema valueType, Object o, Pair<Text, Text> col, String fieldName) throws GoraException {
query.setFields(fieldName);
private int putArray(Mutation m, int count, Object o, Pair<Text, Text> col, String fieldName) {
query.setFields(fieldName);
import java.util.concurrent.ConcurrentNavigableMap;
import java.util.concurrent.ConcurrentSkipListMap;
@SuppressWarnings("rawtypes")
public static ConcurrentSkipListMap map = new ConcurrentSkipListMap();
return deletedRows;
} catch (Exception e) {
@SuppressWarnings("unchecked")
startKey = (K) map.firstKey();
endKey = (K) map.lastKey();
ConcurrentNavigableMap<K,T> submap = map.subMap(startKey, true, endKey, true);
@SuppressWarnings("unchecked")
T obj = (T) map.get(key);
@SuppressWarnings("unchecked")
public void flush() {
map.clear();
}
import java.util.TreeMap;
private TreeMap<K, T> map = new TreeMap<K, T>();
return 0;
}
catch(Exception e){
startKey = map.firstKey();
endKey = map.lastKey();
NavigableMap<K, T> submap = map.subMap(startKey, true, endKey, true);
T obj = map.get(key);
map.clear();
public void flush() { }
import java.util.concurrent.ConcurrentNavigableMap;
import java.util.concurrent.ConcurrentSkipListMap;
@SuppressWarnings("rawtypes")
public static ConcurrentSkipListMap map = new ConcurrentSkipListMap();
return deletedRows;
} catch (Exception e) {
@SuppressWarnings("unchecked")
startKey = (K) map.firstKey();
endKey = (K) map.lastKey();
ConcurrentNavigableMap<K,T> submap = map.subMap(startKey, true, endKey, true);
@SuppressWarnings("unchecked")
T obj = (T) map.get(key);
@SuppressWarnings("unchecked")
} catch (IOException e) {;
throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(GoraCompilerCLI.class);
LOG.error("Must supply at least one source file and an output directory.");
LOG.error("Must supply a directory for output");
LOG.error("Input must be a file.");
LOG.info("Compiler executed SUCCESSFULL.");
LOG.error("Error while compiling schema files. Check that the schemas are properly formatted.");
throw new RuntimeException(e);
LOG.info("Usage: gora-compiler ( -h | --help ) | (<input> [<input>...] <output>)");
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(GoraCompiler.class);
LOG.info("Compiling: {}", src.getAbsolutePath());
LOG.info("Compiled into: {}", dest.getAbsolutePath());
throw new RuntimeException(e);
throw new RuntimeException(e);
throw new RuntimeException(e);
throw new RuntimeException(e);
import java.nio.charset.Charset;
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes(Charset.defaultCharset())));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(QueryCounter.class);
LOG.info("Usage QueryCounter <keyClass> <persistentClass> [dataStoreClass]");
import java.nio.charset.Charset;
String content = new String(page.getContent().array(), Charset.defaultCharset());
throw new RuntimeException(e);
throw new RuntimeException(e);
throw new RuntimeException(e);
import java.util.Locale;
key = key.toLowerCase(Locale.getDefault());
throw new RuntimeException(e);
import java.util.Locale;
value = getProperty(properties, fullKey.toLowerCase(Locale.getDefault()));
throw new RuntimeException(e);
throw new RuntimeException(e);
import java.util.Locale;
NumberFormat nf = NumberFormat.getInstance(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(VersionInfo.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Delete.class);
LOG.info("Usage : {} <node to delete>", Delete.class.getSimpleName());
LOG.info("Delete returned {}", ret);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Generator.class);
LOG.info("num {}", num);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("Running Generator with numMappers={}, numNodes={}", numMappers, numNodes);
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Loop.class);
LOG.info("Verify finished with succees. Total nodes={}", expectedNumNodes);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("Starting iteration = {}", i);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Print.class);
LOG.info("%016x:%016x:%012d:%s\n {} {} {} {}", new Object[] {rs.getKey(),
node.getPrev(), node.getCount(), node.getClient()});
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Verify.class);
sb.append(String.format(Locale.getDefault(), "%016x", ref));
context.write(new Text(String.format(Locale.getDefault(), "%016x", key.get())), new Text(sb.toString()));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Walker.class);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("CQ %d %016x \n {}", new Object [] {t2 - t1, prev});
LOG.info("HQ %d %016x \n {}", new Object [] {t2 - t1, prev});
LOG.info("FSR %d %016x\n {}", new Object[] {t2 - t1, rs.getKey()});
throw new RuntimeException(e);
LOG.info("FSR {}", new Object [] {(t2 - t1)});
public static void main(String[] args) throws NoSuchElementException, InstantiationException, IllegalAccessException, IOException {
throw new IOException(e);
throw new IOException(e);
import java.nio.charset.Charset;
BufferedReader br = new BufferedReader(new InputStreamReader(process.getInputStream(), Charset.defaultCharset()));
LOG.info("Setting dfs.datanode.data.dir.perm to {}",  perms);
LOG.warn("Couldn't get umask {}", e);
import java.util.Locale;
newDocumentField(docFieldName, valueOf(fieldType.toUpperCase(Locale.getDefault())));
Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.getDefault());
import java.util.Locale;
if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("http")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("cloud")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("concurrent")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("loadbalance")) {
throw new RuntimeException(e);
throw new RuntimeException(e);
LOG.warn("Invalid batch size '{}', using default {}", batchSizeString, DEFAULT_BATCH_SIZE);
LOG.warn("Invalid commit within '{}' , using default {}", commitWithinString, DEFAULT_COMMIT_WITHIN);
LOG.warn("Invalid results size '{}' , using default {}", resultsSizeString, DEFAULT_RESULTS_SIZE);
" {} in mapping file.", classes.indexOf(classElement));
log.info("Creating Hadoop Job: {}", job.getJobName());
log.info("Log completed with {}", (success ? "success" : "failure"));
log.info(USAGE);
import java.nio.charset.Charset;
import java.io.InputStreamReader;
import java.io.FileInputStream;
import java.nio.charset.Charset;
import java.util.Locale;
= new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.getDefault());
log.info("Parsing file: {}", input);
BufferedReader reader = new BufferedReader(new InputStreamReader(
new FileInputStream(input), Charset.defaultCharset()));
log.info("finished parsing file. Total number of log lines: {}", lineCount);
log.info("pageview with key: {} deleted", lineNum);
log.info("pageviews with keys between {} and {} are deleted.", startKey, endKey);
log.info("{} :", resultKey);
log.info("Number of pageviews from the query: {}", result.getOffset());
log.info("No result to show");
log.info(pageview.toString());
log.error(USAGE);
log.info(USAGE);
import java.util.concurrent.ConcurrentNavigableMap;
import java.util.concurrent.ConcurrentSkipListMap;
@SuppressWarnings("rawtypes")
public static ConcurrentSkipListMap map = new ConcurrentSkipListMap();
return deletedRows;
} catch (Exception e) {
@SuppressWarnings("unchecked")
startKey = (K) map.firstKey();
endKey = (K) map.lastKey();
ConcurrentNavigableMap<K,T> submap = map.subMap(startKey, true, endKey, true);
@SuppressWarnings("unchecked")
T obj = (T) map.get(key);
@SuppressWarnings("unchecked")
} catch (IOException e) {
LOG.error(e.getMessage());
LOG.error(ioe.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(LicenseHeaders.class);
LOG.error(e.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
LOG.error("Error reading Gora records: {}", e.getMessage());
LOG.warn("Exception at GoraRecordWriter.class while closing datastore: {}", e.getMessage());
LOG.warn("Trace: {}", e.getStackTrace());
LOG.warn("Exception at GoraRecordWriter.class while writing to datastore: {}", e.getMessage());
LOG.warn("Trace: {}", e.getStackTrace());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(BeanFactoryWSImpl.class);
LOG.error(e.getMessage());
LOG.error(e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(WSBackedDataStoreBase.class);
LOG.error(e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(ByteUtils.class);
LOG.error(e.getMessage());
LOG.error(e.getMessage());
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
LOG.error(ioe.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(GoraCompilerCLI.class);
LOG.error("Must supply at least one source file and an output directory.");
LOG.error("Must supply a directory for output");
LOG.error("Input must be a file.");
LOG.info("Compiler executed SUCCESSFULL.");
LOG.error("Error while compiling schema files. Check that the schemas are properly formatted.");
throw new RuntimeException(e);
LOG.info("Usage: gora-compiler ( -h | --help ) | (<input> [<input>...] <output>)");
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(GoraCompiler.class);
LOG.info("Compiling: {}", src.getAbsolutePath());
LOG.info("Compiled into: {}", dest.getAbsolutePath());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(LicenseHeaders.class);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
import java.nio.charset.Charset;
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes(Charset.defaultCharset())));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(QueryCounter.class);
LOG.info("Usage QueryCounter <keyClass> <persistentClass> [dataStoreClass]");
import java.nio.charset.Charset;
String content = new String(page.getContent().array(), Charset.defaultCharset());
LOG.error("Error reading Gora records: {}", e.getMessage());
throw new RuntimeException(e);
LOG.warn("Exception at GoraRecordWriter.class while closing datastore: {}", e.getMessage());
LOG.warn("Trace: {}", e.getStackTrace());
throw new RuntimeException(e);
LOG.warn("Exception at GoraRecordWriter.class while writing to datastore: {}", e.getMessage());
LOG.warn("Trace: {}", e.getStackTrace());
throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(BeanFactoryWSImpl.class);
LOG.error(e.getMessage());
LOG.error(e.getMessage());
import java.util.Locale;
key = key.toLowerCase(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(WSBackedDataStoreBase.class);
LOG.error(e.getMessage());
throw new RuntimeException(e);
import java.util.Locale;
value = getProperty(properties, fullKey.toLowerCase(Locale.getDefault()));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(ByteUtils.class);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
import java.util.Locale;
NumberFormat nf = NumberFormat.getInstance(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(VersionInfo.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Delete.class);
LOG.info("Usage : {} <node to delete>", Delete.class.getSimpleName());
LOG.info("Delete returned {}", ret);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Generator.class);
LOG.info("num {}", num);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("Running Generator with numMappers={}, numNodes={}", numMappers, numNodes);
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Loop.class);
LOG.info("Verify finished with succees. Total nodes={}", expectedNumNodes);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("Starting iteration = {}", i);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Print.class);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("%016x:%016x:%012d:%s\n {} {} {} {}", new Object[] {rs.getKey(),
node.getPrev(), node.getCount(), node.getClient()});
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Verify.class);
sb.append(String.format(Locale.getDefault(), "%016x", ref));
context.write(new Text(String.format(Locale.getDefault(), "%016x", key.get())), new Text(sb.toString()));
LOG.error("Failed to parse command line {}", e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Walker.class);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("CQ %d %016x \n {}", new Object [] {t2 - t1, prev});
LOG.info("HQ %d %016x \n {}", new Object [] {t2 - t1, prev});
LOG.info("FSR %d %016x\n {}", new Object[] {t2 - t1, rs.getKey()});
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.info("FSR {}", new Object [] {(t2 - t1)});
public static void main(String[] args) throws NoSuchElementException, InstantiationException, IllegalAccessException, IOException {
throw new IOException(e);
throw new IOException(e);
import java.nio.charset.Charset;
BufferedReader br = new BufferedReader(new InputStreamReader(process.getInputStream(), Charset.defaultCharset()));
LOG.info("Setting dfs.datanode.data.dir.perm to {}",  perms);
LOG.warn("Couldn't get umask {}", e);
import java.util.Locale;
newDocumentField(docFieldName, valueOf(fieldType.toUpperCase(Locale.getDefault())));
Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.getDefault());
import java.util.Locale;
if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("http")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("cloud")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("concurrent")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("loadbalance")) {
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.warn("Invalid batch size '{}', using default {}", batchSizeString, DEFAULT_BATCH_SIZE);
LOG.warn("Invalid commit within '{}' , using default {}", commitWithinString, DEFAULT_COMMIT_WITHIN);
LOG.warn("Invalid results size '{}' , using default {}", resultsSizeString, DEFAULT_RESULTS_SIZE);
" {} in mapping file.", classes.indexOf(classElement));
log.info("Creating Hadoop Job: {}", job.getJobName());
log.info("Log completed with {}", (success ? "success" : "failure"));
log.info(USAGE);
import java.nio.charset.Charset;
import java.io.InputStreamReader;
import java.io.FileInputStream;
import java.nio.charset.Charset;
import java.util.Locale;
= new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.getDefault());
log.info("Parsing file: {}", input);
BufferedReader reader = new BufferedReader(new InputStreamReader(
new FileInputStream(input), Charset.defaultCharset()));
log.info("finished parsing file. Total number of log lines: {}", lineCount);
log.info("pageview with key: {} deleted", lineNum);
log.info("pageviews with keys between {} and {} are deleted.", startKey, endKey);
log.info("{} :", resultKey);
log.info("Number of pageviews from the query: {}", result.getOffset());
log.info("No result to show");
log.info(pageview.toString());
log.error(USAGE);
log.info(USAGE);
import java.nio.charset.Charset;
String content = new String(webPage.getContent().array(), Charset.defaultCharset());
log.info("Total Web page count: {}", count);
log.info("SparkWordCount debug purpose TokenDatum print starts:");
log.info(key);
log.info(tokenDatumMap.get(key).toString());
log.info("SparkWordCount debug purpose TokenDatum print ends:");
log.info(USAGE);
log.error(USAGE);
log.info("Total Log Count: {}", count);
log.info("MetricDatum count: {}", reducedGoraRdd.count());
LOG.error(e.getMessage());
LOG.error(ioe.getMessage());
LOG.error(e.getMessage());
LOG.error(e.getMessage());
throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(GoraCompilerCLI.class);
LOG.error("Must supply at least one source file and an output directory.");
LOG.error("Must supply a directory for output");
LOG.error("Input must be a file.");
LOG.info("Compiler executed SUCCESSFULL.");
LOG.error("Error while compiling schema files. Check that the schemas are properly formatted.");
throw new RuntimeException(e);
LOG.info("Usage: gora-compiler ( -h | --help ) | (<input> [<input>...] <output>)");
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(GoraCompiler.class);
LOG.info("Compiling: {}", src.getAbsolutePath());
LOG.info("Compiled into: {}", dest.getAbsolutePath());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(LicenseHeaders.class);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
import java.nio.charset.Charset;
page.setContent(ByteBuffer.wrap(CONTENTS[i].getBytes(Charset.defaultCharset())));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(QueryCounter.class);
LOG.info("Usage QueryCounter <keyClass> <persistentClass> [dataStoreClass]");
import java.nio.charset.Charset;
String content = new String(page.getContent().array(), Charset.defaultCharset());
LOG.error("Error reading Gora records: {}", e.getMessage());
throw new RuntimeException(e);
LOG.warn("Exception at GoraRecordWriter.class while closing datastore: {}", e.getMessage());
LOG.warn("Trace: {}", e.getStackTrace());
throw new RuntimeException(e);
LOG.warn("Exception at GoraRecordWriter.class while writing to datastore: {}", e.getMessage());
LOG.warn("Trace: {}", e.getStackTrace());
throw new RuntimeException(e);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(BeanFactoryWSImpl.class);
LOG.error(e.getMessage());
LOG.error(e.getMessage());
import java.util.Locale;
key = key.toLowerCase(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(WSBackedDataStoreBase.class);
LOG.error(e.getMessage());
throw new RuntimeException(e);
import java.util.Locale;
value = getProperty(properties, fullKey.toLowerCase(Locale.getDefault()));
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(ByteUtils.class);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
import java.util.Locale;
NumberFormat nf = NumberFormat.getInstance(Locale.getDefault());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(VersionInfo.class);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Delete.class);
LOG.info("Usage : {} <node to delete>", Delete.class.getSimpleName());
LOG.info("Delete returned {}", ret);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Generator.class);
LOG.info("num {}", num);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("Running Generator with numMappers={}, numNodes={}", numMappers, numNodes);
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Loop.class);
LOG.info("Verify finished with succees. Total nodes={}", expectedNumNodes);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("Starting iteration = {}", i);
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Print.class);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("%016x:%016x:%012d:%s\n {} {} {} {}", new Object[] {rs.getKey(),
node.getPrev(), node.getCount(), node.getClient()});
import java.util.Locale;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Verify.class);
sb.append(String.format(Locale.getDefault(), "%016x", ref));
context.write(new Text(String.format(Locale.getDefault(), "%016x", key.get())), new Text(sb.toString()));
LOG.error("Failed to parse command line {}", e.getMessage());
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(Walker.class);
LOG.error("Failed to parse command line {}", e.getMessage());
LOG.info("CQ %d %016x \n {}", new Object [] {t2 - t1, prev});
LOG.info("HQ %d %016x \n {}", new Object [] {t2 - t1, prev});
LOG.info("FSR %d %016x\n {}", new Object[] {t2 - t1, rs.getKey()});
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.info("FSR {}", new Object [] {(t2 - t1)});
public static void main(String[] args) throws NoSuchElementException, InstantiationException, IllegalAccessException, IOException {
throw new IOException(e);
throw new IOException(e);
import java.nio.charset.Charset;
BufferedReader br = new BufferedReader(new InputStreamReader(process.getInputStream(), Charset.defaultCharset()));
LOG.info("Setting dfs.datanode.data.dir.perm to {}",  perms);
LOG.warn("Couldn't get umask {}", e);
import java.util.Locale;
newDocumentField(docFieldName, valueOf(fieldType.toUpperCase(Locale.getDefault())));
Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.getDefault());
import java.util.Locale;
if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("http")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("cloud")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("concurrent")) {
} else if (solrJServerType.toString().toLowerCase(Locale.getDefault()).equals("loadbalance")) {
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.error(e.getMessage());
throw new RuntimeException(e);
LOG.warn("Invalid batch size '{}', using default {}", batchSizeString, DEFAULT_BATCH_SIZE);
LOG.warn("Invalid commit within '{}' , using default {}", commitWithinString, DEFAULT_COMMIT_WITHIN);
LOG.warn("Invalid results size '{}' , using default {}", resultsSizeString, DEFAULT_RESULTS_SIZE);
" {} in mapping file.", classes.indexOf(classElement));
log.info("Creating Hadoop Job: {}", job.getJobName());
log.info("Log completed with {}", (success ? "success" : "failure"));
log.info(USAGE);
import java.nio.charset.Charset;
import java.io.InputStreamReader;
import java.io.FileInputStream;
import java.nio.charset.Charset;
import java.util.Locale;
= new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.getDefault());
log.info("Parsing file: {}", input);
BufferedReader reader = new BufferedReader(new InputStreamReader(
new FileInputStream(input), Charset.defaultCharset()));
log.info("finished parsing file. Total number of log lines: {}", lineCount);
log.info("pageview with key: {} deleted", lineNum);
log.info("pageviews with keys between {} and {} are deleted.", startKey, endKey);
log.info("{} :", resultKey);
log.info("Number of pageviews from the query: {}", result.getOffset());
log.info("No result to show");
log.info(pageview.toString());
log.error(USAGE);
log.info(USAGE);
flush();
return decodeByte(val) == 1;
assert(job.isComplete());
assert(job.isComplete());
return limit > 0 && offset >= limit;
if (!ret) {
return limit > 0 && offset >= limit;
return Arrays.equals(family, other.family) && Arrays.equals(qualifier, other.qualifier);
return (K) Boolean.valueOf(val[0] != 0);
return decodeByte(val) == 1;
assert(job.isComplete());
assert(job.isComplete());
return limit > 0 && offset >= limit;
if (!ret) {
return limit > 0 && offset >= limit;
return Arrays.equals(family, other.family) && Arrays.equals(qualifier, other.qualifier);
return (K) Boolean.valueOf(val[0] != 0);
public static final int DEFAULT_UNION_SCHEMA = 0;
public static final String UNION_COL_SUFIX = "_UnionIndex";
public static final int DEFAULT_UNION_SCHEMA = 0;
public static final String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
key = ((AccumuloStore<K, T>) dataStore).fromBytes(getKeyClass(), row.toArray());
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<K,T>(query, startKey, endKey, location);
SpecificDatumWriter writer = writerMap.get(schema.getFullName());
SpecificDatumReader<?> reader = readerMap.get(schemaId);
ByteBuffer byteBuffer = ByteBuffer.allocate(array.size() * size);
int n = array.size();
int n = map.size();
int n = map.size();
if (!field.name().contains(CassandraStore.UNION_COL_SUFIX)){
GORA_RESERVED_NAMES.addAll(Arrays.asList(DIRTY_BYTES_FIELD_NAME));
}
}
}
}
}
query = IOUtils.deserialize(conf, in, null);
public void executeQuery() throws Exception {
T newObj = AvroUtils.deepClonePersistent(obj);
return persistentClass.newInstance();
boolean next() throws Exception;
public final boolean next() throws Exception {
protected T getOrCreatePersistent(T persistent) throws Exception {
protected T getOrCreatePersistent(T persistent) throws Exception {
}
Properties properties) throws Exception {
return datumReader.read(object, decoder);
return datumReader.read(object, decoder);
return datumReader.read(object, decoder);
public void start(Path outputDir, int numReducers, boolean concurrent) throws Exception {
byte[] startKey, byte[] endKey, Call<T, R> callable) throws Throwable {
byte[] endKey, Call<T, R> callable, Callback<R> callback) throws Throwable {
getTable().coprocessorService(service, startKey, endKey, callable, callback);
R responsePrototype) throws Throwable {
throws Throwable {
SpecificDatumReader<?> reader = readerMap.get(schemaId);
SpecificDatumWriter writer = writerMap.get(schema.getFullName());
}
if (solrJServerType.toLowerCase(Locale.getDefault()).equals("http")) {
} else if (solrJServerType.toLowerCase(Locale.getDefault()).equals("cloud")) {
} else if (solrJServerType.toLowerCase(Locale.getDefault()).equals("concurrent")) {
} else if (solrJServerType.toLowerCase(Locale.getDefault()).equals("loadbalance")) {
sb.append("\\").append(c);
SpecificDatumReader<?> reader = readerMap.get(schemaId);
SpecificDatumWriter writer = writerMap.get(schemaId);
}
}
}
private void parse(String input) throws Exception {
private void storePageview(long key, Pageview pageview) throws Exception {
private void get(long key) throws Exception {
private void query(long key) throws Exception {
private void query(long startKey, long endKey) throws Exception {
private void deleteByQuery(long startKey, long endKey) throws Exception {
private void printResult(Result<Long, Pageview> result) throws Exception {
private void close() throws Exception {
Map<String, HColumnDescriptor> families = getOrCreateFamilies(tableName);
deleteColumn(key, familyMap.values().iterator().next(), null);
if (DIRTY_BYTES_FIELD_NAME.equals(field.name())) {
if (DIRTY_BYTES_FIELD_NAME.equals(fieldName)) {
System.arraycopy(URLS, 0, SORTED_URLS, 0, URLS.length);
import java.util.Collections;
Collections.addAll(set, arr1);
Collections.addAll(set, arr2);
if (!memberName.contains(CassandraStore.UNION_COL_SUFIX)) {
public static final String PROP_MONGO_DB = "gora.mongodb.db";
public static final String UNION_COL_SUFIX = "_UnionIndex";
public static final int DEFAULT_UNION_SCHEMA = 0;
public static final String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
public static final int DEFAULT_UNION_SCHEMA = 0;
flush();
public static final String UNION_COL_SUFIX = "_UnionIndex";
public static final int DEFAULT_UNION_SCHEMA = 0;
public static final String DIRTY_BYTES_FIELD_NAME = "__g__dirty";
public static final int DEFAULT_UNION_SCHEMA = 0;
flush();
for (byte anA : a) {
b |= fromChar(anA);
for (byte aBin : bin) {
Map<String,Pair<Text,Text>> fieldMap = new HashMap<>();
Map<Pair<Text,Text>,String> columnMap = new HashMap<>();
Map<String,String> tableConfig = new HashMap<>();
} catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {
} catch (AccumuloException | AccumuloSecurityException e) {
Pair<Text,Text> col = new Pair<>(new Text(family), qualifier == null ? null : new Text(qualifier));
} catch (AccumuloException | AccumuloSecurityException | TableExistsException e) {
} catch (AccumuloException | AccumuloSecurityException | TableNotFoundException e) {
currentMap = new DirtyMapWrapper<>(new HashMap<Utf8, Object>());
currentArray = new DirtyListWrapper<>(new ArrayList<>());
currentArray = new DirtyListWrapper<>(new ArrayList<>());
currentMap = new DirtyMapWrapper<>(new HashMap<Utf8, Object>());
String fieldName = mapping.columnMap.get(new Pair<>(entry.getKey().getColumnFamily(),
SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
return new AccumuloResult<>(this, query, scanner);
return new AccumuloQuery<>(this);
Map<String,Map<KeyExtent,List<Range>>> binnedRanges = new HashMap<>();
List<PartitionQuery<K,T>> ret = new ArrayList<>();
HashMap<String,String> hostNameCache = new HashMap<>();
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query, startKey, endKey, location);
} catch (TableNotFoundException | AccumuloException | AccumuloSecurityException e) {
for (Object currentPCassandraRow : pCassandraRow) {
CassandraColumn cColumn = (CassandraColumn) currentPCassandraRow;
if (!fieldName.contains(CassandraStore.UNION_COL_SUFIX)) {
private HashMap<K, Integer> indexMap = new HashMap<>();
List<Object> array = new ArrayList<>();
Map<CharSequence, Object> map = new HashMap<>();
if (!mapKey.toString().contains(CassandraStore.UNION_COL_SUFIX)) {
for (Object currentHColumn : hColumns) {
HColumn<ByteBuffer, ByteBuffer> hColumn = (HColumn<ByteBuffer, ByteBuffer>) currentHColumn;
new ThreadLocal<>();
new ThreadLocal<>();
new ThreadLocal<>();
new ConcurrentHashMap<>();
new ConcurrentHashMap<>();
private static Map<Type, ListSerializer> elementTypeToSerializerMap = new HashMap<>();
private static Map<Class, ListSerializer> fixedClassToSerializerMap = new HashMap<>();
List<byte[]> list = new ArrayList<>(n);
ArrayList<T> array = new ArrayList<>();
private static Map<Type, MapSerializer> valueTypeToSerializerMap = new HashMap<>();
private static Map<Class, MapSerializer> fixedClassToSerializerMap = new HashMap<>();
List<byte[]> list = new ArrayList<>(n);
List<byte[]> list = new ArrayList<>(n);
Map<CharSequence, T> map = new HashMap<>();
private static Map<Class, SpecificFixedSerializer> classToSerializerMap = new HashMap<>();
accessMap = new HashMap<>();
Map<String, HConsistencyLevel> clMap = new HashMap<>();
Map<String, List<String>> map = new HashMap<>();
list = new ArrayList<>();
Map<String, String> map = new HashMap<>();
private List<String> superFamilies = new ArrayList<>();
private Map<String, String> familyMap = new HashMap<>();
private Map<String, String> columnMap = new HashMap<>();
private Map<String, String> columnAttrMap = new HashMap<>();
new HashMap<>();
List<ColumnFamilyDefinition> list = new ArrayList<>();
keyspaceMap = new HashMap<>();
mappingMap  = new HashMap<>();
catch (JDOMException | IOException e) {
private CassandraClient<K, T> cassandraClient = new CassandraClient<>();
new ThreadLocal<>();
new ConcurrentHashMap<>();
CassandraQuery<K, T> cassandraQuery = new CassandraQuery<>();
CassandraResult<K, T> cassandraResult = new CassandraResult<>(this, query);
CassandraResultSet<K> cassandraResultSet = new CassandraResultSet<>();
cassandraRow = new CassandraRow<>();
cassandraRow = new CassandraRow<>();
CassandraQuery<K,T> query = new CassandraQuery<>();
List<PartitionQuery<K,T>> partitions = new ArrayList<>();
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query);
Query<K,T> query = new CassandraQuery<>(this);
Map<CharSequence,Object> valueMap = new HashMap<>();
for (Schema currentSchema : pUnionSchema.getTypes()) {
Type schemaType = currentSchema.getType();
unionSchemaPos;
private static final Set<String> GORA_RESERVED_NAMES = new HashSet<>();
private static final Set<String> GORA_HIDDEN_FIELD_NAMES = new HashSet<>();
List<Schema> newTypeSchemas = new ArrayList<>();
for (Schema currentTypeSchema : schemaTypes) {
newTypeSchemas.add(getSchemaWithDirtySupport(currentTypeSchema, queue));
List<Field> newFields = new ArrayList<>();
relatedLicenses = new HashMap<>();
} catch (SecurityException | NoSuchFieldException | IllegalArgumentException | IllegalAccessException e) {
public static HashMap<String, Integer> URL_INDEXES = new HashMap<>();
return new AvroResult<>(this, (AvroQuery<K,T>)query,
return new AvroQuery<>(this);
return new SpecificDatumWriter<>(schema);
return new SpecificDatumReader<>(schema);
writer = new DataFileWriter<>(getDatumWriter());
return new DataFileAvroResult<>(this, query
return new DataFileAvroResult<>(this, query, reader, fsInput
return new DataFileReader<>(fsInput, getDatumReader());
private List<Filter<K, T>> filters = new ArrayList<>();
filters = new ArrayList<>(size);
protected List<Object> operands = new ArrayList<>();
for (Object operand : operands) {
protected List<Object> operands = new ArrayList<>();
for (Object operand : operands) {
operand = operand.toString();
return new GoraRecordReader<>(partitionQuery, context);
List<InputSplit> splits = new ArrayList<>(queries.size());
datumReader = new SpecificDatumReader<>(schema);
this.datumWriter = new SpecificDatumWriter<>();
return new MemResult<>(this, query, submap);
for (Field otherField : otherFields) {
int index = otherField.pos();
return new MemQuery<>(this);
List<PartitionQuery<K, T>> list = new ArrayList<>();
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query);
return new DirtyIteratorWrapper<>(delegate.iterator(), dirtyFlag);
return new DirtyListIterator<>(getDelegate().listIterator(),
return new DirtyListIterator<>(getDelegate().listIterator(index),
return new DirtyListWrapper<>(getDelegate().subList(fromIndex, toIndex),
return new DirtyCollectionWrapper<>(delegate.values(), dirtyFlag);
return new DirtyEntryWrapper<>(input, dirtyFlag);
} catch (InstantiationException | IllegalAccessException e) {
new HashMap<>();
new HashMap<>();
HashMap<String, Integer> map = new HashMap<>(fieldsLength);
this.beanFactory = new BeanFactoryImpl<>(keyClass, persistentClass);
datumReader = new SpecificDatumReader<>(schema);
datumWriter = new SpecificDatumWriter<>(schema);
List<Field> list = new ArrayList<>();
} catch (ClassNotFoundException | IOException ex) {
queries = new ArrayList<>(splits.size());
queries.add(new FileSplitPartitionQuery<>(query, (FileSplit) split));
HashMap<String, Field> fieldMap = new HashMap<>(fields.size());
SpecificDatumWriter<Persistent> writer = new SpecificDatumWriter<>(
SpecificDatumReader<T> reader = new SpecificDatumReader<>(
try (ByteBufferOutputStream os = new ByteBufferOutputStream()) {
List<ByteBuffer> list = new ArrayList<>();
try (ByteBufferInputStream is = new ByteBufferInputStream(list)) {
List<ByteBuffer> buffers = new ArrayList<>(4);
nodes = new Stack<>();
HashSet<String> set = new HashSet<>();
LinkedHashSet<Set<String>> power = new LinkedHashSet<>();
LinkedHashSet<String> innerSet = new LinkedHashSet<>();
new ArrayList<>();
list.add(new PartitionQueryImpl<>(query, LOCATIONS[i]));
ArrayList<InputSplit> splits = new ArrayList<>(numMappers);
flushed = new HashMap<>();
private ArrayList<Long> refs = new ArrayList<>();
ArrayList<String> flushedEntries = new ArrayList<>();
new HashMap<>();
new HashMap<>();
families = new HashMap<>();
filterUtil = new HBaseFilterUtil<>(this.conf);
ArrayList<Delete> deletes = new ArrayList<>();
return new HBaseQuery<>(this);
List<PartitionQuery<K,T>> partitions = new ArrayList<>(keys.getFirst().length);
PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<>(
return new HBaseGetResult<>(this, query, result);
= new HBaseScannerResult<>(this, query, scanner);
Map<Utf8, Object> map = new HashMap<>();
ArrayList<Object> arrayList = new ArrayList<>();
DirtyListWrapper<Object> dirtyListWrapper = new DirtyListWrapper<>(arrayList);
private final BlockingQueue<HTable> pool = new LinkedBlockingQueue<>();
this.tables = new ThreadLocal<>();
List<String> filters = new ArrayList<>();
new ThreadLocal<>();
new ThreadLocal<>();
new ThreadLocal<>();
new ConcurrentHashMap<>();
new ConcurrentHashMap<>();
private Map<String, FilterFactory<K, T>> factories = new LinkedHashMap<>();
List<String> filters = new ArrayList<>();
List<String> operands = new ArrayList<>(rawOperands.size());
private Map<String, FilterFactory<K, T>> factories = new LinkedHashMap<>();
private HashMap<String, String> classToDocument = new HashMap<>();
private HashMap<String, String> documentToClass = new HashMap<>();
private HashMap<String, DocumentFieldType> documentFields = new HashMap<>();
private static ConcurrentHashMap<String, MongoClient> mapsOfClients = new ConcurrentHashMap<>();
filterUtil = new MongoFilterUtil<>(getConf());
MongoMappingBuilder<K, T> builder = new MongoMappingBuilder<>(this);
credentials = new ArrayList<>();
List<ServerAddress> addrs = new ArrayList<>();
MongoDBResult<K, T> mongoResult = new MongoDBResult<>(this, query);
MongoDBQuery<K, T> query = new MongoDBQuery<>(this);
List<PartitionQuery<K, T>> partitions = new ArrayList<>();
PartitionQueryImpl<K, T> partitionQuery = new PartitionQueryImpl<>(
List<Object> rlist = new ArrayList<>();
return new DirtyListWrapper<>(rlist);
Map<Utf8, Object> rmap = new HashMap<>();
return new DirtyMapWrapper<>(rmap);
private static final Map<String, String> HADOOP_VERSION_TO_IMPL_MAP = new HashMap<>();
HashSet<String> uniqFields = new HashSet<>(Arrays.asList(fields));
mapping = new HashMap<>();
public static final ConcurrentHashMap<String, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<>();
public static final ConcurrentHashMap<String, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<>();
batch = new ArrayList<>(batchSize);
for (Schema currentSchema : pUnionSchema.getTypes()) {
Type schemaType = currentSchema.getType();
return new SolrResult<>(this, query, server, resultsSize);
return new SolrQuery<>(this);
ArrayList<PartitionQuery<K, T>> partitions = new ArrayList<>();
PartitionQueryImpl<K, T> pqi = new PartitionQueryImpl<>(query);
long lineCount = 0;
try (BufferedReader reader = new BufferedReader(new InputStreamReader(
new FileInputStream(input), Charset.defaultCharset()))) {
String line = reader.readLine();
do {
Pageview pageview = parseLine(line);
if (pageview != null) {
}
line = reader.readLine();
} while (line != null);
}
throw new IOException("cannot read from DataInput of instance:"
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(MemStore.class);
return "MemStore";
try{
long deletedRows = 0;
if (!map.isEmpty()) {
startKey = (K) map.firstKey();
}
if (!map.isEmpty()) {
endKey = (K) map.lastKey();
}
ConcurrentNavigableMap<K,T> submap = null;
try {
submap =  map.subMap(startKey, true, endKey, true);
} catch (NullPointerException npe){
LOG.info("Either startKey || endKey || startKey and endKey value(s) is null. "
"No results will be returned for query to MemStore.");
return new MemResult<>(this, query, new ConcurrentSkipListMap<K, T>());
}
if (!map.isEmpty()) {
map.clear();
}
Integer unionIndex = getUnionIndex(cc);
Integer unionIndex = getUnionIndex(hc);
private Integer getUnionIndex(HColumn<ByteBuffer, ByteBuffer> uc){
public Job createJob(Query<K,T> query) throws IOException {
GoraMapper.initMapperJob(job, query, NullWritable.class
public long countQuery(Query<K,T> query) throws Exception {
Job job = createJob(query);
Job job = createJob(query);
public Job createJob(Query<String,WebPage> query
GoraMapper.initMapperJob(job, query, Text.class
Job job = createJob(query, outStore);
private void setInputPath(PartitionQuery<K, T> partitionQuery) throws IOException {
setInputPath(partitionQuery);
, Query<K1,V1> query, boolean reuseObjects)
setInput(job, store.newQuery(), reuseObjects);
Class<K2> outKeyClass,
GoraInputFormat.setInput(job, query, reuseObjects);
initMapperJob(job, dataStore.newQuery(),
Class<K2> outKeyClass,
initMapperJob(job, query, outKeyClass, outValueClass,
DataStore<K, V> dataStore) {
case ARRAY:   return (T)IOUtils.deserialize(val, (SpecificDatumReader<SpecificRecord>)datumReader, (SpecificRecord)object);
case ARRAY:   return IOUtils.serialize((SpecificDatumWriter<SpecificRecord>)datumWriter, (SpecificRecord)o);
SpecificDatumWriter<T> datumWriter, T object)
SpecificDatumWriter<T> datumWriter, T object)
, T object) throws IOException {
serialize(os, datumWriter, object);
, T object) throws IOException {
serialize(os, datumWriter, object);
SpecificDatumReader<T> datumReader, T object)
SpecificDatumReader<T> datumReader, T object)
SpecificDatumReader<T> datumReader, T object)
GoraMapper.initMapperJob(job, query, LongWritable.class, VLongWritable.class, VerifyMapper.class, true);
public void addClassField(String classFieldName,
.addClassField(field.getAttributeValue(ATT_NAME),
fieldValue = IOUtils.deserialize((byte[]) solrValue, reader,
persistent.get(field.pos()));
data = IOUtils.serialize(writer, fieldValue);
serilazeData = IOUtils.serialize(writer, fieldValue);
import org.apache.accumulo.core.security.Authorizations;
import org.apache.accumulo.core.security.Credentials;
private Credentials credentials;
private static byte[] getBytes(Text text) {
byte[] bytes = text.getBytes();
if (bytes.length != text.getLength()) {
bytes = new byte[text.getLength()];
System.arraycopy(text.getBytes(), 0, bytes, 0, bytes.length);
}
return bytes;
}
credentials = new Credentials(user, token);
} catch (AccumuloException | AccumuloSecurityException e) {
} catch (TableExistsException e) {
LOG.debug(e.getMessage(), e);
Scanner scanner = new IsolatedScanner(conn.createScanner(mapping.tableName, Authorizations.EMPTY));
Scanner scanner = new IsolatedScanner(conn.createScanner(mapping.tableName, Authorizations.EMPTY));
tl = TabletLocator.getLocator(conn.getInstance(), new Text(Tables.getTableId(conn.getInstance(), mapping.tableName)));
while (tl.binRanges(credentials, Collections.singletonList(createRange(query)), binnedRanges).size() > 0) {
startKey = followingKey(encoder, getKeyClass(), getBytes(ke.getPrevEndRow()));
startKey = fromBytes(getKeyClass(), getBytes(startRow));
endKey = lastPossibleKey(encoder, getKeyClass(), getBytes(ke.getEndRow()));
endKey = fromBytes(getKeyClass(), getBytes(endRow));
import org.apache.gora.memory.store.MemStore;
String rsContinent = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(),
String rsUser = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_USERNAME, "asf-gora");
String rsApiKey = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_APIKEY, null);
String rs_region = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_REGION, "DFW");
String rs_flavourId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_FLAVORID, null);
String rs_imageId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_IMAGEID, null);
int num_servers = Integer.parseInt(DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_NUM_SERVERS, "10"));
String serverName = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_SERVERNAME, "goraci_test_server");
if (DataStoreFactory.findBooleanProperty(properties, MemStore.class.newInstance(), RS_PUBKEY, "true")) {
"/**\n"
" *Licensed to the Apache Software Foundation (ASF) under one\n"
" *or more contributor license agreements.  See the NOTICE file\n"
" *distributed with this work for additional information\n"
" *regarding copyright ownership.  The ASF licenses this file\n"
" *to you under the Apache License, Version 2.0 (the\"\n"
" *License\"); you may not use this file except in compliance\n"
" *with the License.  You may obtain a copy of the License at\n"
" *\n "
" * http://www.apache.org/licenses/LICENSE-2.0\n"
" * \n"
" *Unless required by applicable law or agreed to in writing, software\n"
" *distributed under the License is distributed on an \"AS IS\" BASIS,\n"
" *WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n"
" *See the License for the specific language governing permissions and\n"
" *limitations under the License.\n"
" */\n";
"/**\n"
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the GNU Affero General Public License as published by\n"
" * the Free Software Foundation, either version 3 of the License, or\n"
" * (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Public License for more details.\n"
" */\n";
"/**\n"
" * COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0\n"
" *\n "
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the Common Development and Distrubtion License as\n"
" * published by the Sun Microsystems, either version 1.0 of the\n"
" * License, or (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Lesser Public License for more details.\n"
" *\n "
" * You should have received a copy of the Common Development and Distrubtion\n"
" * License along with this program.  If not, see\n"
" * <http://www.gnu.org/licenses/gpl-1.0.html>.\n"
" */\n";
"/**\n"
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the GNU Free Documentation License as published by\n"
" * the Free Software Foundation, either version 1.3 of the License, or\n"
" * (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Public License for more details.\n"
" *\n "
" * You should have received a copy of the GNU Free Documentation License\n"
" * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n"
" */\n";
"/**\n"
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the GNU General Public License as\n"
" * published by the Free Software Foundation, either version 1 of the\n"
" * License, or (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Public License for more details.\n"
" *\n "
" * You should have received a copy of the GNU General Public\n"
" * License along with this program.  If not, see\n"
" * <http://www.gnu.org/licenses/gpl-1.0.html>.\n"
" */\n";
"/**\n"
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the GNU General Public License as\n"
" * published by the Free Software Foundation, either version 2 of the\n"
" * License, or (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Public License for more details.\n"
" *\n "
" * You should have received a copy of the GNU General Public\n"
" * License along with this program.  If not, see\n"
" * <http://www.gnu.org/licenses/gpl-2.0.html>.\n"
" */\n";
"/**\n"
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the GNU General Public License as\n"
" * published by the Free Software Foundation, either version 3 of the\n"
" * License, or (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Public License for more details.\n"
" *\n "
" * You should have received a copy of the GNU General Public\n"
" * License along with this program.  If not, see\n"
" * <http://www.gnu.org/licenses/gpl-3.0.html>.\n"
" */\n";
"/**\n"
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the GNU Lesser General Public License as\n"
" * published by the Free Software Foundation, either version 2.1 of the\n"
" * License, or (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Public License for more details.\n"
" *\n "
" * You should have received a copy of the GNU Lesser General Public\n"
" * License along with this program.  If not, see\n"
" * <http://www.gnu.org/licenses/lgpl-2.1.html>.\n"
" */\n";
"/**\n"
" * This program is free software: you can redistribute it and/or modify\n"
" * it under the terms of the GNU Lesser General Public License as\n"
" * published by the Free Software Foundation, either version 3 of the\n"
" * License, or (at your option) any later version.\n"
" *\n "
" * This program is distributed in the hope that it will be useful,\n"
" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"
" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"
" * GNU General Public License for more details.\n"
" *\n "
" * You should have received a copy of the GNU Lesser General Public\n"
" * License along with this program.  If not, see\n"
" * <http://www.gnu.org/licenses/lgpl-3.0.html>.\n"
" */\n";
for (String licenseValue : supportedLicenses) {
String var = (String) this.getClass().getDeclaredField(licenseValue).get(licenseValue);
relatedLicenses.put(licenseValue,var);
}
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"CINode\",\"namespace\":\"org.apache.gora.goraci.generated\",\"fields\":[{\"name\":\"prev\",\"type\":\"long\",\"default\":0},{\"name\":\"client\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"count\",\"type\":\"long\",\"default\":0}]}");
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Flushed\",\"namespace\":\"org.apache.gora.goraci.generated\",\"fields\":[{\"name\":\"count\",\"type\":\"long\",\"default\":0}]}");
import java.util.List;
import java.util.Set;
import org.jclouds.chef.ChefContext;
import org.jclouds.chef.config.ChefProperties;
import org.jclouds.chef.domain.BootstrapConfig;
import org.jclouds.chef.domain.CookbookVersion;
import org.jclouds.chef.predicates.CookbookVersionPredicates;
import org.jclouds.chef.util.RunListBuilder;
import org.jclouds.compute.ComputeServiceContext;
import org.jclouds.compute.RunNodesException;
import org.jclouds.compute.domain.NodeMetadata;
import org.jclouds.compute.domain.TemplateBuilder;
import org.jclouds.domain.JsonBall;
import org.jclouds.scriptbuilder.domain.Statement;
import org.jclouds.sshj.config.SshjSshClientModule;
import static org.jclouds.compute.options.TemplateOptions.Builder.runScript;
import com.google.common.collect.ImmutableSet;
import com.google.inject.Module;
import static com.google.common.collect.Iterables.any;
import static com.google.common.collect.Iterables.concat;
private static NovaApi novaApi = null;
private static String rsContinent = null;
private static String rsUser = null;
private static String rsApiKey = null;
private static String rsRegion = null;
performRackspaceOrchestration(properties);
}
private static void performRackspaceOrchestration(Properties properties) throws InstantiationException, IllegalAccessException, IOException {
rsContinent = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(),
rsUser = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_USERNAME, "asf-gora");
rsApiKey = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_APIKEY, null);
rsRegion = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_REGION, "DFW");
String rsFlavourId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_FLAVORID, null);
String rsImageId = DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_IMAGEID, null);
int numServers = Integer.parseInt(DataStoreFactory.findProperty(properties, MemStore.class.newInstance(), RS_NUM_SERVERS, "10"));
novaApi = ContextBuilder.newBuilder(rsContinent).credentials(rsUser, rsApiKey).buildApi(NovaApi.class);
LOG.info("Defining Rackspace cloudserver continent as: {}, and region: {}.", rsContinent, rsRegion);
ImageApi imageApi = novaApi.getImageApi(rsRegion);
Image image = imageApi.get(rsImageId);
FlavorApi flavorApi = novaApi.getFlavorApi(rsRegion);
Flavor flavor = flavorApi.get(rsFlavourId);
LOG.info("Defining Rackspace cloudserver flavors as: {}, with image id's {}", rsFlavourId, rsImageId);
KeyPairApi keyPairApi = novaApi.getKeyPairApi(rsRegion).get();
ServerApi serverApi = novaApi.getServerApi(rsRegion);
}
boolean filtered = false;
if (operator.equals(Operator.MUST_PASS_ONE)) {
for (Filter<K, T> filter: filters) {
if (!filter.filter(key, persistent)) {
return !filtered;
}
}
} else if (operator.equals(Operator.MUST_PASS_ALL)) {
for (Filter<K, T> filter: filters) {
if (filter.filter(key, persistent)) {
return !filtered;
}
}
} else {
}
return filtered;
private Configuration conf = new Configuration();
private Configuration conf = new Configuration();
int fieldIndex = persistent.getSchema().getField(fieldName).pos();
= (Class<? extends DataStore<K, T>>) ClassLoadingUtils.loadClass(dataStoreClass);
List<MongoCredential> credentials = new ArrayList<>();
LOG.debug("Collection {} has been created for Mongo instance {}.",
LOG.debug(
LOG.debug("Forced synced of database for Mongo instance {}.",
if (bin instanceof String) {
ObjectId id = new ObjectId((String) bin);
result = new Utf8(id.toString());
} else {
result = new Utf8(bin.toString());
}
import org.apache.avro.util.Utf8;
import org.bson.BSONObject;
import java.nio.ByteBuffer;
import java.util.Date;
.get(getLeafName(fieldName));
String lf = getLeafName(fieldName);
return parent.containsField(lf) ? parent.getBoolean(lf) : null;
String lf = getLeafName(fieldName);
return parent.containsField(lf) ? parent.getDouble(lf) : null;
String lf = getLeafName(fieldName);
return parent.containsField(lf) && parent.get(lf) != null ? parent.getInt(lf) : null;
String lf = getLeafName(fieldName);
return parent.containsField(lf) ? parent.getLong(lf) : null;
String lf = getLeafName(fieldName);
return parent.getDate(lf);
switch (operator.toString()) {
case "MUST_PASS_ALL":
break;
case "MUST_PASS_ONE":
break;
default:
= (Class<? extends DataStore<K, T>>) ClassLoadingUtils.loadClass(dataStoreClass);
List<MongoCredential> credentials = new ArrayList<>();
LOG.debug("Collection {} has been created for Mongo instance {}.",
LOG.debug(
LOG.debug("Forced synced of database for Mongo instance {}.",
if (bin instanceof String) {
ObjectId id = new ObjectId((String) bin);
result = new Utf8(id.toString());
} else {
result = new Utf8(bin.toString());
}
import org.apache.avro.util.Utf8;
import org.bson.BSONObject;
import java.nio.ByteBuffer;
import java.util.Date;
.get(getLeafName(fieldName));
String lf = getLeafName(fieldName);
return parent.containsField(lf) ? parent.getBoolean(lf) : null;
String lf = getLeafName(fieldName);
return parent.containsField(lf) ? parent.getDouble(lf) : null;
String lf = getLeafName(fieldName);
return parent.containsField(lf) && parent.get(lf) != null ? parent.getInt(lf) : null;
String lf = getLeafName(fieldName);
return parent.containsField(lf) ? parent.getLong(lf) : null;
String lf = getLeafName(fieldName);
return parent.getDate(lf);
if (table == null) {
throw new IOException("No table was provided.");
}
DataOutputStream dos = null;
dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
dos.close();
} finally {
try {
dos.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataInputStream dis = null;
dis = new DataInputStream(new ByteArrayInputStream(a));
dis.close();
} finally {
try {
dis.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataOutputStream dos = null;
dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
dos.close();
} finally {
try {
dos.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataInputStream dis = null;
dis = new DataInputStream(new ByteArrayInputStream(a));
dis.close();
} finally {
try {
dis.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataOutputStream dos = null;
dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
dos.close();
} finally {
try {
dos.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataInputStream dis = null;
dis = new DataInputStream(new ByteArrayInputStream(a));
dis.close();
} finally {
try {
dis.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataOutputStream dos = null;
dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
dos.close();
} finally {
try {
dos.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataInputStream dis = null;
dis = new DataInputStream(new ByteArrayInputStream(a));
dis.close();
} finally {
try {
dis.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataOutputStream dos = null;
dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
} finally {
try {
dos.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataInputStream dis = null;
dis = new DataInputStream(new ByteArrayInputStream(a));
} finally {
try {
dis.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataInputStream dis = null;
dis = new DataInputStream(new ByteArrayInputStream(a));
} finally {
try {
dis.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
DataOutputStream dos = null;
dos = new DataOutputStream(new FixedByteArrayOutputStream(ret));
} finally {
try {
dos.close();
} catch (IOException ioe) {
throw new RuntimeException(ioe);
}
mappingFile.close();
import org.apache.hadoop.hbase.client.HTable;
HTable hTable = htu.createTable(tableName, cfs);
hTable.close();
HTable hTable = htu.truncateTable(table.getName());
hTable.close();
PersistentBase persistent = (PersistentBase) fieldValue;
PersistentBase newRecord = (PersistentBase) SpecificData.get().newRecord(persistent, persistent.getSchema());
return result.getProgress();
}
catch(Exception e){
return 0;
}
throws IOException, InterruptedException { }
try{
if (counter.isModulo()) {
boolean firstBatch = (this.result == null);
if (! firstBatch) {
this.query.setStartKey(this.result.getKey());
if (this.query.getLimit() == counter.getRecordsMax()) {
}
}
if (this.result != null) {
this.result.close();
}
executeQuery();
if (! firstBatch) {
this.result.next();
}
}
counter.increment();
return this.result.next();
}
catch(Exception e){
LOG.error("Error reading Gora records: {}", e.getMessage());
throw new RuntimeException(e);
}
import org.apache.gora.persistency.impl.PersistentBase;
implements Deserializer<PersistentBase> {
private Class<? extends PersistentBase> persistentClass;
private SpecificDatumReader<PersistentBase> datumReader;
public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
datumReader = new SpecificDatumReader<PersistentBase>(schema);
public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerialization implements Serialization<PersistentBase> {
return PersistentBase.class.isAssignableFrom(c);
public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
private SpecificDatumWriter<PersistentBase> datumWriter;
public void serialize(PersistentBase persistent) throws IOException {
private static<T extends PersistentBase> T getPersistent(T obj, String[] fields) {
public interface Persistent extends Dirtyable {
import org.apache.gora.store.DataStoreFactory;
protected boolean autoCreateSchema;
@Override
autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
this.properties = properties;
import org.apache.gora.persistency.impl.PersistentBase;
public static Schema getSchema(Class<? extends PersistentBase> clazz)
public static String[] getPersistentFieldNames(PersistentBase persistent) {
public static <T extends PersistentBase> T deepClonePersistent(T persistent) {
SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(
import org.apache.gora.persistency.impl.PersistentBase;
throws SecurityException, NoSuchMethodException {
throws SecurityException, NoSuchMethodException {
public static <T> T newInstance(Class<T> clazz)
throws InstantiationException, IllegalAccessException,
SecurityException, NoSuchMethodException, IllegalArgumentException,
InvocationTargetException {
public static Object newInstance(String classStr)
throws InstantiationException, IllegalAccessException,
ClassNotFoundException, SecurityException, IllegalArgumentException,
NoSuchMethodException, InvocationTargetException {
throws IllegalArgumentException, SecurityException,
IllegalAccessException, NoSuchFieldException {
public static <T extends PersistentBase> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz)
throws SecurityException, NoSuchMethodException, IllegalArgumentException,
IllegalAccessException, InvocationTargetException {
import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Locale;
import org.apache.commons.io.FilenameUtils;
import org.apache.gora.util.GoraException;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
private String packageName;
this.dest = dest;
log.info("Compiling {} to {}", src, dest);
if (dynamoDBMap.getTables().isEmpty())
throw new IllegalStateException("There are no tables defined.");
for(String tableName : dynamoDBMap.getTables().keySet()) {
compiler.compile(tableName, dynamoDBMap.getKeySchema(tableName),
dynamoDBMap.getItems(tableName));
log.info("{} written without issues to {}", tableName, dest.getAbsolutePath());
private void compile(String pTableName, ArrayList<KeySchemaElement> arrayList, Map<String, String> map){
setHeaders(packageName);
for (KeySchemaElement pKeySchema : arrayList) {
setKeyAttributes(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
setKeyMethods(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
map.remove(pKeySchema.getAttributeName());
}
setItems(map, 2);
setDefaultMethods(2, pTableName);
log.error("Error while compiling table {}",pTableName, e.getMessage());
throw new RuntimeException(e);
private void setItems(Map<String, String> pItems, int pIden)
throws IOException {
for (String itemName : pItems.keySet()) {
String itemType = "String";
if (pItems.get(itemName).toString().equals("N"))
itemType = "double";
if (pItems.get(itemName).toString().equals("SS"))
itemType = "Set<String>";
if (pItems.get(itemName).toString().equals("SN"))
itemType = "Set<double>";
setItemMethods(itemName, itemType, pIden);
private void setItemMethods(String pItemName, String pItemType, int pIden)
throws IOException {
line(pIden, "@DynamoDBAttribute(attributeName = \""
private void setKeyMethods(KeySchemaElement pKeySchema, String attType,
int pIden) throws IOException {
attType = attType.equals("S") ? "String" : "double";
if (pKeySchema.getKeyType().equals(KeyType.HASH.toString())) {
strBuilder.append("@DynamoDBHashKey(attributeName=\""
"; }");
if (pKeySchema.getKeyType().equals(KeyType.RANGE.toString())) {
strBuilder.append("@DynamoDBRangeKey(attributeName=\""
"; }");
private void setKeyAttributes(KeySchemaElement pKeySchema, String attType,
int pIden) throws IOException {
attType = attType.equals("S") ? "String " : "double ";
if (pKeySchema != null) {
String fullDest = FilenameUtils.normalize
File dir = new File(fullDest);
out = new OutputStreamWriter(new FileOutputStream(new File(dir, name)), Charset.defaultCharset());
if (namespace != null) {
line(0, "import java.util.List;");
line(0, "");
line(0, "import org.apache.avro.Schema.Field;");
line(0, "import org.apache.gora.persistency.Tombstone;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBAttribute;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBHashKey;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTable;");
private void setDefaultMethods(int pIden, String tabName) throws IOException {
line(pIden, "public Tombstone getTombstone() { return null; }");
line(pIden, "public List<Field> getUnmanagedFields() { return null; }");
return name.substring(0,1).toUpperCase(Locale.getDefault())
name.substring(1,name.length());
public static void main(String[] args) {
try {
if (args.length < 2) {
log.error("Usage: Compiler <schema file> <output dir>");
System.exit(1);
}
compileSchema(new File(args[0]), new File(args[1]));
} catch (Exception e) {
log.error("Something went wrong. Please check the input file.", e.getMessage());
throw new RuntimeException(e);
if (doc == null || doc.getRootElement() == null)
". Please check its existance!");
boolean keys = false;
for (Element tableElement : tableElements) {
String tableName = tableElement.getAttributeValue("name");
long readCapacUnits = Long.parseLong(tableElement
.getAttributeValue("readcunit"));
long writeCapacUnits = Long.parseLong(tableElement
.getAttributeValue("writecunit"));
this.packageName = tableElement.getAttributeValue("package");
mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
writeCapacUnits);
log.debug("Table properties have been set for name, package and provisioned throughput.");
List<Element> fieldElements = tableElement.getChildren("attribute");
for (Element fieldElement : fieldElements) {
String key = fieldElement.getAttributeValue("key");
String attributeName = fieldElement.getAttributeValue("name");
mappingBuilder.addAttribute(tableName, attributeName, attributeType);
if (key != null) {
mappingBuilder.setKeySchema(tableName, attributeName, key);
keys = true;
}
log.debug("Attributes for table '{}' have been read.", tableName);
if (!keys)
log.warn("Keys for table '{}' have NOT been set.", tableName);
log.error("Error while performing xml mapping.", ex.getMessage());
throw new RuntimeException(ex);
log.error("An error occured whilst reading the xml mapping file!", ex.getMessage());
public class DynamoDBKey<H, R> {
@Override
public String toString() {
StringBuilder sb = new StringBuilder();
sb.append('[').append(hashKey != null? hashKey.toString():":");
sb.append(']');
return sb.toString();
}
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;
import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBScanExpression;
import com.amazonaws.services.dynamodbv2.model.AttributeValue;
import com.amazonaws.services.dynamodbv2.model.ComparisonOperator;
import com.amazonaws.services.dynamodbv2.model.Condition;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
public static final ComparisonOperator DEFAULT_SCAN_OP = ComparisonOperator.GE;
private ArrayList<KeySchemaElement> keySchema;
private Map<String, String> keyItems;
super(null);
super(dataStore);
private void defineQueryParams() {
if ((query.getStartKey() != null || query.getKey() != null)
&& query.getEndKey() != null) {
DynamoDBQuery.setType(RANGE_QUERY);
} else if (query.getKey() != null || query.getStartKey() != null) {
DynamoDBQuery.setType(SCAN_QUERY);
}
}
public void buildExpression() {
defineQueryParams();
if (DynamoDBQuery.getType().equals(RANGE_QUERY)) {
buildRangeExpression();
} else if (DynamoDBQuery.getType().equals(SCAN_QUERY)) {
buildScanExpression();
} else {
throw new IllegalArgumentException("Query type not supported");
}
private Map<String, AttributeValue> buildHashKey(K qKey) {
Map<String, AttributeValue> hashKey = new HashMap<>();
for (KeySchemaElement key : getKeySchema()) {
AttributeValue attr = new AttributeValue();
if (key.getKeyType().equals(KeyType.HASH.toString())) {
if (keyItems.get(key.getAttributeName()).equals("N")) {
attr.withN(getHashKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("S")) {
attr.withS(getHashKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("B")) {
attr.withB(ByteBuffer.wrap(getHashKey(qKey).toString().getBytes(Charset.defaultCharset())));
} else {
throw new IllegalArgumentException("Data type not supported for "
key.getAttributeName());
}
hashKey.put(key.getAttributeName(), attr);
}
}
if (hashKey.isEmpty()) {
throw new IllegalStateException("No key value has been defined.");
}
return hashKey;
}
private Map<String, AttributeValue> buildRangeKey(K qKey) {
Map<String, AttributeValue> kAttrs = new HashMap<>();
for (KeySchemaElement key : getKeySchema()) {
AttributeValue attr = new AttributeValue();
if (key.getKeyType().equals(KeyType.RANGE.toString())) {
if (keyItems.get(key.getAttributeName()).equals("N")) {
attr.withN(getRangeKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("S")) {
attr.withS(getRangeKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("B")) {
attr.withB(ByteBuffer.wrap(getRangeKey(qKey).toString().getBytes(Charset.defaultCharset())));
} else {
throw new IllegalArgumentException("Data type not supported for "
key.getAttributeName());
}
kAttrs.put(key.getAttributeName(), attr);
}
}
return kAttrs;
public void buildScanExpression() {
K qKey = getKey();
if (qKey == null) {
LOG.warn("No key defined. Trying with startKey.");
qKey = query.getStartKey();
if (qKey == null) {
throw new IllegalStateException("No key has been defined please check");
}
}
ComparisonOperator compOp = getScanCompOp() != null ? getScanCompOp()
: DEFAULT_SCAN_OP;
Map<String, AttributeValue> hashAttrVals = buildHashKey(qKey);
for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
Condition scanFilterHashCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue());
newScanExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
}
Map<String, AttributeValue> rangeAttrVals = buildRangeKey(qKey);
for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue());
newScanExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
}
public void buildRangeExpression() {
DynamoDBScanExpression queryExpression = new DynamoDBScanExpression();
ComparisonOperator compOp = ComparisonOperator.BETWEEN;
Map<String, AttributeValue> hashAttrVals = buildHashKey(query.getStartKey());
Map<String, AttributeValue> endHashAttrVals = buildHashKey(query.getEndKey());
for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
Condition scanFilterHashCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue(), endHashAttrVals.get(en.getKey()));
queryExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
}
Map<String, AttributeValue> rangeAttrVals = buildRangeKey(query.getStartKey());
Map<String, AttributeValue> endRangeAttrVals = buildRangeKey(query.getEndKey());
for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue(), endRangeAttrVals.get(en.getKey()));
queryExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
}
dynamoDBExpression = queryExpression;
for (Method met : key.getClass().getDeclaredMethods()) {
if (met.getName().equals("getHashKey")) {
Object[] params = null;
hashKey = met.invoke(key, params);
break;
}
} catch (IllegalArgumentException e) {
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new IllegalArgumentException(e);
} catch (IllegalAccessException e) {
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
} catch (InvocationTargetException e) {
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
return hashKey;
if(met.getName().equals("getRangeKey")){
Object [] params = null;
rangeKey = met.invoke(key, params);
break;
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new IllegalArgumentException(e);
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
public ArrayList<KeySchemaElement> getKeySchema(){
public void setKeySchema(ArrayList<KeySchemaElement> arrayList) {
this.keySchema = arrayList;
this.setStartKey(query.getStartKey());
this.setEndKey(query.getEndKey());
public void setKeyItems(Map<String, String> items) {
keyItems = items;
}
import static org.apache.gora.dynamodb.store.DynamoDBUtils.DYNAMO_KEY_HASHRANGE;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
private final Map<String, Map<String, String>> tablesToItems;
private final Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas;
public DynamoDBMapping(Map<String, Map<String, String>> tablesToItems2,
Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas,
Map<String, ProvisionedThroughput> provisionedThroughput) {
this.tablesToItems = tablesToItems2;
public Map<String, Map<String, String>> getTables() {
public Map<String, String> getItems(String tableName) {
public ArrayList<KeySchemaElement> getKeySchema(String tableName) {
private Map<String, Map<String, String>> tablesToItems =
new HashMap<String, Map<String, String>>();
private Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas =
new HashMap<String, ArrayList<KeySchemaElement>>();
private Map<String, ProvisionedThroughput> tablesToPrTh =
new HashMap<String, ProvisionedThroughput>();
public void setProvisionedThroughput(String tableName, long readCapUnits,
long writeCapUnits) {
ProvisionedThroughput ptDesc = new ProvisionedThroughput()
.withReadCapacityUnits(readCapUnits).withWriteCapacityUnits(
writeCapUnits);
public void setKeySchema(String tableName, String keyName, String keyType) {
ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
if (kSchema == null) {
kSchema = new ArrayList<KeySchemaElement>();
}
KeyType type = keyType.equals(DYNAMO_KEY_HASHRANGE) ? KeyType.RANGE : KeyType.HASH;
kSchema.add(new KeySchemaElement().withAttributeName(keyName)
.withKeyType(type));
private Map<String, String> getOrCreateTable(String tableName) {
Map<String, String> items = tablesToItems.get(tableName);
items = new HashMap<String, String>();
}
public void addAttribute(String tableName, String attributeName,
String attrType) {
Map<String, String> items = getOrCreateTable(tableName);
items.put(attributeName, attrType);
}
private boolean verifyAllKeySchemas() {
boolean rsl = true;
if (tablesToItems.isEmpty() || tablesToKeySchemas.isEmpty())
rsl = false;
for (String tableName : tablesToItems.keySet()) {
if (tablesToKeySchemas.get(tableName) == null) {
rsl = false;
rsl = verifyKeySchema(tableName);
}
return rsl;
private boolean verifyKeySchema(String tableName) {
ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
boolean hashPk = false;
if (kSchema == null) {
LOG.error("No keys defined for '{}'. Please check your schema!", tableName);
return hashPk;
for (KeySchemaElement ks : kSchema) {
if (ks.getKeyType().equals(KeyType.HASH.toString())) {
hashPk = true;
}
}
return hashPk;
if (tablesToItems.isEmpty())
throw new IllegalStateException("No tables were defined.");
if (!verifyAllKeySchemas())
throw new IllegalStateException("no key schemas defined for table ");
return new DynamoDBMapping(tablesToItems, tablesToKeySchemas,
tablesToPrTh);
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CLI_TYP_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS_TRUE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.ENDPOINT_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.PREF_SCH_NAME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SERIALIZATION_TYPE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SLEEP_DELETE_TIME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.WAIT_TIME;
import java.util.Map;
import org.apache.gora.store.DataStore;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
import com.amazonaws.services.dynamodbv2.model.DeleteTableRequest;
import com.amazonaws.services.dynamodbv2.model.DeleteTableResult;
import com.amazonaws.services.dynamodbv2.model.DescribeTableRequest;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
import com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException;
import com.amazonaws.services.dynamodbv2.model.TableDescription;
public class DynamoDBStore<K, T extends Persistent> implements DataStore<K, T> {
private IDynamoDB<K, T> dynamoDbStore;
private DynamoDBUtils.DynamoDBType serializationType;
private String preferredSchema;
@Override
public void close() {
dynamoDbStore.close();
@Override
public void createSchema() {
dynamoDbStore.createSchema();
}
@Override
public boolean delete(K key) {
return dynamoDbStore.delete(key);
}
@Override
public long deleteByQuery(Query<K, T> query) {
return dynamoDbStore.deleteByQuery(query);
}
@Override
public void deleteSchema() {
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
if (preferredSchema == null) {
LOG.debug("Delete schemas");
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
for (String tableName : getDynamoDbMapping().getTables().keySet())
executeDeleteTableRequest(tableName);
LOG.debug("All schemas deleted successfully.");
} else {
executeDeleteTableRequest(preferredSchema);
@Override
public Result<K, T> execute(Query<K, T> query) {
return dynamoDbStore.execute(query);
}
@Override
public void flush() {
dynamoDbStore.flush();
}
@Override
public T get(K key) {
return dynamoDbStore.get(key);
}
@Override
public T get(K key, String[] fields) {
return dynamoDbStore.get(key, fields);
}
@Override
public BeanFactory<K, T> getBeanFactory() {
@Override
public Class<K> getKeyClass() {
return null;
}
@Override
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> arg0)
throws IOException {
return null;
}
@Override
public Class<T> getPersistentClass() {
return null;
}
@Override
public String getSchemaName() {
return this.getPreferredSchema();
}
@Override
public void initialize(Class<K> keyClass, Class<T> persistentClass,
Properties properties) {
try {
LOG.debug("Initializing DynamoDB store");
setDynamoDBProperties(properties);
dynamoDbStore = DynamoDBFactory.buildDynamoDBStore(getSerializationType());
dynamoDbStore.setDynamoDBStoreHandler(this);
dynamoDbStore.initialize(keyClass, persistentClass, properties);
} catch (Exception e) {
LOG.error("Error while initializing DynamoDB store", e.getMessage());
throw new RuntimeException(e);
}
}
private void setDynamoDBProperties(Properties properties) throws IOException {
setSerializationType(properties.getProperty(SERIALIZATION_TYPE));
PropertiesCredentials creds = DynamoDBUtils.getCredentials(this.getClass());
setPreferredSchema(properties.getProperty(PREF_SCH_NAME));
setDynamoDBClient(DynamoDBUtils.getClient(
properties.getProperty(CLI_TYP_PROP), creds));
getDynamoDBClient().setEndpoint(properties.getProperty(ENDPOINT_PROP));
setDynamoDbMapping(readMapping());
setConsistency(properties.getProperty(CONSISTENCY_READS));
}
@Override
public K newKey() {
return dynamoDbStore.newKey();
}
@Override
public T newPersistent() {
return dynamoDbStore.newPersistent();
}
@Override
public Query<K, T> newQuery() {
return dynamoDbStore.newQuery();
}
@Override
public void put(K key, T value) {
dynamoDbStore.put(key, value);
}
@Override
public boolean schemaExists() {
LOG.info("Verifying schemas.");
TableDescription success = null;
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
if (getPreferredSchema() == null) {
LOG.debug("Verifying schemas");
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
for (String tableName : getDynamoDbMapping().getTables().keySet()) {
success = getTableSchema(tableName);
if (success == null)
return false;
}
} else {
success = getTableSchema(preferredSchema);
}
LOG.info("Finished verifying schemas.");
return (success != null) ? true : false;
}
@Override
public void setBeanFactory(BeanFactory<K, T> arg0) {
}
@Override
public void setKeyClass(Class<K> arg0) {
dynamoDbStore.setKeyClass(arg0);
}
@Override
public void setPersistentClass(Class<T> arg0) {
dynamoDbStore.setPersistentClass(arg0);
}
@Override
public void truncateSchema() {
}
Document doc = builder.build(getClass().getClassLoader()
.getResourceAsStream(MAPPING_FILE));
if (doc == null || doc.getRootElement() == null)
". Please check its existance!");
boolean keys = false;
for (Element tableElement : tableElements) {
long readCapacUnits = Long.parseLong(tableElement
.getAttributeValue("readcunit"));
long writeCapacUnits = Long.parseLong(tableElement
.getAttributeValue("writecunit"));
mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
writeCapacUnits);
List<Element> fieldElements = tableElement.getChildren("attribute");
for (Element fieldElement : fieldElements) {
String key = fieldElement.getAttributeValue("key");
String attributeName = fieldElement.getAttributeValue("name");
mappingBuilder.addAttribute(tableName, attributeName, attributeType);
if (key != null) {
mappingBuilder.setKeySchema(tableName, attributeName, key);
keys = true;
}
if (!keys)
} catch (IOException ex) {
LOG.error("Error while performing xml mapping.", ex.getMessage());
} catch (Exception ex) {
LOG.error("Error while performing xml mapping.", ex.getMessage());
throw new RuntimeException(ex);
public void executeDeleteTableRequest(String pTableName) {
try {
DeleteTableRequest deleteTableRequest = new DeleteTableRequest()
.withTableName(pTableName);
DeleteTableResult result = getDynamoDBClient().deleteTable(
deleteTableRequest);
" deleted successfully.");
} catch (Exception e) {
LOG.debug("Schema: {} deleted.", pTableName, e.getMessage());
throw new RuntimeException(e);
private void waitForTableToBeDeleted(String pTableName) {
Thread.sleep(SLEEP_DELETE_TIME);
} catch (Exception e) {
}
try {
DescribeTableRequest request = new DescribeTableRequest()
.withTableName(pTableName);
TableDescription tableDescription = getDynamoDBClient().describeTable(
request).getTable();
LOG.error(ase.getMessage());
private TableDescription getTableSchema(String tableName) {
try {
DescribeTableRequest describeTableRequest = new DescribeTableRequest()
.withTableName(tableName);
tableDescription = getDynamoDBClient()
.describeTable(describeTableRequest).getTable();
} catch (ResourceNotFoundException e) {
public ArrayList<KeySchemaElement> getTableKeySchema(String tableName) {
return getDynamoDbMapping().getKeySchema(tableName);
public ProvisionedThroughput getTableProvisionedThroughput(String tableName) {
return getDynamoDbMapping().getProvisionedThroughput(tableName);
public Map<String, String> getTableAttributes(String tableName) {
return getDynamoDbMapping().getItems(tableName);
public boolean getConsistencyReads() {
if (getConsistency() != null)
if (getConsistency().equals(CONSISTENCY_READS_TRUE))
return true;
return false;
}
public void setDynamoDbStore(IDynamoDB<K, T> iDynamoDB) {
this.dynamoDbStore = iDynamoDB;
}
private void setSerializationType(String serializationType) {
if (serializationType == null || serializationType.isEmpty()
|| serializationType.equals(DynamoDBUtils.AVRO_SERIALIZATION)) {
LOG.warn("Using AVRO serialization.");
this.serializationType = DynamoDBUtils.DynamoDBType.AVRO;
} else {
LOG.warn("Using DynamoDB serialization.");
this.serializationType = DynamoDBUtils.DynamoDBType.DYNAMO;
}
}
private DynamoDBUtils.DynamoDBType getSerializationType() {
return serializationType;
}
public String getPreferredSchema() {
return preferredSchema;
}
public void setPreferredSchema(String preferredSchema) {
this.preferredSchema = preferredSchema;
}
public AmazonDynamoDB getDynamoDbClient() {
return getDynamoDBClient();
}
public DynamoDBMapping getDynamoDbMapping() {
return mapping;
}
public void setDynamoDbMapping(DynamoDBMapping mapping) {
this.mapping = mapping;
}
public String getConsistency() {
return consistency;
}
public void setConsistency(String consistency) {
this.consistency = consistency;
}
public AmazonDynamoDB getDynamoDBClient() {
return dynamoDBClient;
}
public void setDynamoDBClient(AmazonDynamoDB dynamoDBClient) {
this.dynamoDBClient = dynamoDBClient;
PersistentBase record = (PersistentBase) new BeanFactoryImpl(keyClass, clazz).newPersistent();
if ( unionSchema == null ) {
unionSchema = pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
}
if (serializer != null) {
List<?> genericArray = serializer.fromByteBuffer(byteBuffer);
value = genericArray;
} else {
LOG.warn("Field detected as type Array, however no serializer could be found!")
}
if ( unionSchema == null ) {
unionSchema = pSchema.getTypes().get(CassandraStore.DEFAULT_UNION_SCHEMA);
}
LOG.warn("Field detected as type Array, however no serializer could be found!");
PersistentBase persistent = (PersistentBase) fieldValue;
PersistentBase newRecord = (PersistentBase) SpecificData.get().newRecord(persistent, persistent.getSchema());
return result.getProgress();
}
catch(Exception e){
return 0;
}
throws IOException, InterruptedException { }
try{
if (counter.isModulo()) {
boolean firstBatch = (this.result == null);
if (! firstBatch) {
this.query.setStartKey(this.result.getKey());
if (this.query.getLimit() == counter.getRecordsMax()) {
}
}
if (this.result != null) {
this.result.close();
}
executeQuery();
if (! firstBatch) {
this.result.next();
}
}
counter.increment();
return this.result.next();
}
catch(Exception e){
LOG.error("Error reading Gora records: {}", e.getMessage());
throw new RuntimeException(e);
}
import org.apache.gora.persistency.impl.PersistentBase;
implements Deserializer<PersistentBase> {
private Class<? extends PersistentBase> persistentClass;
private SpecificDatumReader<PersistentBase> datumReader;
public PersistentDeserializer(Class<? extends PersistentBase> c, boolean reuseObjects) {
datumReader = new SpecificDatumReader<PersistentBase>(schema);
public PersistentBase deserialize(PersistentBase persistent) throws IOException {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerialization implements Serialization<PersistentBase> {
return PersistentBase.class.isAssignableFrom(c);
public Deserializer<PersistentBase> getDeserializer(Class<PersistentBase> c) {
public Serializer<PersistentBase> getSerializer(Class<PersistentBase> c) {
import org.apache.gora.persistency.impl.PersistentBase;
public class PersistentSerializer implements Serializer<PersistentBase> {
private SpecificDatumWriter<PersistentBase> datumWriter;
public void serialize(PersistentBase persistent) throws IOException {
private static<T extends PersistentBase> T getPersistent(T obj, String[] fields) {
public interface Persistent extends Dirtyable {
import org.apache.gora.store.DataStoreFactory;
protected boolean autoCreateSchema;
@Override
autoCreateSchema = DataStoreFactory.getAutoCreateSchema(properties, this);
this.properties = properties;
import org.apache.gora.persistency.impl.PersistentBase;
public static Schema getSchema(Class<? extends PersistentBase> clazz)
public static String[] getPersistentFieldNames(PersistentBase persistent) {
public static <T extends PersistentBase> T deepClonePersistent(T persistent) {
SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(
import org.apache.gora.persistency.impl.PersistentBase;
throws SecurityException, NoSuchMethodException {
throws SecurityException, NoSuchMethodException {
public static <T> T newInstance(Class<T> clazz)
throws InstantiationException, IllegalAccessException,
SecurityException, NoSuchMethodException, IllegalArgumentException,
InvocationTargetException {
public static Object newInstance(String classStr)
throws InstantiationException, IllegalAccessException,
ClassNotFoundException, SecurityException, IllegalArgumentException,
NoSuchMethodException, InvocationTargetException {
throws IllegalArgumentException, SecurityException,
IllegalAccessException, NoSuchFieldException {
public static <T extends PersistentBase> SpecificRecordBuilderBase<T> classBuilder(Class<T> clazz)
throws SecurityException, NoSuchMethodException, IllegalArgumentException,
IllegalAccessException, InvocationTargetException {
import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Locale;
import org.apache.commons.io.FilenameUtils;
import org.apache.gora.util.GoraException;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
private String packageName;
this.dest = dest;
log.info("Compiling {} to {}", src, dest);
if (dynamoDBMap.getTables().isEmpty())
throw new IllegalStateException("There are no tables defined.");
for(String tableName : dynamoDBMap.getTables().keySet()) {
compiler.compile(tableName, dynamoDBMap.getKeySchema(tableName),
dynamoDBMap.getItems(tableName));
log.info("{} written without issues to {}", tableName, dest.getAbsolutePath());
private void compile(String pTableName, ArrayList<KeySchemaElement> arrayList, Map<String, String> map){
setHeaders(packageName);
for (KeySchemaElement pKeySchema : arrayList) {
setKeyAttributes(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
setKeyMethods(pKeySchema, map.get(pKeySchema.getAttributeName()), 2);
map.remove(pKeySchema.getAttributeName());
}
setItems(map, 2);
setDefaultMethods(2, pTableName);
log.error("Error while compiling table {}",pTableName, e.getMessage());
throw new RuntimeException(e);
private void setItems(Map<String, String> pItems, int pIden)
throws IOException {
for (String itemName : pItems.keySet()) {
String itemType = "String";
if (pItems.get(itemName).toString().equals("N"))
itemType = "double";
if (pItems.get(itemName).toString().equals("SS"))
itemType = "Set<String>";
if (pItems.get(itemName).toString().equals("SN"))
itemType = "Set<double>";
setItemMethods(itemName, itemType, pIden);
private void setItemMethods(String pItemName, String pItemType, int pIden)
throws IOException {
line(pIden, "@DynamoDBAttribute(attributeName = \""
private void setKeyMethods(KeySchemaElement pKeySchema, String attType,
int pIden) throws IOException {
attType = attType.equals("S") ? "String" : "double";
if (pKeySchema.getKeyType().equals(KeyType.HASH.toString())) {
strBuilder.append("@DynamoDBHashKey(attributeName=\""
"; }");
if (pKeySchema.getKeyType().equals(KeyType.RANGE.toString())) {
strBuilder.append("@DynamoDBRangeKey(attributeName=\""
"; }");
private void setKeyAttributes(KeySchemaElement pKeySchema, String attType,
int pIden) throws IOException {
attType = attType.equals("S") ? "String " : "double ";
if (pKeySchema != null) {
String fullDest = FilenameUtils.normalize
File dir = new File(fullDest);
out = new OutputStreamWriter(new FileOutputStream(new File(dir, name)), Charset.defaultCharset());
if (namespace != null) {
line(0, "import java.util.List;");
line(0, "");
line(0, "import org.apache.avro.Schema.Field;");
line(0, "import org.apache.gora.persistency.Tombstone;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBAttribute;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBHashKey;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBRangeKey;");
line(0, "import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBTable;");
private void setDefaultMethods(int pIden, String tabName) throws IOException {
line(pIden, "public Tombstone getTombstone() { return null; }");
line(pIden, "public List<Field> getUnmanagedFields() { return null; }");
return name.substring(0,1).toUpperCase(Locale.getDefault())
name.substring(1,name.length());
public static void main(String[] args) {
try {
if (args.length < 2) {
log.error("Usage: Compiler <schema file> <output dir>");
System.exit(1);
}
compileSchema(new File(args[0]), new File(args[1]));
} catch (Exception e) {
log.error("Something went wrong. Please check the input file.", e.getMessage());
throw new RuntimeException(e);
if (doc == null || doc.getRootElement() == null)
". Please check its existance!");
boolean keys = false;
for (Element tableElement : tableElements) {
String tableName = tableElement.getAttributeValue("name");
long readCapacUnits = Long.parseLong(tableElement
.getAttributeValue("readcunit"));
long writeCapacUnits = Long.parseLong(tableElement
.getAttributeValue("writecunit"));
this.packageName = tableElement.getAttributeValue("package");
mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
writeCapacUnits);
log.debug("Table properties have been set for name, package and provisioned throughput.");
List<Element> fieldElements = tableElement.getChildren("attribute");
for (Element fieldElement : fieldElements) {
String key = fieldElement.getAttributeValue("key");
String attributeName = fieldElement.getAttributeValue("name");
mappingBuilder.addAttribute(tableName, attributeName, attributeType);
if (key != null) {
mappingBuilder.setKeySchema(tableName, attributeName, key);
keys = true;
}
log.debug("Attributes for table '{}' have been read.", tableName);
if (!keys)
log.warn("Keys for table '{}' have NOT been set.", tableName);
log.error("Error while performing xml mapping.", ex.getMessage());
throw new RuntimeException(ex);
log.error("An error occured whilst reading the xml mapping file!", ex.getMessage());
public class DynamoDBKey<H, R> {
@Override
public String toString() {
StringBuilder sb = new StringBuilder();
sb.append('[').append(hashKey != null? hashKey.toString():":");
sb.append(']');
return sb.toString();
}
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;
import com.amazonaws.services.dynamodbv2.datamodeling.DynamoDBScanExpression;
import com.amazonaws.services.dynamodbv2.model.AttributeValue;
import com.amazonaws.services.dynamodbv2.model.ComparisonOperator;
import com.amazonaws.services.dynamodbv2.model.Condition;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
public static final ComparisonOperator DEFAULT_SCAN_OP = ComparisonOperator.GE;
private ArrayList<KeySchemaElement> keySchema;
private Map<String, String> keyItems;
super(null);
super(dataStore);
private void defineQueryParams() {
if ((query.getStartKey() != null || query.getKey() != null)
&& query.getEndKey() != null) {
DynamoDBQuery.setType(RANGE_QUERY);
} else if (query.getKey() != null || query.getStartKey() != null) {
DynamoDBQuery.setType(SCAN_QUERY);
}
}
public void buildExpression() {
defineQueryParams();
if (DynamoDBQuery.getType().equals(RANGE_QUERY)) {
buildRangeExpression();
} else if (DynamoDBQuery.getType().equals(SCAN_QUERY)) {
buildScanExpression();
} else {
throw new IllegalArgumentException("Query type not supported");
}
private Map<String, AttributeValue> buildHashKey(K qKey) {
Map<String, AttributeValue> hashKey = new HashMap<>();
for (KeySchemaElement key : getKeySchema()) {
AttributeValue attr = new AttributeValue();
if (key.getKeyType().equals(KeyType.HASH.toString())) {
if (keyItems.get(key.getAttributeName()).equals("N")) {
attr.withN(getHashKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("S")) {
attr.withS(getHashKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("B")) {
attr.withB(ByteBuffer.wrap(getHashKey(qKey).toString().getBytes(Charset.defaultCharset())));
} else {
throw new IllegalArgumentException("Data type not supported for "
key.getAttributeName());
}
hashKey.put(key.getAttributeName(), attr);
}
}
if (hashKey.isEmpty()) {
throw new IllegalStateException("No key value has been defined.");
}
return hashKey;
}
private Map<String, AttributeValue> buildRangeKey(K qKey) {
Map<String, AttributeValue> kAttrs = new HashMap<>();
for (KeySchemaElement key : getKeySchema()) {
AttributeValue attr = new AttributeValue();
if (key.getKeyType().equals(KeyType.RANGE.toString())) {
if (keyItems.get(key.getAttributeName()).equals("N")) {
attr.withN(getRangeKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("S")) {
attr.withS(getRangeKey(qKey).toString());
} else if (keyItems.get(key.getAttributeName()).equals("B")) {
attr.withB(ByteBuffer.wrap(getRangeKey(qKey).toString().getBytes(Charset.defaultCharset())));
} else {
throw new IllegalArgumentException("Data type not supported for "
key.getAttributeName());
}
kAttrs.put(key.getAttributeName(), attr);
}
}
return kAttrs;
public void buildScanExpression() {
K qKey = getKey();
if (qKey == null) {
LOG.warn("No key defined. Trying with startKey.");
qKey = query.getStartKey();
if (qKey == null) {
throw new IllegalStateException("No key has been defined please check");
}
}
ComparisonOperator compOp = getScanCompOp() != null ? getScanCompOp()
: DEFAULT_SCAN_OP;
Map<String, AttributeValue> hashAttrVals = buildHashKey(qKey);
for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
Condition scanFilterHashCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue());
newScanExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
}
Map<String, AttributeValue> rangeAttrVals = buildRangeKey(qKey);
for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue());
newScanExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
}
public void buildRangeExpression() {
DynamoDBScanExpression queryExpression = new DynamoDBScanExpression();
ComparisonOperator compOp = ComparisonOperator.BETWEEN;
Map<String, AttributeValue> hashAttrVals = buildHashKey(query.getStartKey());
Map<String, AttributeValue> endHashAttrVals = buildHashKey(query.getEndKey());
for (Entry<String, AttributeValue> en : hashAttrVals.entrySet()) {
Condition scanFilterHashCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue(), endHashAttrVals.get(en.getKey()));
queryExpression.addFilterCondition(en.getKey(), scanFilterHashCondition);
}
Map<String, AttributeValue> rangeAttrVals = buildRangeKey(query.getStartKey());
Map<String, AttributeValue> endRangeAttrVals = buildRangeKey(query.getEndKey());
for (Entry<String, AttributeValue> en : rangeAttrVals.entrySet()) {
Condition scanFilterRangeCondition = new Condition().withComparisonOperator(
compOp.toString()).withAttributeValueList(en.getValue(), endRangeAttrVals.get(en.getKey()));
queryExpression.addFilterCondition(en.getKey(), scanFilterRangeCondition);
}
dynamoDBExpression = queryExpression;
for (Method met : key.getClass().getDeclaredMethods()) {
if (met.getName().equals("getHashKey")) {
Object[] params = null;
hashKey = met.invoke(key, params);
break;
}
} catch (IllegalArgumentException e) {
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new IllegalArgumentException(e);
} catch (IllegalAccessException e) {
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
} catch (InvocationTargetException e) {
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
return hashKey;
if(met.getName().equals("getRangeKey")){
Object [] params = null;
rangeKey = met.invoke(key, params);
break;
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new IllegalArgumentException(e);
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
LOG.info("DynamoDBStore: Error while trying to fetch range key.", e.getMessage());
throw new RuntimeException(e);
public ArrayList<KeySchemaElement> getKeySchema(){
public void setKeySchema(ArrayList<KeySchemaElement> arrayList) {
this.keySchema = arrayList;
this.setStartKey(query.getStartKey());
this.setEndKey(query.getEndKey());
public void setKeyItems(Map<String, String> items) {
keyItems = items;
}
import static org.apache.gora.dynamodb.store.DynamoDBUtils.DYNAMO_KEY_HASHRANGE;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
private final Map<String, Map<String, String>> tablesToItems;
private final Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas;
public DynamoDBMapping(Map<String, Map<String, String>> tablesToItems2,
Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas,
Map<String, ProvisionedThroughput> provisionedThroughput) {
this.tablesToItems = tablesToItems2;
public Map<String, Map<String, String>> getTables() {
public Map<String, String> getItems(String tableName) {
public ArrayList<KeySchemaElement> getKeySchema(String tableName) {
private Map<String, Map<String, String>> tablesToItems =
new HashMap<String, Map<String, String>>();
private Map<String, ArrayList<KeySchemaElement>> tablesToKeySchemas =
new HashMap<String, ArrayList<KeySchemaElement>>();
private Map<String, ProvisionedThroughput> tablesToPrTh =
new HashMap<String, ProvisionedThroughput>();
public void setProvisionedThroughput(String tableName, long readCapUnits,
long writeCapUnits) {
ProvisionedThroughput ptDesc = new ProvisionedThroughput()
.withReadCapacityUnits(readCapUnits).withWriteCapacityUnits(
writeCapUnits);
public void setKeySchema(String tableName, String keyName, String keyType) {
ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
if (kSchema == null) {
kSchema = new ArrayList<KeySchemaElement>();
}
KeyType type = keyType.equals(DYNAMO_KEY_HASHRANGE) ? KeyType.RANGE : KeyType.HASH;
kSchema.add(new KeySchemaElement().withAttributeName(keyName)
.withKeyType(type));
private Map<String, String> getOrCreateTable(String tableName) {
Map<String, String> items = tablesToItems.get(tableName);
items = new HashMap<String, String>();
}
public void addAttribute(String tableName, String attributeName,
String attrType) {
Map<String, String> items = getOrCreateTable(tableName);
items.put(attributeName, attrType);
}
private boolean verifyAllKeySchemas() {
boolean rsl = true;
if (tablesToItems.isEmpty() || tablesToKeySchemas.isEmpty())
rsl = false;
for (String tableName : tablesToItems.keySet()) {
if (tablesToKeySchemas.get(tableName) == null) {
rsl = false;
rsl = verifyKeySchema(tableName);
}
return rsl;
private boolean verifyKeySchema(String tableName) {
ArrayList<KeySchemaElement> kSchema = tablesToKeySchemas.get(tableName);
boolean hashPk = false;
if (kSchema == null) {
LOG.error("No keys defined for '{}'. Please check your schema!", tableName);
return hashPk;
for (KeySchemaElement ks : kSchema) {
if (ks.getKeyType().equals(KeyType.HASH.toString())) {
hashPk = true;
}
}
return hashPk;
if (tablesToItems.isEmpty())
throw new IllegalStateException("No tables were defined.");
if (!verifyAllKeySchemas())
throw new IllegalStateException("no key schemas defined for table ");
return new DynamoDBMapping(tablesToItems, tablesToKeySchemas,
tablesToPrTh);
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CLI_TYP_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.CONSISTENCY_READS_TRUE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.ENDPOINT_PROP;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.MAPPING_FILE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.PREF_SCH_NAME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SERIALIZATION_TYPE;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.SLEEP_DELETE_TIME;
import static org.apache.gora.dynamodb.store.DynamoDBUtils.WAIT_TIME;
import java.util.Map;
import org.apache.gora.store.DataStore;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
import com.amazonaws.services.dynamodbv2.model.DeleteTableRequest;
import com.amazonaws.services.dynamodbv2.model.DeleteTableResult;
import com.amazonaws.services.dynamodbv2.model.DescribeTableRequest;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
import com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException;
import com.amazonaws.services.dynamodbv2.model.TableDescription;
public class DynamoDBStore<K, T extends Persistent> implements DataStore<K, T> {
private IDynamoDB<K, T> dynamoDbStore;
private DynamoDBUtils.DynamoDBType serializationType;
private String preferredSchema;
@Override
public void close() {
dynamoDbStore.close();
@Override
public void createSchema() {
dynamoDbStore.createSchema();
}
@Override
public boolean delete(K key) {
return dynamoDbStore.delete(key);
}
@Override
public long deleteByQuery(Query<K, T> query) {
return dynamoDbStore.deleteByQuery(query);
}
@Override
public void deleteSchema() {
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
if (preferredSchema == null) {
LOG.debug("Delete schemas");
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
for (String tableName : getDynamoDbMapping().getTables().keySet())
executeDeleteTableRequest(tableName);
LOG.debug("All schemas deleted successfully.");
} else {
executeDeleteTableRequest(preferredSchema);
@Override
public Result<K, T> execute(Query<K, T> query) {
return dynamoDbStore.execute(query);
}
@Override
public void flush() {
dynamoDbStore.flush();
}
@Override
public T get(K key) {
return dynamoDbStore.get(key);
}
@Override
public T get(K key, String[] fields) {
return dynamoDbStore.get(key, fields);
}
@Override
public BeanFactory<K, T> getBeanFactory() {
@Override
public Class<K> getKeyClass() {
return null;
}
@Override
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> arg0)
throws IOException {
return null;
}
@Override
public Class<T> getPersistentClass() {
return null;
}
@Override
public String getSchemaName() {
return this.getPreferredSchema();
}
@Override
public void initialize(Class<K> keyClass, Class<T> persistentClass,
Properties properties) {
try {
LOG.debug("Initializing DynamoDB store");
setDynamoDBProperties(properties);
dynamoDbStore = DynamoDBFactory.buildDynamoDBStore(getSerializationType());
dynamoDbStore.setDynamoDBStoreHandler(this);
dynamoDbStore.initialize(keyClass, persistentClass, properties);
} catch (Exception e) {
LOG.error("Error while initializing DynamoDB store", e.getMessage());
throw new RuntimeException(e);
}
}
private void setDynamoDBProperties(Properties properties) throws IOException {
setSerializationType(properties.getProperty(SERIALIZATION_TYPE));
PropertiesCredentials creds = DynamoDBUtils.getCredentials(this.getClass());
setPreferredSchema(properties.getProperty(PREF_SCH_NAME));
setDynamoDBClient(DynamoDBUtils.getClient(
properties.getProperty(CLI_TYP_PROP), creds));
getDynamoDBClient().setEndpoint(properties.getProperty(ENDPOINT_PROP));
setDynamoDbMapping(readMapping());
setConsistency(properties.getProperty(CONSISTENCY_READS));
}
@Override
public K newKey() {
return dynamoDbStore.newKey();
}
@Override
public T newPersistent() {
return dynamoDbStore.newPersistent();
}
@Override
public Query<K, T> newQuery() {
return dynamoDbStore.newQuery();
}
@Override
public void put(K key, T value) {
dynamoDbStore.put(key, value);
}
@Override
public boolean schemaExists() {
LOG.info("Verifying schemas.");
TableDescription success = null;
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
if (getPreferredSchema() == null) {
LOG.debug("Verifying schemas");
if (getDynamoDbMapping().getTables().isEmpty())
throw new IllegalStateException("There are not tables defined.");
for (String tableName : getDynamoDbMapping().getTables().keySet()) {
success = getTableSchema(tableName);
if (success == null)
return false;
}
} else {
success = getTableSchema(preferredSchema);
}
LOG.info("Finished verifying schemas.");
return (success != null) ? true : false;
}
@Override
public void setBeanFactory(BeanFactory<K, T> arg0) {
}
@Override
public void setKeyClass(Class<K> arg0) {
dynamoDbStore.setKeyClass(arg0);
}
@Override
public void setPersistentClass(Class<T> arg0) {
dynamoDbStore.setPersistentClass(arg0);
}
@Override
public void truncateSchema() {
}
Document doc = builder.build(getClass().getClassLoader()
.getResourceAsStream(MAPPING_FILE));
if (doc == null || doc.getRootElement() == null)
". Please check its existance!");
boolean keys = false;
for (Element tableElement : tableElements) {
long readCapacUnits = Long.parseLong(tableElement
.getAttributeValue("readcunit"));
long writeCapacUnits = Long.parseLong(tableElement
.getAttributeValue("writecunit"));
mappingBuilder.setProvisionedThroughput(tableName, readCapacUnits,
writeCapacUnits);
List<Element> fieldElements = tableElement.getChildren("attribute");
for (Element fieldElement : fieldElements) {
String key = fieldElement.getAttributeValue("key");
String attributeName = fieldElement.getAttributeValue("name");
mappingBuilder.addAttribute(tableName, attributeName, attributeType);
if (key != null) {
mappingBuilder.setKeySchema(tableName, attributeName, key);
keys = true;
}
if (!keys)
} catch (IOException ex) {
LOG.error("Error while performing xml mapping.", ex.getMessage());
} catch (Exception ex) {
LOG.error("Error while performing xml mapping.", ex.getMessage());
throw new RuntimeException(ex);
public void executeDeleteTableRequest(String pTableName) {
try {
DeleteTableRequest deleteTableRequest = new DeleteTableRequest()
.withTableName(pTableName);
DeleteTableResult result = getDynamoDBClient().deleteTable(
deleteTableRequest);
" deleted successfully.");
} catch (Exception e) {
LOG.debug("Schema: {} deleted.", pTableName, e.getMessage());
throw new RuntimeException(e);
private void waitForTableToBeDeleted(String pTableName) {
Thread.sleep(SLEEP_DELETE_TIME);
} catch (Exception e) {
}
try {
DescribeTableRequest request = new DescribeTableRequest()
.withTableName(pTableName);
TableDescription tableDescription = getDynamoDBClient().describeTable(
request).getTable();
LOG.error(ase.getMessage());
private TableDescription getTableSchema(String tableName) {
try {
DescribeTableRequest describeTableRequest = new DescribeTableRequest()
.withTableName(tableName);
tableDescription = getDynamoDBClient()
.describeTable(describeTableRequest).getTable();
} catch (ResourceNotFoundException e) {
public ArrayList<KeySchemaElement> getTableKeySchema(String tableName) {
return getDynamoDbMapping().getKeySchema(tableName);
public ProvisionedThroughput getTableProvisionedThroughput(String tableName) {
return getDynamoDbMapping().getProvisionedThroughput(tableName);
public Map<String, String> getTableAttributes(String tableName) {
return getDynamoDbMapping().getItems(tableName);
public boolean getConsistencyReads() {
if (getConsistency() != null)
if (getConsistency().equals(CONSISTENCY_READS_TRUE))
return true;
return false;
}
public void setDynamoDbStore(IDynamoDB<K, T> iDynamoDB) {
this.dynamoDbStore = iDynamoDB;
}
private void setSerializationType(String serializationType) {
if (serializationType == null || serializationType.isEmpty()
|| serializationType.equals(DynamoDBUtils.AVRO_SERIALIZATION)) {
LOG.warn("Using AVRO serialization.");
this.serializationType = DynamoDBUtils.DynamoDBType.AVRO;
} else {
LOG.warn("Using DynamoDB serialization.");
this.serializationType = DynamoDBUtils.DynamoDBType.DYNAMO;
}
}
private DynamoDBUtils.DynamoDBType getSerializationType() {
return serializationType;
}
public String getPreferredSchema() {
return preferredSchema;
}
public void setPreferredSchema(String preferredSchema) {
this.preferredSchema = preferredSchema;
}
public AmazonDynamoDB getDynamoDbClient() {
return getDynamoDBClient();
}
public DynamoDBMapping getDynamoDbMapping() {
return mapping;
}
public void setDynamoDbMapping(DynamoDBMapping mapping) {
this.mapping = mapping;
}
public String getConsistency() {
return consistency;
}
public void setConsistency(String consistency) {
this.consistency = consistency;
}
public AmazonDynamoDB getDynamoDBClient() {
return dynamoDBClient;
}
public void setDynamoDBClient(AmazonDynamoDB dynamoDBClient) {
this.dynamoDBClient = dynamoDBClient;
PersistentBase record = (PersistentBase) new BeanFactoryImpl(keyClass, clazz).newPersistent();
try {
Result<K, T> result = query.execute();
String[] fields = getFieldsToQuery(query.getFields());
boolean isAllFields = Arrays.equals(fields, getFields());
while (result.next()) {
if (isAllFields) {
if (delete(result.getKey())) {
deletedRows;
continue;
}
}
for (String field : fields) {
result.get().clearField(field);
}
deletedRows;
void clearField(String Field);
public void clearField(String field) {
Collection<Field> unmanagedFields = getUnmanagedFields();
Field specificField = getSchema().getField(field);
if (unmanagedFields.contains(specificField)) {
put(specificField.pos(), PersistentData.get().deepCopy(specificField.schema(),
PersistentData.get().getDefaultValue(specificField)));
}
clearDirynessIfFieldIsDirtyable(specificField.pos());
}
@Override
public void clearField(String Field) { }
@Override
public void clearField(String Field) { }
@Override
line(pIden, "public void clearField(String Field) { }");
line(pIden, "@Override");
import java.util.*;
}
} else {
ArrayList<String> excludedFields = new ArrayList<>();
for (String field : getFields()){
if (!Arrays.asList(fields).contains(field)){
excludedFields.add(field);
}
}
T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
if (delete(result.getKey())) {
put(result.getKey(),newClonedObj);
deletedRows;
newObj.clear();
for (String field : fields) {
Field otherField = obj.getSchema().getField(field);
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;
import java.util.NavigableMap;
for (String field : getFields()) {
if (!Arrays.asList(fields).contains(field)) {
@SuppressWarnings("unchecked")
@SuppressWarnings("unused")
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]}");
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"WebPage\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"url\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"content\",\"type\":[\"null\",\"bytes\"],\"default\":null},{\"name\":\"parsedContent\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"default\":null},{\"name\":\"outlinks\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}},{\"name\":\"headers\",\"type\":[\"null\",{\"type\":\"map\",\"values\":[\"null\",\"string\"]}],\"default\":null},{\"name\":\"metadata\",\"type\":{\"type\":\"record\",\"name\":\"Metadata\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]},\"default\":null},{\"name\":\"byteData\",\"type\":{\"type\":\"map\",\"values\":\"bytes\"},\"default\":{}},{\"name\":\"stringData\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}],\"default\":null}");
BYTE_DATA(6, "byteData"),
STRING_DATA(7, "stringData"),
"byteData",
"stringData",
private java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> byteData;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> stringData;
case 6: return byteData;
case 7: return stringData;
case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
return byteData;
}
public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
this.byteData = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(6);
}
public boolean isByteDataDirty() {
return isDirty(6);
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
return stringData;
}
public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
this.stringData = (value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper(value);
setDirty(7);
}
public boolean isStringDataDirty() {
return isDirty(7);
}
private java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> byteData;
private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> stringData;
if (isValidValue(fields()[6], other.byteData)) {
this.byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>) data().deepCopy(fields()[6].schema(), other.byteData);
fieldSetFlags()[6] = true;
}
if (isValidValue(fields()[7], other.stringData)) {
this.stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) data().deepCopy(fields()[7].schema(), other.stringData);
fieldSetFlags()[7] = true;
}
public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
return byteData;
}
public org.apache.gora.examples.generated.WebPage.Builder setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
validate(fields()[6], value);
this.byteData = value;
fieldSetFlags()[6] = true;
return this;
}
public boolean hasByteData() {
return fieldSetFlags()[6];
}
public org.apache.gora.examples.generated.WebPage.Builder clearByteData() {
byteData = null;
fieldSetFlags()[6] = false;
return this;
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
return stringData;
}
public org.apache.gora.examples.generated.WebPage.Builder setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
validate(fields()[7], value);
this.stringData = value;
fieldSetFlags()[7] = true;
return this;
}
public boolean hasStringData() {
return fieldSetFlags()[7];
}
public org.apache.gora.examples.generated.WebPage.Builder clearStringData() {
stringData = null;
fieldSetFlags()[7] = false;
return this;
}
record.byteData = fieldSetFlags()[6] ? this.byteData : (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[6]));
record.stringData = fieldSetFlags()[7] ? this.stringData : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)defaultValue(fields()[7]));
public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isByteDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isStringDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public static final ConcurrentHashMap<Schema, SpecificDatumReader<?>> readerMap = new ConcurrentHashMap<>();
public static final ConcurrentHashMap<Schema, SpecificDatumWriter<?>> writerMap = new ConcurrentHashMap<>();
private SpecificDatumReader getDatumReader(Schema fieldSchema) {
SpecificDatumReader<?> reader = readerMap.get(fieldSchema);
if ((localReader = readerMap.putIfAbsent(fieldSchema, reader)) != null) {
private SpecificDatumWriter getDatumWriter(Schema fieldSchema) {
SpecificDatumWriter writer = writerMap.get(fieldSchema);
writerMap.put(fieldSchema, writer);
SpecificDatumReader reader = getDatumReader(fieldSchema);
SpecificDatumReader unionReader = getDatumReader(fieldSchema);
SpecificDatumWriter writer = getDatumWriter(fieldSchema);
SpecificDatumWriter writer = getDatumWriter(fieldSchema);
private void get(long key, String[] fields) throws Exception {
Pageview pageview = dataStore.get(key, fields);
printPageview(pageview);
}
"           -get <lineNum> <fieldList>\n"
"           -delete <lineNum>\n"
"           -deleteByQuery <startLineNum> <endLineNum>\n";
if(args.length == 2) {
manager.get(Long.parseLong(args[1]));
} else {
String[] fields = args[2].split(",");
manager.get(Long.parseLong(args[1]), fields);
}
implements Result<K, T> {
if(persistent != null) {
return persistent;
}
return dataStore.newPersistent();
.info("Keyclass and nameclass match but mismatching table names "
"' , assuming they are the same.");
.addClassField(field.getAttributeValue(ATT_NAME),
field.getAttributeValue(ATT_FIELD),
field.getAttributeValue(ATT_TYPE));
DataStoreBase<K, T> {
parameters.getDbname(), parameters.getServers() });
LOG.debug(
"Load from DBObject (RECORD), field:{}, schemaType:{}, docField:{}, storeType:{}",
new Object[] { recField.name(), innerSchema.getType(), fieldPath,
innerStoreType });
record.put(
recField.pos(),
fromDBObject(innerSchema, innerStoreType, recField, innerDocField,
innerBson));
final BSONDecorator easybson, final Field f) {
final BSONDecorator easybson, final Field f) {
return new DirtyMapWrapper(rmap);
": Invalid string: unable to convert to ObjectId");
public boolean isReferrerDirty() {
LOG.error("{}  is not found, please check the file.", DEFAULT_MAPPING_FILE);
throw new RuntimeException(ex);
LOG.error("{}  is not found, please check the file.", DEFAULT_MAPPING_FILE);
throw new RuntimeException(ex);
try {
Result<K, T> result = query.execute();
String[] fields = getFieldsToQuery(query.getFields());
boolean isAllFields = Arrays.equals(fields, getFields());
while (result.next()) {
if (isAllFields) {
if (delete(result.getKey())) {
deletedRows;
}
} else {
ArrayList<String> excludedFields = new ArrayList<>();
for (String field : getFields()) {
if (!Arrays.asList(fields).contains(field)) {
excludedFields.add(field);
}
}
T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
if (delete(result.getKey())) {
put(result.getKey(),newClonedObj);
deletedRows;
}
}
newObj.clear();
for (String field : fields) {
Field otherField = obj.getSchema().getField(field);
try {
Result<K, T> result = query.execute();
String[] fields = getFieldsToQuery(query.getFields());
boolean isAllFields = Arrays.equals(fields, getFields());
while (result.next()) {
if (isAllFields) {
if (delete(result.getKey())) {
deletedRows;
}
} else {
ArrayList<String> excludedFields = new ArrayList<>();
for (String field : getFields()) {
if (!Arrays.asList(fields).contains(field)) {
excludedFields.add(field);
}
}
T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
if (delete(result.getKey())) {
put(result.getKey(),newClonedObj);
deletedRows;
}
}
newObj.clear();
for (String field : fields) {
Field otherField = obj.getSchema().getField(field);
@Override
public byte[] encodeShort(short s) throws IOException {
@Override
public byte[] encodeShort(short s, byte[] ret) throws IOException {
try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){
@Override
public short decodeShort(byte[] a) throws IOException {
try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){
@Override
public byte[] encodeInt(int i) throws IOException {
@Override
public byte[] encodeInt(int i, byte[] ret) throws IOException {
try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){
@Override
public int decodeInt(byte[] a) throws IOException {
try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){
@Override
public byte[] encodeLong(long l) throws IOException {
@Override
public byte[] encodeLong(long l, byte[] ret) throws IOException {
try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){
@Override
public long decodeLong(byte[] a) throws IOException {
try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){
@Override
public byte[] encodeDouble(double d) throws IOException {
@Override
public byte[] encodeDouble(double d, byte[] ret) throws IOException {
try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){
@Override
public double decodeDouble(byte[] a) throws IOException {
try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){
@Override
public byte[] encodeFloat(float d) throws IOException {
@Override
public byte[] encodeFloat(float f, byte[] ret) throws IOException {
try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){
@Override
public float decodeFloat(byte[] a) throws IOException {
try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){
@Override
@Override
@Override
@Override
public boolean decodeBoolean(byte[] a) throws IOException {
try (DataInputStream dis = new DataInputStream(new ByteArrayInputStream(a))){
@Override
public byte[] encodeBoolean(boolean b) throws IOException {
@Override
public byte[] encodeBoolean(boolean b, byte[] ret) throws IOException {
try (DataOutputStream dos = new DataOutputStream(new FixedByteArrayOutputStream(ret))){
import java.io.IOException;
public byte[] encodeShort(short s) throws IOException;
public byte[] encodeShort(short s, byte[] ret) throws IOException;
public short decodeShort(byte[] a) throws IOException;
public byte[] encodeInt(int i) throws IOException;
public byte[] encodeInt(int i, byte[] ret) throws IOException;
public int decodeInt(byte[] a) throws IOException;
public byte[] encodeLong(long l) throws IOException;
public byte[] encodeLong(long l, byte[] ret) throws IOException;
public long decodeLong(byte[] a) throws IOException;
public byte[] encodeDouble(double d) throws IOException;
public byte[] encodeDouble(double d, byte[] ret) throws IOException;
public double decodeDouble(byte[] a) throws IOException;
public byte[] encodeFloat(float d) throws IOException;
public byte[] encodeFloat(float f, byte[] ret) throws IOException;
public float decodeFloat(byte[] a) throws IOException;
public boolean decodeBoolean(byte[] val) throws IOException;
public byte[] encodeBoolean(boolean b) throws IOException;
public byte[] encodeBoolean(boolean b, byte[] ret) throws IOException;
private byte[] chars = new byte[] {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'};
return b - '0';
import java.io.IOException;
@Override
public byte[] encodeShort(short s, byte[] ret) throws IOException{
@Override
public short decodeShort(byte[] a) throws IOException{
@Override
public byte[] encodeInt(int i, byte[] ret) throws IOException{
@Override
public int decodeInt(byte[] a) throws IOException{
@Override
public byte[] encodeLong(long l, byte[] ret) throws IOException{
l = l ^ 0x8000000000000000L;
@Override
public long decodeLong(byte[] a) throws IOException {
l = l ^ 0x8000000000000000L;
@Override
public byte[] encodeDouble(double d, byte[] ret) throws IOException {
l = l ^ 0x8000000000000000L;
@Override
public double decodeDouble(byte[] a) throws IOException{
l = l ^ 0x8000000000000000L;
@Override
public byte[] encodeFloat(float f, byte[] ret) throws IOException {
@Override
public float decodeFloat(byte[] a) throws IOException{
byte[] ret = new byte[size];
byte[] copy = new byte[ret.length - 1];
byte[] copy = new byte[minLen];
private final static String UNKOWN = "Unknown type ";
public Object fromBytes(Schema schema, byte[] data) throws IOException {
public static Object fromBytes(Encoder encoder, Schema schema, byte data[]) throws IOException {
byte[] copy = new byte[len];
batchWriterConfig.setMaxLatency(60000L, TimeUnit.MILLISECONDS);
if (mapping.encoder == null || "".equals(mapping.encoder)) {
if ("".equals(qualifier))
private void setFetchColumns(Scanner scanner, String[] fields) {
LOG.error("Mapping not found for field: {}", field);
|| !(o instanceof DirtyMapWrapper)) {
try {
Result<K, T> result = query.execute();
String[] fields = getFieldsToQuery(query.getFields());
boolean isAllFields = Arrays.equals(fields, getFields());
while (result.next()) {
if (isAllFields) {
if (delete(result.getKey())) {
deletedRows;
}
} else {
ArrayList<String> excludedFields = new ArrayList<>();
for (String field : getFields()) {
if (!Arrays.asList(fields).contains(field)) {
excludedFields.add(field);
}
}
T newClonedObj = getPersistent(result.get(),excludedFields.toArray(new String[excludedFields.size()]));
if (delete(result.getKey())) {
put(result.getKey(),newClonedObj);
deletedRows;
}
}
newObj.clear();
for (String field : fields) {
Field otherField = obj.getSchema().getField(field);
Marshaller<T> marshaller = new Marshaller<>(persistentClass);
public static final Logger LOG = LoggerFactory.getLogger(InfinispanStore.class);
private InfinispanClient<K, T> infinispanClient;
private String primaryFieldName;
private int primaryFieldPos;
private int splitSize;
public InfinispanStore(){
}
@Override
public synchronized void initialize(Class<K> keyClass, Class<T> persistentClass, Properties properties) {
try {
if (primaryFieldName!=null) {
LOG.info("Client already initialized; ignoring.");
return;
super.initialize(keyClass, persistentClass, properties);
infinispanClient  = new InfinispanClient<>();
infinispanClient.setConf(conf);
LOG.info("key class: "
keyClass.getCanonicalName()
", persistent class: "
persistentClass.getCanonicalName());
schema = persistentClass.newInstance().getSchema();
splitSize = Integer.valueOf(
properties.getProperty( BUFFER_LIMIT_READ_NAME,
getConf().get(
BUFFER_LIMIT_READ_NAME,
Integer.toString(BUFFER_LIMIT_READ_VALUE))));
primaryFieldPos = 0;
primaryFieldName = schema.getFields().get(0).name();
this.infinispanClient.initialize(keyClass, persistentClass, properties);
} catch (Exception e) {
throw new RuntimeException(e);
}
}
@Override
public void close() {
LOG.debug("close()");
infinispanClient.close();
}
@Override
public void createSchema() {
LOG.debug("createSchema()");
this.infinispanClient.createCache();
}
@Override
public boolean delete(K key) {
this.infinispanClient.deleteByKey(key);
return true;
}
@Override
public long deleteByQuery(Query<K, T> query) {
((InfinispanQuery<K, T>) query).build();
InfinispanQuery<K, T> q = (InfinispanQuery) query;
q.build();
for( T t : q.list()){
infinispanClient.deleteByKey((K) t.get(primaryFieldPos));
}
return q.getResultSize();
}
@Override
public void deleteSchema() {
LOG.debug("deleteSchema()");
this.infinispanClient.dropCache();
}
@Override
public Result<K, T> execute(Query<K, T> query) {
LOG.debug("execute()");
((InfinispanQuery<K,T>)query).build();
InfinispanResult<K,T> result = new InfinispanResult<>(this, (InfinispanQuery<K,T>)query);
return result;
}
@Override
public T get(K key){
return infinispanClient.get(key);
}
@Override
public T get(K key, String[] fields) {
if (fields==null)
InfinispanQuery<K, T> query = new InfinispanQuery<K, T>(this);
query.setKey(key);
query.setFields(fields);
query.build();
Result<K,T> result = query.execute();
try {
result.next();
return result.get();
} catch (Exception e) {
throw new RuntimeException(e);
}
}
@Override
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query)
throws IOException {
LOG.debug("getPartitions()");
List<PartitionQuery<K,T>> locations = ((InfinispanQuery<K,T>)query).split();
List<PartitionQuery<K,T>> splitLocations = new ArrayList<>();
for(PartitionQuery<K,T> location : locations) {
InfinispanQuery<K,T> sizeQuery = (InfinispanQuery<K, T>) ((InfinispanQuery<K, T>) location).clone();
sizeQuery.setFields(primaryFieldName);
sizeQuery.setLimit(1);
sizeQuery.rebuild();
int resultSize = sizeQuery.getResultSize();
long queryLimit = query.getLimit();
long splitLimit = queryLimit>0 ? Math.min((long)resultSize,queryLimit) : resultSize;
if (splitLimit <= splitSize) {
LOG.trace("location returned");
splitLocations.add(location);
continue;
InfinispanQuery<K, T> split = (InfinispanQuery<K, T>) ((InfinispanQuery<K, T>) location).clone();
split.setOffset(i * splitSize);
split.setLimit(splitSize);
split.rebuild();
splitLocations.add(split);
}
}
return splitLocations;
}
@Override
public void flush() {
LOG.debug("flush()");
infinispanClient.flush();
}
@Override
public String getSchemaName() {
LOG.debug("getSchemaName()");
return this.infinispanClient.getCacheName();
}
@Override
public Query<K, T> newQuery() {
LOG.debug("newQuery()");
Query<K, T> query = new InfinispanQuery<>(this);
query.setFields(getFieldsToQuery(null));
return query;
}
@Override
public void put(K key, T obj) {
LOG.trace(obj.toString());
if (obj.get(primaryFieldPos)==null)
obj.put(primaryFieldPos,key);
if (!obj.get(primaryFieldPos).equals(key) )
this.infinispanClient.put(key, obj);
}
@Override
public boolean schemaExists() {
LOG.debug("schemaExists()");
return infinispanClient.cacheExists();
}
public InfinispanClient<K, T> getClient() {
LOG.debug("getClient()");
return infinispanClient;
}
public String getPrimaryFieldName() {
LOG.debug("getPrimaryField()");
return primaryFieldName;
}
public void setPrimaryFieldName(String name){
LOG.debug("getPrimaryFieldName()");
primaryFieldName = name;
}
public int getPrimaryFieldPos(){
LOG.debug("getPrimaryFieldPos()");
return primaryFieldPos;
}
public void setPrimaryFieldPos(int p){
LOG.debug("setPrimaryFieldPos()");
primaryFieldPos = p;
}
public static final String GORA_DEFAULT_CACHE_DATASTORE_KEY = "gora.cache.datastore.default";
@SuppressWarnings("unchecked")
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
Class<K> keyClass, Class<T> persistent, Configuration conf, boolean isCacheEnabled) throws GoraException {
Properties createProps = createProps();
Class<? extends DataStore<K, T>> c;
try {
if (isCacheEnabled) {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultCacheDataStore(createProps));
} else {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
}
} catch (Exception ex) {
throw new GoraException(ex);
}
return createDataStore(c, keyClass, persistent, conf, createProps, null);
}
private static String getDefaultCacheDataStore(Properties properties) {
return getProperty(properties, GORA_DEFAULT_CACHE_DATASTORE_KEY);
}
import java.util.concurrent.ConcurrentSkipListSet;
import javax.cache.Cache;
import javax.cache.CacheManager;
import javax.cache.Caching;
import javax.cache.configuration.MutableCacheEntryListenerConfiguration;
import javax.cache.configuration.MutableConfiguration;
import javax.cache.spi.CachingProvider;
private Cache<K, T> cache;
private CacheManager manager;
private ConcurrentSkipListSet<K> cacheEntryList;
private static final String GORA_DEFAULT_JCACHE_PROVIDER_KEY = "gora.datastore.jcache.provider";
private static final Logger LOG = LoggerFactory.getLogger(JCacheStore.class);
CachingProvider cachingProvider = Caching.getCachingProvider(
properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY)
);
manager = cachingProvider.getCacheManager();
cacheEntryList = new ConcurrentSkipListSet<>();
MutableConfiguration<K, T> config = new MutableConfiguration<K, T>();
config.setTypes(keyClass, persistentClass);
config.setReadThrough(true);
config.setWriteThrough(true);
config.setCacheLoaderFactory(JCacheCacheFactoryBuilder.factoryOfCacheLoader(keyClass,persistentClass));
config.setCacheWriterFactory(JCacheCacheFactoryBuilder.factoryOfCacheWriter(keyClass,persistentClass));
config.addCacheEntryListenerConfiguration(
new MutableCacheEntryListenerConfiguration<>(
JCacheCacheFactoryBuilder.factoryOfEntryListener(new JCacheCacheEntryListener<K,T>(cacheEntryList)),
null, true, true
)
);
cache = manager.createCache(persistentClass.getSimpleName(),config);
public T get(K key){
return cache.get(key);
}
@Override
cache.put(key,val);
return cache.remove(key);
Collection<T>, java.io.Serializable {
final class DirtyFlag implements Dirtyable, java.io.Serializable {
final class DirtyIteratorWrapper<T> implements Iterator<T>, java.io.Serializable {
public class DirtyMapWrapper<K, V> implements Map<K, V>, Dirtyable, java.io.Serializable {
import java.io.Serializable;
Persistent, java.io.Serializable {
private byte[] __g__dirty;
__g__dirty = new byte[getFieldsCount()];
return java.nio.ByteBuffer.wrap(__g__dirty);
public class JCacheQuery<K, T extends PersistentBase> extends QueryBase<K, T> {
public JCacheQuery(DataStore<K, T> dataStore) {
import java.util.Iterator;
import java.util.NavigableSet;
public class JCacheResult<K, T extends PersistentBase> extends ResultBase<K, T> {
private NavigableSet<K> cacheKeySet;
private Iterator<K> iterator;
public JCacheResult(DataStore<K, T> dataStore, Query<K, T> query) {
public JCacheResult(DataStore<K, T> dataStore, Query<K, T> query, NavigableSet<K> cacheKeySet) {
super(dataStore, query);
this.cacheKeySet = cacheKeySet;
this.iterator = cacheKeySet.iterator();
}
public JCacheStore<K, T> getDataStore() {
return (JCacheStore<K, T>) super.getDataStore();
}
if (!iterator.hasNext()) {
return false;
}
key = iterator.next();
persistent = dataStore.get(key);
private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheEntryListenerFactory.class);
import org.apache.gora.store.DataStore;
factoryOfCacheLoader(DataStore<K, T> dataStore) {
return new JCacheCacheLoaderFactory<>(new JCacheCacheLoader<>(dataStore));
factoryOfCacheWriter(DataStore<K, T> dataStore) {
return new JCacheCacheWriterFactory<>(new JCacheCacheWriter<>(dataStore));
public JCacheCacheLoader(DataStore<K, T> dataStore) {
this.dataStore = dataStore;
private transient JCacheCacheLoader<K, T> instance;
public JCacheCacheLoaderFactory(JCacheCacheLoader<K, T> instance) {
this.instance = instance;
public JCacheCacheLoader<K, T> create() {
return (JCacheCacheLoader<K, T>) this.instance;
public JCacheCacheWriter(DataStore<K, T> dataStore) {
this.dataStore = dataStore;
private transient JCacheCacheWriter<K,T> instance;
public JCacheCacheWriterFactory(JCacheCacheWriter<K,T> instance) {
this.instance = instance;
return (JCacheCacheWriter<K,T>)this.instance;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Arrays;
import java.util.ArrayList;
import com.hazelcast.cache.HazelcastCachingProvider;
import com.hazelcast.cache.ICache;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.Member;
import com.hazelcast.core.Partition;
import org.apache.avro.Schema;
import org.apache.gora.jcache.query.JCacheResult;
import org.apache.gora.query.impl.PartitionQueryImpl;
import org.apache.gora.store.DataStore;
import org.apache.gora.store.DataStoreFactory;
import org.apache.gora.util.AvroUtils;
import org.apache.gora.util.GoraException;
import org.apache.hadoop.conf.Configuration;
private ICache<K, T> cache;
private static final String GORA_DEFAULT_JCACHE_NAMESPACE = "gora.jcache.namespace";
private DataStore<K, T> persistentDataStore;
private MutableConfiguration<K, T> cacheConfig;
private HazelcastInstance hazelcastInstance;
try {
this.persistentDataStore = DataStoreFactory.getDataStore(keyClass, persistentClass,
new Configuration());
} catch (GoraException ex) {
LOG.error("Couldn't initialize persistent DataStore");
}
hazelcastInstance = Hazelcast.newHazelcastInstance();
Properties providerProperties = new Properties();
providerProperties.setProperty( HazelcastCachingProvider.HAZELCAST_INSTANCE_NAME,
hazelcastInstance.getName());
try {
manager = cachingProvider.getCacheManager(new URI(GORA_DEFAULT_JCACHE_NAMESPACE), null, providerProperties);
} catch (URISyntaxException ex) {
LOG.error("Couldn't initialize cache manager to a bounded hazelcast instance");
manager = cachingProvider.getCacheManager();
}
cacheConfig = new MutableConfiguration<K, T>();
cacheConfig.setTypes(keyClass, persistentClass);
cacheConfig.setReadThrough(true);
cacheConfig.setWriteThrough(true);
cacheConfig.setStoreByValue(true);
cacheConfig.setCacheLoaderFactory(JCacheCacheFactoryBuilder
.factoryOfCacheLoader(this.persistentDataStore));
cacheConfig.setCacheWriterFactory(JCacheCacheFactoryBuilder
.factoryOfCacheWriter(this.persistentDataStore));
cacheConfig.addCacheEntryListenerConfiguration(
JCacheCacheFactoryBuilder
.factoryOfEntryListener(new JCacheCacheEntryListener<K,T>(cacheEntryList)),
cache = manager.createCache(persistentClass.getSimpleName(),
cacheConfig).unwrap(ICache.class);
return super.persistentClass.getSimpleName();
if (manager.getCache(super.getPersistentClass().getSimpleName()) == null) {
cache = manager.createCache(persistentClass.getSimpleName(),
cacheConfig).unwrap(ICache.class);
}
persistentDataStore.createSchema();
manager.destroyCache(super.getPersistentClass().getSimpleName());
persistentDataStore.deleteSchema();
return (manager.getCache(super.getPersistentClass().getSimpleName()) != null)
&& persistentDataStore.schemaExists();
T persitent = (T) cache.get(key);
if (persitent == null) {
return null;
}
return getPersistent(persitent, fields);
}
private static <T extends PersistentBase> T getPersistent(T persitent, String[] fields) {
List<Schema.Field> otherFields = persitent.getSchema().getFields();
String[] otherFieldStrings = new String[otherFields.size()];
otherFieldStrings[i] = otherFields.get(i).name();
}
if (Arrays.equals(fields, otherFieldStrings)) {
return persitent;
}
T clonedPersistent = AvroUtils.deepClonePersistent(persitent);
clonedPersistent.clear();
for (String field : fields) {
Schema.Field otherField = persitent.getSchema().getField(field);
int index = otherField.pos();
clonedPersistent.put(index, persitent.get(index));
}
return clonedPersistent;
public long deleteByQuery(Query<K, T> query) {
try {
long deletedRows = 0;
Result<K, T> result = query.execute();
String[] fields = getFieldsToQuery(query.getFields());
boolean isAllFields = Arrays.equals(fields, getFields());
while (result.next()) {
if (isAllFields) {
if (delete(result.getKey())) {
deletedRows;
}
} else {
ArrayList<String> excludedFields = new ArrayList<>();
for (String field : getFields()) {
if (!Arrays.asList(fields).contains(field)) {
excludedFields.add(field);
}
}
T newClonedObj = getPersistent(result.get(),
excludedFields.toArray(new String[excludedFields.size()]));
if (delete(result.getKey())) {
put(result.getKey(), newClonedObj);
deletedRows;
}
}
}
return deletedRows;
} catch (Exception e) {
return 0;
}
public Result<K, T> execute(Query<K, T> query) {
K startKey = query.getStartKey();
K endKey = query.getEndKey();
if (startKey == null) {
if (!cacheEntryList.isEmpty()) {
startKey = (K) cacheEntryList.first();
}
}
if (endKey == null) {
if (!cacheEntryList.isEmpty()) {
endKey = (K) cacheEntryList.last();
}
}
query.setFields(getFieldsToQuery(query.getFields()));
ConcurrentSkipListSet<K> cacheEntrySubList = null;
try {
cacheEntrySubList = (ConcurrentSkipListSet<K>) cacheEntryList.subSet(startKey, true, endKey, true);
} catch (NullPointerException npe) {
LOG.error("NPE occurred while executing the query for JCacheStore");
return new JCacheResult<>(this, query, new ConcurrentSkipListSet<K>());
}
return new JCacheResult<>(this, query, cacheEntrySubList);
public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query) throws IOException {
List<PartitionQuery<K, T>> partitions = new ArrayList<>();
try {
Member[] clusterMembers = new Member[hazelcastInstance.getCluster().getMembers().size()];
this.hazelcastInstance.getCluster().getMembers().toArray(clusterMembers);
for (Member member : clusterMembers) {
JCacheResult<K, T> result = ((JCacheResult<K, T>) query.execute());
ConcurrentSkipListSet<K> memberOwnedCacheEntries = new ConcurrentSkipListSet<>();
while (result.next()) {
K key = result.getKey();
Partition partition = hazelcastInstance.getPartitionService().getPartition(key);
if (partition.getOwner().getUuid().equals(member.getUuid())) {
memberOwnedCacheEntries.add(key);
}
}
PartitionQueryImpl<K, T> partition = new PartitionQueryImpl<>(
query, memberOwnedCacheEntries.first(),
memberOwnedCacheEntries.last(), member.getSocketAddress().getHostString());
partitions.add(partition);
}
} catch (java.lang.Exception ex) {
LOG.error("Exception occurred while partitioning the query based on Hazelcast partitions.");
return null;
}
return partitions;
persistentDataStore.flush();
flush();
if (!cache.isDestroyed()) {
cache.destroy();
}
if (!manager.isClosed()) {
manager.close();
}
persistentDataStore.close();
import java.util.concurrent.TimeUnit;
import com.hazelcast.config.CacheConfig;
import com.hazelcast.config.EvictionConfig;
import com.hazelcast.config.EvictionPolicy;
import javax.cache.configuration.FactoryBuilder;
import javax.cache.expiry.AccessedExpiryPolicy;
import javax.cache.expiry.ModifiedExpiryPolicy;
import javax.cache.expiry.CreatedExpiryPolicy;
import javax.cache.expiry.TouchedExpiryPolicy;
import javax.cache.expiry.Duration;
private static final String GORA_DEFAULT_JCACHE_PROVIDER_KEY = "gora.datastore.jcache.provider";
private static final String JCACHE_READ_THROUGH_PROPERTY_KEY = "jcache.read.through.enable";
private static final String JCACHE_WRITE_THROUGH_PROPERTY_KEY = "jcache.write.through.enable";
private static final String JCACHE_STORE_BY_VALUE_PROPERTY_KEY = "jcache.store.by.value.enable";
private static final String JCACHE_STATISTICS_PROPERTY_KEY = "jcache.statistics.enable";
private static final String JCACHE_MANAGEMENT_PROPERTY_KEY = "jcache.management.enable";
private static final String JCACHE_CACHE_NAMESPACE_PROPERTY_KEY = "jcache.cache.namespace";
private static final String JCACHE_EVICTION_POLICY_PROPERTY_KEY = "jcache.eviction.policy";
private static final String JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY = "jcache.eviction.max.size.policy";
private static final String JCACHE_EVICTION_SIZE_PROPERTY_KEY = "jcache.eviction.size";
private static final String JCACHE_EXPIRE_POLICY_PROPERTY_KEY = "jcache.expire.policy";
private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy";
private static final String JCACHE_ACCESSED_EXPIRY_IDENTIFIER = "ACCESSED";
private static final String JCACHE_CREATED_EXPIRY_IDENTIFIER = "CREATED";
private static final String JCACHE_MODIFIED_EXPIRY_IDENTIFIER = "MODIFIED";
private static final String JCACHE_TOUCHED_EXPIRY_IDENTIFIER = "TOUCHED";
private String goraCacheNamespace = GORA_DEFAULT_JCACHE_NAMESPACE;
private CacheConfig<K, T> cacheConfig;
if (properties.getProperty(JCACHE_CACHE_NAMESPACE_PROPERTY_KEY) != null) {
goraCacheNamespace = properties.getProperty(JCACHE_CACHE_NAMESPACE_PROPERTY_KEY);
}
manager = cachingProvider.getCacheManager(new URI(goraCacheNamespace), null, providerProperties);
LOG.error("Couldn't initialize cache manager to bounded hazelcast instance");
cacheConfig = new CacheConfig<K, T>();
if (properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY) != null) {
cacheConfig.setReadThrough(Boolean.valueOf(properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY) != null) {
cacheConfig.setWriteThrough(Boolean.valueOf(properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY) != null) {
cacheConfig.setStoreByValue(Boolean.valueOf(properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY) != null) {
cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY) != null) {
cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY) != null) {
cacheConfig.getEvictionConfig()
.setEvictionPolicy(EvictionPolicy.valueOf(properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY) != null) {
cacheConfig.getEvictionConfig()
.setMaximumSizePolicy(EvictionConfig.MaxSizePolicy
.valueOf(properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY) != null) {
cacheConfig.getEvictionConfig()
.setSize(Integer.valueOf(properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY) != null) {
String expiryPolicyIdentifier = properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY);
if (expiryPolicyIdentifier.equals(JCACHE_ACCESSED_EXPIRY_IDENTIFIER)){
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new AccessedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
} else if (expiryPolicyIdentifier.equals(JCACHE_CREATED_EXPIRY_IDENTIFIER)){
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new CreatedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
} else if (expiryPolicyIdentifier.equals(JCACHE_MODIFIED_EXPIRY_IDENTIFIER)){
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new ModifiedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
} else if (expiryPolicyIdentifier.equals(JCACHE_TOUCHED_EXPIRY_IDENTIFIER)){
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new TouchedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
}
}
import org.apache.hadoop.mapreduce.task.JobContextImpl;
conf.getStrings("io.serializations"),
Job job = Job.getInstance(conf);
return new JobContextImpl(job.getConfiguration(), null);
return new JobContextImpl(conf, null);
private static final String DEFAULT_TEMPLATES_PATH = "/org/apache/gora/compiler/templates/" ;
compiler.setTemplateDir(DEFAULT_TEMPLATES_PATH);
public static Schema compileSchema(String sourceSchema, File dest, String templatesPath) throws IOException {
Schema.Parser parser = new Schema.Parser();
if (templatesPath == null) {
templatesPath = DEFAULT_TEMPLATES_PATH ;
}
LOG.info("Compiling source schema from String into {} using templates in {}", dest.getPath(), templatesPath);
Schema newSchema = parser.parse(sourceSchema);
GoraCompiler compiler = new GoraCompiler(newSchema);
compiler.setTemplateDir(templatesPath);
LOG.info("Compiled avro into: {}", dest.getAbsolutePath());
return newSchema;
}
@SuppressWarnings({ "unchecked" })
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
String dataStoreClass, String keyClass, String persistentClass, Properties props, Configuration conf)
throws GoraException {
try {
Class<? extends DataStore<K,T>> c
= (Class<? extends DataStore<K, T>>) Class.forName(dataStoreClass);
Class<K> k = (Class<K>) ClassLoadingUtils.loadClass(keyClass);
Class<T> p = (Class<T>) ClassLoadingUtils.loadClass(persistentClass);
return createDataStore(c, k, p, conf, props, null);
} catch(GoraException ex) {
throw ex;
} catch (Exception ex) {
throw new GoraException(ex);
}
}
import java.io.InputStream;
import org.apache.commons.io.IOUtils;
public static final String XML_MAPPING_DEFINITION = "gora.mapping" ;
InputStream mappingInputStream ;
if (getConf().get(XML_MAPPING_DEFINITION, null) != null) {
mappingInputStream = IOUtils.toInputStream(getConf().get(XML_MAPPING_DEFINITION, null)) ;
}
else {
mappingInputStream = getClass().getClassLoader().getResourceAsStream(getConf().get(PARSE_MAPPING_FILE_KEY, DEFAULT_MAPPING_FILE)) ;
}
mapping = readMapping(mappingInputStream);
private HBaseMapping readMapping(InputStream mappingStream) throws IOException {
Document doc = builder.build(mappingStream);
mappingStream) ;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import javax.cache.event.CacheEntryExpiredListener;
import javax.cache.event.CacheEntryUpdatedListener;
import javax.cache.event.CacheEntryRemovedListener;
CacheEntryRemovedListener<K, T>, CacheEntryUpdatedListener<K, T>, CacheEntryExpiredListener<K, T> {
private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheEntryListener.class);
@Override
public void onUpdated(Iterable<CacheEntryEvent<? extends K, ? extends T>> cacheEntryEvents)
throws CacheEntryListenerException {
for (CacheEntryEvent<? extends K, ? extends T> event : cacheEntryEvents) {
}
}
@Override
public void onExpired(Iterable<CacheEntryEvent<? extends K, ? extends T>> cacheEntryEvents)
throws CacheEntryListenerException {
for (CacheEntryEvent<? extends K, ? extends T> event : cacheEntryEvents) {
}
}
public static final long serialVersionUID = 201305101634L;
private transient JCacheCacheEntryListener<K, T> instance;
LOG.info("JCache cache entry listener factory initialized successfully.");
public static final long serialVersionUID = 201305101626L;
LOG.info("JCache cache entry loader factory initialized successfully.");
public static final long serialVersionUID = 201205101621L;
LOG.info("JCache entry writer factory initialized successfully.");
import com.hazelcast.config.InMemoryFormat;
private static final String HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY = "jcache.cache.inmemory.format";
private static final String HAZELCAST_CACHE_BINARY_IN_MEMORY_FORMAT_IDENTIFIER = "BINARY";
private static final String HAZELCAST_CACHE_OBJECT_IN_MEMORY_FORMAT_IDENTIFIER = "OBJECT";
private static final String HAZELCAST_CACHE_NATIVE_IN_MEMORY_FORMAT_IDENTIFIER = "NATIVE";
if (properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY) != null) {
String inMemoryFormat = properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY);
if (inMemoryFormat.equals(HAZELCAST_CACHE_BINARY_IN_MEMORY_FORMAT_IDENTIFIER) ||
inMemoryFormat.equals(HAZELCAST_CACHE_OBJECT_IN_MEMORY_FORMAT_IDENTIFIER) ||
inMemoryFormat.equals(HAZELCAST_CACHE_NATIVE_IN_MEMORY_FORMAT_IDENTIFIER)) {
cacheConfig.setInMemoryFormat(InMemoryFormat.valueOf(inMemoryFormat));
}
}
LOG.info("JCache Gora datastore initialized successfully.");
LOG.info("Created schema on persistent store and initialized cache for persistent bean "
super.getPersistentClass().getSimpleName());
LOG.info("Deleted schema on persistent store and destroyed cache for persistent bean "
super.getPersistentClass().getSimpleName());
LOG.error("Exception occured while deleting entries from JCache Gora datastore. Hence returning 0");
LOG.error("NPE occurred while executing the query for JCacheStore. Hence returning empty entry set.");
LOG.info("JCache Gora datastore flushed successfully.");
LOG.info("JCache Gora datastore destroyed successfully.");
import org.apache.avro.SchemaNormalization;
public static long fingerprint64(Schema schema) {
return SchemaNormalization.parsingFingerprint64(schema);
}
private static final long serialVersionUID = -6468893522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522296148698L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522296178608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468894522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522496148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522236148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
Collection<T> {
final class DirtyFlag implements Dirtyable {
final class DirtyIteratorWrapper<T> implements Iterator<T> {
public class DirtyMapWrapper<K, V> implements Map<K, V>, Dirtyable {
Persistent, java.io.Externalizable {
private ByteBuffer __g__dirty;
__g__dirty = ByteBuffer.wrap(new byte[getFieldsCount()]);
public ByteBuffer getDirtyBytes() {
return __g__dirty;
}
public void setDirtyBytes(ByteBuffer __g__dirty) {
this.__g__dirty = __g__dirty;
private final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(getSchema());
private final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(getSchema());
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final String JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY ="jcache.auto.create.cache";
if (properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY) != null) {
Boolean createCache = Boolean.valueOf(properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY));
if (createCache) {
cache = manager.createCache(persistentClass.getSimpleName(),
cacheConfig).unwrap(ICache.class);
}
} else {
cache = manager.createCache(persistentClass.getSimpleName(),
cacheConfig).unwrap(ICache.class);
}
if (manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) == null) {
cacheEntryList.clear();
cacheEntryList.clear();
return (manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) != null);
if (fields != null && fields.length > 0) {
for (String field : fields) {
Schema.Field otherField = persitent.getSchema().getField(field);
int index = otherField.pos();
clonedPersistent.put(index, persitent.get(index));
}
} else {
for (String field : otherFieldStrings) {
Schema.Field otherField = persitent.getSchema().getField(field);
int index = otherField.pos();
clonedPersistent.put(index, persitent.get(index));
}
cacheEntryList.clear();
if (!cache.isDestroyed() && !manager.isClosed()) {
private static final long serialVersionUID = -6468793522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
package org.apache.gora.tutorial.log.generated;
private static final long serialVersionUID = -6468893522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
public class JCacheStore<K, T extends PersistentBase> extends DataStoreBase<K, T> {
private static final String JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY = "jcache.auto.create.cache";
private ICache<K, T> cache;
private CacheManager manager;
private ConcurrentSkipListSet<K> cacheEntryList;
private String goraCacheNamespace = GORA_DEFAULT_JCACHE_NAMESPACE;
private static <T extends PersistentBase> T getPersistent(T persitent, String[] fields) {
List<Schema.Field> otherFields = persitent.getSchema().getFields();
String[] otherFieldStrings = new String[otherFields.size()];
otherFieldStrings[i] = otherFields.get(i).name();
}
if (Arrays.equals(fields, otherFieldStrings)) {
return persitent;
}
T clonedPersistent = AvroUtils.deepClonePersistent(persitent);
clonedPersistent.clear();
if (fields != null && fields.length > 0) {
for (String field : fields) {
Schema.Field otherField = persitent.getSchema().getField(field);
int index = otherField.pos();
clonedPersistent.put(index, persitent.get(index));
}
} else {
for (String field : otherFieldStrings) {
Schema.Field otherField = persitent.getSchema().getField(field);
int index = otherField.pos();
clonedPersistent.put(index, persitent.get(index));
}
}
return clonedPersistent;
}
properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY)
providerProperties.setProperty(HazelcastCachingProvider.HAZELCAST_INSTANCE_NAME,
if (expiryPolicyIdentifier.equals(JCACHE_ACCESSED_EXPIRY_IDENTIFIER)) {
} else if (expiryPolicyIdentifier.equals(JCACHE_CREATED_EXPIRY_IDENTIFIER)) {
} else if (expiryPolicyIdentifier.equals(JCACHE_MODIFIED_EXPIRY_IDENTIFIER)) {
} else if (expiryPolicyIdentifier.equals(JCACHE_TOUCHED_EXPIRY_IDENTIFIER)) {
.factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
public T get(K key) {
cache.put(key, val);
public Query<K, T> newQuery() {
LOG.info("JCache cache writer factory initialized successfully.");
} else {
cacheConfig.setReadThrough(true);
} else {
cacheConfig.setWriteThrough(true);
private static final long serialVersionUID = -6468893532296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468883532296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private java.nio.ByteBuffer __g__dirty;
__g__dirty = java.nio.ByteBuffer.wrap(new byte[getFieldsCount()]);
private static final long serialVersionUID = -7468893532296148608L;
public static final org.apache.avro.Schema SCHEMA$ =
new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
import java.io.ObjectInput;
import java.io.ObjectOutput;
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
import org.apache.avro.SchemaNormalization;
public static long fingerprint64(Schema schema) {
return SchemaNormalization.parsingFingerprint64(schema);
}
private static final long serialVersionUID = -6468893522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522296148698L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522296178608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468894522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522496148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893522236148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
Collection<T> {
import java.io.ObjectInput;
import java.io.ObjectOutput;
Persistent, java.io.Externalizable {
public ByteBuffer getDirtyBytes() {
public void setDirtyBytes(ByteBuffer __g__dirty) {
this.__g__dirty = __g__dirty;
}
public static final String GORA_DEFAULT_CACHE_DATASTORE_KEY = "gora.cache.datastore.default";
@SuppressWarnings("unchecked")
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
Class<K> keyClass, Class<T> persistent, Configuration conf, boolean isCacheEnabled) throws GoraException {
Properties createProps = createProps();
Class<? extends DataStore<K, T>> c;
try {
if (isCacheEnabled) {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultCacheDataStore(createProps));
} else {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
}
} catch (Exception ex) {
throw new GoraException(ex);
}
return createDataStore(c, keyClass, persistent, conf, createProps, null);
}
private static String getDefaultCacheDataStore(Properties properties) {
return getProperty(properties, GORA_DEFAULT_CACHE_DATASTORE_KEY);
}
private static final long serialVersionUID = -7468893532296148608L;
public static final org.apache.avro.Schema SCHEMA$ =
new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write
(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read
(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468893532296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468883532296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
private static final long serialVersionUID = -6468793522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
package org.apache.gora.tutorial.log.generated;
private static final long serialVersionUID = -6468893522296148608L;
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
private int current;
this.current = 0;
if (cacheKeySet.size() == 0) {
return 1;
}
float progress = ((float) current / (float) cacheKeySet.size());
return progress;
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
private int current;
this.current = 0;
if (cacheKeySet.size() == 0) {
return 1;
}
float progress = ((float) current / (float) cacheKeySet.size());
return progress;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
private static final Logger LOG = LoggerFactory.getLogger(JCacheResult.class);
LOG.info("Results set pointer is now moved to key {}.", key);
LOG.info("Cache entry added on key {}.", event.getKey().toString());
LOG.info("Cache entry removed on key {}.", event.getKey().toString());
LOG.info("Cache entry updated set on key {}.", event.getKey().toString());
LOG.warn("Cache entry expired on key {}.", event.getKey().toString());
LOG.info("Loaded data bean from persistent datastore on key {}.", key.toString());
LOG.info("Loaded data bean from persistent datastore on key {}.", key.toString());
LOG.info("Written data bean to persistent datastore on key {}.", entry.getKey().toString());
LOG.info("Deleted data bean from persistent datastore on key {}.", key.toString());
import java.util.Iterator;
import java.util.Properties;
import com.hazelcast.client.HazelcastClient;
import com.hazelcast.client.config.ClientConfig;
import com.hazelcast.client.config.XmlClientConfigBuilder;
import com.hazelcast.config.Config;
import com.hazelcast.config.ClasspathXmlConfig;
import com.hazelcast.config.EvictionConfig;
import javax.cache.Cache;
private static final String GORA_DEFAULT_JCACHE_HAZELCAST_CONFIG_KEY = "gora.datastore.jcache.hazelcast.config";
private static final String HAZELCAST_SERVER_CACHE_PROVIDER_IDENTIFIER = "Server";
LOG.error("Couldn't initialize persistent DataStore.", ex);
if (properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY)
.contains(HAZELCAST_SERVER_CACHE_PROVIDER_IDENTIFIER)) {
Config config = new ClasspathXmlConfig(properties.getProperty(GORA_DEFAULT_JCACHE_HAZELCAST_CONFIG_KEY));
hazelcastInstance = Hazelcast.newHazelcastInstance(config);
} else {
try {
ClientConfig config =
new XmlClientConfigBuilder(properties.getProperty(GORA_DEFAULT_JCACHE_HAZELCAST_CONFIG_KEY)).build();
hazelcastInstance = HazelcastClient.newHazelcastClient(config);
} catch (IOException ex) {
LOG.error("Couldn't locate the client side cache provider configuration.", ex);
}
}
LOG.error("Couldn't initialize cache manager to bounded hazelcast instance.", ex);
null, true, true));
if (manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) == null) {
cache = manager.createCache(persistentClass.getSimpleName(),
cacheConfig).unwrap(ICache.class);
} else {
cache = manager.getCache(super.getPersistentClass().getSimpleName(),
keyClass, persistentClass).unwrap(ICache.class);
this.populateLocalCacheEntrySet(cache.iterator());
}
LOG.info("Created schema on persistent store and initialized cache for persistent bean {}."
, super.getPersistentClass().getSimpleName());
LOG.info("Deleted schema on persistent store and destroyed cache for persistent bean {}."
, super.getPersistentClass().getSimpleName());
LOG.info("JCache Gora datastore deleled {} rows from Persistent datastore.", deletedRows);
LOG.error("Exception occurred while deleting entries from JCache Gora datastore. Hence returning 0.", e);
LOG.error("NPE occurred while executing the query for JCacheStore. Hence returning empty entry set.", npe);
LOG.error("Exception occurred while partitioning the query based on Hazelcast partitions.", ex);
LOG.info("Query is partitioned to {} number of partitions.", partitions.size());
cache.close();
hazelcastInstance.shutdown();
private void populateLocalCacheEntrySet(Iterator<Cache.Entry<K, T>> cacheEntryIterator) {
cacheEntryList.clear();
while (cacheEntryIterator.hasNext()) {
cacheEntryList.add(cacheEntryIterator.next().getKey());
}
LOG.info("Populated local cache entry set with respect to remote cache provider.");
}
CacheEntryRemovedListener<K, T>, CacheEntryUpdatedListener<K, T>,
CacheEntryExpiredListener<K, T>, java.io.Serializable {
private transient ConcurrentSkipListSet<K> cacheEntryList;
if (cacheEntryList == null) {
return;
}
if (cacheEntryList == null) {
return;
}
if (cacheEntryList == null) {
return;
}
if (cacheEntryList == null) {
return;
}
public void setCacheEntryList(ConcurrentSkipListSet<K> cacheEntryList) {
this.cacheEntryList = cacheEntryList;
}
public class JCacheCacheEntryListenerFactory<K, T extends PersistentBase>
private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheEntryListenerFactory.class);
private JCacheCacheEntryListener<K, T> instance;
if (this == other) {
} else if (other != null && this.getClass() == other.getClass()) {
JCacheCacheEntryListenerFactory that = (JCacheCacheEntryListenerFactory) other;
public static <K, T extends PersistentBase> Factory<JCacheCacheLoader<K, T>>
public static <K, T extends PersistentBase> Factory<JCacheCacheWriter<K, T>>
public static <K, T extends PersistentBase> Factory<JCacheCacheEntryListener<K, T>>
public class JCacheCacheLoader<K, T extends PersistentBase> implements CacheLoader<K, T>, java.io.Serializable {
private transient DataStore<K, T> dataStore;
this.dataStore = dataStore;
public void setDataStore(DataStore<K, T> dataStore) {
this.dataStore = dataStore;
}
implements Factory<JCacheCacheLoader<K, T>> {
private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheLoaderFactory.class);
private JCacheCacheLoader<K, T> instance;
public class JCacheCacheWriter<K, T extends PersistentBase> implements CacheWriter<K, T>, java.io.Serializable {
private transient DataStore<K, T> dataStore;
public void setDataStore(DataStore<K, T> dataStore) {
this.dataStore = dataStore;
}
public class JCacheCacheWriterFactory<K, T extends PersistentBase> implements Factory<JCacheCacheWriter<K, T>> {
private static final Logger LOG = LoggerFactory.getLogger(JCacheCacheWriterFactory.class);
private JCacheCacheWriter<K, T> instance;
public JCacheCacheWriterFactory(JCacheCacheWriter<K, T> instance) {
public JCacheCacheWriter<K, T> create() {
return (JCacheCacheWriter<K, T>) this.instance;
import java.util.List;
import java.util.Properties;
import javax.cache.configuration.CacheEntryListenerConfiguration;
CachingProvider cachingProvider = Caching.getCachingProvider
(properties.getProperty(GORA_DEFAULT_JCACHE_PROVIDER_KEY));
if (((properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY) != null) &&
Boolean.valueOf(properties.getProperty(JCACHE_AUTO_CREATE_CACHE_PROPERTY_KEY)))
|| ((manager.getCache(super.getPersistentClass().getSimpleName(), keyClass, persistentClass) == null))) {
cacheEntryList = new ConcurrentSkipListSet<>();
cacheConfig = new CacheConfig<K, T>();
cacheConfig.setTypes(keyClass, persistentClass);
if (properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY) != null) {
cacheConfig.setReadThrough(Boolean.valueOf(properties.getProperty(JCACHE_READ_THROUGH_PROPERTY_KEY)));
cacheConfig.setReadThrough(true);
if (properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY) != null) {
cacheConfig.setWriteThrough(Boolean.valueOf(properties.getProperty(JCACHE_WRITE_THROUGH_PROPERTY_KEY)));
} else {
cacheConfig.setWriteThrough(true);
}
if (properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY) != null) {
cacheConfig.setStoreByValue(Boolean.valueOf(properties.getProperty(JCACHE_STORE_BY_VALUE_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY) != null) {
cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_STATISTICS_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY) != null) {
cacheConfig.setStatisticsEnabled(Boolean.valueOf(properties.getProperty(JCACHE_MANAGEMENT_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY) != null) {
cacheConfig.getEvictionConfig()
.setEvictionPolicy(EvictionPolicy.valueOf(properties.getProperty(JCACHE_EVICTION_POLICY_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY) != null) {
cacheConfig.getEvictionConfig()
.setMaximumSizePolicy(EvictionConfig.MaxSizePolicy
.valueOf(properties.getProperty(JCACHE_EVICTION_MAX_SIZE_POLICY_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY) != null) {
cacheConfig.getEvictionConfig()
.setSize(Integer.valueOf(properties.getProperty(JCACHE_EVICTION_SIZE_PROPERTY_KEY)));
}
if (properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY) != null) {
String expiryPolicyIdentifier = properties.getProperty(JCACHE_EXPIRE_POLICY_PROPERTY_KEY);
if (expiryPolicyIdentifier.equals(JCACHE_ACCESSED_EXPIRY_IDENTIFIER)) {
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new AccessedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
} else if (expiryPolicyIdentifier.equals(JCACHE_CREATED_EXPIRY_IDENTIFIER)) {
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new CreatedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
} else if (expiryPolicyIdentifier.equals(JCACHE_MODIFIED_EXPIRY_IDENTIFIER)) {
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new ModifiedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
} else if (expiryPolicyIdentifier.equals(JCACHE_TOUCHED_EXPIRY_IDENTIFIER)) {
cacheConfig.setExpiryPolicyFactory(FactoryBuilder.factoryOf(
new TouchedExpiryPolicy(new Duration(TimeUnit.SECONDS,
Integer.valueOf(properties.getProperty(JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY))))
));
}
}
if (properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY) != null) {
String inMemoryFormat = properties.getProperty(HAZELCAST_CACHE_IN_MEMORY_FORMAT_PROPERTY_KEY);
if (inMemoryFormat.equals(HAZELCAST_CACHE_BINARY_IN_MEMORY_FORMAT_IDENTIFIER) ||
inMemoryFormat.equals(HAZELCAST_CACHE_OBJECT_IN_MEMORY_FORMAT_IDENTIFIER) ||
inMemoryFormat.equals(HAZELCAST_CACHE_NATIVE_IN_MEMORY_FORMAT_IDENTIFIER)) {
cacheConfig.setInMemoryFormat(InMemoryFormat.valueOf(inMemoryFormat));
}
}
cacheConfig.setCacheLoaderFactory(JCacheCacheFactoryBuilder
.factoryOfCacheLoader(this.persistentDataStore));
cacheConfig.setCacheWriterFactory(JCacheCacheFactoryBuilder
.factoryOfCacheWriter(this.persistentDataStore));
cacheConfig.addCacheEntryListenerConfiguration(
new MutableCacheEntryListenerConfiguration<>(
JCacheCacheFactoryBuilder
.factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
null, true, true));
cache = manager.createCache(persistentClass.getSimpleName(),
cacheConfig).unwrap(ICache.class);
} else {
cache = manager.getCache(super.getPersistentClass().getSimpleName(),
keyClass, persistentClass).unwrap(ICache.class);
this.populateLocalCacheEntrySet(cache);
this.populateLocalCacheConfig(cache);
cache.removeAll();
private void populateLocalCacheEntrySet(ICache<K, T> cache) {
cacheEntryList = new ConcurrentSkipListSet<>();
Iterator<Cache.Entry<K, T>> cacheEntryIterator = cache.iterator();
cacheConfig = cache.getConfiguration(CacheConfig.class);
Iterator<CacheEntryListenerConfiguration<K, T>> itr =
cacheConfig.getCacheEntryListenerConfigurations().iterator();
while (itr.hasNext()) {
JCacheCacheEntryListenerFactory<K, T> listenerFac = (JCacheCacheEntryListenerFactory<K, T>)
((MutableCacheEntryListenerConfiguration) itr.next()).getCacheEntryListenerFactory();
listenerFac.create().setCacheEntryList(cacheEntryList);
break;
}
private void populateLocalCacheConfig(ICache<K, T> cache) {
cacheConfig = cache.getConfiguration(CacheConfig.class);
((JCacheCacheLoaderFactory) cacheConfig.getCacheLoaderFactory())
.create().setDataStore(this.persistentDataStore);
((JCacheCacheWriterFactory) cacheConfig.getCacheWriterFactory())
.create().setDataStore(this.persistentDataStore);
LOG.info("Populated transient cache loader/writer in local cache configuration.");
}
CacheEntryExpiredListener<K, T> {
private ConcurrentSkipListSet<K> cacheEntryList;
private transient JCacheCacheEntryListener<K, T> instance;
factoryOfCacheLoader(DataStore<K, T> dataStore, Class<K> keyClass, Class<T> persistentClass) {
return new JCacheCacheLoaderFactory<>(new JCacheCacheLoader<>(dataStore), keyClass, persistentClass);
factoryOfCacheWriter(DataStore<K, T> dataStore, Class<K> keyClass, Class<T> persistentClass) {
return new JCacheCacheWriterFactory<>(new JCacheCacheWriter<>(dataStore), keyClass, persistentClass);
public class JCacheCacheLoader<K, T extends PersistentBase> implements CacheLoader<K, T> {
private DataStore<K, T> dataStore;
import org.apache.gora.store.DataStoreFactory;
import org.apache.gora.util.GoraException;
import org.apache.hadoop.conf.Configuration;
private transient JCacheCacheLoader<K, T> instance;
private Class<K> keyClass;
private Class<T> persistentClass;
public JCacheCacheLoaderFactory(JCacheCacheLoader<K, T> instance,
Class<K> keyClass,
Class<T> persistentClass) {
this.keyClass = keyClass;
this.persistentClass = persistentClass;
if (this.instance != null) {
return (JCacheCacheLoader<K, T>) this.instance;
} else {
try {
this.instance = new JCacheCacheLoader<>(DataStoreFactory
.getDataStore(keyClass, persistentClass, new Configuration()));
} catch (GoraException ex) {
LOG.error("Couldn't initialize persistent dataStore for cache loader.", ex);
return null;
}
return (JCacheCacheLoader<K, T>) this.instance;
}
public class JCacheCacheWriter<K, T extends PersistentBase> implements CacheWriter<K, T> {
private DataStore<K, T> dataStore;
import org.apache.gora.store.DataStoreFactory;
import org.apache.gora.util.GoraException;
import org.apache.hadoop.conf.Configuration;
private transient JCacheCacheWriter<K, T> instance;
private Class<K> keyClass;
private Class<T> persistentClass;
public JCacheCacheWriterFactory(JCacheCacheWriter<K, T> instance,
Class<K> keyClass,
Class<T> persistentClass) {
this.keyClass = keyClass;
this.persistentClass = persistentClass;
if (this.instance != null) {
return (JCacheCacheWriter<K, T>) this.instance;
} else {
try {
this.instance = new JCacheCacheWriter<>(DataStoreFactory
.getDataStore(keyClass, persistentClass, new Configuration()));
} catch (GoraException ex) {
LOG.error("Couldn't initialize persistent dataStore for cache writer.", ex);
return null;
}
return (JCacheCacheWriter<K, T>) this.instance;
}
.factoryOfCacheLoader(this.persistentDataStore, keyClass, persistentClass));
.factoryOfCacheWriter(this.persistentDataStore, keyClass, persistentClass));
cache.registerCacheEntryListener(new MutableCacheEntryListenerConfiguration<>(
JCacheCacheFactoryBuilder
.factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
null, true, true));
cache.registerCacheEntryListener(new MutableCacheEntryListenerConfiguration<>(
JCacheCacheFactoryBuilder
.factoryOfEntryListener(new JCacheCacheEntryListener<K, T>(cacheEntryList)),
null, true, true));
import org.apache.gora.util.IOUtils;
final SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
final byte[] byteData = IOUtils.serialize(writer,o);
m.put(col.getFirst(), col.getSecond(), new Value(byteData));
final SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(persistent.getSchema());
final byte[] byteData;
byteData = IOUtils.serialize(writer, persistent);
final SpecificDatumReader<T> reader = new SpecificDatumReader<>((Class<T>) persistent.getClass());
return IOUtils.deserialize(byteData, reader, null);
private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy.duration";
private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy.duration";
import org.apache.gora.query.Query;
import org.apache.gora.query.Query;
final Map<String, Object>  result;
partition.setConf(this.getConf());
partition.setConf(this.getConf());
import org.apache.hadoop.hbase.client.*;
private volatile Admin admin;
admin = ConnectionFactory.createConnection(getConf()).getAdmin();
admin.disableTable(mapping.getTable().getTableName());
admin.deleteTable(mapping.getTable().getTableName());
return admin.tableExists(mapping.getTable().getTableName());
}
if (put.size() > 0) {
table.put(put);
delete.addFamily(hcol.getFamily());
delete.addColumn(hcol.getFamily(), qualifier);
put.addColumn(hcol.getFamily(), qualifier, serializedBytes);
delete.addFamily(hcol.getFamily());
delete.addColumn(hcol.getFamily(), qualifier);
put.addColumn(hcol.getFamily(), qualifier, serializedBytes);
import java.util.ArrayList;
import java.util.concurrent.ConcurrentLinkedDeque;
import java.util.concurrent.ConcurrentLinkedQueue;
import org.apache.hadoop.hbase.client.*;
public class HBaseTableConnection {
private final Connection connection;
private final RegionLocator regionLocator;
private final ThreadLocal<ConcurrentLinkedQueue<Mutation>> buffers;
private final ThreadLocal<Table> tables;
private final BlockingQueue<Table> tPool = new LinkedBlockingQueue<>();
private final BlockingQueue<ConcurrentLinkedQueue<Mutation>> bPool = new LinkedBlockingQueue<>();
this.buffers = new ThreadLocal<>();
this.connection = ConnectionFactory.createConnection(conf);
this.regionLocator = this.connection.getRegionLocator(this.tableName);
private Table getTable() throws IOException {
Table table = tables.get();
table = connection.getTable(tableName);
private ConcurrentLinkedQueue<Mutation> getBuffer() throws IOException {
ConcurrentLinkedQueue<Mutation> buffer = buffers.get();
if (buffer == null) {
buffer = new ConcurrentLinkedQueue<>();
bPool.add(buffer);
buffers.set(buffer);
}
return buffer;
}
public void flushCommits() throws IOException {
BufferedMutator bufMutator = connection.getBufferedMutator(this.tableName);
for (ConcurrentLinkedQueue<Mutation> buffer : bPool) {
for (Mutation m: buffer) {
bufMutator.mutate(m);
bufMutator.flush();
}
}
bufMutator.close();
}
flushCommits();
for (Table table : tPool) {
return regionLocator.getStartEndKeys();
return regionLocator.getRegionLocation(bs);
public boolean[] existsAll(List<Get> list) throws IOException {
return getTable().existsAll(list);
}
getBuffer().add(put);
getBuffer().addAll(puts);
getBuffer().add(delete);
getBuffer().addAll(deletes);
import org.apache.gora.query.Query;
private static final String JCACHE_EXPIRE_POLICY_DURATION_PROPERTY_KEY = "jcache.expire.policy.duration";
partition.setConf(this.getConf());
import org.apache.gora.query.Query;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
LOG.error("Can not load {} from gora.properties. Setting to default value: {}.", SCANNER_CACHING_PROPERTIES_KEY, SCANNER_CACHING_PROPERTIES_DEFAULT);
closeHBaseAdmin();
closeHBaseAdmin();
closeHBaseAdmin();
return newInstance(result, fields);
delete.addFamily(col.family);
delete.addColumn(col.family, col.qualifier);
LOG.warn("Mismatching schema's names. Mappingfile schema: '{}'. PersistentClass schema's name: '{}'. Assuming they are the same.", tableNameFromMapping, tableName);
LOG.warn("Invalid Scanner Caching optimization value. Cannot set to: {}.", numRows) ;
private void closeHBaseAdmin(){
try {
admin.close();
} catch (IOException ioe) {
LOG.error("An error occured whilst closing HBase Admin", ioe);
}
}
import org.apache.hadoop.hbase.client.BufferedMutator;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Mutation;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.RegionLocator;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.Table;
@SuppressWarnings("unused")
bufMutator.flush();
try (DataInputBuffer buffer = new DataInputBuffer()) {
buffer.reset(in, in.length);
return deserialize(conf, buffer, obj);
}
public synchronized void createSchema() {
package org.apache.gora.examples.generated;
@SuppressWarnings("all")
"name",
"dateOfBirth",
"ssn",
"salary",
"boss",
"webpage",
case 0: return name;
case 1: return dateOfBirth;
case 2: return ssn;
case 3: return salary;
case 4: return boss;
case 5: return webpage;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
@SuppressWarnings(value="unchecked")
case 0: name = (java.lang.CharSequence)(value); break;
case 1: dateOfBirth = (java.lang.Long)(value); break;
case 2: ssn = (java.lang.CharSequence)(value); break;
case 3: salary = (java.lang.Integer)(value); break;
case 4: boss = (java.lang.Object)(value); break;
case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Employee> {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getName() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setName(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isNameDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getDateOfBirth() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setDateOfBirth(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDateOfBirthDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getSsn() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSsn(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSsnDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getSalary() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSalary(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSalaryDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Object getBoss() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setBoss(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isBossDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isWebpageDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -7621464515167921131L;
"v1",
"v2",
case 0: return v1;
case 1: return v2;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: v1 = (java.lang.Integer)(value); break;
case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<ImmutableFields> {
super(org.apache.gora.examples.generated.ImmutableFields.SCHEMA$);
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getV1() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV1(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV1Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.V2 getV2() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV2(org.apache.gora.examples.generated.V2 value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV2Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.examples.generated;
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":{}}]}");
private static final long serialVersionUID = -7097391446015721734L;
"version",
"data",
case 0: return version;
case 1: return data;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: version = (java.lang.Integer)(value); break;
case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Metadata> {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getVersion() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setVersion(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isVersionDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -4481652577902636424L;
"count",
case 0: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: count = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<TokenDatum> {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -6538763924317658547L;
"v3",
case 0: return v3;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: v3 = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<V2> {
super(org.apache.gora.examples.generated.V2.SCHEMA$);
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getV3() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV3(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV3Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -2829100587222969501L;
"url",
"content",
"parsedContent",
"outlinks",
"headers",
"metadata",
"byteData",
"stringData",
case 0: return url;
case 1: return content;
case 2: return parsedContent;
case 3: return outlinks;
case 4: return headers;
case 5: return metadata;
case 6: return byteData;
case 7: return stringData;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: url = (java.lang.CharSequence)(value); break;
case 1: content = (java.nio.ByteBuffer)(value); break;
case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<WebPage> {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.nio.ByteBuffer getContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setContent(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isContentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isParsedContentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isOutlinksDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHeadersDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetadataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isByteDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isStringDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.goraci.generated;
private static final long serialVersionUID = 1014651356631895518L;
"prev",
"client",
"count",
case 0: return prev;
case 1: return client;
case 2: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: prev = (java.lang.Long)(value); break;
case 1: client = (java.lang.CharSequence)(value); break;
case 2: count = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<CINode> {
super(org.apache.gora.goraci.generated.CINode.SCHEMA$);
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Long getPrev() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setPrev(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isPrevDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getClient() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setClient(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isClientDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.goraci.generated;
private static final long serialVersionUID = -8888031915401438521L;
"count",
case 0: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: count = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Flushed> {
super(org.apache.gora.goraci.generated.Flushed.SCHEMA$);
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Long getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.tutorial.log.generated;
@SuppressWarnings("all")
private static final long serialVersionUID = 8278557845311856507L;
"metricDimension",
"timestamp",
"metric",
case 0: return metricDimension;
case 1: return timestamp;
case 2: return metric;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: metricDimension = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: metric = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<MetricDatum> {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getMetricDimension() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetricDimension(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDimensionDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getMetric() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetric(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final long serialVersionUID = -6136058768384995982L;
"url",
"timestamp",
"ip",
"httpMethod",
"httpStatusCode",
"responseSize",
"referrer",
"userAgent",
case 0: return url;
case 1: return timestamp;
case 2: return ip;
case 3: return httpMethod;
case 4: return httpStatusCode;
case 5: return responseSize;
case 6: return referrer;
case 7: return userAgent;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: url = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: ip = (java.lang.CharSequence)(value); break;
case 3: httpMethod = (java.lang.CharSequence)(value); break;
case 4: httpStatusCode = (java.lang.Integer)(value); break;
case 5: responseSize = (java.lang.Integer)(value); break;
case 6: referrer = (java.lang.CharSequence)(value); break;
case 7: userAgent = (java.lang.CharSequence)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Pageview> {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
persistent = datumReader.read(reuseObjects ? persistent : null, decoder);
byte[] __g__dirty = new byte[persistent.getFieldsCount()];
decoder.readFixed(__g__dirty);
persistent.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
return persistent;
encoder.writeFixed(persistent.getDirtyBytes().array());
throws IOException, InterruptedException {
page.setUrl("hola");
context.write(new Text(key), page);
}
throws IOException, InterruptedException {
LOG.info(key.toString());
LOG.info(val.toString());
LOG.info(String.valueOf(val.isDirty()));
}
public Job createJob(DataStore<String, WebPage> inStore, Query<String, WebPage> query
, DataStore<String, WebPage> outStore) throws IOException {
public int mapReduceSerialization(DataStore<String, WebPage> inStore,
DataStore<String, WebPage> outStore)
throws IOException, InterruptedException, ClassNotFoundException {
Query<String, WebPage> query = inStore.newQuery();
query.setFields("url");
DataStore<String, WebPage> inStore;
DataStore<String, WebPage> outStore;
if (args.length > 0) {
if (args.length > 1) {
import org.apache.avro.Schema;
public Schema getSchema() ;
import org.apache.avro.Schema;
@Override
public Schema getSchema() { return null; }
import org.apache.avro.Schema;
@Override
public Schema getSchema() { return null; }
public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Metadata\",\"namespace\":\"org.apache.gora.examples.generated\",\"fields\":[{\"name\":\"version\",\"type\":\"int\",\"default\":0},{\"name\":\"data\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"default\":null}]}");
package org.apache.gora.examples.generated;
"name",
"dateOfBirth",
"ssn",
"salary",
"boss",
"webpage",
case 0: return name;
case 1: return dateOfBirth;
case 2: return ssn;
case 3: return salary;
case 4: return boss;
case 5: return webpage;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
@SuppressWarnings(value="unchecked")
case 0: name = (java.lang.CharSequence)(value); break;
case 1: dateOfBirth = (java.lang.Long)(value); break;
case 2: ssn = (java.lang.CharSequence)(value); break;
case 3: salary = (java.lang.Integer)(value); break;
case 4: boss = (java.lang.Object)(value); break;
case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Employee> {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getName() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setName(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isNameDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getDateOfBirth() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setDateOfBirth(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDateOfBirthDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getSsn() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSsn(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSsnDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getSalary() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSalary(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSalaryDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Object getBoss() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setBoss(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isBossDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isWebpageDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -7621464515167921131L;
"v1",
"v2",
case 0: return v1;
case 1: return v2;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: v1 = (java.lang.Integer)(value); break;
case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<ImmutableFields> {
super(org.apache.gora.examples.generated.ImmutableFields.SCHEMA$);
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getV1() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV1(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV1Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.V2 getV2() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV2(org.apache.gora.examples.generated.V2 value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV2Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -7097391446015721734L;
"version",
"data",
case 0: return version;
case 1: return data;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: version = (java.lang.Integer)(value); break;
case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Metadata> {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getVersion() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setVersion(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isVersionDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -4481652577902636424L;
"count",
case 0: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: count = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<TokenDatum> {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -6538763924317658547L;
"v3",
case 0: return v3;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: v3 = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<V2> {
super(org.apache.gora.examples.generated.V2.SCHEMA$);
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Integer getV3() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV3(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV3Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -2829100587222969501L;
"url",
"content",
"parsedContent",
"outlinks",
"headers",
"metadata",
"byteData",
"stringData",
case 0: return url;
case 1: return content;
case 2: return parsedContent;
case 3: return outlinks;
case 4: return headers;
case 5: return metadata;
case 6: return byteData;
case 7: return stringData;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: url = (java.lang.CharSequence)(value); break;
case 1: content = (java.nio.ByteBuffer)(value); break;
case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<WebPage> {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.nio.ByteBuffer getContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setContent(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isContentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isParsedContentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isOutlinksDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHeadersDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetadataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isByteDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isStringDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
package org.apache.gora.goraci.generated;
private static final long serialVersionUID = 1014651356631895518L;
"prev",
"client",
"count",
case 0: return prev;
case 1: return client;
case 2: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: prev = (java.lang.Long)(value); break;
case 1: client = (java.lang.CharSequence)(value); break;
case 2: count = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<CINode> {
super(org.apache.gora.goraci.generated.CINode.SCHEMA$);
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Long getPrev() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setPrev(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isPrevDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getClient() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setClient(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isClientDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.goraci.generated;
private static final long serialVersionUID = -8888031915401438521L;
"count",
case 0: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: count = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Flushed> {
super(org.apache.gora.goraci.generated.Flushed.SCHEMA$);
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.Long getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
package org.apache.gora.tutorial.log.generated;
private static final long serialVersionUID = 8278557845311856507L;
"metricDimension",
"timestamp",
"metric",
case 0: return metricDimension;
case 1: return timestamp;
case 2: return metric;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: metricDimension = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: metric = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<MetricDatum> {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getMetricDimension() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetricDimension(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDimensionDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getMetric() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetric(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final long serialVersionUID = -6136058768384995982L;
"url",
"timestamp",
"ip",
"httpMethod",
"httpStatusCode",
"responseSize",
"referrer",
"userAgent",
case 0: return url;
case 1: return timestamp;
case 2: return ip;
case 3: return httpMethod;
case 4: return httpStatusCode;
case 5: return responseSize;
case 6: return referrer;
case 7: return userAgent;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: url = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: ip = (java.lang.CharSequence)(value); break;
case 3: httpMethod = (java.lang.CharSequence)(value); break;
case 4: httpStatusCode = (java.lang.Integer)(value); break;
case 5: responseSize = (java.lang.Integer)(value); break;
case 6: referrer = (java.lang.CharSequence)(value); break;
case 7: userAgent = (java.lang.CharSequence)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Pageview> {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
import org.apache.gora.util.IOUtils;
final SpecificDatumWriter<Object> writer = new SpecificDatumWriter<>(field.schema());
final byte[] byteData = IOUtils.serialize(writer,o);
m.put(col.getFirst(), col.getSecond(), new Value(byteData));
import org.apache.avro.SchemaNormalization;
public static long fingerprint64(Schema schema) {
return SchemaNormalization.parsingFingerprint64(schema);
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -6468893522296148608L;
"name",
"dateOfBirth",
"ssn",
"salary",
"boss",
"webpage",
case 0: return name;
case 1: return dateOfBirth;
case 2: return ssn;
case 3: return salary;
case 4: return boss;
case 5: return webpage;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
@SuppressWarnings(value="unchecked")
case 0: name = (java.lang.CharSequence)(value); break;
case 1: dateOfBirth = (java.lang.Long)(value); break;
case 2: ssn = (java.lang.CharSequence)(value); break;
case 3: salary = (java.lang.Integer)(value); break;
case 4: boss = (java.lang.Object)(value); break;
case 5: webpage = (org.apache.gora.examples.generated.WebPage)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Employee> {
super(org.apache.gora.examples.generated.Employee.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
public static final class Tombstone extends Employee implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getName() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setName(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isNameDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getDateOfBirth() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setDateOfBirth(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDateOfBirthDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getSsn() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSsn(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSsnDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getSalary() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setSalary(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isSalaryDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Object getBoss() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setBoss(java.lang.Object value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isBossDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.WebPage getWebpage() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setWebpage(org.apache.gora.examples.generated.WebPage value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isWebpageDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -7621464515167921131L;
"v1",
"v2",
case 0: return v1;
case 1: return v2;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: v1 = (java.lang.Integer)(value); break;
case 1: v2 = (org.apache.gora.examples.generated.V2)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<ImmutableFields> {
super(org.apache.gora.examples.generated.ImmutableFields.SCHEMA$);
return this;
return this;
return TOMBSTONE;
public static final class Tombstone extends ImmutableFields implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getV1() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV1(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV1Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.V2 getV2() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV2(org.apache.gora.examples.generated.V2 value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV2Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -7097391446015721734L;
"version",
"data",
case 0: return version;
case 1: return data;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: version = (java.lang.Integer)(value); break;
case 1: data = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Metadata> {
super(org.apache.gora.examples.generated.Metadata.SCHEMA$);
return this;
return this;
return TOMBSTONE;
public static final class Tombstone extends Metadata implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getVersion() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setVersion(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isVersionDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -4481652577902636424L;
"count",
case 0: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: count = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<TokenDatum> {
super(org.apache.gora.examples.generated.TokenDatum.SCHEMA$);
return this;
return TOMBSTONE;
public static final class Tombstone extends TokenDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -6538763924317658547L;
"v3",
case 0: return v3;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: v3 = (java.lang.Integer)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<V2> {
super(org.apache.gora.examples.generated.V2.SCHEMA$);
return this;
return TOMBSTONE;
public static final class Tombstone extends V2 implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Integer getV3() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setV3(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isV3Dirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
package org.apache.gora.examples.generated;
private static final long serialVersionUID = -2829100587222969501L;
"url",
"content",
"parsedContent",
"outlinks",
"headers",
"metadata",
"byteData",
"stringData",
case 0: return url;
case 1: return content;
case 2: return parsedContent;
case 3: return outlinks;
case 4: return headers;
case 5: return metadata;
case 6: return byteData;
case 7: return stringData;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: url = (java.lang.CharSequence)(value); break;
case 1: content = (java.nio.ByteBuffer)(value); break;
case 2: parsedContent = (java.util.List<java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyListWrapper((java.util.List)value)); break;
case 3: outlinks = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 4: headers = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)(value); break;
case 5: metadata = (org.apache.gora.examples.generated.Metadata)(value); break;
case 6: byteData = (java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
case 7: stringData = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)((value instanceof org.apache.gora.persistency.Dirtyable) ? value : new org.apache.gora.persistency.impl.DirtyMapWrapper((java.util.Map)value)); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<WebPage> {
super(org.apache.gora.examples.generated.WebPage.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
public static final class Tombstone extends WebPage implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.nio.ByteBuffer getContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setContent(java.nio.ByteBuffer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isContentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.List<java.lang.CharSequence> getParsedContent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setParsedContent(java.util.List<java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isParsedContentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getOutlinks() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setOutlinks(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isOutlinksDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getHeaders() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHeaders(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHeadersDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public org.apache.gora.examples.generated.Metadata getMetadata() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetadata(org.apache.gora.examples.generated.Metadata value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetadataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> getByteData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setByteData(java.util.Map<java.lang.CharSequence,java.nio.ByteBuffer> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isByteDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getStringData() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setStringData(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isStringDataDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
import org.apache.hadoop.mapreduce.task.JobContextImpl;
conf.getStrings("io.serializations"),
Job job = Job.getInstance(conf);
return new JobContextImpl(job.getConfiguration(), null);
return new JobContextImpl(conf, null);
Collection<T> {
import java.io.ObjectInput;
import java.io.ObjectOutput;
Persistent, java.io.Externalizable {
public ByteBuffer getDirtyBytes() {
public void setDirtyBytes(ByteBuffer __g__dirty) {
this.__g__dirty = __g__dirty;
}
public static final String GORA_DEFAULT_CACHE_DATASTORE_KEY = "gora.cache.datastore.default";
@SuppressWarnings("unchecked")
public static <K, T extends Persistent> DataStore<K, T> getDataStore(
Class<K> keyClass, Class<T> persistent, Configuration conf, boolean isCacheEnabled) throws GoraException {
Properties createProps = createProps();
Class<? extends DataStore<K, T>> c;
try {
if (isCacheEnabled) {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultCacheDataStore(createProps));
} else {
c = (Class<? extends DataStore<K, T>>) Class.forName(getDefaultDataStore(createProps));
}
} catch (Exception ex) {
throw new GoraException(ex);
}
return createDataStore(c, keyClass, persistent, conf, createProps, null);
}
private static String getDefaultCacheDataStore(Properties properties) {
return getProperty(properties, GORA_DEFAULT_CACHE_DATASTORE_KEY);
}
final SpecificDatumWriter<PersistentBase> writer = new SpecificDatumWriter<>(persistent.getSchema());
final byte[] byteData;
byteData = IOUtils.serialize(writer, persistent);
final SpecificDatumReader<T> reader = new SpecificDatumReader<>((Class<T>) persistent.getClass());
return IOUtils.deserialize(byteData, reader, null);
try (DataInputBuffer buffer = new DataInputBuffer()) {
buffer.reset(in, in.length);
return deserialize(conf, buffer, obj);
}
private static final long serialVersionUID = -7468893532296148608L;
public static final org.apache.avro.Schema SCHEMA$ =
new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MockPersistent\",\"namespace\":\"org.apache.gora.mock.persistency\",\"fields\":[{\"name\":\"foo\",\"type\":\"int\"},{\"name\":\"baz\",\"type\":\"int\"}]}");
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
package org.apache.gora.goraci.generated;
private static final long serialVersionUID = 1014651356631895518L;
"prev",
"client",
"count",
case 0: return prev;
case 1: return client;
case 2: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: prev = (java.lang.Long)(value); break;
case 1: client = (java.lang.CharSequence)(value); break;
case 2: count = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<CINode> {
super(org.apache.gora.goraci.generated.CINode.SCHEMA$);
return this;
return this;
return this;
return TOMBSTONE;
public static final class Tombstone extends CINode implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Long getPrev() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setPrev(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isPrevDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getClient() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setClient(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isClientDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
package org.apache.gora.goraci.generated;
private static final long serialVersionUID = -8888031915401438521L;
"count",
case 0: return count;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: count = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Flushed> {
super(org.apache.gora.goraci.generated.Flushed.SCHEMA$);
return this;
return TOMBSTONE;
public static final class Tombstone extends Flushed implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.Long getCount() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setCount(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isCountDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
import org.apache.gora.query.Query;
public synchronized void createSchema() {
import org.apache.gora.query.Query;
package org.apache.gora.tutorial.log.generated;
private static final long serialVersionUID = 8278557845311856507L;
"metricDimension",
"timestamp",
"metric",
case 0: return metricDimension;
case 1: return timestamp;
case 2: return metric;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: metricDimension = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: metric = (java.lang.Long)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<MetricDatum> {
super(org.apache.gora.tutorial.log.generated.MetricDatum.SCHEMA$);
return this;
return this;
return this;
return TOMBSTONE;
public static final class Tombstone extends MetricDatum implements org.apache.gora.persistency.Tombstone {
private Tombstone() { }
public java.lang.CharSequence getMetricDimension() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetricDimension(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDimensionDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getMetric() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setMetric(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isMetricDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
package org.apache.gora.tutorial.log.generated;
private static final long serialVersionUID = -6136058768384995982L;
"url",
"timestamp",
"ip",
"httpMethod",
"httpStatusCode",
"responseSize",
"referrer",
"userAgent",
case 0: return url;
case 1: return timestamp;
case 2: return ip;
case 3: return httpMethod;
case 4: return httpStatusCode;
case 5: return responseSize;
case 6: return referrer;
case 7: return userAgent;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
case 0: url = (java.lang.CharSequence)(value); break;
case 1: timestamp = (java.lang.Long)(value); break;
case 2: ip = (java.lang.CharSequence)(value); break;
case 3: httpMethod = (java.lang.CharSequence)(value); break;
case 4: httpStatusCode = (java.lang.Integer)(value); break;
case 5: responseSize = (java.lang.Integer)(value); break;
case 6: referrer = (java.lang.CharSequence)(value); break;
case 7: userAgent = (java.lang.CharSequence)(value); break;
default: throw new org.apache.avro.AvroRuntimeException("Bad index");
java.nio.ByteBuffer input) {
implements org.apache.avro.data.RecordBuilder<Pageview> {
super(org.apache.gora.tutorial.log.generated.Pageview.SCHEMA$);
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return this;
return TOMBSTONE;
private Tombstone() { }
public java.lang.CharSequence getUrl() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUrl(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUrlDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Long getTimestamp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setTimestamp(java.lang.Long value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isTimestampDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getIp() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setIp(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isIpDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getHttpMethod() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpMethod(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpMethodDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getHttpStatusCode() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setHttpStatusCode(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isHttpStatusCodeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.Integer getResponseSize() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setResponseSize(java.lang.Integer value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isResponseSizeDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getReferrer() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setReferrer(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
public java.lang.CharSequence getUserAgent() {
throw new java.lang.UnsupportedOperationException("Get is not supported on tombstones");
}
public void setUserAgent(java.lang.CharSequence value) {
throw new java.lang.UnsupportedOperationException("Set is not supported on tombstones");
}
public boolean isUserAgentDirty() {
throw new java.lang.UnsupportedOperationException("IsDirty is not supported on tombstones");
}
}
private static final org.apache.avro.io.DatumWriter
DATUM_WRITER$ = new org.apache.avro.specific.SpecificDatumWriter(SCHEMA$);
private static final org.apache.avro.io.DatumReader
DATUM_READER$ = new org.apache.avro.specific.SpecificDatumReader(SCHEMA$);
@Override
public void writeExternal(java.io.ObjectOutput out)
throws java.io.IOException {
out.write(super.getDirtyBytes().array());
DATUM_WRITER$.write(this, org.apache.avro.io.EncoderFactory.get()
.directBinaryEncoder((java.io.OutputStream) out,
null));
}
@Override
public void readExternal(java.io.ObjectInput in)
throws java.io.IOException {
byte[] __g__dirty = new byte[getFieldsCount()];
in.read(__g__dirty);
super.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
DATUM_READER$.read(this, org.apache.avro.io.DecoderFactory.get()
.directBinaryDecoder((java.io.InputStream) in,
null));
}
}
persistent = datumReader.read(reuseObjects ? persistent : null, decoder);
byte[] __g__dirty = new byte[persistent.getFieldsCount()];
decoder.readFixed(__g__dirty);
persistent.setDirtyBytes(java.nio.ByteBuffer.wrap(__g__dirty));
return persistent;
encoder.writeFixed(persistent.getDirtyBytes().array());
import static com.mongodb.AuthenticationMechanism.*;
if (params.getAuthenticationType() != null) {
credentials.add(createCredential(params.getAuthenticationType(), params.getLogin(), params.getDbname(), params.getSecret()));
private MongoCredential createCredential(String authenticationType, String username, String database, String password) {
MongoCredential credential = null;
if (authenticationType.equals(PLAIN.getMechanismName())) {
credential = MongoCredential.createPlainCredential(username, database, password.toCharArray());
} else if (authenticationType.equals(SCRAM_SHA_1.getMechanismName())) {
credential = MongoCredential.createScramSha1Credential(username, database, password.toCharArray());
} else if (authenticationType.equals(MONGODB_CR.getMechanismName())) {
credential = MongoCredential.createMongoCRCredential(username, database, password.toCharArray());
} else if (authenticationType.equals(GSSAPI.getMechanismName())) {
credential = MongoCredential.createGSSAPICredential(username);
} else if (authenticationType.equals(MONGODB_X509.getMechanismName())) {
credential = MongoCredential.createMongoX509Credential(username);
} else {
LOG.error("Error while initializing MongoDB store: Invalid Authentication type.");
throw new RuntimeException("Error while initializing MongoDB store: Invalid Authentication type.");
}
return credential;
}
public static final String PROP_MONGO_AUTHENTICATION_TYPE = "gora.mongodb.authentication.type";
private final String authenticationType;
private MongoStoreParameters(String mappingFile, String servers, String dbname, String authenticationType, String login, String secret, String readPreference, String writeConcern) {
this.authenticationType = authenticationType;
public String getAuthenticationType() {
return authenticationType;
}
String vPropMongoAuthenticationType = properties.getProperty(PROP_MONGO_AUTHENTICATION_TYPE);
vPropMongoAuthenticationType = conf.get(PROP_MONGO_AUTHENTICATION_TYPE, vPropMongoAuthenticationType);
return new MongoStoreParameters(vPropMappingFile, vPropMongoServers, vPropMongoDb, vPropMongoAuthenticationType, vPropMongoLogin, vPropMongoSecret, vPropMongoRead, vPropMongoWrite);
