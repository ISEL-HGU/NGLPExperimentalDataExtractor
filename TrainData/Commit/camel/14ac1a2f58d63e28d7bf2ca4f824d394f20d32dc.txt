 * The spark component can be used to send RDD or DataFrame jobs to Apache Spark
 * cluster.
     * Builder for endpoint for the Apache Spark component.
    public interface SparkEndpointBuilder extends EndpointProducerBuilder {
         * Type of the endpoint (rdd, dataframe, hive).
         * The option is a
         * <code>org.apache.camel.component.spark.EndpointType</code> type.
         * @group producer
        default SparkEndpointBuilder endpointType(EndpointType endpointType) {
            setProperty("endpointType", endpointType);
         * Type of the endpoint (rdd, dataframe, hive).
         * The option will be converted to a
         * <code>org.apache.camel.component.spark.EndpointType</code> type.
         * @group producer
        default SparkEndpointBuilder endpointType(String endpointType) {
            setProperty("endpointType", endpointType);
         * Indicates if results should be collected or counted.
         * @group producer
        default SparkEndpointBuilder collect(boolean collect) {
            setProperty("collect", collect);
         * Indicates if results should be collected or counted.
         * @group producer
        default SparkEndpointBuilder collect(String collect) {
            setProperty("collect", collect);
         * DataFrame to compute against.
         * The option is a
         * <code>org.apache.spark.sql.Dataset&lt;org.apache.spark.sql.Row&gt;</code> type.
         * @group producer
        default SparkEndpointBuilder dataFrame(Object dataFrame) {
            setProperty("dataFrame", dataFrame);
         * DataFrame to compute against.
         * The option will be converted to a
         * <code>org.apache.spark.sql.Dataset&lt;org.apache.spark.sql.Row&gt;</code> type.
         * @group producer
        default SparkEndpointBuilder dataFrame(String dataFrame) {
            setProperty("dataFrame", dataFrame);
         * Function performing action against an DataFrame.
         * The option is a
         * <code>org.apache.camel.component.spark.DataFrameCallback</code> type.
         * @group producer
        default SparkEndpointBuilder dataFrameCallback(Object dataFrameCallback) {
            setProperty("dataFrameCallback", dataFrameCallback);
         * Function performing action against an DataFrame.
         * The option will be converted to a
         * <code>org.apache.camel.component.spark.DataFrameCallback</code> type.
         * @group producer
        default SparkEndpointBuilder dataFrameCallback(String dataFrameCallback) {
            setProperty("dataFrameCallback", dataFrameCallback);
         * RDD to compute against.
         * The option is a <code>org.apache.spark.api.java.JavaRDDLike</code>
         * type.
         * @group producer
        default SparkEndpointBuilder rdd(Object rdd) {
            setProperty("rdd", rdd);
         * RDD to compute against.
         * The option will be converted to a
         * <code>org.apache.spark.api.java.JavaRDDLike</code> type.
         * @group producer
        default SparkEndpointBuilder rdd(String rdd) {
            setProperty("rdd", rdd);
            return this;
        }
        /**
         * Function performing action against an RDD.
         * The option is a
         * <code>org.apache.camel.component.spark.RddCallback</code> type.
         * @group producer
         */
        default SparkEndpointBuilder rddCallback(Object rddCallback) {
            setProperty("rddCallback", rddCallback);
            return this;
        }
        /**
         * Function performing action against an RDD.
         * The option will be converted to a
         * <code>org.apache.camel.component.spark.RddCallback</code> type.
         * @group producer
         */
        default SparkEndpointBuilder rddCallback(String rddCallback) {
            setProperty("rddCallback", rddCallback);
     * Advanced builder for endpoint for the Apache Spark component.
                EndpointProducerBuilder {

     * Proxy enum for <code>org.apache.camel.component.spark.EndpointType</code>
     * enum.
     */
    enum EndpointType {
        rdd,
        dataframe,
        hive;
    }
    /**
     * The spark component can be used to send RDD or DataFrame jobs to Apache
     * Spark cluster.
     * Maven coordinates: org.apache.camel:camel-spark
                super("spark", path);
