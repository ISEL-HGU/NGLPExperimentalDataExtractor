 * The spark component can be used to send RDD or DataFrame jobs to Apache Spark
 * cluster.
     * Builder for endpoint for the Apache Spark component.
    public interface SparkEndpointBuilder extends EndpointProducerBuilder {
         * Indicates if results should be collected or counted.
         * Group: producer
        default SparkEndpointBuilder collect(boolean collect) {
            setProperty("collect", collect);
         * Indicates if results should be collected or counted.
         * Group: producer
        default SparkEndpointBuilder collect(String collect) {
            setProperty("collect", collect);
         * DataFrame to compute against.
         * The option is a:
         * <code>org.apache.spark.sql.Dataset&lt;org.apache.spark.sql.Row&gt;</code> type.
         * Group: producer
        default SparkEndpointBuilder dataFrame(Object dataFrame) {
            setProperty("dataFrame", dataFrame);
         * DataFrame to compute against.
         * The option will be converted to a
         * <code>org.apache.spark.sql.Dataset&lt;org.apache.spark.sql.Row&gt;</code> type.
         * Group: producer
        default SparkEndpointBuilder dataFrame(String dataFrame) {
            setProperty("dataFrame", dataFrame);
         * Function performing action against an DataFrame.
         * The option is a:
         * <code>org.apache.camel.component.spark.DataFrameCallback</code> type.
         * Group: producer
        default SparkEndpointBuilder dataFrameCallback(Object dataFrameCallback) {
            setProperty("dataFrameCallback", dataFrameCallback);
         * Function performing action against an DataFrame.
         * The option will be converted to a
         * <code>org.apache.camel.component.spark.DataFrameCallback</code> type.
         * Group: producer
        default SparkEndpointBuilder dataFrameCallback(String dataFrameCallback) {
            setProperty("dataFrameCallback", dataFrameCallback);
         * RDD to compute against.
         * The option is a: <code>org.apache.spark.api.java.JavaRDDLike</code>
         * type.
         * Group: producer
        default SparkEndpointBuilder rdd(Object rdd) {
            setProperty("rdd", rdd);
         * RDD to compute against.
         * The option will be converted to a
         * <code>org.apache.spark.api.java.JavaRDDLike</code> type.
         * Group: producer
        default SparkEndpointBuilder rdd(String rdd) {
            setProperty("rdd", rdd);
            return this;
        }
        /**
         * Function performing action against an RDD.
         * 
         * The option is a:
         * <code>org.apache.camel.component.spark.RddCallback</code> type.
         * 
         * Group: producer
         */
        default SparkEndpointBuilder rddCallback(Object rddCallback) {
            setProperty("rddCallback", rddCallback);
            return this;
        }
        /**
         * Function performing action against an RDD.
         * 
         * The option will be converted to a
         * <code>org.apache.camel.component.spark.RddCallback</code> type.
         * 
         * Group: producer
         */
        default SparkEndpointBuilder rddCallback(String rddCallback) {
            setProperty("rddCallback", rddCallback);
     * Advanced builder for endpoint for the Apache Spark component.
                EndpointProducerBuilder {
     * Apache Spark (camel-spark)
     * The spark component can be used to send RDD or DataFrame jobs to Apache
     * Spark cluster.
     * Category: bigdata,iot
     * Available as of version: 2.17
     * Maven coordinates: org.apache.camel:camel-spark
     * Syntax: <code>spark:endpointType</code>
     * Path parameter: endpointType (required)
     * Type of the endpoint (rdd, dataframe, hive).
     * The value can be one of: rdd, dataframe, hive
                super("spark", path);
