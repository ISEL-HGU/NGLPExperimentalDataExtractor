import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.lucene.index.IndexableField;
import java.util.*;
public class LuceneStore<K, T extends PersistentBase>
        extends FileBackedDataStoreBase<K, T> implements Configurable {
  private Directory dir;
                         Properties properties) throws GoraException {
    try {
      dir = FSDirectory.open(FileSystems.getDefault().getPath(dataStoreOutputPath));
    } catch (IOException e) {
  public long deleteByQuery(Query<K, T> query) {
      LuceneQuery<K, T> q = (LuceneQuery<K, T>) query;
      LuceneResult<K, T> r = (LuceneResult<K, T>) q.execute();
      if (query.getFields() == null || (query.getFields().length == getFields().length)) {
        // Delete them
        writer.deleteDocuments(q.toLuceneQuery());
        searcherManager.maybeRefresh();
      } else {
        Query<K,T>  selectQuery = this.newQuery();
        selectQuery.setStartKey(q.getStartKey());
        selectQuery.setEndKey(q.getEndKey());
        LuceneResult<K, T> selectResult = (LuceneResult<K, T>) selectQuery.execute();
        ScoreDoc[] scoreDocs =  selectResult.getScoreDocs();
        HashSet<String> fields = new HashSet<>();
        fields.addAll(mapping.getLuceneFields());
        IndexSearcher searcher = selectResult.getSearcher();
        if (scoreDocs.length > 0) {
          for (ScoreDoc scoreDoc : scoreDocs) {
            Document doc = searcher.doc(scoreDoc.doc, fields);
            for (String avroField : query.getFields()) {
              String docField = mapping.getLuceneField(avroField);
              if (doc.getField(docField) != null) {
                doc.removeField(docField);
              }
            }
            String key = doc.get(getMapping().getPrimaryKey());
            doc.removeField(getMapping().getPrimaryKey());
            doc.removeField(getMapping().getPrimaryKey());
            writer.updateDocument(new Term(mapping.getPrimaryKey(), key), doc);
            searcherManager.maybeRefresh();
          }
        }
        //selectResult.close();
      }
      r = (LuceneResult<K, T>) q.execute();
    } else {
        return newInstance(doc, fields.toArray(a));
  private Object convertDocFieldToAvroUnion(final Schema fieldSchema,
                                            final Schema.Field field,
                                            final String sf,
                                            final Document doc) throws IOException {
    Object result;
    Schema.Type type0 = fieldSchema.getTypes().get(0).getType();
    Schema.Type type1 = fieldSchema.getTypes().get(1).getType();

    if (!type0.equals(type1)
            && (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {
      Schema innerSchema = null;
      if (type0.equals(Schema.Type.NULL)) {
        innerSchema = fieldSchema.getTypes().get(1);
      } else {
        innerSchema = fieldSchema.getTypes().get(0);
      }

      result = convertToIndexableFieldToAvroField(doc, field, innerSchema, sf);
    } else {
      throw new GoraException("LuceneStore only supports Union of two types field.");
    }
    return result;
  }


  private SpecificDatumReader getDatumReader(Schema fieldSchema) {
    // reuse
    return new SpecificDatumReader(fieldSchema);
  }

  private Object convertToIndexableFieldToAvroField(final Document doc,
                                                    final Schema.Field field,
                                                    final Schema fieldSchema,
                                                    final String sf) throws IOException {
    Object result = null;
    T persistent = newPersistent();
    Object sv;
    switch (fieldSchema.getType()) {
      case MAP:
      case ARRAY:
      case RECORD:
        sv = doc.getBinaryValue(sf);
        if (sv == null) {
          break;
        }
        BytesRef b = (BytesRef) sv;
        SpecificDatumReader reader = getDatumReader(fieldSchema);
        result = IOUtils.deserialize(b.bytes, reader, persistent.get(field.pos()));
        break;
      case UNION:
        result = convertDocFieldToAvroUnion(fieldSchema, field, sf, doc);
        break;
      case ENUM:
        sv = doc.get(sf);
        if (sv == null) {
          break;
        }
        result = AvroUtils.getEnumValue(fieldSchema, (String) sv);
        break;
      case BYTES:
        sv = doc.getBinaryValue(sf);
        if (sv == null) {
          break;
        }
        result = ByteBuffer.wrap(((BytesRef) sv).bytes);
        break;
      default:
        sv = doc.get(sf);
        if (sv == null) {
          break;
        }
        result = convertLuceneFieldToAvroField(fieldSchema.getType(), sv);
    }
    return result;
  }

    if (fields == null) {
      fields = fieldMap.keySet().toArray(new String[fieldMap.size()]);
    for (String f : fields) {
      org.apache.avro.Schema.Field field = fieldMap.get(f);
      if (pk.equals(f)) {
      Schema fieldSchema = field.schema();
      Object fieldValue = convertToIndexableFieldToAvroField(doc, field, fieldSchema, sf);
      if (fieldValue == null) {
        continue;
      persistent.put(field.pos(), fieldValue);
      persistent.setDirty(field.pos());
  private Object convertLuceneFieldToAvroField(Type t, Object o) {
    Object result = null;
    switch (t) {
        // Could we combine this with the BYTES section below and
        // either fix the size of the array or not depending on Type?
        // This might be a buffer copy. Do we need to pad if the
        // fixed sized data is smaller than the type? Do we truncate
        // if the data is larger than the type?
        LOG.error("Fixed-sized fields are not supported yet");
        break;
        result = Boolean.parseBoolean((String) o);
        break;
        result = Double.parseDouble((String) o);
        break;
        result = Float.parseFloat((String) o);
        break;
        result = Integer.parseInt((String) o);
        break;
        result = Long.parseLong((String) o);
        break;
        result = new Utf8(o.toString());
        break;
        LOG.error("Unknown field type: {}", t);
    return result;
  public Query<K, T> newQuery() {
    return new LuceneQuery<>(this);
  }

  private IndexableField convertAvroUnionToDocumentField(final String sf,
                                                         final Schema fieldSchema,
                                                         final Object value) {
    IndexableField result;
    Schema.Type type0 = fieldSchema.getTypes().get(0).getType();
    Schema.Type type1 = fieldSchema.getTypes().get(1).getType();

    if (!type0.equals(type1)
            && (type0.equals(Schema.Type.NULL) || type1.equals(Schema.Type.NULL))) {
      Schema innerSchema = null;
      if (type0.equals(Schema.Type.NULL)) {
        innerSchema = fieldSchema.getTypes().get(1);
      } else {
        innerSchema = fieldSchema.getTypes().get(0);
      }
      result = convertToIndexableField(sf, innerSchema, value);
    } else {
      throw new IllegalStateException("LuceneStore only supports Union of two types field.");
    }
    return result;
  }

  private SpecificDatumWriter getDatumWriter(Schema fieldSchema) {
    return new SpecificDatumWriter(fieldSchema);
  }

  private IndexableField convertToIndexableField(String sf, Schema fieldSchema, Object o) {
    IndexableField result = null;
    switch (fieldSchema.getType()) {
      case MAP: //TODO: These should be handled better
      case ARRAY:
      case RECORD:
        // For now we'll just store the bytes
        byte[] data = new byte[0];
        try {
          SpecificDatumWriter writer = getDatumWriter(fieldSchema);
          data = IOUtils.serialize(writer, o);
        } catch (IOException e) {
          LOG.error("Error occurred while serializing record", e);
        }
        result = new StoredField(sf, data);
        break;
      case UNION:
        result = convertAvroUnionToDocumentField(sf, fieldSchema, o);
        break;
      case BYTES:
        result = new StoredField(sf, ((ByteBuffer) o).array());
        break;
      case ENUM:
      case STRING:
        //TODO make this Text based on a mapping.xml attribute
        result = new StringField(sf, o.toString(), Store.YES);
        break;
      case BOOLEAN:
        result = new StringField(sf, o.toString(), Store.YES);
        break;
      case DOUBLE:
        result = new StoredField(sf, (Double) o);
        break;
      case FLOAT:
        result = new StoredField(sf, (Float) o);
        break;
      case INT:
        result = new StoredField(sf, (Integer) o);
        break;
      case LONG:
        result = new StoredField(sf, (Long) o);
        break;
      default:
        LOG.error("Unknown field type: {}", fieldSchema.getType());
    }
    return result;
    for (org.apache.avro.Schema.Field field : fields) {
      if (sf == null) {
      doc.add(convertToIndexableField(sf, fieldSchema, o));
    LOG.info("DOCUMENT: {}", doc);
      LOG.info("DOCUMENT: {}", doc);
      } else {
  protected Result<K, T> executePartial(FileSplitPartitionQuery<K, T> arg0)
  protected Result<K, T> executeQuery(Query<K, T> query) throws IOException {
  public List<PartitionQuery<K, T>> getPartitions(Query<K, T> query) {
      dir.close();
